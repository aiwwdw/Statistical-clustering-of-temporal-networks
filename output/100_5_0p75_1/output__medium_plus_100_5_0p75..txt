nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  1%|          | 1/100 [15:25<25:26:56, 925.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  2%|▏         | 2/100 [30:20<24:41:57, 907.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  3%|▎         | 3/100 [47:29<25:56:42, 962.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  4%|▍         | 4/100 [1:02:55<25:17:30, 948.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  5%|▌         | 5/100 [1:15:40<23:16:44, 882.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  6%|▌         | 6/100 [1:30:47<23:15:37, 890.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  7%|▋         | 7/100 [1:42:12<21:16:36, 823.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  8%|▊         | 8/100 [1:58:54<22:29:31, 880.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  9%|▉         | 9/100 [2:11:37<21:19:28, 843.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 10%|█         | 10/100 [2:24:55<20:44:15, 829.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 11%|█         | 11/100 [2:36:41<19:34:22, 791.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 12%|█▏        | 12/100 [2:53:35<21:00:38, 859.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 13%|█▎        | 13/100 [3:09:14<21:21:14, 883.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 14%|█▍        | 14/100 [3:22:02<20:16:24, 848.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 15%|█▌        | 15/100 [3:36:40<20:14:48, 857.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 16%|█▌        | 16/100 [3:50:23<19:45:39, 846.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 17%|█▋        | 17/100 [4:07:08<20:37:20, 894.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 18%|█▊        | 18/100 [4:23:48<21:05:45, 926.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 19%|█▉        | 19/100 [4:40:45<21:27:10, 953.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 20%|██        | 20/100 [4:56:18<21:03:18, 947.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 21%|██        | 21/100 [5:09:56<19:56:24, 908.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 22%|██▏       | 22/100 [5:26:51<20:22:36, 940.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 23%|██▎       | 23/100 [5:41:02<19:32:32, 913.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 24%|██▍       | 24/100 [5:56:55<19:32:02, 925.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 25%|██▌       | 25/100 [6:13:03<19:32:46, 938.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 26%|██▌       | 26/100 [6:29:24<19:32:56, 951.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 27%|██▋       | 27/100 [6:45:49<19:29:39, 961.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 28%|██▊       | 28/100 [7:01:22<19:03:16, 952.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 29%|██▉       | 29/100 [7:17:21<18:49:43, 954.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 30%|███       | 30/100 [7:30:17<17:31:19, 901.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 31%|███       | 31/100 [7:44:28<16:58:57, 886.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 32%|███▏      | 32/100 [8:01:21<17:27:18, 924.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 33%|███▎      | 33/100 [8:18:49<17:53:20, 961.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 34%|███▍      | 34/100 [8:32:39<16:53:56, 921.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11638.555498623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20929.44140625
inf tensor(20929.4414, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12196.40625
tensor(20929.4414, grad_fn=<NegBackward0>) tensor(12196.4062, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11776.73828125
tensor(12196.4062, grad_fn=<NegBackward0>) tensor(11776.7383, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11662.0810546875
tensor(11776.7383, grad_fn=<NegBackward0>) tensor(11662.0811, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11653.9775390625
tensor(11662.0811, grad_fn=<NegBackward0>) tensor(11653.9775, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11653.470703125
tensor(11653.9775, grad_fn=<NegBackward0>) tensor(11653.4707, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11651.7392578125
tensor(11653.4707, grad_fn=<NegBackward0>) tensor(11651.7393, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11651.58984375
tensor(11651.7393, grad_fn=<NegBackward0>) tensor(11651.5898, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11651.490234375
tensor(11651.5898, grad_fn=<NegBackward0>) tensor(11651.4902, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11651.419921875
tensor(11651.4902, grad_fn=<NegBackward0>) tensor(11651.4199, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11651.3681640625
tensor(11651.4199, grad_fn=<NegBackward0>) tensor(11651.3682, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11651.3310546875
tensor(11651.3682, grad_fn=<NegBackward0>) tensor(11651.3311, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11651.298828125
tensor(11651.3311, grad_fn=<NegBackward0>) tensor(11651.2988, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11651.275390625
tensor(11651.2988, grad_fn=<NegBackward0>) tensor(11651.2754, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11651.2578125
tensor(11651.2754, grad_fn=<NegBackward0>) tensor(11651.2578, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11651.240234375
tensor(11651.2578, grad_fn=<NegBackward0>) tensor(11651.2402, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11651.2255859375
tensor(11651.2402, grad_fn=<NegBackward0>) tensor(11651.2256, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11651.21484375
tensor(11651.2256, grad_fn=<NegBackward0>) tensor(11651.2148, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11651.205078125
tensor(11651.2148, grad_fn=<NegBackward0>) tensor(11651.2051, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11651.1943359375
tensor(11651.2051, grad_fn=<NegBackward0>) tensor(11651.1943, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11651.177734375
tensor(11651.1943, grad_fn=<NegBackward0>) tensor(11651.1777, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11642.013671875
tensor(11651.1777, grad_fn=<NegBackward0>) tensor(11642.0137, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11631.740234375
tensor(11642.0137, grad_fn=<NegBackward0>) tensor(11631.7402, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11631.732421875
tensor(11631.7402, grad_fn=<NegBackward0>) tensor(11631.7324, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11631.7275390625
tensor(11631.7324, grad_fn=<NegBackward0>) tensor(11631.7275, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11631.7333984375
tensor(11631.7275, grad_fn=<NegBackward0>) tensor(11631.7334, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11631.7197265625
tensor(11631.7275, grad_fn=<NegBackward0>) tensor(11631.7197, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11631.7158203125
tensor(11631.7197, grad_fn=<NegBackward0>) tensor(11631.7158, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11631.712890625
tensor(11631.7158, grad_fn=<NegBackward0>) tensor(11631.7129, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11631.7119140625
tensor(11631.7129, grad_fn=<NegBackward0>) tensor(11631.7119, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11631.7099609375
tensor(11631.7119, grad_fn=<NegBackward0>) tensor(11631.7100, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11631.7080078125
tensor(11631.7100, grad_fn=<NegBackward0>) tensor(11631.7080, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11631.705078125
tensor(11631.7080, grad_fn=<NegBackward0>) tensor(11631.7051, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11631.703125
tensor(11631.7051, grad_fn=<NegBackward0>) tensor(11631.7031, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11631.7021484375
tensor(11631.7031, grad_fn=<NegBackward0>) tensor(11631.7021, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11631.701171875
tensor(11631.7021, grad_fn=<NegBackward0>) tensor(11631.7012, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11631.69921875
tensor(11631.7012, grad_fn=<NegBackward0>) tensor(11631.6992, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11631.697265625
tensor(11631.6992, grad_fn=<NegBackward0>) tensor(11631.6973, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11631.703125
tensor(11631.6973, grad_fn=<NegBackward0>) tensor(11631.7031, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11631.6953125
tensor(11631.6973, grad_fn=<NegBackward0>) tensor(11631.6953, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11631.693359375
tensor(11631.6953, grad_fn=<NegBackward0>) tensor(11631.6934, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11631.6923828125
tensor(11631.6934, grad_fn=<NegBackward0>) tensor(11631.6924, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11631.693359375
tensor(11631.6924, grad_fn=<NegBackward0>) tensor(11631.6934, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11631.697265625
tensor(11631.6924, grad_fn=<NegBackward0>) tensor(11631.6973, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11631.689453125
tensor(11631.6924, grad_fn=<NegBackward0>) tensor(11631.6895, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11631.6904296875
tensor(11631.6895, grad_fn=<NegBackward0>) tensor(11631.6904, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11631.689453125
tensor(11631.6895, grad_fn=<NegBackward0>) tensor(11631.6895, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11631.689453125
tensor(11631.6895, grad_fn=<NegBackward0>) tensor(11631.6895, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11631.6875
tensor(11631.6895, grad_fn=<NegBackward0>) tensor(11631.6875, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11631.6875
tensor(11631.6875, grad_fn=<NegBackward0>) tensor(11631.6875, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11631.6865234375
tensor(11631.6875, grad_fn=<NegBackward0>) tensor(11631.6865, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11631.6826171875
tensor(11631.6865, grad_fn=<NegBackward0>) tensor(11631.6826, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11631.6748046875
tensor(11631.6826, grad_fn=<NegBackward0>) tensor(11631.6748, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11631.67578125
tensor(11631.6748, grad_fn=<NegBackward0>) tensor(11631.6758, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11631.6748046875
tensor(11631.6748, grad_fn=<NegBackward0>) tensor(11631.6748, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11631.6748046875
tensor(11631.6748, grad_fn=<NegBackward0>) tensor(11631.6748, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11631.673828125
tensor(11631.6748, grad_fn=<NegBackward0>) tensor(11631.6738, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11631.6826171875
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6826, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11631.6728515625
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6729, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11631.6787109375
tensor(11631.6729, grad_fn=<NegBackward0>) tensor(11631.6787, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11631.671875
tensor(11631.6729, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11631.671875
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11631.6728515625
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6729, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11631.671875
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11631.671875
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11631.671875
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11631.671875
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11631.6689453125
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6689, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11631.669921875
tensor(11631.6689, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11631.6748046875
tensor(11631.6689, grad_fn=<NegBackward0>) tensor(11631.6748, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11631.6748046875
tensor(11631.6689, grad_fn=<NegBackward0>) tensor(11631.6748, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11631.708984375
tensor(11631.6689, grad_fn=<NegBackward0>) tensor(11631.7090, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11631.671875
tensor(11631.6689, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.7310, 0.2690],
        [0.2072, 0.7928]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6255, 0.3745], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3965, 0.0973],
         [0.6410, 0.2002]],

        [[0.6639, 0.1073],
         [0.5504, 0.5312]],

        [[0.5839, 0.0997],
         [0.5663, 0.5899]],

        [[0.6825, 0.0954],
         [0.5189, 0.5918]],

        [[0.7004, 0.0981],
         [0.6604, 0.6014]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20425.6171875
inf tensor(20425.6172, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12143.6650390625
tensor(20425.6172, grad_fn=<NegBackward0>) tensor(12143.6650, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11670.099609375
tensor(12143.6650, grad_fn=<NegBackward0>) tensor(11670.0996, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11650.4814453125
tensor(11670.0996, grad_fn=<NegBackward0>) tensor(11650.4814, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11648.431640625
tensor(11650.4814, grad_fn=<NegBackward0>) tensor(11648.4316, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11648.0556640625
tensor(11648.4316, grad_fn=<NegBackward0>) tensor(11648.0557, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11647.8818359375
tensor(11648.0557, grad_fn=<NegBackward0>) tensor(11647.8818, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11639.2294921875
tensor(11647.8818, grad_fn=<NegBackward0>) tensor(11639.2295, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11631.9208984375
tensor(11639.2295, grad_fn=<NegBackward0>) tensor(11631.9209, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11631.87890625
tensor(11631.9209, grad_fn=<NegBackward0>) tensor(11631.8789, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11631.8388671875
tensor(11631.8789, grad_fn=<NegBackward0>) tensor(11631.8389, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11631.8115234375
tensor(11631.8389, grad_fn=<NegBackward0>) tensor(11631.8115, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11631.791015625
tensor(11631.8115, grad_fn=<NegBackward0>) tensor(11631.7910, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11631.7734375
tensor(11631.7910, grad_fn=<NegBackward0>) tensor(11631.7734, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11631.76171875
tensor(11631.7734, grad_fn=<NegBackward0>) tensor(11631.7617, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11631.748046875
tensor(11631.7617, grad_fn=<NegBackward0>) tensor(11631.7480, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11631.7412109375
tensor(11631.7480, grad_fn=<NegBackward0>) tensor(11631.7412, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11631.732421875
tensor(11631.7412, grad_fn=<NegBackward0>) tensor(11631.7324, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11631.7265625
tensor(11631.7324, grad_fn=<NegBackward0>) tensor(11631.7266, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11631.7197265625
tensor(11631.7266, grad_fn=<NegBackward0>) tensor(11631.7197, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11631.71484375
tensor(11631.7197, grad_fn=<NegBackward0>) tensor(11631.7148, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11631.7099609375
tensor(11631.7148, grad_fn=<NegBackward0>) tensor(11631.7100, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11631.7060546875
tensor(11631.7100, grad_fn=<NegBackward0>) tensor(11631.7061, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11631.7021484375
tensor(11631.7061, grad_fn=<NegBackward0>) tensor(11631.7021, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11631.705078125
tensor(11631.7021, grad_fn=<NegBackward0>) tensor(11631.7051, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11631.6982421875
tensor(11631.7021, grad_fn=<NegBackward0>) tensor(11631.6982, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11631.6962890625
tensor(11631.6982, grad_fn=<NegBackward0>) tensor(11631.6963, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11631.693359375
tensor(11631.6963, grad_fn=<NegBackward0>) tensor(11631.6934, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11631.6904296875
tensor(11631.6934, grad_fn=<NegBackward0>) tensor(11631.6904, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11631.6904296875
tensor(11631.6904, grad_fn=<NegBackward0>) tensor(11631.6904, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11631.6884765625
tensor(11631.6904, grad_fn=<NegBackward0>) tensor(11631.6885, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11631.6865234375
tensor(11631.6885, grad_fn=<NegBackward0>) tensor(11631.6865, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11631.693359375
tensor(11631.6865, grad_fn=<NegBackward0>) tensor(11631.6934, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11631.6845703125
tensor(11631.6865, grad_fn=<NegBackward0>) tensor(11631.6846, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11631.6826171875
tensor(11631.6846, grad_fn=<NegBackward0>) tensor(11631.6826, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11631.681640625
tensor(11631.6826, grad_fn=<NegBackward0>) tensor(11631.6816, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11631.6806640625
tensor(11631.6816, grad_fn=<NegBackward0>) tensor(11631.6807, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11631.6806640625
tensor(11631.6807, grad_fn=<NegBackward0>) tensor(11631.6807, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11631.6796875
tensor(11631.6807, grad_fn=<NegBackward0>) tensor(11631.6797, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11631.677734375
tensor(11631.6797, grad_fn=<NegBackward0>) tensor(11631.6777, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11631.6767578125
tensor(11631.6777, grad_fn=<NegBackward0>) tensor(11631.6768, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11631.6787109375
tensor(11631.6768, grad_fn=<NegBackward0>) tensor(11631.6787, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11631.67578125
tensor(11631.6768, grad_fn=<NegBackward0>) tensor(11631.6758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11631.6767578125
tensor(11631.6758, grad_fn=<NegBackward0>) tensor(11631.6768, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11631.67578125
tensor(11631.6758, grad_fn=<NegBackward0>) tensor(11631.6758, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11631.67578125
tensor(11631.6758, grad_fn=<NegBackward0>) tensor(11631.6758, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11631.673828125
tensor(11631.6758, grad_fn=<NegBackward0>) tensor(11631.6738, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11631.677734375
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6777, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11631.673828125
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6738, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11631.685546875
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6855, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11631.673828125
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6738, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11631.6796875
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6797, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11631.6728515625
tensor(11631.6738, grad_fn=<NegBackward0>) tensor(11631.6729, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11631.6728515625
tensor(11631.6729, grad_fn=<NegBackward0>) tensor(11631.6729, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11631.6728515625
tensor(11631.6729, grad_fn=<NegBackward0>) tensor(11631.6729, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11631.671875
tensor(11631.6729, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11631.6708984375
tensor(11631.6719, grad_fn=<NegBackward0>) tensor(11631.6709, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11631.669921875
tensor(11631.6709, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11631.671875
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11631.6708984375
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6709, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11631.669921875
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11631.669921875
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11631.669921875
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11631.669921875
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11631.669921875
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11631.6689453125
tensor(11631.6699, grad_fn=<NegBackward0>) tensor(11631.6689, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11631.66796875
tensor(11631.6689, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11631.6708984375
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6709, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11631.66796875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11631.669921875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11631.6689453125
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6689, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11631.66796875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11631.6689453125
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6689, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11631.6708984375
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6709, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11631.66796875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11631.6875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6875, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11631.66796875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11631.669921875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11631.66796875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11631.669921875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6699, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11631.66796875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11631.6689453125
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6689, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11631.6669921875
tensor(11631.6680, grad_fn=<NegBackward0>) tensor(11631.6670, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11631.66796875
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11631.66796875
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11631.66796875
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11631.6669921875
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6670, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11631.6689453125
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6689, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11631.6669921875
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6670, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11631.66796875
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6680, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11631.671875
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6719, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11631.6640625
tensor(11631.6670, grad_fn=<NegBackward0>) tensor(11631.6641, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11631.640625
tensor(11631.6641, grad_fn=<NegBackward0>) tensor(11631.6406, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11631.634765625
tensor(11631.6406, grad_fn=<NegBackward0>) tensor(11631.6348, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11631.640625
tensor(11631.6348, grad_fn=<NegBackward0>) tensor(11631.6406, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11631.6435546875
tensor(11631.6348, grad_fn=<NegBackward0>) tensor(11631.6436, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11631.6298828125
tensor(11631.6348, grad_fn=<NegBackward0>) tensor(11631.6299, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11631.6396484375
tensor(11631.6299, grad_fn=<NegBackward0>) tensor(11631.6396, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11631.6357421875
tensor(11631.6299, grad_fn=<NegBackward0>) tensor(11631.6357, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11631.6298828125
tensor(11631.6299, grad_fn=<NegBackward0>) tensor(11631.6299, grad_fn=<NegBackward0>)
pi: tensor([[0.7957, 0.2043],
        [0.2690, 0.7310]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3710, 0.6290], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.0972],
         [0.5873, 0.3964]],

        [[0.5900, 0.1073],
         [0.7261, 0.6102]],

        [[0.5439, 0.0997],
         [0.6756, 0.7018]],

        [[0.7170, 0.0954],
         [0.5426, 0.6587]],

        [[0.6697, 0.0981],
         [0.7141, 0.6720]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11631.671875, 11631.630859375]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11331.552692079336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23150.29296875
inf tensor(23150.2930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11975.935546875
tensor(23150.2930, grad_fn=<NegBackward0>) tensor(11975.9355, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11701.4736328125
tensor(11975.9355, grad_fn=<NegBackward0>) tensor(11701.4736, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11502.3251953125
tensor(11701.4736, grad_fn=<NegBackward0>) tensor(11502.3252, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11480.658203125
tensor(11502.3252, grad_fn=<NegBackward0>) tensor(11480.6582, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11466.380859375
tensor(11480.6582, grad_fn=<NegBackward0>) tensor(11466.3809, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11442.572265625
tensor(11466.3809, grad_fn=<NegBackward0>) tensor(11442.5723, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11416.876953125
tensor(11442.5723, grad_fn=<NegBackward0>) tensor(11416.8770, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11403.4130859375
tensor(11416.8770, grad_fn=<NegBackward0>) tensor(11403.4131, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11380.072265625
tensor(11403.4131, grad_fn=<NegBackward0>) tensor(11380.0723, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11365.548828125
tensor(11380.0723, grad_fn=<NegBackward0>) tensor(11365.5488, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11345.509765625
tensor(11365.5488, grad_fn=<NegBackward0>) tensor(11345.5098, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11345.421875
tensor(11345.5098, grad_fn=<NegBackward0>) tensor(11345.4219, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11345.384765625
tensor(11345.4219, grad_fn=<NegBackward0>) tensor(11345.3848, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11345.36328125
tensor(11345.3848, grad_fn=<NegBackward0>) tensor(11345.3633, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11345.3466796875
tensor(11345.3633, grad_fn=<NegBackward0>) tensor(11345.3467, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11345.3330078125
tensor(11345.3467, grad_fn=<NegBackward0>) tensor(11345.3330, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11345.3193359375
tensor(11345.3330, grad_fn=<NegBackward0>) tensor(11345.3193, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11345.30078125
tensor(11345.3193, grad_fn=<NegBackward0>) tensor(11345.3008, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11338.4775390625
tensor(11345.3008, grad_fn=<NegBackward0>) tensor(11338.4775, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11337.74609375
tensor(11338.4775, grad_fn=<NegBackward0>) tensor(11337.7461, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11330.6259765625
tensor(11337.7461, grad_fn=<NegBackward0>) tensor(11330.6260, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11330.6181640625
tensor(11330.6260, grad_fn=<NegBackward0>) tensor(11330.6182, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11330.6142578125
tensor(11330.6182, grad_fn=<NegBackward0>) tensor(11330.6143, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11330.611328125
tensor(11330.6143, grad_fn=<NegBackward0>) tensor(11330.6113, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11330.607421875
tensor(11330.6113, grad_fn=<NegBackward0>) tensor(11330.6074, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11330.6025390625
tensor(11330.6074, grad_fn=<NegBackward0>) tensor(11330.6025, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11330.6015625
tensor(11330.6025, grad_fn=<NegBackward0>) tensor(11330.6016, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11330.599609375
tensor(11330.6016, grad_fn=<NegBackward0>) tensor(11330.5996, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11330.595703125
tensor(11330.5996, grad_fn=<NegBackward0>) tensor(11330.5957, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11330.5927734375
tensor(11330.5957, grad_fn=<NegBackward0>) tensor(11330.5928, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11323.90625
tensor(11330.5928, grad_fn=<NegBackward0>) tensor(11323.9062, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11323.904296875
tensor(11323.9062, grad_fn=<NegBackward0>) tensor(11323.9043, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11323.904296875
tensor(11323.9043, grad_fn=<NegBackward0>) tensor(11323.9043, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11323.9013671875
tensor(11323.9043, grad_fn=<NegBackward0>) tensor(11323.9014, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11323.90234375
tensor(11323.9014, grad_fn=<NegBackward0>) tensor(11323.9023, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11323.8994140625
tensor(11323.9014, grad_fn=<NegBackward0>) tensor(11323.8994, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11323.8994140625
tensor(11323.8994, grad_fn=<NegBackward0>) tensor(11323.8994, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11323.8984375
tensor(11323.8994, grad_fn=<NegBackward0>) tensor(11323.8984, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11323.896484375
tensor(11323.8984, grad_fn=<NegBackward0>) tensor(11323.8965, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11323.8955078125
tensor(11323.8965, grad_fn=<NegBackward0>) tensor(11323.8955, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11323.8955078125
tensor(11323.8955, grad_fn=<NegBackward0>) tensor(11323.8955, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11323.8955078125
tensor(11323.8955, grad_fn=<NegBackward0>) tensor(11323.8955, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11323.8955078125
tensor(11323.8955, grad_fn=<NegBackward0>) tensor(11323.8955, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11323.892578125
tensor(11323.8955, grad_fn=<NegBackward0>) tensor(11323.8926, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11323.8916015625
tensor(11323.8926, grad_fn=<NegBackward0>) tensor(11323.8916, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11323.892578125
tensor(11323.8916, grad_fn=<NegBackward0>) tensor(11323.8926, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11323.8935546875
tensor(11323.8916, grad_fn=<NegBackward0>) tensor(11323.8936, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11323.892578125
tensor(11323.8916, grad_fn=<NegBackward0>) tensor(11323.8926, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11323.8896484375
tensor(11323.8916, grad_fn=<NegBackward0>) tensor(11323.8896, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11323.890625
tensor(11323.8896, grad_fn=<NegBackward0>) tensor(11323.8906, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11323.8896484375
tensor(11323.8896, grad_fn=<NegBackward0>) tensor(11323.8896, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11323.8896484375
tensor(11323.8896, grad_fn=<NegBackward0>) tensor(11323.8896, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11323.892578125
tensor(11323.8896, grad_fn=<NegBackward0>) tensor(11323.8926, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11323.8876953125
tensor(11323.8896, grad_fn=<NegBackward0>) tensor(11323.8877, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11323.888671875
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8887, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11323.888671875
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8887, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11323.888671875
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8887, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11323.8876953125
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8877, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11323.88671875
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11323.88671875
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11323.8876953125
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8877, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11323.88671875
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11323.88671875
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11323.88671875
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11323.8857421875
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11323.8857421875
tensor(11323.8857, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11323.884765625
tensor(11323.8857, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11323.884765625
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11323.884765625
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11323.884765625
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11323.884765625
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11323.8857421875
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11323.8837890625
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8838, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11323.8837890625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8838, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11323.888671875
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8887, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11323.88671875
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11323.8837890625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8838, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11323.8857421875
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11323.8837890625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8838, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11323.8837890625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8838, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11323.916015625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.9160, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11323.939453125
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.9395, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11323.8828125
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8828, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11323.8837890625
tensor(11323.8828, grad_fn=<NegBackward0>) tensor(11323.8838, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11323.8818359375
tensor(11323.8828, grad_fn=<NegBackward0>) tensor(11323.8818, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11323.88671875
tensor(11323.8818, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11323.8818359375
tensor(11323.8818, grad_fn=<NegBackward0>) tensor(11323.8818, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11323.884765625
tensor(11323.8818, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11323.8828125
tensor(11323.8818, grad_fn=<NegBackward0>) tensor(11323.8828, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11323.884765625
tensor(11323.8818, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11323.970703125
tensor(11323.8818, grad_fn=<NegBackward0>) tensor(11323.9707, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11323.89453125
tensor(11323.8818, grad_fn=<NegBackward0>) tensor(11323.8945, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.8089, 0.1911],
        [0.2678, 0.7322]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5494, 0.4506], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.0934],
         [0.6911, 0.4090]],

        [[0.5625, 0.1045],
         [0.6464, 0.6114]],

        [[0.6434, 0.1037],
         [0.7168, 0.5351]],

        [[0.5888, 0.1070],
         [0.6378, 0.7229]],

        [[0.6141, 0.1044],
         [0.7204, 0.6724]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919979558369874
Average Adjusted Rand Index: 0.99199877740731
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22122.951171875
inf tensor(22122.9512, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11984.1220703125
tensor(22122.9512, grad_fn=<NegBackward0>) tensor(11984.1221, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11982.4755859375
tensor(11984.1221, grad_fn=<NegBackward0>) tensor(11982.4756, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11923.7177734375
tensor(11982.4756, grad_fn=<NegBackward0>) tensor(11923.7178, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11530.2724609375
tensor(11923.7178, grad_fn=<NegBackward0>) tensor(11530.2725, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11469.359375
tensor(11530.2725, grad_fn=<NegBackward0>) tensor(11469.3594, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11422.79296875
tensor(11469.3594, grad_fn=<NegBackward0>) tensor(11422.7930, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11355.572265625
tensor(11422.7930, grad_fn=<NegBackward0>) tensor(11355.5723, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11341.126953125
tensor(11355.5723, grad_fn=<NegBackward0>) tensor(11341.1270, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11331.4873046875
tensor(11341.1270, grad_fn=<NegBackward0>) tensor(11331.4873, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11324.798828125
tensor(11331.4873, grad_fn=<NegBackward0>) tensor(11324.7988, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11324.3828125
tensor(11324.7988, grad_fn=<NegBackward0>) tensor(11324.3828, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11324.3095703125
tensor(11324.3828, grad_fn=<NegBackward0>) tensor(11324.3096, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11324.259765625
tensor(11324.3096, grad_fn=<NegBackward0>) tensor(11324.2598, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11324.220703125
tensor(11324.2598, grad_fn=<NegBackward0>) tensor(11324.2207, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11324.19140625
tensor(11324.2207, grad_fn=<NegBackward0>) tensor(11324.1914, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11324.1669921875
tensor(11324.1914, grad_fn=<NegBackward0>) tensor(11324.1670, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11324.146484375
tensor(11324.1670, grad_fn=<NegBackward0>) tensor(11324.1465, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11324.130859375
tensor(11324.1465, grad_fn=<NegBackward0>) tensor(11324.1309, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11324.1171875
tensor(11324.1309, grad_fn=<NegBackward0>) tensor(11324.1172, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11324.10546875
tensor(11324.1172, grad_fn=<NegBackward0>) tensor(11324.1055, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11324.0947265625
tensor(11324.1055, grad_fn=<NegBackward0>) tensor(11324.0947, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11324.0859375
tensor(11324.0947, grad_fn=<NegBackward0>) tensor(11324.0859, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11324.080078125
tensor(11324.0859, grad_fn=<NegBackward0>) tensor(11324.0801, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11324.0693359375
tensor(11324.0801, grad_fn=<NegBackward0>) tensor(11324.0693, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11324.05078125
tensor(11324.0693, grad_fn=<NegBackward0>) tensor(11324.0508, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11324.005859375
tensor(11324.0508, grad_fn=<NegBackward0>) tensor(11324.0059, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11324.0
tensor(11324.0059, grad_fn=<NegBackward0>) tensor(11324., grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11323.96484375
tensor(11324., grad_fn=<NegBackward0>) tensor(11323.9648, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11323.9287109375
tensor(11323.9648, grad_fn=<NegBackward0>) tensor(11323.9287, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11323.923828125
tensor(11323.9287, grad_fn=<NegBackward0>) tensor(11323.9238, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11323.9208984375
tensor(11323.9238, grad_fn=<NegBackward0>) tensor(11323.9209, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11323.91796875
tensor(11323.9209, grad_fn=<NegBackward0>) tensor(11323.9180, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11323.916015625
tensor(11323.9180, grad_fn=<NegBackward0>) tensor(11323.9160, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11323.9130859375
tensor(11323.9160, grad_fn=<NegBackward0>) tensor(11323.9131, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11323.9111328125
tensor(11323.9131, grad_fn=<NegBackward0>) tensor(11323.9111, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11323.91015625
tensor(11323.9111, grad_fn=<NegBackward0>) tensor(11323.9102, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11323.908203125
tensor(11323.9102, grad_fn=<NegBackward0>) tensor(11323.9082, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11323.90625
tensor(11323.9082, grad_fn=<NegBackward0>) tensor(11323.9062, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11323.9033203125
tensor(11323.9062, grad_fn=<NegBackward0>) tensor(11323.9033, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11323.90234375
tensor(11323.9033, grad_fn=<NegBackward0>) tensor(11323.9023, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11323.9111328125
tensor(11323.9023, grad_fn=<NegBackward0>) tensor(11323.9111, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11323.8984375
tensor(11323.9023, grad_fn=<NegBackward0>) tensor(11323.8984, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11323.8974609375
tensor(11323.8984, grad_fn=<NegBackward0>) tensor(11323.8975, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11323.8955078125
tensor(11323.8975, grad_fn=<NegBackward0>) tensor(11323.8955, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11323.8955078125
tensor(11323.8955, grad_fn=<NegBackward0>) tensor(11323.8955, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11323.90234375
tensor(11323.8955, grad_fn=<NegBackward0>) tensor(11323.9023, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11323.892578125
tensor(11323.8955, grad_fn=<NegBackward0>) tensor(11323.8926, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11323.892578125
tensor(11323.8926, grad_fn=<NegBackward0>) tensor(11323.8926, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11323.8916015625
tensor(11323.8926, grad_fn=<NegBackward0>) tensor(11323.8916, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11323.890625
tensor(11323.8916, grad_fn=<NegBackward0>) tensor(11323.8906, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11323.890625
tensor(11323.8906, grad_fn=<NegBackward0>) tensor(11323.8906, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11323.8896484375
tensor(11323.8906, grad_fn=<NegBackward0>) tensor(11323.8896, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11323.90234375
tensor(11323.8896, grad_fn=<NegBackward0>) tensor(11323.9023, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11323.888671875
tensor(11323.8896, grad_fn=<NegBackward0>) tensor(11323.8887, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11323.8876953125
tensor(11323.8887, grad_fn=<NegBackward0>) tensor(11323.8877, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11323.888671875
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8887, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11323.8896484375
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8896, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11323.88671875
tensor(11323.8877, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11323.88671875
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8867, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11323.8857421875
tensor(11323.8867, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11323.8857421875
tensor(11323.8857, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11323.884765625
tensor(11323.8857, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11323.8857421875
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11323.8837890625
tensor(11323.8848, grad_fn=<NegBackward0>) tensor(11323.8838, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11323.8857421875
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11323.8857421875
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8857, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11323.884765625
tensor(11323.8838, grad_fn=<NegBackward0>) tensor(11323.8848, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.7328, 0.2672],
        [0.1911, 0.8089]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4497, 0.5503], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4095, 0.0934],
         [0.6509, 0.1939]],

        [[0.6701, 0.1046],
         [0.6622, 0.6668]],

        [[0.5975, 0.1038],
         [0.6049, 0.6616]],

        [[0.5149, 0.1079],
         [0.6439, 0.6645]],

        [[0.7083, 0.1044],
         [0.6318, 0.6965]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919979558369874
Average Adjusted Rand Index: 0.99199877740731
[0.9919979558369874, 0.9919979558369874] [0.99199877740731, 0.99199877740731] [11323.89453125, 11323.884765625]
-------------------------------------
This iteration is 2
True Objective function: Loss = -11583.710628394521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21930.306640625
inf tensor(21930.3066, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12355.8603515625
tensor(21930.3066, grad_fn=<NegBackward0>) tensor(12355.8604, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12129.7646484375
tensor(12355.8604, grad_fn=<NegBackward0>) tensor(12129.7646, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11689.091796875
tensor(12129.7646, grad_fn=<NegBackward0>) tensor(11689.0918, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11622.1748046875
tensor(11689.0918, grad_fn=<NegBackward0>) tensor(11622.1748, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11598.0751953125
tensor(11622.1748, grad_fn=<NegBackward0>) tensor(11598.0752, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11597.529296875
tensor(11598.0752, grad_fn=<NegBackward0>) tensor(11597.5293, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11597.2685546875
tensor(11597.5293, grad_fn=<NegBackward0>) tensor(11597.2686, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11597.0673828125
tensor(11597.2686, grad_fn=<NegBackward0>) tensor(11597.0674, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11589.5556640625
tensor(11597.0674, grad_fn=<NegBackward0>) tensor(11589.5557, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11584.24609375
tensor(11589.5557, grad_fn=<NegBackward0>) tensor(11584.2461, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11584.1826171875
tensor(11584.2461, grad_fn=<NegBackward0>) tensor(11584.1826, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11584.1337890625
tensor(11584.1826, grad_fn=<NegBackward0>) tensor(11584.1338, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11584.060546875
tensor(11584.1338, grad_fn=<NegBackward0>) tensor(11584.0605, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11572.4404296875
tensor(11584.0605, grad_fn=<NegBackward0>) tensor(11572.4404, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11572.4150390625
tensor(11572.4404, grad_fn=<NegBackward0>) tensor(11572.4150, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11572.39453125
tensor(11572.4150, grad_fn=<NegBackward0>) tensor(11572.3945, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11572.3779296875
tensor(11572.3945, grad_fn=<NegBackward0>) tensor(11572.3779, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11572.3623046875
tensor(11572.3779, grad_fn=<NegBackward0>) tensor(11572.3623, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11572.3505859375
tensor(11572.3623, grad_fn=<NegBackward0>) tensor(11572.3506, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11572.337890625
tensor(11572.3506, grad_fn=<NegBackward0>) tensor(11572.3379, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11572.3251953125
tensor(11572.3379, grad_fn=<NegBackward0>) tensor(11572.3252, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11572.2978515625
tensor(11572.3252, grad_fn=<NegBackward0>) tensor(11572.2979, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11572.2880859375
tensor(11572.2979, grad_fn=<NegBackward0>) tensor(11572.2881, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11572.28125
tensor(11572.2881, grad_fn=<NegBackward0>) tensor(11572.2812, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11572.275390625
tensor(11572.2812, grad_fn=<NegBackward0>) tensor(11572.2754, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11572.2705078125
tensor(11572.2754, grad_fn=<NegBackward0>) tensor(11572.2705, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11572.265625
tensor(11572.2705, grad_fn=<NegBackward0>) tensor(11572.2656, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11572.259765625
tensor(11572.2656, grad_fn=<NegBackward0>) tensor(11572.2598, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11572.2548828125
tensor(11572.2598, grad_fn=<NegBackward0>) tensor(11572.2549, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11572.2451171875
tensor(11572.2549, grad_fn=<NegBackward0>) tensor(11572.2451, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11571.6201171875
tensor(11572.2451, grad_fn=<NegBackward0>) tensor(11571.6201, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11571.611328125
tensor(11571.6201, grad_fn=<NegBackward0>) tensor(11571.6113, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11571.607421875
tensor(11571.6113, grad_fn=<NegBackward0>) tensor(11571.6074, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11571.603515625
tensor(11571.6074, grad_fn=<NegBackward0>) tensor(11571.6035, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11571.6025390625
tensor(11571.6035, grad_fn=<NegBackward0>) tensor(11571.6025, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11571.599609375
tensor(11571.6025, grad_fn=<NegBackward0>) tensor(11571.5996, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11571.59765625
tensor(11571.5996, grad_fn=<NegBackward0>) tensor(11571.5977, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11571.5966796875
tensor(11571.5977, grad_fn=<NegBackward0>) tensor(11571.5967, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11571.595703125
tensor(11571.5967, grad_fn=<NegBackward0>) tensor(11571.5957, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11571.59375
tensor(11571.5957, grad_fn=<NegBackward0>) tensor(11571.5938, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11571.5927734375
tensor(11571.5938, grad_fn=<NegBackward0>) tensor(11571.5928, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11571.591796875
tensor(11571.5928, grad_fn=<NegBackward0>) tensor(11571.5918, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11571.591796875
tensor(11571.5918, grad_fn=<NegBackward0>) tensor(11571.5918, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11571.5888671875
tensor(11571.5918, grad_fn=<NegBackward0>) tensor(11571.5889, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11571.587890625
tensor(11571.5889, grad_fn=<NegBackward0>) tensor(11571.5879, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11571.5888671875
tensor(11571.5879, grad_fn=<NegBackward0>) tensor(11571.5889, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11571.5869140625
tensor(11571.5879, grad_fn=<NegBackward0>) tensor(11571.5869, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11571.5869140625
tensor(11571.5869, grad_fn=<NegBackward0>) tensor(11571.5869, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11571.5849609375
tensor(11571.5869, grad_fn=<NegBackward0>) tensor(11571.5850, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11571.583984375
tensor(11571.5850, grad_fn=<NegBackward0>) tensor(11571.5840, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11571.583984375
tensor(11571.5840, grad_fn=<NegBackward0>) tensor(11571.5840, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11571.583984375
tensor(11571.5840, grad_fn=<NegBackward0>) tensor(11571.5840, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11571.58203125
tensor(11571.5840, grad_fn=<NegBackward0>) tensor(11571.5820, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11571.58203125
tensor(11571.5820, grad_fn=<NegBackward0>) tensor(11571.5820, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11571.58203125
tensor(11571.5820, grad_fn=<NegBackward0>) tensor(11571.5820, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11571.58203125
tensor(11571.5820, grad_fn=<NegBackward0>) tensor(11571.5820, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11571.5810546875
tensor(11571.5820, grad_fn=<NegBackward0>) tensor(11571.5811, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11571.58203125
tensor(11571.5811, grad_fn=<NegBackward0>) tensor(11571.5820, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11571.580078125
tensor(11571.5811, grad_fn=<NegBackward0>) tensor(11571.5801, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11571.5791015625
tensor(11571.5801, grad_fn=<NegBackward0>) tensor(11571.5791, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11571.58203125
tensor(11571.5791, grad_fn=<NegBackward0>) tensor(11571.5820, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11571.5791015625
tensor(11571.5791, grad_fn=<NegBackward0>) tensor(11571.5791, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11571.5859375
tensor(11571.5791, grad_fn=<NegBackward0>) tensor(11571.5859, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11571.578125
tensor(11571.5791, grad_fn=<NegBackward0>) tensor(11571.5781, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11571.5791015625
tensor(11571.5781, grad_fn=<NegBackward0>) tensor(11571.5791, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11571.576171875
tensor(11571.5781, grad_fn=<NegBackward0>) tensor(11571.5762, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11571.583984375
tensor(11571.5762, grad_fn=<NegBackward0>) tensor(11571.5840, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11571.576171875
tensor(11571.5762, grad_fn=<NegBackward0>) tensor(11571.5762, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11571.5791015625
tensor(11571.5762, grad_fn=<NegBackward0>) tensor(11571.5791, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11571.5751953125
tensor(11571.5762, grad_fn=<NegBackward0>) tensor(11571.5752, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11571.587890625
tensor(11571.5752, grad_fn=<NegBackward0>) tensor(11571.5879, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11571.578125
tensor(11571.5752, grad_fn=<NegBackward0>) tensor(11571.5781, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11571.6474609375
tensor(11571.5752, grad_fn=<NegBackward0>) tensor(11571.6475, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11571.5751953125
tensor(11571.5752, grad_fn=<NegBackward0>) tensor(11571.5752, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11571.5732421875
tensor(11571.5752, grad_fn=<NegBackward0>) tensor(11571.5732, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11571.599609375
tensor(11571.5732, grad_fn=<NegBackward0>) tensor(11571.5996, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11571.5732421875
tensor(11571.5732, grad_fn=<NegBackward0>) tensor(11571.5732, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11571.599609375
tensor(11571.5732, grad_fn=<NegBackward0>) tensor(11571.5996, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11571.572265625
tensor(11571.5732, grad_fn=<NegBackward0>) tensor(11571.5723, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11571.7392578125
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.7393, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11571.5732421875
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5732, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11571.580078125
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5801, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11571.572265625
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5723, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11571.5732421875
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5732, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11571.572265625
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5723, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11571.572265625
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5723, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11571.5810546875
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5811, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11571.572265625
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5723, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11571.57421875
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5742, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11571.5712890625
tensor(11571.5723, grad_fn=<NegBackward0>) tensor(11571.5713, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11571.5732421875
tensor(11571.5713, grad_fn=<NegBackward0>) tensor(11571.5732, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11571.5869140625
tensor(11571.5713, grad_fn=<NegBackward0>) tensor(11571.5869, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11571.57421875
tensor(11571.5713, grad_fn=<NegBackward0>) tensor(11571.5742, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11571.576171875
tensor(11571.5713, grad_fn=<NegBackward0>) tensor(11571.5762, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11571.572265625
tensor(11571.5713, grad_fn=<NegBackward0>) tensor(11571.5723, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.7473, 0.2527],
        [0.2658, 0.7342]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5000, 0.5000], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.1071],
         [0.6949, 0.4057]],

        [[0.6157, 0.0950],
         [0.7047, 0.5231]],

        [[0.6025, 0.1088],
         [0.5765, 0.7212]],

        [[0.7002, 0.0900],
         [0.5961, 0.7091]],

        [[0.5404, 0.1102],
         [0.6079, 0.7261]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320230862317
Average Adjusted Rand Index: 0.9841609542216967
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21518.265625
inf tensor(21518.2656, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12356.1474609375
tensor(21518.2656, grad_fn=<NegBackward0>) tensor(12356.1475, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12003.0302734375
tensor(12356.1475, grad_fn=<NegBackward0>) tensor(12003.0303, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11631.345703125
tensor(12003.0303, grad_fn=<NegBackward0>) tensor(11631.3457, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11600.6416015625
tensor(11631.3457, grad_fn=<NegBackward0>) tensor(11600.6416, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11600.01171875
tensor(11600.6416, grad_fn=<NegBackward0>) tensor(11600.0117, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11594.55859375
tensor(11600.0117, grad_fn=<NegBackward0>) tensor(11594.5586, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11594.392578125
tensor(11594.5586, grad_fn=<NegBackward0>) tensor(11594.3926, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11594.287109375
tensor(11594.3926, grad_fn=<NegBackward0>) tensor(11594.2871, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11594.212890625
tensor(11594.2871, grad_fn=<NegBackward0>) tensor(11594.2129, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11594.16015625
tensor(11594.2129, grad_fn=<NegBackward0>) tensor(11594.1602, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11594.115234375
tensor(11594.1602, grad_fn=<NegBackward0>) tensor(11594.1152, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11594.068359375
tensor(11594.1152, grad_fn=<NegBackward0>) tensor(11594.0684, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11593.5224609375
tensor(11594.0684, grad_fn=<NegBackward0>) tensor(11593.5225, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11593.498046875
tensor(11593.5225, grad_fn=<NegBackward0>) tensor(11593.4980, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11593.4814453125
tensor(11593.4980, grad_fn=<NegBackward0>) tensor(11593.4814, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11593.4658203125
tensor(11593.4814, grad_fn=<NegBackward0>) tensor(11593.4658, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11593.455078125
tensor(11593.4658, grad_fn=<NegBackward0>) tensor(11593.4551, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11593.4453125
tensor(11593.4551, grad_fn=<NegBackward0>) tensor(11593.4453, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11593.435546875
tensor(11593.4453, grad_fn=<NegBackward0>) tensor(11593.4355, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11593.4208984375
tensor(11593.4355, grad_fn=<NegBackward0>) tensor(11593.4209, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11578.9755859375
tensor(11593.4209, grad_fn=<NegBackward0>) tensor(11578.9756, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11578.966796875
tensor(11578.9756, grad_fn=<NegBackward0>) tensor(11578.9668, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11578.9638671875
tensor(11578.9668, grad_fn=<NegBackward0>) tensor(11578.9639, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11578.9580078125
tensor(11578.9639, grad_fn=<NegBackward0>) tensor(11578.9580, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11578.9541015625
tensor(11578.9580, grad_fn=<NegBackward0>) tensor(11578.9541, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11578.9501953125
tensor(11578.9541, grad_fn=<NegBackward0>) tensor(11578.9502, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11578.9462890625
tensor(11578.9502, grad_fn=<NegBackward0>) tensor(11578.9463, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11578.9453125
tensor(11578.9463, grad_fn=<NegBackward0>) tensor(11578.9453, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11578.9384765625
tensor(11578.9453, grad_fn=<NegBackward0>) tensor(11578.9385, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11578.8955078125
tensor(11578.9385, grad_fn=<NegBackward0>) tensor(11578.8955, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11578.8935546875
tensor(11578.8955, grad_fn=<NegBackward0>) tensor(11578.8936, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11578.8916015625
tensor(11578.8936, grad_fn=<NegBackward0>) tensor(11578.8916, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11578.888671875
tensor(11578.8916, grad_fn=<NegBackward0>) tensor(11578.8887, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11578.890625
tensor(11578.8887, grad_fn=<NegBackward0>) tensor(11578.8906, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11578.8857421875
tensor(11578.8887, grad_fn=<NegBackward0>) tensor(11578.8857, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11578.8857421875
tensor(11578.8857, grad_fn=<NegBackward0>) tensor(11578.8857, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11578.8837890625
tensor(11578.8857, grad_fn=<NegBackward0>) tensor(11578.8838, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11578.8828125
tensor(11578.8838, grad_fn=<NegBackward0>) tensor(11578.8828, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11578.8984375
tensor(11578.8828, grad_fn=<NegBackward0>) tensor(11578.8984, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11578.880859375
tensor(11578.8828, grad_fn=<NegBackward0>) tensor(11578.8809, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11578.8798828125
tensor(11578.8809, grad_fn=<NegBackward0>) tensor(11578.8799, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11578.8798828125
tensor(11578.8799, grad_fn=<NegBackward0>) tensor(11578.8799, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11578.876953125
tensor(11578.8799, grad_fn=<NegBackward0>) tensor(11578.8770, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11578.884765625
tensor(11578.8770, grad_fn=<NegBackward0>) tensor(11578.8848, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11578.8759765625
tensor(11578.8770, grad_fn=<NegBackward0>) tensor(11578.8760, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11578.876953125
tensor(11578.8760, grad_fn=<NegBackward0>) tensor(11578.8770, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11578.875
tensor(11578.8760, grad_fn=<NegBackward0>) tensor(11578.8750, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11578.875
tensor(11578.8750, grad_fn=<NegBackward0>) tensor(11578.8750, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11578.8779296875
tensor(11578.8750, grad_fn=<NegBackward0>) tensor(11578.8779, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11578.8740234375
tensor(11578.8750, grad_fn=<NegBackward0>) tensor(11578.8740, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11578.8828125
tensor(11578.8740, grad_fn=<NegBackward0>) tensor(11578.8828, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11578.8720703125
tensor(11578.8740, grad_fn=<NegBackward0>) tensor(11578.8721, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11578.8779296875
tensor(11578.8721, grad_fn=<NegBackward0>) tensor(11578.8779, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11578.8720703125
tensor(11578.8721, grad_fn=<NegBackward0>) tensor(11578.8721, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11578.8779296875
tensor(11578.8721, grad_fn=<NegBackward0>) tensor(11578.8779, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11578.87109375
tensor(11578.8721, grad_fn=<NegBackward0>) tensor(11578.8711, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11578.8720703125
tensor(11578.8711, grad_fn=<NegBackward0>) tensor(11578.8721, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11578.8701171875
tensor(11578.8711, grad_fn=<NegBackward0>) tensor(11578.8701, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11578.876953125
tensor(11578.8701, grad_fn=<NegBackward0>) tensor(11578.8770, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11578.8701171875
tensor(11578.8701, grad_fn=<NegBackward0>) tensor(11578.8701, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11578.8701171875
tensor(11578.8701, grad_fn=<NegBackward0>) tensor(11578.8701, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11578.869140625
tensor(11578.8701, grad_fn=<NegBackward0>) tensor(11578.8691, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11578.869140625
tensor(11578.8691, grad_fn=<NegBackward0>) tensor(11578.8691, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11578.8701171875
tensor(11578.8691, grad_fn=<NegBackward0>) tensor(11578.8701, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11578.8681640625
tensor(11578.8691, grad_fn=<NegBackward0>) tensor(11578.8682, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11578.87890625
tensor(11578.8682, grad_fn=<NegBackward0>) tensor(11578.8789, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11578.8681640625
tensor(11578.8682, grad_fn=<NegBackward0>) tensor(11578.8682, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11578.8671875
tensor(11578.8682, grad_fn=<NegBackward0>) tensor(11578.8672, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11578.8662109375
tensor(11578.8672, grad_fn=<NegBackward0>) tensor(11578.8662, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11578.8662109375
tensor(11578.8662, grad_fn=<NegBackward0>) tensor(11578.8662, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11578.8671875
tensor(11578.8662, grad_fn=<NegBackward0>) tensor(11578.8672, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11578.8671875
tensor(11578.8662, grad_fn=<NegBackward0>) tensor(11578.8672, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11578.8681640625
tensor(11578.8662, grad_fn=<NegBackward0>) tensor(11578.8682, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11578.865234375
tensor(11578.8662, grad_fn=<NegBackward0>) tensor(11578.8652, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11578.9169921875
tensor(11578.8652, grad_fn=<NegBackward0>) tensor(11578.9170, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11578.865234375
tensor(11578.8652, grad_fn=<NegBackward0>) tensor(11578.8652, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11578.876953125
tensor(11578.8652, grad_fn=<NegBackward0>) tensor(11578.8770, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11578.8642578125
tensor(11578.8652, grad_fn=<NegBackward0>) tensor(11578.8643, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11578.8759765625
tensor(11578.8643, grad_fn=<NegBackward0>) tensor(11578.8760, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11578.8662109375
tensor(11578.8643, grad_fn=<NegBackward0>) tensor(11578.8662, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11578.86328125
tensor(11578.8643, grad_fn=<NegBackward0>) tensor(11578.8633, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11578.86328125
tensor(11578.8633, grad_fn=<NegBackward0>) tensor(11578.8633, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11574.904296875
tensor(11578.8633, grad_fn=<NegBackward0>) tensor(11574.9043, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11571.5966796875
tensor(11574.9043, grad_fn=<NegBackward0>) tensor(11571.5967, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11571.63671875
tensor(11571.5967, grad_fn=<NegBackward0>) tensor(11571.6367, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11571.603515625
tensor(11571.5967, grad_fn=<NegBackward0>) tensor(11571.6035, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11571.6591796875
tensor(11571.5967, grad_fn=<NegBackward0>) tensor(11571.6592, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11571.6015625
tensor(11571.5967, grad_fn=<NegBackward0>) tensor(11571.6016, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11571.5947265625
tensor(11571.5967, grad_fn=<NegBackward0>) tensor(11571.5947, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11571.5947265625
tensor(11571.5947, grad_fn=<NegBackward0>) tensor(11571.5947, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11571.5947265625
tensor(11571.5947, grad_fn=<NegBackward0>) tensor(11571.5947, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11571.5986328125
tensor(11571.5947, grad_fn=<NegBackward0>) tensor(11571.5986, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11571.5966796875
tensor(11571.5947, grad_fn=<NegBackward0>) tensor(11571.5967, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11571.6162109375
tensor(11571.5947, grad_fn=<NegBackward0>) tensor(11571.6162, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11571.595703125
tensor(11571.5947, grad_fn=<NegBackward0>) tensor(11571.5957, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11571.6005859375
tensor(11571.5947, grad_fn=<NegBackward0>) tensor(11571.6006, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.7321, 0.2679],
        [0.2531, 0.7469]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5005, 0.4995], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4055, 0.1073],
         [0.6840, 0.1989]],

        [[0.5100, 0.0950],
         [0.6370, 0.6943]],

        [[0.6550, 0.1088],
         [0.5690, 0.7249]],

        [[0.5492, 0.0900],
         [0.5820, 0.7093]],

        [[0.7097, 0.1101],
         [0.6256, 0.7294]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320230862317
Average Adjusted Rand Index: 0.9841609542216967
[0.9840320230862317, 0.9840320230862317] [0.9841609542216967, 0.9841609542216967] [11571.572265625, 11571.6005859375]
-------------------------------------
This iteration is 3
True Objective function: Loss = -11371.602373623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21970.193359375
inf tensor(21970.1934, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12051.556640625
tensor(21970.1934, grad_fn=<NegBackward0>) tensor(12051.5566, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12034.875
tensor(12051.5566, grad_fn=<NegBackward0>) tensor(12034.8750, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11862.5830078125
tensor(12034.8750, grad_fn=<NegBackward0>) tensor(11862.5830, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11608.869140625
tensor(11862.5830, grad_fn=<NegBackward0>) tensor(11608.8691, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11580.7529296875
tensor(11608.8691, grad_fn=<NegBackward0>) tensor(11580.7529, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11565.82421875
tensor(11580.7529, grad_fn=<NegBackward0>) tensor(11565.8242, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11565.32421875
tensor(11565.8242, grad_fn=<NegBackward0>) tensor(11565.3242, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11564.58203125
tensor(11565.3242, grad_fn=<NegBackward0>) tensor(11564.5820, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11558.0341796875
tensor(11564.5820, grad_fn=<NegBackward0>) tensor(11558.0342, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11557.904296875
tensor(11558.0342, grad_fn=<NegBackward0>) tensor(11557.9043, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11557.818359375
tensor(11557.9043, grad_fn=<NegBackward0>) tensor(11557.8184, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11557.7470703125
tensor(11557.8184, grad_fn=<NegBackward0>) tensor(11557.7471, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11557.1259765625
tensor(11557.7471, grad_fn=<NegBackward0>) tensor(11557.1260, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11556.8076171875
tensor(11557.1260, grad_fn=<NegBackward0>) tensor(11556.8076, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11556.767578125
tensor(11556.8076, grad_fn=<NegBackward0>) tensor(11556.7676, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11556.7314453125
tensor(11556.7676, grad_fn=<NegBackward0>) tensor(11556.7314, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11556.375
tensor(11556.7314, grad_fn=<NegBackward0>) tensor(11556.3750, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11555.708984375
tensor(11556.3750, grad_fn=<NegBackward0>) tensor(11555.7090, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11555.4404296875
tensor(11555.7090, grad_fn=<NegBackward0>) tensor(11555.4404, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11554.48046875
tensor(11555.4404, grad_fn=<NegBackward0>) tensor(11554.4805, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11554.1611328125
tensor(11554.4805, grad_fn=<NegBackward0>) tensor(11554.1611, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11550.4375
tensor(11554.1611, grad_fn=<NegBackward0>) tensor(11550.4375, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11550.421875
tensor(11550.4375, grad_fn=<NegBackward0>) tensor(11550.4219, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11550.4091796875
tensor(11550.4219, grad_fn=<NegBackward0>) tensor(11550.4092, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11550.3994140625
tensor(11550.4092, grad_fn=<NegBackward0>) tensor(11550.3994, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11550.3896484375
tensor(11550.3994, grad_fn=<NegBackward0>) tensor(11550.3896, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11550.3740234375
tensor(11550.3896, grad_fn=<NegBackward0>) tensor(11550.3740, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11550.310546875
tensor(11550.3740, grad_fn=<NegBackward0>) tensor(11550.3105, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11550.3037109375
tensor(11550.3105, grad_fn=<NegBackward0>) tensor(11550.3037, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11550.2861328125
tensor(11550.3037, grad_fn=<NegBackward0>) tensor(11550.2861, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11550.216796875
tensor(11550.2861, grad_fn=<NegBackward0>) tensor(11550.2168, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11550.2001953125
tensor(11550.2168, grad_fn=<NegBackward0>) tensor(11550.2002, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11550.1982421875
tensor(11550.2002, grad_fn=<NegBackward0>) tensor(11550.1982, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11550.1962890625
tensor(11550.1982, grad_fn=<NegBackward0>) tensor(11550.1963, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11550.1923828125
tensor(11550.1963, grad_fn=<NegBackward0>) tensor(11550.1924, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11550.19140625
tensor(11550.1924, grad_fn=<NegBackward0>) tensor(11550.1914, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11550.1884765625
tensor(11550.1914, grad_fn=<NegBackward0>) tensor(11550.1885, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11550.193359375
tensor(11550.1885, grad_fn=<NegBackward0>) tensor(11550.1934, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11550.189453125
tensor(11550.1885, grad_fn=<NegBackward0>) tensor(11550.1895, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11550.1806640625
tensor(11550.1885, grad_fn=<NegBackward0>) tensor(11550.1807, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11550.1767578125
tensor(11550.1807, grad_fn=<NegBackward0>) tensor(11550.1768, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11550.1630859375
tensor(11550.1768, grad_fn=<NegBackward0>) tensor(11550.1631, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11547.0078125
tensor(11550.1631, grad_fn=<NegBackward0>) tensor(11547.0078, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11546.9853515625
tensor(11547.0078, grad_fn=<NegBackward0>) tensor(11546.9854, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11546.9814453125
tensor(11546.9854, grad_fn=<NegBackward0>) tensor(11546.9814, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11546.98046875
tensor(11546.9814, grad_fn=<NegBackward0>) tensor(11546.9805, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11546.9794921875
tensor(11546.9805, grad_fn=<NegBackward0>) tensor(11546.9795, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11546.9775390625
tensor(11546.9795, grad_fn=<NegBackward0>) tensor(11546.9775, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11546.9765625
tensor(11546.9775, grad_fn=<NegBackward0>) tensor(11546.9766, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11546.9755859375
tensor(11546.9766, grad_fn=<NegBackward0>) tensor(11546.9756, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11546.9736328125
tensor(11546.9756, grad_fn=<NegBackward0>) tensor(11546.9736, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11546.9716796875
tensor(11546.9736, grad_fn=<NegBackward0>) tensor(11546.9717, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11546.974609375
tensor(11546.9717, grad_fn=<NegBackward0>) tensor(11546.9746, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11546.970703125
tensor(11546.9717, grad_fn=<NegBackward0>) tensor(11546.9707, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11546.9677734375
tensor(11546.9707, grad_fn=<NegBackward0>) tensor(11546.9678, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11546.9794921875
tensor(11546.9678, grad_fn=<NegBackward0>) tensor(11546.9795, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11546.966796875
tensor(11546.9678, grad_fn=<NegBackward0>) tensor(11546.9668, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11546.966796875
tensor(11546.9668, grad_fn=<NegBackward0>) tensor(11546.9668, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11546.9775390625
tensor(11546.9668, grad_fn=<NegBackward0>) tensor(11546.9775, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11546.966796875
tensor(11546.9668, grad_fn=<NegBackward0>) tensor(11546.9668, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11546.966796875
tensor(11546.9668, grad_fn=<NegBackward0>) tensor(11546.9668, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11546.9658203125
tensor(11546.9668, grad_fn=<NegBackward0>) tensor(11546.9658, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11546.966796875
tensor(11546.9658, grad_fn=<NegBackward0>) tensor(11546.9668, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11546.96484375
tensor(11546.9658, grad_fn=<NegBackward0>) tensor(11546.9648, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11546.9638671875
tensor(11546.9648, grad_fn=<NegBackward0>) tensor(11546.9639, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11546.9638671875
tensor(11546.9639, grad_fn=<NegBackward0>) tensor(11546.9639, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11546.9697265625
tensor(11546.9639, grad_fn=<NegBackward0>) tensor(11546.9697, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11546.9638671875
tensor(11546.9639, grad_fn=<NegBackward0>) tensor(11546.9639, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11546.962890625
tensor(11546.9639, grad_fn=<NegBackward0>) tensor(11546.9629, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11546.962890625
tensor(11546.9629, grad_fn=<NegBackward0>) tensor(11546.9629, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11546.9619140625
tensor(11546.9629, grad_fn=<NegBackward0>) tensor(11546.9619, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11546.9609375
tensor(11546.9619, grad_fn=<NegBackward0>) tensor(11546.9609, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11546.9609375
tensor(11546.9609, grad_fn=<NegBackward0>) tensor(11546.9609, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11546.966796875
tensor(11546.9609, grad_fn=<NegBackward0>) tensor(11546.9668, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11546.9599609375
tensor(11546.9609, grad_fn=<NegBackward0>) tensor(11546.9600, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11546.958984375
tensor(11546.9600, grad_fn=<NegBackward0>) tensor(11546.9590, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11546.9482421875
tensor(11546.9590, grad_fn=<NegBackward0>) tensor(11546.9482, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11546.9306640625
tensor(11546.9482, grad_fn=<NegBackward0>) tensor(11546.9307, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11546.931640625
tensor(11546.9307, grad_fn=<NegBackward0>) tensor(11546.9316, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11546.9423828125
tensor(11546.9307, grad_fn=<NegBackward0>) tensor(11546.9424, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11546.9296875
tensor(11546.9307, grad_fn=<NegBackward0>) tensor(11546.9297, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11546.9306640625
tensor(11546.9297, grad_fn=<NegBackward0>) tensor(11546.9307, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11547.0185546875
tensor(11546.9297, grad_fn=<NegBackward0>) tensor(11547.0186, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11546.9287109375
tensor(11546.9297, grad_fn=<NegBackward0>) tensor(11546.9287, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11546.92578125
tensor(11546.9287, grad_fn=<NegBackward0>) tensor(11546.9258, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11546.923828125
tensor(11546.9258, grad_fn=<NegBackward0>) tensor(11546.9238, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11546.923828125
tensor(11546.9238, grad_fn=<NegBackward0>) tensor(11546.9238, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11546.923828125
tensor(11546.9238, grad_fn=<NegBackward0>) tensor(11546.9238, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11546.9228515625
tensor(11546.9238, grad_fn=<NegBackward0>) tensor(11546.9229, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11546.923828125
tensor(11546.9229, grad_fn=<NegBackward0>) tensor(11546.9238, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11546.921875
tensor(11546.9229, grad_fn=<NegBackward0>) tensor(11546.9219, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11546.927734375
tensor(11546.9219, grad_fn=<NegBackward0>) tensor(11546.9277, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11546.9501953125
tensor(11546.9219, grad_fn=<NegBackward0>) tensor(11546.9502, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11546.9345703125
tensor(11546.9219, grad_fn=<NegBackward0>) tensor(11546.9346, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11546.92578125
tensor(11546.9219, grad_fn=<NegBackward0>) tensor(11546.9258, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11546.923828125
tensor(11546.9219, grad_fn=<NegBackward0>) tensor(11546.9238, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.7018, 0.2982],
        [0.5207, 0.4793]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5097, 0.4903], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2078, 0.0969],
         [0.5047, 0.4007]],

        [[0.5875, 0.1066],
         [0.7216, 0.5329]],

        [[0.6522, 0.1054],
         [0.6054, 0.7036]],

        [[0.6512, 0.1024],
         [0.5721, 0.5291]],

        [[0.5503, 0.0989],
         [0.6439, 0.6838]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.05818181818181818
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.552429697311595
Average Adjusted Rand Index: 0.803636018840359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21017.892578125
inf tensor(21017.8926, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12041.3447265625
tensor(21017.8926, grad_fn=<NegBackward0>) tensor(12041.3447, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11797.974609375
tensor(12041.3447, grad_fn=<NegBackward0>) tensor(11797.9746, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11398.7041015625
tensor(11797.9746, grad_fn=<NegBackward0>) tensor(11398.7041, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11378.236328125
tensor(11398.7041, grad_fn=<NegBackward0>) tensor(11378.2363, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11377.1435546875
tensor(11378.2363, grad_fn=<NegBackward0>) tensor(11377.1436, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11374.5
tensor(11377.1436, grad_fn=<NegBackward0>) tensor(11374.5000, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11374.337890625
tensor(11374.5000, grad_fn=<NegBackward0>) tensor(11374.3379, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11374.2412109375
tensor(11374.3379, grad_fn=<NegBackward0>) tensor(11374.2412, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11374.173828125
tensor(11374.2412, grad_fn=<NegBackward0>) tensor(11374.1738, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11369.4990234375
tensor(11374.1738, grad_fn=<NegBackward0>) tensor(11369.4990, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11369.451171875
tensor(11369.4990, grad_fn=<NegBackward0>) tensor(11369.4512, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11369.4208984375
tensor(11369.4512, grad_fn=<NegBackward0>) tensor(11369.4209, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11369.3955078125
tensor(11369.4209, grad_fn=<NegBackward0>) tensor(11369.3955, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11369.376953125
tensor(11369.3955, grad_fn=<NegBackward0>) tensor(11369.3770, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11369.3623046875
tensor(11369.3770, grad_fn=<NegBackward0>) tensor(11369.3623, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11369.3505859375
tensor(11369.3623, grad_fn=<NegBackward0>) tensor(11369.3506, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11369.33984375
tensor(11369.3506, grad_fn=<NegBackward0>) tensor(11369.3398, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11369.3310546875
tensor(11369.3398, grad_fn=<NegBackward0>) tensor(11369.3311, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11369.32421875
tensor(11369.3311, grad_fn=<NegBackward0>) tensor(11369.3242, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11369.3173828125
tensor(11369.3242, grad_fn=<NegBackward0>) tensor(11369.3174, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11369.310546875
tensor(11369.3174, grad_fn=<NegBackward0>) tensor(11369.3105, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11369.306640625
tensor(11369.3105, grad_fn=<NegBackward0>) tensor(11369.3066, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11369.3017578125
tensor(11369.3066, grad_fn=<NegBackward0>) tensor(11369.3018, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11369.2978515625
tensor(11369.3018, grad_fn=<NegBackward0>) tensor(11369.2979, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11369.294921875
tensor(11369.2979, grad_fn=<NegBackward0>) tensor(11369.2949, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11369.291015625
tensor(11369.2949, grad_fn=<NegBackward0>) tensor(11369.2910, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11369.2880859375
tensor(11369.2910, grad_fn=<NegBackward0>) tensor(11369.2881, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11369.28515625
tensor(11369.2881, grad_fn=<NegBackward0>) tensor(11369.2852, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11369.2841796875
tensor(11369.2852, grad_fn=<NegBackward0>) tensor(11369.2842, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11369.2802734375
tensor(11369.2842, grad_fn=<NegBackward0>) tensor(11369.2803, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11369.279296875
tensor(11369.2803, grad_fn=<NegBackward0>) tensor(11369.2793, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11369.2783203125
tensor(11369.2793, grad_fn=<NegBackward0>) tensor(11369.2783, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11369.27734375
tensor(11369.2783, grad_fn=<NegBackward0>) tensor(11369.2773, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11369.2763671875
tensor(11369.2773, grad_fn=<NegBackward0>) tensor(11369.2764, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11369.2744140625
tensor(11369.2764, grad_fn=<NegBackward0>) tensor(11369.2744, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11369.2724609375
tensor(11369.2744, grad_fn=<NegBackward0>) tensor(11369.2725, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11369.26953125
tensor(11369.2725, grad_fn=<NegBackward0>) tensor(11369.2695, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11369.2626953125
tensor(11369.2695, grad_fn=<NegBackward0>) tensor(11369.2627, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11369.26171875
tensor(11369.2627, grad_fn=<NegBackward0>) tensor(11369.2617, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11369.259765625
tensor(11369.2617, grad_fn=<NegBackward0>) tensor(11369.2598, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11369.2451171875
tensor(11369.2598, grad_fn=<NegBackward0>) tensor(11369.2451, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11363.556640625
tensor(11369.2451, grad_fn=<NegBackward0>) tensor(11363.5566, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11363.552734375
tensor(11363.5566, grad_fn=<NegBackward0>) tensor(11363.5527, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11363.552734375
tensor(11363.5527, grad_fn=<NegBackward0>) tensor(11363.5527, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11363.5517578125
tensor(11363.5527, grad_fn=<NegBackward0>) tensor(11363.5518, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11363.55078125
tensor(11363.5518, grad_fn=<NegBackward0>) tensor(11363.5508, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11363.55078125
tensor(11363.5508, grad_fn=<NegBackward0>) tensor(11363.5508, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11363.5498046875
tensor(11363.5508, grad_fn=<NegBackward0>) tensor(11363.5498, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11363.5498046875
tensor(11363.5498, grad_fn=<NegBackward0>) tensor(11363.5498, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11363.55078125
tensor(11363.5498, grad_fn=<NegBackward0>) tensor(11363.5508, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11363.5478515625
tensor(11363.5498, grad_fn=<NegBackward0>) tensor(11363.5479, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11363.55859375
tensor(11363.5479, grad_fn=<NegBackward0>) tensor(11363.5586, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11363.546875
tensor(11363.5479, grad_fn=<NegBackward0>) tensor(11363.5469, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11363.5478515625
tensor(11363.5469, grad_fn=<NegBackward0>) tensor(11363.5479, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11363.546875
tensor(11363.5469, grad_fn=<NegBackward0>) tensor(11363.5469, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11363.5478515625
tensor(11363.5469, grad_fn=<NegBackward0>) tensor(11363.5479, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11363.546875
tensor(11363.5469, grad_fn=<NegBackward0>) tensor(11363.5469, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11363.5458984375
tensor(11363.5469, grad_fn=<NegBackward0>) tensor(11363.5459, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11363.544921875
tensor(11363.5459, grad_fn=<NegBackward0>) tensor(11363.5449, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11363.546875
tensor(11363.5449, grad_fn=<NegBackward0>) tensor(11363.5469, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11363.544921875
tensor(11363.5449, grad_fn=<NegBackward0>) tensor(11363.5449, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11363.5517578125
tensor(11363.5449, grad_fn=<NegBackward0>) tensor(11363.5518, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11363.544921875
tensor(11363.5449, grad_fn=<NegBackward0>) tensor(11363.5449, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11363.5439453125
tensor(11363.5449, grad_fn=<NegBackward0>) tensor(11363.5439, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11363.5439453125
tensor(11363.5439, grad_fn=<NegBackward0>) tensor(11363.5439, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11363.5439453125
tensor(11363.5439, grad_fn=<NegBackward0>) tensor(11363.5439, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11363.5439453125
tensor(11363.5439, grad_fn=<NegBackward0>) tensor(11363.5439, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11363.54296875
tensor(11363.5439, grad_fn=<NegBackward0>) tensor(11363.5430, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11363.5439453125
tensor(11363.5430, grad_fn=<NegBackward0>) tensor(11363.5439, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11363.54296875
tensor(11363.5430, grad_fn=<NegBackward0>) tensor(11363.5430, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11363.544921875
tensor(11363.5430, grad_fn=<NegBackward0>) tensor(11363.5449, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11363.5458984375
tensor(11363.5430, grad_fn=<NegBackward0>) tensor(11363.5459, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11363.5478515625
tensor(11363.5430, grad_fn=<NegBackward0>) tensor(11363.5479, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11363.55078125
tensor(11363.5430, grad_fn=<NegBackward0>) tensor(11363.5508, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11363.548828125
tensor(11363.5430, grad_fn=<NegBackward0>) tensor(11363.5488, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7984, 0.2016],
        [0.2822, 0.7178]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5095, 0.4905], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1969, 0.0970],
         [0.5229, 0.4071]],

        [[0.5537, 0.1065],
         [0.5054, 0.5626]],

        [[0.6525, 0.1070],
         [0.6981, 0.6841]],

        [[0.6828, 0.0989],
         [0.7020, 0.6397]],

        [[0.5255, 0.0986],
         [0.6119, 0.6073]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919990296688364
Average Adjusted Rand Index: 0.99199877740731
[0.552429697311595, 0.9919990296688364] [0.803636018840359, 0.99199877740731] [11546.923828125, 11363.548828125]
-------------------------------------
This iteration is 4
True Objective function: Loss = -11576.221801582491
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20886.1796875
inf tensor(20886.1797, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12396.2353515625
tensor(20886.1797, grad_fn=<NegBackward0>) tensor(12396.2354, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11788.5947265625
tensor(12396.2354, grad_fn=<NegBackward0>) tensor(11788.5947, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11638.15625
tensor(11788.5947, grad_fn=<NegBackward0>) tensor(11638.1562, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11633.2177734375
tensor(11638.1562, grad_fn=<NegBackward0>) tensor(11633.2178, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11622.998046875
tensor(11633.2178, grad_fn=<NegBackward0>) tensor(11622.9980, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11622.8369140625
tensor(11622.9980, grad_fn=<NegBackward0>) tensor(11622.8369, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11619.3603515625
tensor(11622.8369, grad_fn=<NegBackward0>) tensor(11619.3604, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11619.2939453125
tensor(11619.3604, grad_fn=<NegBackward0>) tensor(11619.2939, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11619.240234375
tensor(11619.2939, grad_fn=<NegBackward0>) tensor(11619.2402, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11616.0224609375
tensor(11619.2402, grad_fn=<NegBackward0>) tensor(11616.0225, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11615.98828125
tensor(11616.0225, grad_fn=<NegBackward0>) tensor(11615.9883, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11615.9365234375
tensor(11615.9883, grad_fn=<NegBackward0>) tensor(11615.9365, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11606.9736328125
tensor(11615.9365, grad_fn=<NegBackward0>) tensor(11606.9736, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11606.9443359375
tensor(11606.9736, grad_fn=<NegBackward0>) tensor(11606.9443, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11606.9208984375
tensor(11606.9443, grad_fn=<NegBackward0>) tensor(11606.9209, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11606.908203125
tensor(11606.9209, grad_fn=<NegBackward0>) tensor(11606.9082, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11606.8994140625
tensor(11606.9082, grad_fn=<NegBackward0>) tensor(11606.8994, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11606.8876953125
tensor(11606.8994, grad_fn=<NegBackward0>) tensor(11606.8877, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11598.455078125
tensor(11606.8877, grad_fn=<NegBackward0>) tensor(11598.4551, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11598.4501953125
tensor(11598.4551, grad_fn=<NegBackward0>) tensor(11598.4502, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11598.4462890625
tensor(11598.4502, grad_fn=<NegBackward0>) tensor(11598.4463, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11598.443359375
tensor(11598.4463, grad_fn=<NegBackward0>) tensor(11598.4434, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11598.439453125
tensor(11598.4434, grad_fn=<NegBackward0>) tensor(11598.4395, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11598.4375
tensor(11598.4395, grad_fn=<NegBackward0>) tensor(11598.4375, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11598.4345703125
tensor(11598.4375, grad_fn=<NegBackward0>) tensor(11598.4346, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11598.4326171875
tensor(11598.4346, grad_fn=<NegBackward0>) tensor(11598.4326, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11598.4296875
tensor(11598.4326, grad_fn=<NegBackward0>) tensor(11598.4297, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11598.4267578125
tensor(11598.4297, grad_fn=<NegBackward0>) tensor(11598.4268, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11598.4267578125
tensor(11598.4268, grad_fn=<NegBackward0>) tensor(11598.4268, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11598.423828125
tensor(11598.4268, grad_fn=<NegBackward0>) tensor(11598.4238, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11598.421875
tensor(11598.4238, grad_fn=<NegBackward0>) tensor(11598.4219, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11598.3359375
tensor(11598.4219, grad_fn=<NegBackward0>) tensor(11598.3359, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11591.3115234375
tensor(11598.3359, grad_fn=<NegBackward0>) tensor(11591.3115, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11591.310546875
tensor(11591.3115, grad_fn=<NegBackward0>) tensor(11591.3105, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11591.30859375
tensor(11591.3105, grad_fn=<NegBackward0>) tensor(11591.3086, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11591.30859375
tensor(11591.3086, grad_fn=<NegBackward0>) tensor(11591.3086, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11591.306640625
tensor(11591.3086, grad_fn=<NegBackward0>) tensor(11591.3066, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11591.3154296875
tensor(11591.3066, grad_fn=<NegBackward0>) tensor(11591.3154, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11582.7060546875
tensor(11591.3066, grad_fn=<NegBackward0>) tensor(11582.7061, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11582.7021484375
tensor(11582.7061, grad_fn=<NegBackward0>) tensor(11582.7021, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11582.701171875
tensor(11582.7021, grad_fn=<NegBackward0>) tensor(11582.7012, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11582.7001953125
tensor(11582.7012, grad_fn=<NegBackward0>) tensor(11582.7002, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11582.701171875
tensor(11582.7002, grad_fn=<NegBackward0>) tensor(11582.7012, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11582.69921875
tensor(11582.7002, grad_fn=<NegBackward0>) tensor(11582.6992, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11582.69921875
tensor(11582.6992, grad_fn=<NegBackward0>) tensor(11582.6992, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11572.818359375
tensor(11582.6992, grad_fn=<NegBackward0>) tensor(11572.8184, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11571.94140625
tensor(11572.8184, grad_fn=<NegBackward0>) tensor(11571.9414, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11571.9404296875
tensor(11571.9414, grad_fn=<NegBackward0>) tensor(11571.9404, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11571.9580078125
tensor(11571.9404, grad_fn=<NegBackward0>) tensor(11571.9580, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11571.9384765625
tensor(11571.9404, grad_fn=<NegBackward0>) tensor(11571.9385, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11571.9384765625
tensor(11571.9385, grad_fn=<NegBackward0>) tensor(11571.9385, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11571.9384765625
tensor(11571.9385, grad_fn=<NegBackward0>) tensor(11571.9385, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11571.9482421875
tensor(11571.9385, grad_fn=<NegBackward0>) tensor(11571.9482, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11571.9296875
tensor(11571.9385, grad_fn=<NegBackward0>) tensor(11571.9297, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11571.9130859375
tensor(11571.9297, grad_fn=<NegBackward0>) tensor(11571.9131, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11571.912109375
tensor(11571.9131, grad_fn=<NegBackward0>) tensor(11571.9121, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11571.9130859375
tensor(11571.9121, grad_fn=<NegBackward0>) tensor(11571.9131, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11571.9150390625
tensor(11571.9121, grad_fn=<NegBackward0>) tensor(11571.9150, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11571.91015625
tensor(11571.9121, grad_fn=<NegBackward0>) tensor(11571.9102, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11571.91015625
tensor(11571.9102, grad_fn=<NegBackward0>) tensor(11571.9102, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11571.91015625
tensor(11571.9102, grad_fn=<NegBackward0>) tensor(11571.9102, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11571.91015625
tensor(11571.9102, grad_fn=<NegBackward0>) tensor(11571.9102, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11571.9091796875
tensor(11571.9102, grad_fn=<NegBackward0>) tensor(11571.9092, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11571.9091796875
tensor(11571.9092, grad_fn=<NegBackward0>) tensor(11571.9092, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11571.912109375
tensor(11571.9092, grad_fn=<NegBackward0>) tensor(11571.9121, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11571.908203125
tensor(11571.9092, grad_fn=<NegBackward0>) tensor(11571.9082, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11571.9365234375
tensor(11571.9082, grad_fn=<NegBackward0>) tensor(11571.9365, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11571.9091796875
tensor(11571.9082, grad_fn=<NegBackward0>) tensor(11571.9092, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11571.9560546875
tensor(11571.9082, grad_fn=<NegBackward0>) tensor(11571.9561, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11571.9091796875
tensor(11571.9082, grad_fn=<NegBackward0>) tensor(11571.9092, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11571.9111328125
tensor(11571.9082, grad_fn=<NegBackward0>) tensor(11571.9111, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.8050, 0.1950],
        [0.2201, 0.7799]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3949, 0.0968],
         [0.5082, 0.1987]],

        [[0.5868, 0.1028],
         [0.7299, 0.5881]],

        [[0.5386, 0.1013],
         [0.6589, 0.5992]],

        [[0.6487, 0.0939],
         [0.6717, 0.6272]],

        [[0.7276, 0.1043],
         [0.5536, 0.6008]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21439.908203125
inf tensor(21439.9082, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12386.322265625
tensor(21439.9082, grad_fn=<NegBackward0>) tensor(12386.3223, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12115.82421875
tensor(12386.3223, grad_fn=<NegBackward0>) tensor(12115.8242, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11921.6552734375
tensor(12115.8242, grad_fn=<NegBackward0>) tensor(11921.6553, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11786.8564453125
tensor(11921.6553, grad_fn=<NegBackward0>) tensor(11786.8564, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11729.6689453125
tensor(11786.8564, grad_fn=<NegBackward0>) tensor(11729.6689, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11729.07421875
tensor(11729.6689, grad_fn=<NegBackward0>) tensor(11729.0742, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11695.7099609375
tensor(11729.0742, grad_fn=<NegBackward0>) tensor(11695.7100, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11674.935546875
tensor(11695.7100, grad_fn=<NegBackward0>) tensor(11674.9355, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11669.578125
tensor(11674.9355, grad_fn=<NegBackward0>) tensor(11669.5781, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11658.322265625
tensor(11669.5781, grad_fn=<NegBackward0>) tensor(11658.3223, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11658.26171875
tensor(11658.3223, grad_fn=<NegBackward0>) tensor(11658.2617, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11658.2177734375
tensor(11658.2617, grad_fn=<NegBackward0>) tensor(11658.2178, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11650.8203125
tensor(11658.2178, grad_fn=<NegBackward0>) tensor(11650.8203, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11650.787109375
tensor(11650.8203, grad_fn=<NegBackward0>) tensor(11650.7871, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11650.7626953125
tensor(11650.7871, grad_fn=<NegBackward0>) tensor(11650.7627, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11650.73828125
tensor(11650.7627, grad_fn=<NegBackward0>) tensor(11650.7383, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11642.935546875
tensor(11650.7383, grad_fn=<NegBackward0>) tensor(11642.9355, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11633.111328125
tensor(11642.9355, grad_fn=<NegBackward0>) tensor(11633.1113, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11633.0966796875
tensor(11633.1113, grad_fn=<NegBackward0>) tensor(11633.0967, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11633.08203125
tensor(11633.0967, grad_fn=<NegBackward0>) tensor(11633.0820, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11633.0673828125
tensor(11633.0820, grad_fn=<NegBackward0>) tensor(11633.0674, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11633.0595703125
tensor(11633.0674, grad_fn=<NegBackward0>) tensor(11633.0596, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11633.0546875
tensor(11633.0596, grad_fn=<NegBackward0>) tensor(11633.0547, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11633.046875
tensor(11633.0547, grad_fn=<NegBackward0>) tensor(11633.0469, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11633.037109375
tensor(11633.0469, grad_fn=<NegBackward0>) tensor(11633.0371, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11627.1884765625
tensor(11633.0371, grad_fn=<NegBackward0>) tensor(11627.1885, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11627.185546875
tensor(11627.1885, grad_fn=<NegBackward0>) tensor(11627.1855, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11627.1806640625
tensor(11627.1855, grad_fn=<NegBackward0>) tensor(11627.1807, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11627.177734375
tensor(11627.1807, grad_fn=<NegBackward0>) tensor(11627.1777, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11627.173828125
tensor(11627.1777, grad_fn=<NegBackward0>) tensor(11627.1738, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11627.1728515625
tensor(11627.1738, grad_fn=<NegBackward0>) tensor(11627.1729, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11627.169921875
tensor(11627.1729, grad_fn=<NegBackward0>) tensor(11627.1699, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11627.16796875
tensor(11627.1699, grad_fn=<NegBackward0>) tensor(11627.1680, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11627.166015625
tensor(11627.1680, grad_fn=<NegBackward0>) tensor(11627.1660, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11627.1640625
tensor(11627.1660, grad_fn=<NegBackward0>) tensor(11627.1641, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11627.162109375
tensor(11627.1641, grad_fn=<NegBackward0>) tensor(11627.1621, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11627.1611328125
tensor(11627.1621, grad_fn=<NegBackward0>) tensor(11627.1611, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11627.1591796875
tensor(11627.1611, grad_fn=<NegBackward0>) tensor(11627.1592, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11627.158203125
tensor(11627.1592, grad_fn=<NegBackward0>) tensor(11627.1582, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11627.158203125
tensor(11627.1582, grad_fn=<NegBackward0>) tensor(11627.1582, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11627.1552734375
tensor(11627.1582, grad_fn=<NegBackward0>) tensor(11627.1553, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11623.076171875
tensor(11627.1553, grad_fn=<NegBackward0>) tensor(11623.0762, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11623.0751953125
tensor(11623.0762, grad_fn=<NegBackward0>) tensor(11623.0752, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11623.0791015625
tensor(11623.0752, grad_fn=<NegBackward0>) tensor(11623.0791, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11623.0732421875
tensor(11623.0752, grad_fn=<NegBackward0>) tensor(11623.0732, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11623.072265625
tensor(11623.0732, grad_fn=<NegBackward0>) tensor(11623.0723, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11623.072265625
tensor(11623.0723, grad_fn=<NegBackward0>) tensor(11623.0723, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11623.0712890625
tensor(11623.0723, grad_fn=<NegBackward0>) tensor(11623.0713, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11623.0703125
tensor(11623.0713, grad_fn=<NegBackward0>) tensor(11623.0703, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11623.0693359375
tensor(11623.0703, grad_fn=<NegBackward0>) tensor(11623.0693, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11623.0703125
tensor(11623.0693, grad_fn=<NegBackward0>) tensor(11623.0703, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11623.068359375
tensor(11623.0693, grad_fn=<NegBackward0>) tensor(11623.0684, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11623.078125
tensor(11623.0684, grad_fn=<NegBackward0>) tensor(11623.0781, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11623.068359375
tensor(11623.0684, grad_fn=<NegBackward0>) tensor(11623.0684, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11618.150390625
tensor(11623.0684, grad_fn=<NegBackward0>) tensor(11618.1504, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11618.1484375
tensor(11618.1504, grad_fn=<NegBackward0>) tensor(11618.1484, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11618.1494140625
tensor(11618.1484, grad_fn=<NegBackward0>) tensor(11618.1494, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11618.1484375
tensor(11618.1484, grad_fn=<NegBackward0>) tensor(11618.1484, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11618.1484375
tensor(11618.1484, grad_fn=<NegBackward0>) tensor(11618.1484, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11618.1484375
tensor(11618.1484, grad_fn=<NegBackward0>) tensor(11618.1484, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11618.150390625
tensor(11618.1484, grad_fn=<NegBackward0>) tensor(11618.1504, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11609.0048828125
tensor(11618.1484, grad_fn=<NegBackward0>) tensor(11609.0049, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11609.0048828125
tensor(11609.0049, grad_fn=<NegBackward0>) tensor(11609.0049, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11595.470703125
tensor(11609.0049, grad_fn=<NegBackward0>) tensor(11595.4707, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11584.66796875
tensor(11595.4707, grad_fn=<NegBackward0>) tensor(11584.6680, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11584.666015625
tensor(11584.6680, grad_fn=<NegBackward0>) tensor(11584.6660, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11584.6708984375
tensor(11584.6660, grad_fn=<NegBackward0>) tensor(11584.6709, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11584.7119140625
tensor(11584.6660, grad_fn=<NegBackward0>) tensor(11584.7119, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11584.66796875
tensor(11584.6660, grad_fn=<NegBackward0>) tensor(11584.6680, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11584.6708984375
tensor(11584.6660, grad_fn=<NegBackward0>) tensor(11584.6709, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11584.673828125
tensor(11584.6660, grad_fn=<NegBackward0>) tensor(11584.6738, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.7936, 0.2064],
        [0.2286, 0.7714]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4901, 0.5099], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3983, 0.0968],
         [0.6422, 0.1975]],

        [[0.6428, 0.1028],
         [0.6072, 0.6690]],

        [[0.6321, 0.1014],
         [0.5978, 0.6594]],

        [[0.6460, 0.0991],
         [0.6123, 0.7290]],

        [[0.5448, 0.1043],
         [0.5045, 0.7222]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320418465675
Average Adjusted Rand Index: 0.9841606232309879
[1.0, 0.9840320418465675] [1.0, 0.9841606232309879] [11571.9111328125, 11584.673828125]
-------------------------------------
This iteration is 5
True Objective function: Loss = -11690.846514248846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24032.1875
inf tensor(24032.1875, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12396.22265625
tensor(24032.1875, grad_fn=<NegBackward0>) tensor(12396.2227, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12290.30078125
tensor(12396.2227, grad_fn=<NegBackward0>) tensor(12290.3008, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11825.939453125
tensor(12290.3008, grad_fn=<NegBackward0>) tensor(11825.9395, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11699.58984375
tensor(11825.9395, grad_fn=<NegBackward0>) tensor(11699.5898, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11686.94140625
tensor(11699.5898, grad_fn=<NegBackward0>) tensor(11686.9414, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11686.818359375
tensor(11686.9414, grad_fn=<NegBackward0>) tensor(11686.8184, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11686.7529296875
tensor(11686.8184, grad_fn=<NegBackward0>) tensor(11686.7529, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11686.7109375
tensor(11686.7529, grad_fn=<NegBackward0>) tensor(11686.7109, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11686.681640625
tensor(11686.7109, grad_fn=<NegBackward0>) tensor(11686.6816, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11686.662109375
tensor(11686.6816, grad_fn=<NegBackward0>) tensor(11686.6621, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11686.646484375
tensor(11686.6621, grad_fn=<NegBackward0>) tensor(11686.6465, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11686.6337890625
tensor(11686.6465, grad_fn=<NegBackward0>) tensor(11686.6338, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11686.625
tensor(11686.6338, grad_fn=<NegBackward0>) tensor(11686.6250, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11686.6171875
tensor(11686.6250, grad_fn=<NegBackward0>) tensor(11686.6172, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11686.609375
tensor(11686.6172, grad_fn=<NegBackward0>) tensor(11686.6094, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11686.6044921875
tensor(11686.6094, grad_fn=<NegBackward0>) tensor(11686.6045, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11686.599609375
tensor(11686.6045, grad_fn=<NegBackward0>) tensor(11686.5996, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11686.5966796875
tensor(11686.5996, grad_fn=<NegBackward0>) tensor(11686.5967, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11686.5927734375
tensor(11686.5967, grad_fn=<NegBackward0>) tensor(11686.5928, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11686.5927734375
tensor(11686.5928, grad_fn=<NegBackward0>) tensor(11686.5928, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11686.5869140625
tensor(11686.5928, grad_fn=<NegBackward0>) tensor(11686.5869, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11686.583984375
tensor(11686.5869, grad_fn=<NegBackward0>) tensor(11686.5840, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11686.5830078125
tensor(11686.5840, grad_fn=<NegBackward0>) tensor(11686.5830, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11686.5791015625
tensor(11686.5830, grad_fn=<NegBackward0>) tensor(11686.5791, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11686.578125
tensor(11686.5791, grad_fn=<NegBackward0>) tensor(11686.5781, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11686.5771484375
tensor(11686.5781, grad_fn=<NegBackward0>) tensor(11686.5771, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11686.5771484375
tensor(11686.5771, grad_fn=<NegBackward0>) tensor(11686.5771, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11686.5751953125
tensor(11686.5771, grad_fn=<NegBackward0>) tensor(11686.5752, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11686.5732421875
tensor(11686.5752, grad_fn=<NegBackward0>) tensor(11686.5732, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11686.572265625
tensor(11686.5732, grad_fn=<NegBackward0>) tensor(11686.5723, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11686.5712890625
tensor(11686.5723, grad_fn=<NegBackward0>) tensor(11686.5713, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11686.5703125
tensor(11686.5713, grad_fn=<NegBackward0>) tensor(11686.5703, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11686.5712890625
tensor(11686.5703, grad_fn=<NegBackward0>) tensor(11686.5713, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11686.568359375
tensor(11686.5703, grad_fn=<NegBackward0>) tensor(11686.5684, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11686.568359375
tensor(11686.5684, grad_fn=<NegBackward0>) tensor(11686.5684, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11686.5693359375
tensor(11686.5684, grad_fn=<NegBackward0>) tensor(11686.5693, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11686.5673828125
tensor(11686.5684, grad_fn=<NegBackward0>) tensor(11686.5674, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11686.56640625
tensor(11686.5674, grad_fn=<NegBackward0>) tensor(11686.5664, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11686.56640625
tensor(11686.5664, grad_fn=<NegBackward0>) tensor(11686.5664, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11686.56640625
tensor(11686.5664, grad_fn=<NegBackward0>) tensor(11686.5664, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11686.5654296875
tensor(11686.5664, grad_fn=<NegBackward0>) tensor(11686.5654, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11686.564453125
tensor(11686.5654, grad_fn=<NegBackward0>) tensor(11686.5645, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11686.5654296875
tensor(11686.5645, grad_fn=<NegBackward0>) tensor(11686.5654, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11686.56640625
tensor(11686.5645, grad_fn=<NegBackward0>) tensor(11686.5664, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11686.564453125
tensor(11686.5645, grad_fn=<NegBackward0>) tensor(11686.5645, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11686.564453125
tensor(11686.5645, grad_fn=<NegBackward0>) tensor(11686.5645, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11686.5634765625
tensor(11686.5645, grad_fn=<NegBackward0>) tensor(11686.5635, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11686.5634765625
tensor(11686.5635, grad_fn=<NegBackward0>) tensor(11686.5635, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11686.5625
tensor(11686.5635, grad_fn=<NegBackward0>) tensor(11686.5625, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11686.5634765625
tensor(11686.5625, grad_fn=<NegBackward0>) tensor(11686.5635, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11686.5625
tensor(11686.5625, grad_fn=<NegBackward0>) tensor(11686.5625, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11686.5634765625
tensor(11686.5625, grad_fn=<NegBackward0>) tensor(11686.5635, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11686.5615234375
tensor(11686.5625, grad_fn=<NegBackward0>) tensor(11686.5615, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11686.5634765625
tensor(11686.5615, grad_fn=<NegBackward0>) tensor(11686.5635, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11686.5615234375
tensor(11686.5615, grad_fn=<NegBackward0>) tensor(11686.5615, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11686.5595703125
tensor(11686.5615, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11686.560546875
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5605, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11686.5615234375
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5615, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11686.5693359375
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5693, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11686.5595703125
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11686.5595703125
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11686.5595703125
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11686.5595703125
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11686.5634765625
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5635, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11686.5693359375
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5693, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11686.5625
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5625, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11686.5595703125
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11686.5693359375
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5693, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11686.5595703125
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11686.5595703125
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11686.560546875
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5605, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11686.55859375
tensor(11686.5596, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11686.5595703125
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11686.560546875
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5605, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11686.55859375
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11686.5673828125
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5674, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11686.55859375
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11686.5615234375
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5615, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11686.55859375
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11686.5595703125
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5596, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11686.55859375
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11686.5576171875
tensor(11686.5586, grad_fn=<NegBackward0>) tensor(11686.5576, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11686.5615234375
tensor(11686.5576, grad_fn=<NegBackward0>) tensor(11686.5615, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11686.55859375
tensor(11686.5576, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11686.55859375
tensor(11686.5576, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11686.55859375
tensor(11686.5576, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11686.55859375
tensor(11686.5576, grad_fn=<NegBackward0>) tensor(11686.5586, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.7551, 0.2449],
        [0.2326, 0.7674]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6101, 0.3899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3963, 0.0979],
         [0.6406, 0.1976]],

        [[0.7191, 0.1038],
         [0.5289, 0.5246]],

        [[0.7275, 0.1010],
         [0.5908, 0.5306]],

        [[0.6456, 0.0947],
         [0.5665, 0.6787]],

        [[0.6484, 0.1011],
         [0.6730, 0.5473]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23840.189453125
inf tensor(23840.1895, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12544.9931640625
tensor(23840.1895, grad_fn=<NegBackward0>) tensor(12544.9932, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12212.8154296875
tensor(12544.9932, grad_fn=<NegBackward0>) tensor(12212.8154, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11956.267578125
tensor(12212.8154, grad_fn=<NegBackward0>) tensor(11956.2676, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11939.98828125
tensor(11956.2676, grad_fn=<NegBackward0>) tensor(11939.9883, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11921.6640625
tensor(11939.9883, grad_fn=<NegBackward0>) tensor(11921.6641, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11901.01953125
tensor(11921.6641, grad_fn=<NegBackward0>) tensor(11901.0195, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11900.3828125
tensor(11901.0195, grad_fn=<NegBackward0>) tensor(11900.3828, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11887.0947265625
tensor(11900.3828, grad_fn=<NegBackward0>) tensor(11887.0947, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11883.744140625
tensor(11887.0947, grad_fn=<NegBackward0>) tensor(11883.7441, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11883.208984375
tensor(11883.7441, grad_fn=<NegBackward0>) tensor(11883.2090, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11883.1640625
tensor(11883.2090, grad_fn=<NegBackward0>) tensor(11883.1641, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11883.1416015625
tensor(11883.1641, grad_fn=<NegBackward0>) tensor(11883.1416, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11883.1220703125
tensor(11883.1416, grad_fn=<NegBackward0>) tensor(11883.1221, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11880.109375
tensor(11883.1221, grad_fn=<NegBackward0>) tensor(11880.1094, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11880.0771484375
tensor(11880.1094, grad_fn=<NegBackward0>) tensor(11880.0771, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11880.06640625
tensor(11880.0771, grad_fn=<NegBackward0>) tensor(11880.0664, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11880.0595703125
tensor(11880.0664, grad_fn=<NegBackward0>) tensor(11880.0596, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11880.05078125
tensor(11880.0596, grad_fn=<NegBackward0>) tensor(11880.0508, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11880.0478515625
tensor(11880.0508, grad_fn=<NegBackward0>) tensor(11880.0479, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11880.0419921875
tensor(11880.0479, grad_fn=<NegBackward0>) tensor(11880.0420, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11880.03515625
tensor(11880.0420, grad_fn=<NegBackward0>) tensor(11880.0352, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11880.033203125
tensor(11880.0352, grad_fn=<NegBackward0>) tensor(11880.0332, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11880.029296875
tensor(11880.0332, grad_fn=<NegBackward0>) tensor(11880.0293, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11880.0244140625
tensor(11880.0293, grad_fn=<NegBackward0>) tensor(11880.0244, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11880.0234375
tensor(11880.0244, grad_fn=<NegBackward0>) tensor(11880.0234, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11880.0224609375
tensor(11880.0234, grad_fn=<NegBackward0>) tensor(11880.0225, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11880.0205078125
tensor(11880.0225, grad_fn=<NegBackward0>) tensor(11880.0205, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11880.015625
tensor(11880.0205, grad_fn=<NegBackward0>) tensor(11880.0156, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11880.0146484375
tensor(11880.0156, grad_fn=<NegBackward0>) tensor(11880.0146, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11879.9853515625
tensor(11880.0146, grad_fn=<NegBackward0>) tensor(11879.9854, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11879.982421875
tensor(11879.9854, grad_fn=<NegBackward0>) tensor(11879.9824, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11879.982421875
tensor(11879.9824, grad_fn=<NegBackward0>) tensor(11879.9824, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11879.98046875
tensor(11879.9824, grad_fn=<NegBackward0>) tensor(11879.9805, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11879.978515625
tensor(11879.9805, grad_fn=<NegBackward0>) tensor(11879.9785, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11879.9775390625
tensor(11879.9785, grad_fn=<NegBackward0>) tensor(11879.9775, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11879.9765625
tensor(11879.9775, grad_fn=<NegBackward0>) tensor(11879.9766, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11879.9765625
tensor(11879.9766, grad_fn=<NegBackward0>) tensor(11879.9766, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11879.9755859375
tensor(11879.9766, grad_fn=<NegBackward0>) tensor(11879.9756, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11879.974609375
tensor(11879.9756, grad_fn=<NegBackward0>) tensor(11879.9746, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11879.9755859375
tensor(11879.9746, grad_fn=<NegBackward0>) tensor(11879.9756, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11879.9736328125
tensor(11879.9746, grad_fn=<NegBackward0>) tensor(11879.9736, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11879.9736328125
tensor(11879.9736, grad_fn=<NegBackward0>) tensor(11879.9736, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11879.9736328125
tensor(11879.9736, grad_fn=<NegBackward0>) tensor(11879.9736, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11879.97265625
tensor(11879.9736, grad_fn=<NegBackward0>) tensor(11879.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11879.9716796875
tensor(11879.9727, grad_fn=<NegBackward0>) tensor(11879.9717, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11879.970703125
tensor(11879.9717, grad_fn=<NegBackward0>) tensor(11879.9707, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11879.97265625
tensor(11879.9707, grad_fn=<NegBackward0>) tensor(11879.9727, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11879.9697265625
tensor(11879.9707, grad_fn=<NegBackward0>) tensor(11879.9697, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11879.9755859375
tensor(11879.9697, grad_fn=<NegBackward0>) tensor(11879.9756, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11879.9677734375
tensor(11879.9697, grad_fn=<NegBackward0>) tensor(11879.9678, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11879.9482421875
tensor(11879.9678, grad_fn=<NegBackward0>) tensor(11879.9482, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11879.9482421875
tensor(11879.9482, grad_fn=<NegBackward0>) tensor(11879.9482, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11879.9501953125
tensor(11879.9482, grad_fn=<NegBackward0>) tensor(11879.9502, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11879.947265625
tensor(11879.9482, grad_fn=<NegBackward0>) tensor(11879.9473, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11879.947265625
tensor(11879.9473, grad_fn=<NegBackward0>) tensor(11879.9473, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11879.947265625
tensor(11879.9473, grad_fn=<NegBackward0>) tensor(11879.9473, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11879.9580078125
tensor(11879.9473, grad_fn=<NegBackward0>) tensor(11879.9580, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11879.947265625
tensor(11879.9473, grad_fn=<NegBackward0>) tensor(11879.9473, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11879.9501953125
tensor(11879.9473, grad_fn=<NegBackward0>) tensor(11879.9502, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11879.9462890625
tensor(11879.9473, grad_fn=<NegBackward0>) tensor(11879.9463, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11879.9462890625
tensor(11879.9463, grad_fn=<NegBackward0>) tensor(11879.9463, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11879.947265625
tensor(11879.9463, grad_fn=<NegBackward0>) tensor(11879.9473, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11879.9443359375
tensor(11879.9463, grad_fn=<NegBackward0>) tensor(11879.9443, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11879.943359375
tensor(11879.9443, grad_fn=<NegBackward0>) tensor(11879.9434, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11879.9443359375
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9443, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11879.9453125
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9453, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11879.9443359375
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9443, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11879.9501953125
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9502, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11879.943359375
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9434, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11879.9443359375
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9443, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11879.947265625
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9473, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11879.943359375
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9434, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11879.951171875
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9512, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11879.9443359375
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9443, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11879.9453125
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9453, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11880.013671875
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11880.0137, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11879.94140625
tensor(11879.9434, grad_fn=<NegBackward0>) tensor(11879.9414, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11879.943359375
tensor(11879.9414, grad_fn=<NegBackward0>) tensor(11879.9434, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11879.943359375
tensor(11879.9414, grad_fn=<NegBackward0>) tensor(11879.9434, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11879.962890625
tensor(11879.9414, grad_fn=<NegBackward0>) tensor(11879.9629, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11879.9423828125
tensor(11879.9414, grad_fn=<NegBackward0>) tensor(11879.9424, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11879.953125
tensor(11879.9414, grad_fn=<NegBackward0>) tensor(11879.9531, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.5099, 0.4901],
        [0.4065, 0.5935]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6090, 0.3910], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3848, 0.0979],
         [0.6848, 0.2338]],

        [[0.6528, 0.1039],
         [0.6433, 0.6551]],

        [[0.6469, 0.0951],
         [0.6807, 0.6876]],

        [[0.7070, 0.0928],
         [0.5204, 0.6540]],

        [[0.6737, 0.1002],
         [0.5989, 0.6309]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 83
Adjusted Rand Index: 0.43024722446959873
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4397781013480388
Average Adjusted Rand Index: 0.8780486612236491
[1.0, 0.4397781013480388] [1.0, 0.8780486612236491] [11686.55859375, 11879.953125]
-------------------------------------
This iteration is 6
True Objective function: Loss = -11379.4990221895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23289.013671875
inf tensor(23289.0137, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11371.95703125
tensor(23289.0137, grad_fn=<NegBackward0>) tensor(11371.9570, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11369.4921875
tensor(11371.9570, grad_fn=<NegBackward0>) tensor(11369.4922, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11369.220703125
tensor(11369.4922, grad_fn=<NegBackward0>) tensor(11369.2207, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11369.091796875
tensor(11369.2207, grad_fn=<NegBackward0>) tensor(11369.0918, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11369.029296875
tensor(11369.0918, grad_fn=<NegBackward0>) tensor(11369.0293, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11368.990234375
tensor(11369.0293, grad_fn=<NegBackward0>) tensor(11368.9902, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11368.96484375
tensor(11368.9902, grad_fn=<NegBackward0>) tensor(11368.9648, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11368.9365234375
tensor(11368.9648, grad_fn=<NegBackward0>) tensor(11368.9365, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11368.9248046875
tensor(11368.9365, grad_fn=<NegBackward0>) tensor(11368.9248, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11368.9150390625
tensor(11368.9248, grad_fn=<NegBackward0>) tensor(11368.9150, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11368.908203125
tensor(11368.9150, grad_fn=<NegBackward0>) tensor(11368.9082, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11368.90234375
tensor(11368.9082, grad_fn=<NegBackward0>) tensor(11368.9023, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11368.8984375
tensor(11368.9023, grad_fn=<NegBackward0>) tensor(11368.8984, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11368.8935546875
tensor(11368.8984, grad_fn=<NegBackward0>) tensor(11368.8936, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11368.8896484375
tensor(11368.8936, grad_fn=<NegBackward0>) tensor(11368.8896, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11368.8876953125
tensor(11368.8896, grad_fn=<NegBackward0>) tensor(11368.8877, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11368.884765625
tensor(11368.8877, grad_fn=<NegBackward0>) tensor(11368.8848, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11368.8837890625
tensor(11368.8848, grad_fn=<NegBackward0>) tensor(11368.8838, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11368.8818359375
tensor(11368.8838, grad_fn=<NegBackward0>) tensor(11368.8818, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11368.8798828125
tensor(11368.8818, grad_fn=<NegBackward0>) tensor(11368.8799, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11368.87890625
tensor(11368.8799, grad_fn=<NegBackward0>) tensor(11368.8789, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11368.876953125
tensor(11368.8789, grad_fn=<NegBackward0>) tensor(11368.8770, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11368.876953125
tensor(11368.8770, grad_fn=<NegBackward0>) tensor(11368.8770, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11368.8759765625
tensor(11368.8770, grad_fn=<NegBackward0>) tensor(11368.8760, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11368.875
tensor(11368.8760, grad_fn=<NegBackward0>) tensor(11368.8750, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11368.8740234375
tensor(11368.8750, grad_fn=<NegBackward0>) tensor(11368.8740, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11368.873046875
tensor(11368.8740, grad_fn=<NegBackward0>) tensor(11368.8730, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11368.873046875
tensor(11368.8730, grad_fn=<NegBackward0>) tensor(11368.8730, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11368.873046875
tensor(11368.8730, grad_fn=<NegBackward0>) tensor(11368.8730, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11368.87109375
tensor(11368.8730, grad_fn=<NegBackward0>) tensor(11368.8711, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11368.873046875
tensor(11368.8711, grad_fn=<NegBackward0>) tensor(11368.8730, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11368.869140625
tensor(11368.8711, grad_fn=<NegBackward0>) tensor(11368.8691, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11368.8701171875
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8701, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11368.8701171875
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8701, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -11368.869140625
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11368.8701171875
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8701, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11368.869140625
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8691, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11368.869140625
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8691, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11368.869140625
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8691, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11368.8681640625
tensor(11368.8691, grad_fn=<NegBackward0>) tensor(11368.8682, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11368.8681640625
tensor(11368.8682, grad_fn=<NegBackward0>) tensor(11368.8682, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11368.8671875
tensor(11368.8682, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11368.8671875
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11368.8671875
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11368.869140625
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8691, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11368.8681640625
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8682, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11368.8671875
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11368.8671875
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11368.8671875
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11368.8662109375
tensor(11368.8672, grad_fn=<NegBackward0>) tensor(11368.8662, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11368.8671875
tensor(11368.8662, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11368.8671875
tensor(11368.8662, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11368.8671875
tensor(11368.8662, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11368.865234375
tensor(11368.8662, grad_fn=<NegBackward0>) tensor(11368.8652, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11368.8671875
tensor(11368.8652, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11368.8662109375
tensor(11368.8652, grad_fn=<NegBackward0>) tensor(11368.8662, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11368.8671875
tensor(11368.8652, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11368.8662109375
tensor(11368.8652, grad_fn=<NegBackward0>) tensor(11368.8662, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11368.8671875
tensor(11368.8652, grad_fn=<NegBackward0>) tensor(11368.8672, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.7722, 0.2278],
        [0.2130, 0.7870]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3969, 0.6031], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4014, 0.1037],
         [0.6967, 0.2013]],

        [[0.5584, 0.0911],
         [0.5819, 0.5649]],

        [[0.6451, 0.1140],
         [0.5613, 0.7036]],

        [[0.5496, 0.1015],
         [0.5585, 0.6047]],

        [[0.5848, 0.0922],
         [0.7222, 0.5499]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.991999181468012
Average Adjusted Rand Index: 0.9919945110819786
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21679.43359375
inf tensor(21679.4336, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12053.0380859375
tensor(21679.4336, grad_fn=<NegBackward0>) tensor(12053.0381, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12040.615234375
tensor(12053.0381, grad_fn=<NegBackward0>) tensor(12040.6152, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11975.1982421875
tensor(12040.6152, grad_fn=<NegBackward0>) tensor(11975.1982, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11643.654296875
tensor(11975.1982, grad_fn=<NegBackward0>) tensor(11643.6543, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11642.904296875
tensor(11643.6543, grad_fn=<NegBackward0>) tensor(11642.9043, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11642.7021484375
tensor(11642.9043, grad_fn=<NegBackward0>) tensor(11642.7021, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11642.61328125
tensor(11642.7021, grad_fn=<NegBackward0>) tensor(11642.6133, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11642.5615234375
tensor(11642.6133, grad_fn=<NegBackward0>) tensor(11642.5615, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11642.525390625
tensor(11642.5615, grad_fn=<NegBackward0>) tensor(11642.5254, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11642.4990234375
tensor(11642.5254, grad_fn=<NegBackward0>) tensor(11642.4990, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11642.48046875
tensor(11642.4990, grad_fn=<NegBackward0>) tensor(11642.4805, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11642.4638671875
tensor(11642.4805, grad_fn=<NegBackward0>) tensor(11642.4639, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11642.4521484375
tensor(11642.4639, grad_fn=<NegBackward0>) tensor(11642.4521, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11642.4443359375
tensor(11642.4521, grad_fn=<NegBackward0>) tensor(11642.4443, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11642.435546875
tensor(11642.4443, grad_fn=<NegBackward0>) tensor(11642.4355, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11642.4287109375
tensor(11642.4355, grad_fn=<NegBackward0>) tensor(11642.4287, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11642.423828125
tensor(11642.4287, grad_fn=<NegBackward0>) tensor(11642.4238, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11642.4169921875
tensor(11642.4238, grad_fn=<NegBackward0>) tensor(11642.4170, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11642.38671875
tensor(11642.4170, grad_fn=<NegBackward0>) tensor(11642.3867, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11642.376953125
tensor(11642.3867, grad_fn=<NegBackward0>) tensor(11642.3770, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11642.375
tensor(11642.3770, grad_fn=<NegBackward0>) tensor(11642.3750, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11642.37109375
tensor(11642.3750, grad_fn=<NegBackward0>) tensor(11642.3711, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11642.37109375
tensor(11642.3711, grad_fn=<NegBackward0>) tensor(11642.3711, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11642.3671875
tensor(11642.3711, grad_fn=<NegBackward0>) tensor(11642.3672, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11642.3662109375
tensor(11642.3672, grad_fn=<NegBackward0>) tensor(11642.3662, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11642.365234375
tensor(11642.3662, grad_fn=<NegBackward0>) tensor(11642.3652, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11642.3623046875
tensor(11642.3652, grad_fn=<NegBackward0>) tensor(11642.3623, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11642.359375
tensor(11642.3623, grad_fn=<NegBackward0>) tensor(11642.3594, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11640.6162109375
tensor(11642.3594, grad_fn=<NegBackward0>) tensor(11640.6162, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11640.5458984375
tensor(11640.6162, grad_fn=<NegBackward0>) tensor(11640.5459, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11640.54296875
tensor(11640.5459, grad_fn=<NegBackward0>) tensor(11640.5430, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11640.5419921875
tensor(11640.5430, grad_fn=<NegBackward0>) tensor(11640.5420, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11640.5400390625
tensor(11640.5420, grad_fn=<NegBackward0>) tensor(11640.5400, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11640.541015625
tensor(11640.5400, grad_fn=<NegBackward0>) tensor(11640.5410, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11640.541015625
tensor(11640.5400, grad_fn=<NegBackward0>) tensor(11640.5410, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -11640.5390625
tensor(11640.5400, grad_fn=<NegBackward0>) tensor(11640.5391, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11640.548828125
tensor(11640.5391, grad_fn=<NegBackward0>) tensor(11640.5488, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11640.5380859375
tensor(11640.5391, grad_fn=<NegBackward0>) tensor(11640.5381, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11640.5380859375
tensor(11640.5381, grad_fn=<NegBackward0>) tensor(11640.5381, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11640.53515625
tensor(11640.5381, grad_fn=<NegBackward0>) tensor(11640.5352, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11640.5361328125
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5361, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11640.53515625
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5352, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11640.5361328125
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5361, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11640.53515625
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5352, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11640.5380859375
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5381, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11640.5390625
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5391, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11640.5380859375
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5381, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -11640.533203125
tensor(11640.5352, grad_fn=<NegBackward0>) tensor(11640.5332, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11640.533203125
tensor(11640.5332, grad_fn=<NegBackward0>) tensor(11640.5332, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11640.5458984375
tensor(11640.5332, grad_fn=<NegBackward0>) tensor(11640.5459, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11640.5322265625
tensor(11640.5332, grad_fn=<NegBackward0>) tensor(11640.5322, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11640.53125
tensor(11640.5322, grad_fn=<NegBackward0>) tensor(11640.5312, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11640.529296875
tensor(11640.5312, grad_fn=<NegBackward0>) tensor(11640.5293, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11640.529296875
tensor(11640.5293, grad_fn=<NegBackward0>) tensor(11640.5293, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11640.5283203125
tensor(11640.5293, grad_fn=<NegBackward0>) tensor(11640.5283, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11640.529296875
tensor(11640.5283, grad_fn=<NegBackward0>) tensor(11640.5293, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11640.5400390625
tensor(11640.5283, grad_fn=<NegBackward0>) tensor(11640.5400, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11640.5283203125
tensor(11640.5283, grad_fn=<NegBackward0>) tensor(11640.5283, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11640.5341796875
tensor(11640.5283, grad_fn=<NegBackward0>) tensor(11640.5342, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11640.52734375
tensor(11640.5283, grad_fn=<NegBackward0>) tensor(11640.5273, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11640.5263671875
tensor(11640.5273, grad_fn=<NegBackward0>) tensor(11640.5264, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11640.52734375
tensor(11640.5264, grad_fn=<NegBackward0>) tensor(11640.5273, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11640.52734375
tensor(11640.5264, grad_fn=<NegBackward0>) tensor(11640.5273, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11640.52734375
tensor(11640.5264, grad_fn=<NegBackward0>) tensor(11640.5273, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11640.52734375
tensor(11640.5264, grad_fn=<NegBackward0>) tensor(11640.5273, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11640.52734375
tensor(11640.5264, grad_fn=<NegBackward0>) tensor(11640.5273, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.6224, 0.3776],
        [0.6108, 0.3892]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9733, 0.0267], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2064, 0.0691],
         [0.6381, 0.4036]],

        [[0.5610, 0.0921],
         [0.5402, 0.5930]],

        [[0.6404, 0.1141],
         [0.5647, 0.5825]],

        [[0.5924, 0.0979],
         [0.5827, 0.7232]],

        [[0.6612, 0.0922],
         [0.7284, 0.6150]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: -0.012494332208171696
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 32
Adjusted Rand Index: 0.12402048831758941
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.3023050414487731
Average Adjusted Rand Index: 0.6062997423038621
[0.991999181468012, 0.3023050414487731] [0.9919945110819786, 0.6062997423038621] [11368.8671875, 11640.52734375]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11518.173416400363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21268.875
inf tensor(21268.8750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12253.75390625
tensor(21268.8750, grad_fn=<NegBackward0>) tensor(12253.7539, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12123.70703125
tensor(12253.7539, grad_fn=<NegBackward0>) tensor(12123.7070, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11574.9853515625
tensor(12123.7070, grad_fn=<NegBackward0>) tensor(11574.9854, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11542.916015625
tensor(11574.9854, grad_fn=<NegBackward0>) tensor(11542.9160, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11533.9560546875
tensor(11542.9160, grad_fn=<NegBackward0>) tensor(11533.9561, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11533.5498046875
tensor(11533.9561, grad_fn=<NegBackward0>) tensor(11533.5498, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11531.609375
tensor(11533.5498, grad_fn=<NegBackward0>) tensor(11531.6094, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11531.400390625
tensor(11531.6094, grad_fn=<NegBackward0>) tensor(11531.4004, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11519.955078125
tensor(11531.4004, grad_fn=<NegBackward0>) tensor(11519.9551, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11519.1171875
tensor(11519.9551, grad_fn=<NegBackward0>) tensor(11519.1172, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11516.2783203125
tensor(11519.1172, grad_fn=<NegBackward0>) tensor(11516.2783, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11516.232421875
tensor(11516.2783, grad_fn=<NegBackward0>) tensor(11516.2324, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11516.193359375
tensor(11516.2324, grad_fn=<NegBackward0>) tensor(11516.1934, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11516.1669921875
tensor(11516.1934, grad_fn=<NegBackward0>) tensor(11516.1670, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11516.14453125
tensor(11516.1670, grad_fn=<NegBackward0>) tensor(11516.1445, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11516.126953125
tensor(11516.1445, grad_fn=<NegBackward0>) tensor(11516.1270, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11516.11328125
tensor(11516.1270, grad_fn=<NegBackward0>) tensor(11516.1133, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11516.1015625
tensor(11516.1133, grad_fn=<NegBackward0>) tensor(11516.1016, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11516.0966796875
tensor(11516.1016, grad_fn=<NegBackward0>) tensor(11516.0967, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11516.0810546875
tensor(11516.0967, grad_fn=<NegBackward0>) tensor(11516.0811, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11516.0732421875
tensor(11516.0811, grad_fn=<NegBackward0>) tensor(11516.0732, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11516.06640625
tensor(11516.0732, grad_fn=<NegBackward0>) tensor(11516.0664, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11516.060546875
tensor(11516.0664, grad_fn=<NegBackward0>) tensor(11516.0605, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11516.0556640625
tensor(11516.0605, grad_fn=<NegBackward0>) tensor(11516.0557, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11516.0517578125
tensor(11516.0557, grad_fn=<NegBackward0>) tensor(11516.0518, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11516.046875
tensor(11516.0518, grad_fn=<NegBackward0>) tensor(11516.0469, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11516.0419921875
tensor(11516.0469, grad_fn=<NegBackward0>) tensor(11516.0420, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11516.025390625
tensor(11516.0420, grad_fn=<NegBackward0>) tensor(11516.0254, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11508.517578125
tensor(11516.0254, grad_fn=<NegBackward0>) tensor(11508.5176, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11508.513671875
tensor(11508.5176, grad_fn=<NegBackward0>) tensor(11508.5137, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11508.5107421875
tensor(11508.5137, grad_fn=<NegBackward0>) tensor(11508.5107, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11508.5078125
tensor(11508.5107, grad_fn=<NegBackward0>) tensor(11508.5078, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11508.4833984375
tensor(11508.5078, grad_fn=<NegBackward0>) tensor(11508.4834, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11507.119140625
tensor(11508.4834, grad_fn=<NegBackward0>) tensor(11507.1191, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11507.11328125
tensor(11507.1191, grad_fn=<NegBackward0>) tensor(11507.1133, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11507.111328125
tensor(11507.1133, grad_fn=<NegBackward0>) tensor(11507.1113, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11507.1142578125
tensor(11507.1113, grad_fn=<NegBackward0>) tensor(11507.1143, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11507.107421875
tensor(11507.1113, grad_fn=<NegBackward0>) tensor(11507.1074, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11507.107421875
tensor(11507.1074, grad_fn=<NegBackward0>) tensor(11507.1074, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11507.10546875
tensor(11507.1074, grad_fn=<NegBackward0>) tensor(11507.1055, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11507.1044921875
tensor(11507.1055, grad_fn=<NegBackward0>) tensor(11507.1045, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11507.1044921875
tensor(11507.1045, grad_fn=<NegBackward0>) tensor(11507.1045, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11507.1025390625
tensor(11507.1045, grad_fn=<NegBackward0>) tensor(11507.1025, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11507.1015625
tensor(11507.1025, grad_fn=<NegBackward0>) tensor(11507.1016, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11507.1015625
tensor(11507.1016, grad_fn=<NegBackward0>) tensor(11507.1016, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11507.099609375
tensor(11507.1016, grad_fn=<NegBackward0>) tensor(11507.0996, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11507.1005859375
tensor(11507.0996, grad_fn=<NegBackward0>) tensor(11507.1006, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11507.09765625
tensor(11507.0996, grad_fn=<NegBackward0>) tensor(11507.0977, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11507.0908203125
tensor(11507.0977, grad_fn=<NegBackward0>) tensor(11507.0908, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11507.0791015625
tensor(11507.0908, grad_fn=<NegBackward0>) tensor(11507.0791, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11507.0732421875
tensor(11507.0791, grad_fn=<NegBackward0>) tensor(11507.0732, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11507.072265625
tensor(11507.0732, grad_fn=<NegBackward0>) tensor(11507.0723, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11507.0732421875
tensor(11507.0723, grad_fn=<NegBackward0>) tensor(11507.0732, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11507.072265625
tensor(11507.0723, grad_fn=<NegBackward0>) tensor(11507.0723, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11507.0693359375
tensor(11507.0723, grad_fn=<NegBackward0>) tensor(11507.0693, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11507.0712890625
tensor(11507.0693, grad_fn=<NegBackward0>) tensor(11507.0713, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11507.072265625
tensor(11507.0693, grad_fn=<NegBackward0>) tensor(11507.0723, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11507.0693359375
tensor(11507.0693, grad_fn=<NegBackward0>) tensor(11507.0693, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11507.0693359375
tensor(11507.0693, grad_fn=<NegBackward0>) tensor(11507.0693, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11507.0712890625
tensor(11507.0693, grad_fn=<NegBackward0>) tensor(11507.0713, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11507.0712890625
tensor(11507.0693, grad_fn=<NegBackward0>) tensor(11507.0713, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11507.068359375
tensor(11507.0693, grad_fn=<NegBackward0>) tensor(11507.0684, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11507.068359375
tensor(11507.0684, grad_fn=<NegBackward0>) tensor(11507.0684, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11507.0673828125
tensor(11507.0684, grad_fn=<NegBackward0>) tensor(11507.0674, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11507.0693359375
tensor(11507.0674, grad_fn=<NegBackward0>) tensor(11507.0693, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11507.06640625
tensor(11507.0674, grad_fn=<NegBackward0>) tensor(11507.0664, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11507.0654296875
tensor(11507.0664, grad_fn=<NegBackward0>) tensor(11507.0654, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11507.0673828125
tensor(11507.0654, grad_fn=<NegBackward0>) tensor(11507.0674, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11507.06640625
tensor(11507.0654, grad_fn=<NegBackward0>) tensor(11507.0664, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11507.06640625
tensor(11507.0654, grad_fn=<NegBackward0>) tensor(11507.0664, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11507.06640625
tensor(11507.0654, grad_fn=<NegBackward0>) tensor(11507.0664, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11507.0625
tensor(11507.0654, grad_fn=<NegBackward0>) tensor(11507.0625, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11507.0615234375
tensor(11507.0625, grad_fn=<NegBackward0>) tensor(11507.0615, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11507.0595703125
tensor(11507.0615, grad_fn=<NegBackward0>) tensor(11507.0596, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11507.060546875
tensor(11507.0596, grad_fn=<NegBackward0>) tensor(11507.0605, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11507.111328125
tensor(11507.0596, grad_fn=<NegBackward0>) tensor(11507.1113, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11507.064453125
tensor(11507.0596, grad_fn=<NegBackward0>) tensor(11507.0645, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11507.0595703125
tensor(11507.0596, grad_fn=<NegBackward0>) tensor(11507.0596, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11507.0595703125
tensor(11507.0596, grad_fn=<NegBackward0>) tensor(11507.0596, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11507.05859375
tensor(11507.0596, grad_fn=<NegBackward0>) tensor(11507.0586, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11507.0595703125
tensor(11507.0586, grad_fn=<NegBackward0>) tensor(11507.0596, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11507.068359375
tensor(11507.0586, grad_fn=<NegBackward0>) tensor(11507.0684, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11507.0595703125
tensor(11507.0586, grad_fn=<NegBackward0>) tensor(11507.0596, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11507.0595703125
tensor(11507.0586, grad_fn=<NegBackward0>) tensor(11507.0596, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11507.060546875
tensor(11507.0586, grad_fn=<NegBackward0>) tensor(11507.0605, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7502, 0.2498],
        [0.2881, 0.7119]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4754, 0.5246], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2019, 0.0978],
         [0.5390, 0.3931]],

        [[0.7001, 0.1026],
         [0.5847, 0.5654]],

        [[0.6265, 0.0975],
         [0.6901, 0.7071]],

        [[0.7058, 0.1045],
         [0.5483, 0.5805]],

        [[0.5159, 0.0921],
         [0.5890, 0.5539]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9840320243133913
Average Adjusted Rand Index: 0.9839989969312853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21319.013671875
inf tensor(21319.0137, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12260.341796875
tensor(21319.0137, grad_fn=<NegBackward0>) tensor(12260.3418, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12237.4326171875
tensor(12260.3418, grad_fn=<NegBackward0>) tensor(12237.4326, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11946.32421875
tensor(12237.4326, grad_fn=<NegBackward0>) tensor(11946.3242, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11722.64453125
tensor(11946.3242, grad_fn=<NegBackward0>) tensor(11722.6445, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11707.9423828125
tensor(11722.6445, grad_fn=<NegBackward0>) tensor(11707.9424, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11689.205078125
tensor(11707.9424, grad_fn=<NegBackward0>) tensor(11689.2051, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11688.5458984375
tensor(11689.2051, grad_fn=<NegBackward0>) tensor(11688.5459, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11688.33203125
tensor(11688.5459, grad_fn=<NegBackward0>) tensor(11688.3320, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11688.2109375
tensor(11688.3320, grad_fn=<NegBackward0>) tensor(11688.2109, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11688.126953125
tensor(11688.2109, grad_fn=<NegBackward0>) tensor(11688.1270, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11688.0615234375
tensor(11688.1270, grad_fn=<NegBackward0>) tensor(11688.0615, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11688.00390625
tensor(11688.0615, grad_fn=<NegBackward0>) tensor(11688.0039, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11686.7041015625
tensor(11688.0039, grad_fn=<NegBackward0>) tensor(11686.7041, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11677.6416015625
tensor(11686.7041, grad_fn=<NegBackward0>) tensor(11677.6416, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11677.53515625
tensor(11677.6416, grad_fn=<NegBackward0>) tensor(11677.5352, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11676.3974609375
tensor(11677.5352, grad_fn=<NegBackward0>) tensor(11676.3975, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11676.294921875
tensor(11676.3975, grad_fn=<NegBackward0>) tensor(11676.2949, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11676.1689453125
tensor(11676.2949, grad_fn=<NegBackward0>) tensor(11676.1689, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11676.0947265625
tensor(11676.1689, grad_fn=<NegBackward0>) tensor(11676.0947, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11676.08203125
tensor(11676.0947, grad_fn=<NegBackward0>) tensor(11676.0820, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11676.0703125
tensor(11676.0820, grad_fn=<NegBackward0>) tensor(11676.0703, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11676.060546875
tensor(11676.0703, grad_fn=<NegBackward0>) tensor(11676.0605, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11676.0517578125
tensor(11676.0605, grad_fn=<NegBackward0>) tensor(11676.0518, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11675.0380859375
tensor(11676.0518, grad_fn=<NegBackward0>) tensor(11675.0381, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11669.2568359375
tensor(11675.0381, grad_fn=<NegBackward0>) tensor(11669.2568, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11668.8154296875
tensor(11669.2568, grad_fn=<NegBackward0>) tensor(11668.8154, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11668.353515625
tensor(11668.8154, grad_fn=<NegBackward0>) tensor(11668.3535, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11668.318359375
tensor(11668.3535, grad_fn=<NegBackward0>) tensor(11668.3184, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11668.3125
tensor(11668.3184, grad_fn=<NegBackward0>) tensor(11668.3125, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11668.30859375
tensor(11668.3125, grad_fn=<NegBackward0>) tensor(11668.3086, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11668.302734375
tensor(11668.3086, grad_fn=<NegBackward0>) tensor(11668.3027, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11668.2626953125
tensor(11668.3027, grad_fn=<NegBackward0>) tensor(11668.2627, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11668.259765625
tensor(11668.2627, grad_fn=<NegBackward0>) tensor(11668.2598, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11668.2578125
tensor(11668.2598, grad_fn=<NegBackward0>) tensor(11668.2578, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11668.255859375
tensor(11668.2578, grad_fn=<NegBackward0>) tensor(11668.2559, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11668.2529296875
tensor(11668.2559, grad_fn=<NegBackward0>) tensor(11668.2529, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11668.2509765625
tensor(11668.2529, grad_fn=<NegBackward0>) tensor(11668.2510, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11668.2470703125
tensor(11668.2510, grad_fn=<NegBackward0>) tensor(11668.2471, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11663.3125
tensor(11668.2471, grad_fn=<NegBackward0>) tensor(11663.3125, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11663.25
tensor(11663.3125, grad_fn=<NegBackward0>) tensor(11663.2500, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11661.75
tensor(11663.2500, grad_fn=<NegBackward0>) tensor(11661.7500, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11661.708984375
tensor(11661.7500, grad_fn=<NegBackward0>) tensor(11661.7090, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11661.693359375
tensor(11661.7090, grad_fn=<NegBackward0>) tensor(11661.6934, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11661.689453125
tensor(11661.6934, grad_fn=<NegBackward0>) tensor(11661.6895, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11661.6953125
tensor(11661.6895, grad_fn=<NegBackward0>) tensor(11661.6953, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11661.689453125
tensor(11661.6895, grad_fn=<NegBackward0>) tensor(11661.6895, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11661.6875
tensor(11661.6895, grad_fn=<NegBackward0>) tensor(11661.6875, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11661.693359375
tensor(11661.6875, grad_fn=<NegBackward0>) tensor(11661.6934, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11661.68359375
tensor(11661.6875, grad_fn=<NegBackward0>) tensor(11661.6836, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11661.68359375
tensor(11661.6836, grad_fn=<NegBackward0>) tensor(11661.6836, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11661.6708984375
tensor(11661.6836, grad_fn=<NegBackward0>) tensor(11661.6709, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11658.693359375
tensor(11661.6709, grad_fn=<NegBackward0>) tensor(11658.6934, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11658.6943359375
tensor(11658.6934, grad_fn=<NegBackward0>) tensor(11658.6943, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11658.6923828125
tensor(11658.6934, grad_fn=<NegBackward0>) tensor(11658.6924, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11658.7060546875
tensor(11658.6924, grad_fn=<NegBackward0>) tensor(11658.7061, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11658.689453125
tensor(11658.6924, grad_fn=<NegBackward0>) tensor(11658.6895, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11658.6884765625
tensor(11658.6895, grad_fn=<NegBackward0>) tensor(11658.6885, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11658.6865234375
tensor(11658.6885, grad_fn=<NegBackward0>) tensor(11658.6865, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11658.693359375
tensor(11658.6865, grad_fn=<NegBackward0>) tensor(11658.6934, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11658.6865234375
tensor(11658.6865, grad_fn=<NegBackward0>) tensor(11658.6865, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11658.685546875
tensor(11658.6865, grad_fn=<NegBackward0>) tensor(11658.6855, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11658.685546875
tensor(11658.6855, grad_fn=<NegBackward0>) tensor(11658.6855, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11658.6904296875
tensor(11658.6855, grad_fn=<NegBackward0>) tensor(11658.6904, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11658.685546875
tensor(11658.6855, grad_fn=<NegBackward0>) tensor(11658.6855, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11658.6875
tensor(11658.6855, grad_fn=<NegBackward0>) tensor(11658.6875, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11658.6845703125
tensor(11658.6855, grad_fn=<NegBackward0>) tensor(11658.6846, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11658.70703125
tensor(11658.6846, grad_fn=<NegBackward0>) tensor(11658.7070, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11658.689453125
tensor(11658.6846, grad_fn=<NegBackward0>) tensor(11658.6895, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11658.68359375
tensor(11658.6846, grad_fn=<NegBackward0>) tensor(11658.6836, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11658.68359375
tensor(11658.6836, grad_fn=<NegBackward0>) tensor(11658.6836, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11658.681640625
tensor(11658.6836, grad_fn=<NegBackward0>) tensor(11658.6816, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11658.68359375
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.6836, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11658.681640625
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.6816, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11658.681640625
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.6816, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11658.681640625
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.6816, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11658.681640625
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.6816, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11658.6826171875
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.6826, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11658.7041015625
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.7041, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11658.6806640625
tensor(11658.6816, grad_fn=<NegBackward0>) tensor(11658.6807, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11658.6806640625
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6807, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11658.6806640625
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6807, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11658.681640625
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6816, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11658.6806640625
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6807, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11658.681640625
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6816, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11658.6806640625
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6807, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11658.685546875
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6855, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11658.6806640625
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6807, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11658.70703125
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.7070, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11658.6787109375
tensor(11658.6807, grad_fn=<NegBackward0>) tensor(11658.6787, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11658.70703125
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.7070, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11658.6796875
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6797, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11658.6865234375
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6865, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11658.6787109375
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6787, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11658.7373046875
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.7373, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11658.6796875
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6797, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11658.6787109375
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6787, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11658.6796875
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6797, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11658.6787109375
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6787, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11658.6787109375
tensor(11658.6787, grad_fn=<NegBackward0>) tensor(11658.6787, grad_fn=<NegBackward0>)
pi: tensor([[0.4680, 0.5320],
        [0.4136, 0.5864]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5239, 0.4761], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3843, 0.0978],
         [0.6266, 0.2194]],

        [[0.7171, 0.1019],
         [0.5939, 0.5267]],

        [[0.6407, 0.0959],
         [0.6984, 0.7191]],

        [[0.5639, 0.1045],
         [0.5795, 0.6014]],

        [[0.5686, 0.0918],
         [0.6032, 0.5480]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 74
Adjusted Rand Index: 0.2236482808306714
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.4833880953179123
Average Adjusted Rand Index: 0.8287286530974196
[0.9840320243133913, 0.4833880953179123] [0.9839989969312853, 0.8287286530974196] [11507.060546875, 11658.6796875]
-------------------------------------
This iteration is 8
True Objective function: Loss = -11795.5009753145
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22797.59765625
inf tensor(22797.5977, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12822.9619140625
tensor(22797.5977, grad_fn=<NegBackward0>) tensor(12822.9619, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12380.0546875
tensor(12822.9619, grad_fn=<NegBackward0>) tensor(12380.0547, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12157.8642578125
tensor(12380.0547, grad_fn=<NegBackward0>) tensor(12157.8643, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12015.9365234375
tensor(12157.8643, grad_fn=<NegBackward0>) tensor(12015.9365, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12008.021484375
tensor(12015.9365, grad_fn=<NegBackward0>) tensor(12008.0215, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11991.990234375
tensor(12008.0215, grad_fn=<NegBackward0>) tensor(11991.9902, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11976.07421875
tensor(11991.9902, grad_fn=<NegBackward0>) tensor(11976.0742, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11975.8359375
tensor(11976.0742, grad_fn=<NegBackward0>) tensor(11975.8359, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11969.2744140625
tensor(11975.8359, grad_fn=<NegBackward0>) tensor(11969.2744, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11969.2158203125
tensor(11969.2744, grad_fn=<NegBackward0>) tensor(11969.2158, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11969.169921875
tensor(11969.2158, grad_fn=<NegBackward0>) tensor(11969.1699, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11969.1376953125
tensor(11969.1699, grad_fn=<NegBackward0>) tensor(11969.1377, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11969.1123046875
tensor(11969.1377, grad_fn=<NegBackward0>) tensor(11969.1123, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11969.08984375
tensor(11969.1123, grad_fn=<NegBackward0>) tensor(11969.0898, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11969.0771484375
tensor(11969.0898, grad_fn=<NegBackward0>) tensor(11969.0771, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11969.05859375
tensor(11969.0771, grad_fn=<NegBackward0>) tensor(11969.0586, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11969.0458984375
tensor(11969.0586, grad_fn=<NegBackward0>) tensor(11969.0459, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11969.0341796875
tensor(11969.0459, grad_fn=<NegBackward0>) tensor(11969.0342, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11969.0185546875
tensor(11969.0342, grad_fn=<NegBackward0>) tensor(11969.0186, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11969.0068359375
tensor(11969.0186, grad_fn=<NegBackward0>) tensor(11969.0068, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11968.9990234375
tensor(11969.0068, grad_fn=<NegBackward0>) tensor(11968.9990, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11968.9912109375
tensor(11968.9990, grad_fn=<NegBackward0>) tensor(11968.9912, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11968.9853515625
tensor(11968.9912, grad_fn=<NegBackward0>) tensor(11968.9854, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11968.982421875
tensor(11968.9854, grad_fn=<NegBackward0>) tensor(11968.9824, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11968.9765625
tensor(11968.9824, grad_fn=<NegBackward0>) tensor(11968.9766, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11968.97265625
tensor(11968.9766, grad_fn=<NegBackward0>) tensor(11968.9727, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11968.96875
tensor(11968.9727, grad_fn=<NegBackward0>) tensor(11968.9688, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11968.966796875
tensor(11968.9688, grad_fn=<NegBackward0>) tensor(11968.9668, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11968.9755859375
tensor(11968.9668, grad_fn=<NegBackward0>) tensor(11968.9756, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11968.962890625
tensor(11968.9668, grad_fn=<NegBackward0>) tensor(11968.9629, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11968.95703125
tensor(11968.9629, grad_fn=<NegBackward0>) tensor(11968.9570, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11968.9560546875
tensor(11968.9570, grad_fn=<NegBackward0>) tensor(11968.9561, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11968.955078125
tensor(11968.9561, grad_fn=<NegBackward0>) tensor(11968.9551, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11968.953125
tensor(11968.9551, grad_fn=<NegBackward0>) tensor(11968.9531, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11968.953125
tensor(11968.9531, grad_fn=<NegBackward0>) tensor(11968.9531, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11968.9501953125
tensor(11968.9531, grad_fn=<NegBackward0>) tensor(11968.9502, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11968.9482421875
tensor(11968.9502, grad_fn=<NegBackward0>) tensor(11968.9482, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11968.9482421875
tensor(11968.9482, grad_fn=<NegBackward0>) tensor(11968.9482, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11968.9501953125
tensor(11968.9482, grad_fn=<NegBackward0>) tensor(11968.9502, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11968.9453125
tensor(11968.9482, grad_fn=<NegBackward0>) tensor(11968.9453, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11968.943359375
tensor(11968.9453, grad_fn=<NegBackward0>) tensor(11968.9434, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11968.9443359375
tensor(11968.9434, grad_fn=<NegBackward0>) tensor(11968.9443, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11968.9423828125
tensor(11968.9434, grad_fn=<NegBackward0>) tensor(11968.9424, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11968.9423828125
tensor(11968.9424, grad_fn=<NegBackward0>) tensor(11968.9424, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11968.94921875
tensor(11968.9424, grad_fn=<NegBackward0>) tensor(11968.9492, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11968.939453125
tensor(11968.9424, grad_fn=<NegBackward0>) tensor(11968.9395, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11968.9423828125
tensor(11968.9395, grad_fn=<NegBackward0>) tensor(11968.9424, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11968.9384765625
tensor(11968.9395, grad_fn=<NegBackward0>) tensor(11968.9385, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11968.94140625
tensor(11968.9385, grad_fn=<NegBackward0>) tensor(11968.9414, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11968.9462890625
tensor(11968.9385, grad_fn=<NegBackward0>) tensor(11968.9463, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11968.9365234375
tensor(11968.9385, grad_fn=<NegBackward0>) tensor(11968.9365, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11968.9375
tensor(11968.9365, grad_fn=<NegBackward0>) tensor(11968.9375, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11968.9365234375
tensor(11968.9365, grad_fn=<NegBackward0>) tensor(11968.9365, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11968.939453125
tensor(11968.9365, grad_fn=<NegBackward0>) tensor(11968.9395, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11968.935546875
tensor(11968.9365, grad_fn=<NegBackward0>) tensor(11968.9355, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11968.935546875
tensor(11968.9355, grad_fn=<NegBackward0>) tensor(11968.9355, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11968.9365234375
tensor(11968.9355, grad_fn=<NegBackward0>) tensor(11968.9365, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11968.93359375
tensor(11968.9355, grad_fn=<NegBackward0>) tensor(11968.9336, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11968.9326171875
tensor(11968.9336, grad_fn=<NegBackward0>) tensor(11968.9326, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11968.9345703125
tensor(11968.9326, grad_fn=<NegBackward0>) tensor(11968.9346, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11968.9326171875
tensor(11968.9326, grad_fn=<NegBackward0>) tensor(11968.9326, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11968.9345703125
tensor(11968.9326, grad_fn=<NegBackward0>) tensor(11968.9346, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11968.9345703125
tensor(11968.9326, grad_fn=<NegBackward0>) tensor(11968.9346, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11968.93359375
tensor(11968.9326, grad_fn=<NegBackward0>) tensor(11968.9336, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11968.93359375
tensor(11968.9326, grad_fn=<NegBackward0>) tensor(11968.9336, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11968.9423828125
tensor(11968.9326, grad_fn=<NegBackward0>) tensor(11968.9424, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.6516, 0.3484],
        [0.2314, 0.7686]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5598, 0.4402], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4042, 0.0979],
         [0.5242, 0.2307]],

        [[0.5303, 0.1019],
         [0.6819, 0.6804]],

        [[0.6102, 0.0842],
         [0.5999, 0.6609]],

        [[0.6171, 0.0967],
         [0.5898, 0.6605]],

        [[0.6179, 0.1010],
         [0.7280, 0.6381]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 76
Adjusted Rand Index: 0.26187050359712233
Global Adjusted Rand Index: 0.4778296251827597
Average Adjusted Rand Index: 0.8443736618829876
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22077.822265625
inf tensor(22077.8223, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12286.541015625
tensor(22077.8223, grad_fn=<NegBackward0>) tensor(12286.5410, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12179.5615234375
tensor(12286.5410, grad_fn=<NegBackward0>) tensor(12179.5615, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12175.78125
tensor(12179.5615, grad_fn=<NegBackward0>) tensor(12175.7812, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12170.9091796875
tensor(12175.7812, grad_fn=<NegBackward0>) tensor(12170.9092, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12024.9345703125
tensor(12170.9092, grad_fn=<NegBackward0>) tensor(12024.9346, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11966.28125
tensor(12024.9346, grad_fn=<NegBackward0>) tensor(11966.2812, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11966.0576171875
tensor(11966.2812, grad_fn=<NegBackward0>) tensor(11966.0576, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11965.943359375
tensor(11966.0576, grad_fn=<NegBackward0>) tensor(11965.9434, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11965.865234375
tensor(11965.9434, grad_fn=<NegBackward0>) tensor(11965.8652, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11965.822265625
tensor(11965.8652, grad_fn=<NegBackward0>) tensor(11965.8223, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11965.791015625
tensor(11965.8223, grad_fn=<NegBackward0>) tensor(11965.7910, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11965.767578125
tensor(11965.7910, grad_fn=<NegBackward0>) tensor(11965.7676, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11965.748046875
tensor(11965.7676, grad_fn=<NegBackward0>) tensor(11965.7480, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11965.7333984375
tensor(11965.7480, grad_fn=<NegBackward0>) tensor(11965.7334, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11965.7216796875
tensor(11965.7334, grad_fn=<NegBackward0>) tensor(11965.7217, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11965.7119140625
tensor(11965.7217, grad_fn=<NegBackward0>) tensor(11965.7119, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11965.7041015625
tensor(11965.7119, grad_fn=<NegBackward0>) tensor(11965.7041, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11965.697265625
tensor(11965.7041, grad_fn=<NegBackward0>) tensor(11965.6973, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11965.6923828125
tensor(11965.6973, grad_fn=<NegBackward0>) tensor(11965.6924, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11965.6865234375
tensor(11965.6924, grad_fn=<NegBackward0>) tensor(11965.6865, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11965.68359375
tensor(11965.6865, grad_fn=<NegBackward0>) tensor(11965.6836, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11965.6796875
tensor(11965.6836, grad_fn=<NegBackward0>) tensor(11965.6797, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11965.67578125
tensor(11965.6797, grad_fn=<NegBackward0>) tensor(11965.6758, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11965.671875
tensor(11965.6758, grad_fn=<NegBackward0>) tensor(11965.6719, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11965.669921875
tensor(11965.6719, grad_fn=<NegBackward0>) tensor(11965.6699, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11965.6748046875
tensor(11965.6699, grad_fn=<NegBackward0>) tensor(11965.6748, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11965.666015625
tensor(11965.6699, grad_fn=<NegBackward0>) tensor(11965.6660, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11965.6650390625
tensor(11965.6660, grad_fn=<NegBackward0>) tensor(11965.6650, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11965.6630859375
tensor(11965.6650, grad_fn=<NegBackward0>) tensor(11965.6631, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11965.66015625
tensor(11965.6631, grad_fn=<NegBackward0>) tensor(11965.6602, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11965.6591796875
tensor(11965.6602, grad_fn=<NegBackward0>) tensor(11965.6592, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11965.6572265625
tensor(11965.6592, grad_fn=<NegBackward0>) tensor(11965.6572, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11965.65625
tensor(11965.6572, grad_fn=<NegBackward0>) tensor(11965.6562, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11965.658203125
tensor(11965.6562, grad_fn=<NegBackward0>) tensor(11965.6582, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11965.6552734375
tensor(11965.6562, grad_fn=<NegBackward0>) tensor(11965.6553, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11965.6533203125
tensor(11965.6553, grad_fn=<NegBackward0>) tensor(11965.6533, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11965.6533203125
tensor(11965.6533, grad_fn=<NegBackward0>) tensor(11965.6533, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11965.6533203125
tensor(11965.6533, grad_fn=<NegBackward0>) tensor(11965.6533, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11965.65234375
tensor(11965.6533, grad_fn=<NegBackward0>) tensor(11965.6523, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11965.650390625
tensor(11965.6523, grad_fn=<NegBackward0>) tensor(11965.6504, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11965.6494140625
tensor(11965.6504, grad_fn=<NegBackward0>) tensor(11965.6494, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11965.6494140625
tensor(11965.6494, grad_fn=<NegBackward0>) tensor(11965.6494, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11965.6474609375
tensor(11965.6494, grad_fn=<NegBackward0>) tensor(11965.6475, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11965.6484375
tensor(11965.6475, grad_fn=<NegBackward0>) tensor(11965.6484, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11965.6494140625
tensor(11965.6475, grad_fn=<NegBackward0>) tensor(11965.6494, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11965.6474609375
tensor(11965.6475, grad_fn=<NegBackward0>) tensor(11965.6475, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11965.666015625
tensor(11965.6475, grad_fn=<NegBackward0>) tensor(11965.6660, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11965.6474609375
tensor(11965.6475, grad_fn=<NegBackward0>) tensor(11965.6475, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11965.646484375
tensor(11965.6475, grad_fn=<NegBackward0>) tensor(11965.6465, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11965.6484375
tensor(11965.6465, grad_fn=<NegBackward0>) tensor(11965.6484, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11965.6455078125
tensor(11965.6465, grad_fn=<NegBackward0>) tensor(11965.6455, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11965.6455078125
tensor(11965.6455, grad_fn=<NegBackward0>) tensor(11965.6455, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11965.6455078125
tensor(11965.6455, grad_fn=<NegBackward0>) tensor(11965.6455, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11965.6455078125
tensor(11965.6455, grad_fn=<NegBackward0>) tensor(11965.6455, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11965.64453125
tensor(11965.6455, grad_fn=<NegBackward0>) tensor(11965.6445, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11965.6455078125
tensor(11965.6445, grad_fn=<NegBackward0>) tensor(11965.6455, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11965.6455078125
tensor(11965.6445, grad_fn=<NegBackward0>) tensor(11965.6455, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11965.6435546875
tensor(11965.6445, grad_fn=<NegBackward0>) tensor(11965.6436, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11965.6435546875
tensor(11965.6436, grad_fn=<NegBackward0>) tensor(11965.6436, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11965.64453125
tensor(11965.6436, grad_fn=<NegBackward0>) tensor(11965.6445, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11965.642578125
tensor(11965.6436, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11965.642578125
tensor(11965.6426, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11965.6435546875
tensor(11965.6426, grad_fn=<NegBackward0>) tensor(11965.6436, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11965.642578125
tensor(11965.6426, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11965.64453125
tensor(11965.6426, grad_fn=<NegBackward0>) tensor(11965.6445, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11965.6416015625
tensor(11965.6426, grad_fn=<NegBackward0>) tensor(11965.6416, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11965.642578125
tensor(11965.6416, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11965.642578125
tensor(11965.6416, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11965.6416015625
tensor(11965.6416, grad_fn=<NegBackward0>) tensor(11965.6416, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11965.640625
tensor(11965.6416, grad_fn=<NegBackward0>) tensor(11965.6406, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11965.6630859375
tensor(11965.6406, grad_fn=<NegBackward0>) tensor(11965.6631, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11965.640625
tensor(11965.6406, grad_fn=<NegBackward0>) tensor(11965.6406, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11965.642578125
tensor(11965.6406, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11965.642578125
tensor(11965.6406, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11965.6591796875
tensor(11965.6406, grad_fn=<NegBackward0>) tensor(11965.6592, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11965.642578125
tensor(11965.6406, grad_fn=<NegBackward0>) tensor(11965.6426, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11965.6474609375
tensor(11965.6406, grad_fn=<NegBackward0>) tensor(11965.6475, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7643, 0.2357],
        [0.3383, 0.6617]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4426, 0.5574], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2326, 0.0979],
         [0.6966, 0.4033]],

        [[0.7099, 0.1019],
         [0.6107, 0.6887]],

        [[0.6975, 0.0842],
         [0.5569, 0.6968]],

        [[0.6229, 0.0967],
         [0.6989, 0.5067]],

        [[0.6401, 0.1008],
         [0.7245, 0.7189]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 22
Adjusted Rand Index: 0.30572038927130313
Global Adjusted Rand Index: 0.4668020673660686
Average Adjusted Rand Index: 0.8531436390178235
[0.4778296251827597, 0.4668020673660686] [0.8443736618829876, 0.8531436390178235] [11968.9423828125, 11965.6474609375]
-------------------------------------
This iteration is 9
True Objective function: Loss = -11619.995723073027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22127.57421875
inf tensor(22127.5742, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12345.2705078125
tensor(22127.5742, grad_fn=<NegBackward0>) tensor(12345.2705, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12247.345703125
tensor(12345.2705, grad_fn=<NegBackward0>) tensor(12247.3457, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11783.2568359375
tensor(12247.3457, grad_fn=<NegBackward0>) tensor(11783.2568, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11729.669921875
tensor(11783.2568, grad_fn=<NegBackward0>) tensor(11729.6699, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11726.6142578125
tensor(11729.6699, grad_fn=<NegBackward0>) tensor(11726.6143, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11725.78125
tensor(11726.6143, grad_fn=<NegBackward0>) tensor(11725.7812, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11709.20703125
tensor(11725.7812, grad_fn=<NegBackward0>) tensor(11709.2070, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11701.232421875
tensor(11709.2070, grad_fn=<NegBackward0>) tensor(11701.2324, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11701.12109375
tensor(11701.2324, grad_fn=<NegBackward0>) tensor(11701.1211, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11695.4658203125
tensor(11701.1211, grad_fn=<NegBackward0>) tensor(11695.4658, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11686.2548828125
tensor(11695.4658, grad_fn=<NegBackward0>) tensor(11686.2549, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11686.12890625
tensor(11686.2549, grad_fn=<NegBackward0>) tensor(11686.1289, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11686.1005859375
tensor(11686.1289, grad_fn=<NegBackward0>) tensor(11686.1006, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11686.0791015625
tensor(11686.1006, grad_fn=<NegBackward0>) tensor(11686.0791, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11685.6083984375
tensor(11686.0791, grad_fn=<NegBackward0>) tensor(11685.6084, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11676.9697265625
tensor(11685.6084, grad_fn=<NegBackward0>) tensor(11676.9697, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11675.029296875
tensor(11676.9697, grad_fn=<NegBackward0>) tensor(11675.0293, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11664.423828125
tensor(11675.0293, grad_fn=<NegBackward0>) tensor(11664.4238, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11664.4130859375
tensor(11664.4238, grad_fn=<NegBackward0>) tensor(11664.4131, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11664.3896484375
tensor(11664.4131, grad_fn=<NegBackward0>) tensor(11664.3896, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11657.3466796875
tensor(11664.3896, grad_fn=<NegBackward0>) tensor(11657.3467, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11657.33984375
tensor(11657.3467, grad_fn=<NegBackward0>) tensor(11657.3398, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11657.3369140625
tensor(11657.3398, grad_fn=<NegBackward0>) tensor(11657.3369, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11657.33203125
tensor(11657.3369, grad_fn=<NegBackward0>) tensor(11657.3320, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11657.3271484375
tensor(11657.3320, grad_fn=<NegBackward0>) tensor(11657.3271, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11657.3232421875
tensor(11657.3271, grad_fn=<NegBackward0>) tensor(11657.3232, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11657.3212890625
tensor(11657.3232, grad_fn=<NegBackward0>) tensor(11657.3213, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11657.3173828125
tensor(11657.3213, grad_fn=<NegBackward0>) tensor(11657.3174, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11657.310546875
tensor(11657.3174, grad_fn=<NegBackward0>) tensor(11657.3105, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11657.30859375
tensor(11657.3105, grad_fn=<NegBackward0>) tensor(11657.3086, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11657.306640625
tensor(11657.3086, grad_fn=<NegBackward0>) tensor(11657.3066, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11657.3046875
tensor(11657.3066, grad_fn=<NegBackward0>) tensor(11657.3047, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11657.3037109375
tensor(11657.3047, grad_fn=<NegBackward0>) tensor(11657.3037, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11657.3037109375
tensor(11657.3037, grad_fn=<NegBackward0>) tensor(11657.3037, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11657.3076171875
tensor(11657.3037, grad_fn=<NegBackward0>) tensor(11657.3076, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11657.30078125
tensor(11657.3037, grad_fn=<NegBackward0>) tensor(11657.3008, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11657.298828125
tensor(11657.3008, grad_fn=<NegBackward0>) tensor(11657.2988, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11657.298828125
tensor(11657.2988, grad_fn=<NegBackward0>) tensor(11657.2988, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11657.296875
tensor(11657.2988, grad_fn=<NegBackward0>) tensor(11657.2969, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11657.296875
tensor(11657.2969, grad_fn=<NegBackward0>) tensor(11657.2969, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11657.294921875
tensor(11657.2969, grad_fn=<NegBackward0>) tensor(11657.2949, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11651.4931640625
tensor(11657.2949, grad_fn=<NegBackward0>) tensor(11651.4932, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11651.4580078125
tensor(11651.4932, grad_fn=<NegBackward0>) tensor(11651.4580, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11651.45703125
tensor(11651.4580, grad_fn=<NegBackward0>) tensor(11651.4570, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11651.4580078125
tensor(11651.4570, grad_fn=<NegBackward0>) tensor(11651.4580, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11651.4609375
tensor(11651.4570, grad_fn=<NegBackward0>) tensor(11651.4609, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11651.4560546875
tensor(11651.4570, grad_fn=<NegBackward0>) tensor(11651.4561, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11651.455078125
tensor(11651.4561, grad_fn=<NegBackward0>) tensor(11651.4551, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11651.455078125
tensor(11651.4551, grad_fn=<NegBackward0>) tensor(11651.4551, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11651.455078125
tensor(11651.4551, grad_fn=<NegBackward0>) tensor(11651.4551, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11651.4541015625
tensor(11651.4551, grad_fn=<NegBackward0>) tensor(11651.4541, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11651.4541015625
tensor(11651.4541, grad_fn=<NegBackward0>) tensor(11651.4541, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11651.4521484375
tensor(11651.4541, grad_fn=<NegBackward0>) tensor(11651.4521, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11651.451171875
tensor(11651.4521, grad_fn=<NegBackward0>) tensor(11651.4512, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11643.013671875
tensor(11651.4512, grad_fn=<NegBackward0>) tensor(11643.0137, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11643.01171875
tensor(11643.0137, grad_fn=<NegBackward0>) tensor(11643.0117, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11643.001953125
tensor(11643.0117, grad_fn=<NegBackward0>) tensor(11643.0020, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11636.2421875
tensor(11643.0020, grad_fn=<NegBackward0>) tensor(11636.2422, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11636.2412109375
tensor(11636.2422, grad_fn=<NegBackward0>) tensor(11636.2412, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11636.2451171875
tensor(11636.2412, grad_fn=<NegBackward0>) tensor(11636.2451, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11636.2421875
tensor(11636.2412, grad_fn=<NegBackward0>) tensor(11636.2422, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11636.240234375
tensor(11636.2412, grad_fn=<NegBackward0>) tensor(11636.2402, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11636.2431640625
tensor(11636.2402, grad_fn=<NegBackward0>) tensor(11636.2432, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11636.2470703125
tensor(11636.2402, grad_fn=<NegBackward0>) tensor(11636.2471, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11636.2412109375
tensor(11636.2402, grad_fn=<NegBackward0>) tensor(11636.2412, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11636.2392578125
tensor(11636.2402, grad_fn=<NegBackward0>) tensor(11636.2393, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11636.240234375
tensor(11636.2393, grad_fn=<NegBackward0>) tensor(11636.2402, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11636.240234375
tensor(11636.2393, grad_fn=<NegBackward0>) tensor(11636.2402, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11636.240234375
tensor(11636.2393, grad_fn=<NegBackward0>) tensor(11636.2402, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11636.2412109375
tensor(11636.2393, grad_fn=<NegBackward0>) tensor(11636.2412, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11636.23828125
tensor(11636.2393, grad_fn=<NegBackward0>) tensor(11636.2383, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11636.2392578125
tensor(11636.2383, grad_fn=<NegBackward0>) tensor(11636.2393, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11636.2392578125
tensor(11636.2383, grad_fn=<NegBackward0>) tensor(11636.2393, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11636.23828125
tensor(11636.2383, grad_fn=<NegBackward0>) tensor(11636.2383, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11636.2392578125
tensor(11636.2383, grad_fn=<NegBackward0>) tensor(11636.2393, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11636.23828125
tensor(11636.2383, grad_fn=<NegBackward0>) tensor(11636.2383, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11636.2373046875
tensor(11636.2383, grad_fn=<NegBackward0>) tensor(11636.2373, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11636.2431640625
tensor(11636.2373, grad_fn=<NegBackward0>) tensor(11636.2432, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11633.3251953125
tensor(11636.2373, grad_fn=<NegBackward0>) tensor(11633.3252, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11633.2099609375
tensor(11633.3252, grad_fn=<NegBackward0>) tensor(11633.2100, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11633.2119140625
tensor(11633.2100, grad_fn=<NegBackward0>) tensor(11633.2119, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11633.2109375
tensor(11633.2100, grad_fn=<NegBackward0>) tensor(11633.2109, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11633.2109375
tensor(11633.2100, grad_fn=<NegBackward0>) tensor(11633.2109, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11633.21875
tensor(11633.2100, grad_fn=<NegBackward0>) tensor(11633.2188, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11633.21484375
tensor(11633.2100, grad_fn=<NegBackward0>) tensor(11633.2148, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7471, 0.2529],
        [0.3124, 0.6876]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4526, 0.5474], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2018, 0.1024],
         [0.5135, 0.4027]],

        [[0.5425, 0.1008],
         [0.6347, 0.6428]],

        [[0.6340, 0.1211],
         [0.6293, 0.5209]],

        [[0.5651, 0.1020],
         [0.6780, 0.6443]],

        [[0.5180, 0.0969],
         [0.7056, 0.5548]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9524809382781522
Average Adjusted Rand Index: 0.9526458914025623
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20081.435546875
inf tensor(20081.4355, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12347.7802734375
tensor(20081.4355, grad_fn=<NegBackward0>) tensor(12347.7803, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12294.537109375
tensor(12347.7803, grad_fn=<NegBackward0>) tensor(12294.5371, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11707.470703125
tensor(12294.5371, grad_fn=<NegBackward0>) tensor(11707.4707, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11678.9697265625
tensor(11707.4707, grad_fn=<NegBackward0>) tensor(11678.9697, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11667.748046875
tensor(11678.9697, grad_fn=<NegBackward0>) tensor(11667.7480, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11667.501953125
tensor(11667.7480, grad_fn=<NegBackward0>) tensor(11667.5020, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11667.36328125
tensor(11667.5020, grad_fn=<NegBackward0>) tensor(11667.3633, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11667.28515625
tensor(11667.3633, grad_fn=<NegBackward0>) tensor(11667.2852, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11667.23046875
tensor(11667.2852, grad_fn=<NegBackward0>) tensor(11667.2305, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11667.1923828125
tensor(11667.2305, grad_fn=<NegBackward0>) tensor(11667.1924, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11667.1611328125
tensor(11667.1924, grad_fn=<NegBackward0>) tensor(11667.1611, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11661.9306640625
tensor(11667.1611, grad_fn=<NegBackward0>) tensor(11661.9307, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11661.849609375
tensor(11661.9307, grad_fn=<NegBackward0>) tensor(11661.8496, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11661.8349609375
tensor(11661.8496, grad_fn=<NegBackward0>) tensor(11661.8350, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11661.8212890625
tensor(11661.8350, grad_fn=<NegBackward0>) tensor(11661.8213, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11661.810546875
tensor(11661.8213, grad_fn=<NegBackward0>) tensor(11661.8105, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11661.802734375
tensor(11661.8105, grad_fn=<NegBackward0>) tensor(11661.8027, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11661.79296875
tensor(11661.8027, grad_fn=<NegBackward0>) tensor(11661.7930, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11659.37109375
tensor(11661.7930, grad_fn=<NegBackward0>) tensor(11659.3711, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11659.365234375
tensor(11659.3711, grad_fn=<NegBackward0>) tensor(11659.3652, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11659.3583984375
tensor(11659.3652, grad_fn=<NegBackward0>) tensor(11659.3584, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11651.998046875
tensor(11659.3584, grad_fn=<NegBackward0>) tensor(11651.9980, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11651.994140625
tensor(11651.9980, grad_fn=<NegBackward0>) tensor(11651.9941, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11651.9892578125
tensor(11651.9941, grad_fn=<NegBackward0>) tensor(11651.9893, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11651.9873046875
tensor(11651.9893, grad_fn=<NegBackward0>) tensor(11651.9873, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11651.984375
tensor(11651.9873, grad_fn=<NegBackward0>) tensor(11651.9844, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11651.9814453125
tensor(11651.9844, grad_fn=<NegBackward0>) tensor(11651.9814, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11651.9794921875
tensor(11651.9814, grad_fn=<NegBackward0>) tensor(11651.9795, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11651.9599609375
tensor(11651.9795, grad_fn=<NegBackward0>) tensor(11651.9600, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11643.4052734375
tensor(11651.9600, grad_fn=<NegBackward0>) tensor(11643.4053, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11643.4013671875
tensor(11643.4053, grad_fn=<NegBackward0>) tensor(11643.4014, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11643.4013671875
tensor(11643.4014, grad_fn=<NegBackward0>) tensor(11643.4014, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11643.3876953125
tensor(11643.4014, grad_fn=<NegBackward0>) tensor(11643.3877, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11643.384765625
tensor(11643.3877, grad_fn=<NegBackward0>) tensor(11643.3848, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11643.3837890625
tensor(11643.3848, grad_fn=<NegBackward0>) tensor(11643.3838, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11643.3818359375
tensor(11643.3838, grad_fn=<NegBackward0>) tensor(11643.3818, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11643.380859375
tensor(11643.3818, grad_fn=<NegBackward0>) tensor(11643.3809, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11643.380859375
tensor(11643.3809, grad_fn=<NegBackward0>) tensor(11643.3809, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11643.3798828125
tensor(11643.3809, grad_fn=<NegBackward0>) tensor(11643.3799, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11643.3798828125
tensor(11643.3799, grad_fn=<NegBackward0>) tensor(11643.3799, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11643.3779296875
tensor(11643.3799, grad_fn=<NegBackward0>) tensor(11643.3779, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11643.376953125
tensor(11643.3779, grad_fn=<NegBackward0>) tensor(11643.3770, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11643.3779296875
tensor(11643.3770, grad_fn=<NegBackward0>) tensor(11643.3779, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11643.376953125
tensor(11643.3770, grad_fn=<NegBackward0>) tensor(11643.3770, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11643.3759765625
tensor(11643.3770, grad_fn=<NegBackward0>) tensor(11643.3760, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11643.3759765625
tensor(11643.3760, grad_fn=<NegBackward0>) tensor(11643.3760, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11643.375
tensor(11643.3760, grad_fn=<NegBackward0>) tensor(11643.3750, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11643.384765625
tensor(11643.3750, grad_fn=<NegBackward0>) tensor(11643.3848, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11643.375
tensor(11643.3750, grad_fn=<NegBackward0>) tensor(11643.3750, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11643.390625
tensor(11643.3750, grad_fn=<NegBackward0>) tensor(11643.3906, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11643.375
tensor(11643.3750, grad_fn=<NegBackward0>) tensor(11643.3750, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11643.3720703125
tensor(11643.3750, grad_fn=<NegBackward0>) tensor(11643.3721, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11643.3720703125
tensor(11643.3721, grad_fn=<NegBackward0>) tensor(11643.3721, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11643.3740234375
tensor(11643.3721, grad_fn=<NegBackward0>) tensor(11643.3740, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11643.373046875
tensor(11643.3721, grad_fn=<NegBackward0>) tensor(11643.3730, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11643.37109375
tensor(11643.3721, grad_fn=<NegBackward0>) tensor(11643.3711, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11643.3779296875
tensor(11643.3711, grad_fn=<NegBackward0>) tensor(11643.3779, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11643.369140625
tensor(11643.3711, grad_fn=<NegBackward0>) tensor(11643.3691, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11643.37109375
tensor(11643.3691, grad_fn=<NegBackward0>) tensor(11643.3711, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11643.375
tensor(11643.3691, grad_fn=<NegBackward0>) tensor(11643.3750, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11643.3701171875
tensor(11643.3691, grad_fn=<NegBackward0>) tensor(11643.3701, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11643.3701171875
tensor(11643.3691, grad_fn=<NegBackward0>) tensor(11643.3701, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -11643.3701171875
tensor(11643.3691, grad_fn=<NegBackward0>) tensor(11643.3701, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[0.7510, 0.2490],
        [0.3154, 0.6846]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4531, 0.5469], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.1025],
         [0.6221, 0.4053]],

        [[0.6574, 0.1008],
         [0.6647, 0.6013]],

        [[0.6663, 0.1288],
         [0.6203, 0.6272]],

        [[0.7248, 0.1020],
         [0.5983, 0.5354]],

        [[0.5343, 0.0970],
         [0.6928, 0.6524]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080890789891884
time is 3
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.9368977927366425
Average Adjusted Rand Index: 0.9377796879291971
[0.9524809382781522, 0.9368977927366425] [0.9526458914025623, 0.9377796879291971] [11633.21484375, 11643.3701171875]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11222.276078605853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20521.48046875
inf tensor(20521.4805, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11783.8017578125
tensor(20521.4805, grad_fn=<NegBackward0>) tensor(11783.8018, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11780.4130859375
tensor(11783.8018, grad_fn=<NegBackward0>) tensor(11780.4131, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11774.0771484375
tensor(11780.4131, grad_fn=<NegBackward0>) tensor(11774.0771, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11443.572265625
tensor(11774.0771, grad_fn=<NegBackward0>) tensor(11443.5723, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11298.822265625
tensor(11443.5723, grad_fn=<NegBackward0>) tensor(11298.8223, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11284.033203125
tensor(11298.8223, grad_fn=<NegBackward0>) tensor(11284.0332, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11249.2109375
tensor(11284.0332, grad_fn=<NegBackward0>) tensor(11249.2109, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11237.173828125
tensor(11249.2109, grad_fn=<NegBackward0>) tensor(11237.1738, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11236.455078125
tensor(11237.1738, grad_fn=<NegBackward0>) tensor(11236.4551, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11236.33984375
tensor(11236.4551, grad_fn=<NegBackward0>) tensor(11236.3398, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11236.2841796875
tensor(11236.3398, grad_fn=<NegBackward0>) tensor(11236.2842, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11236.2451171875
tensor(11236.2842, grad_fn=<NegBackward0>) tensor(11236.2451, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11236.21484375
tensor(11236.2451, grad_fn=<NegBackward0>) tensor(11236.2148, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11236.189453125
tensor(11236.2148, grad_fn=<NegBackward0>) tensor(11236.1895, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11236.1669921875
tensor(11236.1895, grad_fn=<NegBackward0>) tensor(11236.1670, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11236.1435546875
tensor(11236.1670, grad_fn=<NegBackward0>) tensor(11236.1436, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11236.123046875
tensor(11236.1436, grad_fn=<NegBackward0>) tensor(11236.1230, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11236.1123046875
tensor(11236.1230, grad_fn=<NegBackward0>) tensor(11236.1123, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11236.1025390625
tensor(11236.1123, grad_fn=<NegBackward0>) tensor(11236.1025, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11236.095703125
tensor(11236.1025, grad_fn=<NegBackward0>) tensor(11236.0957, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11236.0888671875
tensor(11236.0957, grad_fn=<NegBackward0>) tensor(11236.0889, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11236.0830078125
tensor(11236.0889, grad_fn=<NegBackward0>) tensor(11236.0830, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11236.0771484375
tensor(11236.0830, grad_fn=<NegBackward0>) tensor(11236.0771, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11236.0732421875
tensor(11236.0771, grad_fn=<NegBackward0>) tensor(11236.0732, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11236.068359375
tensor(11236.0732, grad_fn=<NegBackward0>) tensor(11236.0684, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11225.693359375
tensor(11236.0684, grad_fn=<NegBackward0>) tensor(11225.6934, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11225.638671875
tensor(11225.6934, grad_fn=<NegBackward0>) tensor(11225.6387, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11225.6357421875
tensor(11225.6387, grad_fn=<NegBackward0>) tensor(11225.6357, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11225.62890625
tensor(11225.6357, grad_fn=<NegBackward0>) tensor(11225.6289, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11225.626953125
tensor(11225.6289, grad_fn=<NegBackward0>) tensor(11225.6270, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11225.6259765625
tensor(11225.6270, grad_fn=<NegBackward0>) tensor(11225.6260, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11225.6220703125
tensor(11225.6260, grad_fn=<NegBackward0>) tensor(11225.6221, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11225.6201171875
tensor(11225.6221, grad_fn=<NegBackward0>) tensor(11225.6201, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11225.6171875
tensor(11225.6201, grad_fn=<NegBackward0>) tensor(11225.6172, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11225.595703125
tensor(11225.6172, grad_fn=<NegBackward0>) tensor(11225.5957, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11225.5927734375
tensor(11225.5957, grad_fn=<NegBackward0>) tensor(11225.5928, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11225.591796875
tensor(11225.5928, grad_fn=<NegBackward0>) tensor(11225.5918, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11225.5908203125
tensor(11225.5918, grad_fn=<NegBackward0>) tensor(11225.5908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11225.58984375
tensor(11225.5908, grad_fn=<NegBackward0>) tensor(11225.5898, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11225.5908203125
tensor(11225.5898, grad_fn=<NegBackward0>) tensor(11225.5908, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11225.5888671875
tensor(11225.5898, grad_fn=<NegBackward0>) tensor(11225.5889, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11225.5869140625
tensor(11225.5889, grad_fn=<NegBackward0>) tensor(11225.5869, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11225.5869140625
tensor(11225.5869, grad_fn=<NegBackward0>) tensor(11225.5869, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11225.6015625
tensor(11225.5869, grad_fn=<NegBackward0>) tensor(11225.6016, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11225.5869140625
tensor(11225.5869, grad_fn=<NegBackward0>) tensor(11225.5869, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11225.5859375
tensor(11225.5869, grad_fn=<NegBackward0>) tensor(11225.5859, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11225.5927734375
tensor(11225.5859, grad_fn=<NegBackward0>) tensor(11225.5928, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11225.583984375
tensor(11225.5859, grad_fn=<NegBackward0>) tensor(11225.5840, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11225.583984375
tensor(11225.5840, grad_fn=<NegBackward0>) tensor(11225.5840, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11225.583984375
tensor(11225.5840, grad_fn=<NegBackward0>) tensor(11225.5840, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11225.583984375
tensor(11225.5840, grad_fn=<NegBackward0>) tensor(11225.5840, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11225.583984375
tensor(11225.5840, grad_fn=<NegBackward0>) tensor(11225.5840, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11225.5810546875
tensor(11225.5840, grad_fn=<NegBackward0>) tensor(11225.5811, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11225.58203125
tensor(11225.5811, grad_fn=<NegBackward0>) tensor(11225.5820, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11225.58203125
tensor(11225.5811, grad_fn=<NegBackward0>) tensor(11225.5820, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11225.580078125
tensor(11225.5811, grad_fn=<NegBackward0>) tensor(11225.5801, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11225.5791015625
tensor(11225.5801, grad_fn=<NegBackward0>) tensor(11225.5791, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11225.5791015625
tensor(11225.5791, grad_fn=<NegBackward0>) tensor(11225.5791, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11225.578125
tensor(11225.5791, grad_fn=<NegBackward0>) tensor(11225.5781, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11225.5791015625
tensor(11225.5781, grad_fn=<NegBackward0>) tensor(11225.5791, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11225.5810546875
tensor(11225.5781, grad_fn=<NegBackward0>) tensor(11225.5811, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11214.7744140625
tensor(11225.5781, grad_fn=<NegBackward0>) tensor(11214.7744, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11214.7763671875
tensor(11214.7744, grad_fn=<NegBackward0>) tensor(11214.7764, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11214.7734375
tensor(11214.7744, grad_fn=<NegBackward0>) tensor(11214.7734, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11214.7734375
tensor(11214.7734, grad_fn=<NegBackward0>) tensor(11214.7734, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11214.7734375
tensor(11214.7734, grad_fn=<NegBackward0>) tensor(11214.7734, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11214.7734375
tensor(11214.7734, grad_fn=<NegBackward0>) tensor(11214.7734, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11214.7724609375
tensor(11214.7734, grad_fn=<NegBackward0>) tensor(11214.7725, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11214.7734375
tensor(11214.7725, grad_fn=<NegBackward0>) tensor(11214.7734, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11214.771484375
tensor(11214.7725, grad_fn=<NegBackward0>) tensor(11214.7715, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11214.7744140625
tensor(11214.7715, grad_fn=<NegBackward0>) tensor(11214.7744, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11214.771484375
tensor(11214.7715, grad_fn=<NegBackward0>) tensor(11214.7715, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11214.7724609375
tensor(11214.7715, grad_fn=<NegBackward0>) tensor(11214.7725, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11214.7724609375
tensor(11214.7715, grad_fn=<NegBackward0>) tensor(11214.7725, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11214.7734375
tensor(11214.7715, grad_fn=<NegBackward0>) tensor(11214.7734, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11214.7724609375
tensor(11214.7715, grad_fn=<NegBackward0>) tensor(11214.7725, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11214.8046875
tensor(11214.7715, grad_fn=<NegBackward0>) tensor(11214.8047, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7709, 0.2291],
        [0.2922, 0.7078]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5949, 0.4051], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.1038],
         [0.5786, 0.3896]],

        [[0.6855, 0.0915],
         [0.5272, 0.5551]],

        [[0.6494, 0.0966],
         [0.7233, 0.6272]],

        [[0.6401, 0.1076],
         [0.7141, 0.6748]],

        [[0.5446, 0.0951],
         [0.5728, 0.5436]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21818.84765625
inf tensor(21818.8477, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11782.3359375
tensor(21818.8477, grad_fn=<NegBackward0>) tensor(11782.3359, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11778.7900390625
tensor(11782.3359, grad_fn=<NegBackward0>) tensor(11778.7900, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11776.0
tensor(11778.7900, grad_fn=<NegBackward0>) tensor(11776., grad_fn=<NegBackward0>)
Iteration 400: Loss = -11628.8603515625
tensor(11776., grad_fn=<NegBackward0>) tensor(11628.8604, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11338.9384765625
tensor(11628.8604, grad_fn=<NegBackward0>) tensor(11338.9385, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11333.162109375
tensor(11338.9385, grad_fn=<NegBackward0>) tensor(11333.1621, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11324.4345703125
tensor(11333.1621, grad_fn=<NegBackward0>) tensor(11324.4346, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11322.125
tensor(11324.4346, grad_fn=<NegBackward0>) tensor(11322.1250, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11322.0595703125
tensor(11322.1250, grad_fn=<NegBackward0>) tensor(11322.0596, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11322.03125
tensor(11322.0596, grad_fn=<NegBackward0>) tensor(11322.0312, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11322.0126953125
tensor(11322.0312, grad_fn=<NegBackward0>) tensor(11322.0127, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11321.9970703125
tensor(11322.0127, grad_fn=<NegBackward0>) tensor(11321.9971, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11321.984375
tensor(11321.9971, grad_fn=<NegBackward0>) tensor(11321.9844, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11321.958984375
tensor(11321.9844, grad_fn=<NegBackward0>) tensor(11321.9590, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11321.9501953125
tensor(11321.9590, grad_fn=<NegBackward0>) tensor(11321.9502, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11321.826171875
tensor(11321.9502, grad_fn=<NegBackward0>) tensor(11321.8262, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11321.78125
tensor(11321.8262, grad_fn=<NegBackward0>) tensor(11321.7812, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11321.748046875
tensor(11321.7812, grad_fn=<NegBackward0>) tensor(11321.7480, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11321.744140625
tensor(11321.7480, grad_fn=<NegBackward0>) tensor(11321.7441, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11321.740234375
tensor(11321.7441, grad_fn=<NegBackward0>) tensor(11321.7402, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11321.732421875
tensor(11321.7402, grad_fn=<NegBackward0>) tensor(11321.7324, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11321.7314453125
tensor(11321.7324, grad_fn=<NegBackward0>) tensor(11321.7314, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11321.728515625
tensor(11321.7314, grad_fn=<NegBackward0>) tensor(11321.7285, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11321.7255859375
tensor(11321.7285, grad_fn=<NegBackward0>) tensor(11321.7256, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11321.7216796875
tensor(11321.7256, grad_fn=<NegBackward0>) tensor(11321.7217, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11321.720703125
tensor(11321.7217, grad_fn=<NegBackward0>) tensor(11321.7207, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11321.720703125
tensor(11321.7207, grad_fn=<NegBackward0>) tensor(11321.7207, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11321.7177734375
tensor(11321.7207, grad_fn=<NegBackward0>) tensor(11321.7178, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11321.7158203125
tensor(11321.7178, grad_fn=<NegBackward0>) tensor(11321.7158, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11321.7158203125
tensor(11321.7158, grad_fn=<NegBackward0>) tensor(11321.7158, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11321.712890625
tensor(11321.7158, grad_fn=<NegBackward0>) tensor(11321.7129, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11321.7119140625
tensor(11321.7129, grad_fn=<NegBackward0>) tensor(11321.7119, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11321.7119140625
tensor(11321.7119, grad_fn=<NegBackward0>) tensor(11321.7119, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11321.7109375
tensor(11321.7119, grad_fn=<NegBackward0>) tensor(11321.7109, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11321.7109375
tensor(11321.7109, grad_fn=<NegBackward0>) tensor(11321.7109, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11321.708984375
tensor(11321.7109, grad_fn=<NegBackward0>) tensor(11321.7090, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11321.708984375
tensor(11321.7090, grad_fn=<NegBackward0>) tensor(11321.7090, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11321.708984375
tensor(11321.7090, grad_fn=<NegBackward0>) tensor(11321.7090, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11321.70703125
tensor(11321.7090, grad_fn=<NegBackward0>) tensor(11321.7070, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11321.7080078125
tensor(11321.7070, grad_fn=<NegBackward0>) tensor(11321.7080, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11321.70703125
tensor(11321.7070, grad_fn=<NegBackward0>) tensor(11321.7070, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11321.70703125
tensor(11321.7070, grad_fn=<NegBackward0>) tensor(11321.7070, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11321.705078125
tensor(11321.7070, grad_fn=<NegBackward0>) tensor(11321.7051, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11321.70703125
tensor(11321.7051, grad_fn=<NegBackward0>) tensor(11321.7070, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11321.7060546875
tensor(11321.7051, grad_fn=<NegBackward0>) tensor(11321.7061, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11321.7041015625
tensor(11321.7051, grad_fn=<NegBackward0>) tensor(11321.7041, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11321.6953125
tensor(11321.7041, grad_fn=<NegBackward0>) tensor(11321.6953, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11321.693359375
tensor(11321.6953, grad_fn=<NegBackward0>) tensor(11321.6934, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11321.6953125
tensor(11321.6934, grad_fn=<NegBackward0>) tensor(11321.6953, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11321.6943359375
tensor(11321.6934, grad_fn=<NegBackward0>) tensor(11321.6943, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11321.6953125
tensor(11321.6934, grad_fn=<NegBackward0>) tensor(11321.6953, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11321.6943359375
tensor(11321.6934, grad_fn=<NegBackward0>) tensor(11321.6943, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -11321.6943359375
tensor(11321.6934, grad_fn=<NegBackward0>) tensor(11321.6943, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.7090, 0.2910],
        [0.3049, 0.6951]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9091, 0.0909], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.0977],
         [0.5356, 0.3879]],

        [[0.5047, 0.0915],
         [0.6471, 0.6157]],

        [[0.6383, 0.0966],
         [0.6406, 0.7187]],

        [[0.6248, 0.1075],
         [0.6903, 0.6810]],

        [[0.6540, 0.0949],
         [0.6109, 0.5061]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.026008250167835027
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6451159594017802
Average Adjusted Rand Index: 0.7947983499664331
[1.0, 0.6451159594017802] [1.0, 0.7947983499664331] [11214.8046875, 11321.6943359375]
-------------------------------------
This iteration is 11
True Objective function: Loss = -11543.107153817185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22250.4609375
inf tensor(22250.4609, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12386.6748046875
tensor(22250.4609, grad_fn=<NegBackward0>) tensor(12386.6748, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12344.826171875
tensor(12386.6748, grad_fn=<NegBackward0>) tensor(12344.8262, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12341.716796875
tensor(12344.8262, grad_fn=<NegBackward0>) tensor(12341.7168, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12327.0625
tensor(12341.7168, grad_fn=<NegBackward0>) tensor(12327.0625, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11984.4921875
tensor(12327.0625, grad_fn=<NegBackward0>) tensor(11984.4922, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11922.6181640625
tensor(11984.4922, grad_fn=<NegBackward0>) tensor(11922.6182, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11831.4716796875
tensor(11922.6182, grad_fn=<NegBackward0>) tensor(11831.4717, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11788.2412109375
tensor(11831.4717, grad_fn=<NegBackward0>) tensor(11788.2412, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11787.4267578125
tensor(11788.2412, grad_fn=<NegBackward0>) tensor(11787.4268, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11742.681640625
tensor(11787.4268, grad_fn=<NegBackward0>) tensor(11742.6816, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11701.853515625
tensor(11742.6816, grad_fn=<NegBackward0>) tensor(11701.8535, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11699.439453125
tensor(11701.8535, grad_fn=<NegBackward0>) tensor(11699.4395, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11672.779296875
tensor(11699.4395, grad_fn=<NegBackward0>) tensor(11672.7793, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11672.7646484375
tensor(11672.7793, grad_fn=<NegBackward0>) tensor(11672.7646, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11671.98046875
tensor(11672.7646, grad_fn=<NegBackward0>) tensor(11671.9805, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11653.91015625
tensor(11671.9805, grad_fn=<NegBackward0>) tensor(11653.9102, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11645.4208984375
tensor(11653.9102, grad_fn=<NegBackward0>) tensor(11645.4209, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11634.2255859375
tensor(11645.4209, grad_fn=<NegBackward0>) tensor(11634.2256, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11634.2216796875
tensor(11634.2256, grad_fn=<NegBackward0>) tensor(11634.2217, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11634.2080078125
tensor(11634.2217, grad_fn=<NegBackward0>) tensor(11634.2080, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11622.4697265625
tensor(11634.2080, grad_fn=<NegBackward0>) tensor(11622.4697, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11622.466796875
tensor(11622.4697, grad_fn=<NegBackward0>) tensor(11622.4668, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11622.462890625
tensor(11622.4668, grad_fn=<NegBackward0>) tensor(11622.4629, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11609.9033203125
tensor(11622.4629, grad_fn=<NegBackward0>) tensor(11609.9033, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11600.544921875
tensor(11609.9033, grad_fn=<NegBackward0>) tensor(11600.5449, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11600.5439453125
tensor(11600.5449, grad_fn=<NegBackward0>) tensor(11600.5439, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11600.5439453125
tensor(11600.5439, grad_fn=<NegBackward0>) tensor(11600.5439, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11600.5390625
tensor(11600.5439, grad_fn=<NegBackward0>) tensor(11600.5391, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11600.5390625
tensor(11600.5391, grad_fn=<NegBackward0>) tensor(11600.5391, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11600.5361328125
tensor(11600.5391, grad_fn=<NegBackward0>) tensor(11600.5361, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11600.5361328125
tensor(11600.5361, grad_fn=<NegBackward0>) tensor(11600.5361, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11600.53515625
tensor(11600.5361, grad_fn=<NegBackward0>) tensor(11600.5352, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11600.533203125
tensor(11600.5352, grad_fn=<NegBackward0>) tensor(11600.5332, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11600.533203125
tensor(11600.5332, grad_fn=<NegBackward0>) tensor(11600.5332, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11600.533203125
tensor(11600.5332, grad_fn=<NegBackward0>) tensor(11600.5332, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11600.533203125
tensor(11600.5332, grad_fn=<NegBackward0>) tensor(11600.5332, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11600.54296875
tensor(11600.5332, grad_fn=<NegBackward0>) tensor(11600.5430, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11600.5322265625
tensor(11600.5332, grad_fn=<NegBackward0>) tensor(11600.5322, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11600.533203125
tensor(11600.5322, grad_fn=<NegBackward0>) tensor(11600.5332, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11600.53125
tensor(11600.5322, grad_fn=<NegBackward0>) tensor(11600.5312, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11600.533203125
tensor(11600.5312, grad_fn=<NegBackward0>) tensor(11600.5332, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11600.5302734375
tensor(11600.5312, grad_fn=<NegBackward0>) tensor(11600.5303, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11600.53515625
tensor(11600.5303, grad_fn=<NegBackward0>) tensor(11600.5352, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11600.5302734375
tensor(11600.5303, grad_fn=<NegBackward0>) tensor(11600.5303, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11600.53125
tensor(11600.5303, grad_fn=<NegBackward0>) tensor(11600.5312, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11600.529296875
tensor(11600.5303, grad_fn=<NegBackward0>) tensor(11600.5293, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11600.5322265625
tensor(11600.5293, grad_fn=<NegBackward0>) tensor(11600.5322, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11600.5302734375
tensor(11600.5293, grad_fn=<NegBackward0>) tensor(11600.5303, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11600.5302734375
tensor(11600.5293, grad_fn=<NegBackward0>) tensor(11600.5303, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11600.5302734375
tensor(11600.5293, grad_fn=<NegBackward0>) tensor(11600.5303, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -11600.5283203125
tensor(11600.5293, grad_fn=<NegBackward0>) tensor(11600.5283, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11600.5400390625
tensor(11600.5283, grad_fn=<NegBackward0>) tensor(11600.5400, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11600.5283203125
tensor(11600.5283, grad_fn=<NegBackward0>) tensor(11600.5283, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11600.52734375
tensor(11600.5283, grad_fn=<NegBackward0>) tensor(11600.5273, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11600.5302734375
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5303, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11600.5283203125
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5283, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11600.5283203125
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5283, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11600.5322265625
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5322, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11600.52734375
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5273, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11600.52734375
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5273, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11600.541015625
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5410, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11600.5263671875
tensor(11600.5273, grad_fn=<NegBackward0>) tensor(11600.5264, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11600.52734375
tensor(11600.5264, grad_fn=<NegBackward0>) tensor(11600.5273, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11600.5263671875
tensor(11600.5264, grad_fn=<NegBackward0>) tensor(11600.5264, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11591.3232421875
tensor(11600.5264, grad_fn=<NegBackward0>) tensor(11591.3232, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11591.322265625
tensor(11591.3232, grad_fn=<NegBackward0>) tensor(11591.3223, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11578.6455078125
tensor(11591.3223, grad_fn=<NegBackward0>) tensor(11578.6455, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11578.6474609375
tensor(11578.6455, grad_fn=<NegBackward0>) tensor(11578.6475, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11578.6494140625
tensor(11578.6455, grad_fn=<NegBackward0>) tensor(11578.6494, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11578.6533203125
tensor(11578.6455, grad_fn=<NegBackward0>) tensor(11578.6533, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11578.64453125
tensor(11578.6455, grad_fn=<NegBackward0>) tensor(11578.6445, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11578.642578125
tensor(11578.6445, grad_fn=<NegBackward0>) tensor(11578.6426, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11578.642578125
tensor(11578.6426, grad_fn=<NegBackward0>) tensor(11578.6426, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11578.642578125
tensor(11578.6426, grad_fn=<NegBackward0>) tensor(11578.6426, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11578.6435546875
tensor(11578.6426, grad_fn=<NegBackward0>) tensor(11578.6436, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11578.6416015625
tensor(11578.6426, grad_fn=<NegBackward0>) tensor(11578.6416, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11578.6416015625
tensor(11578.6416, grad_fn=<NegBackward0>) tensor(11578.6416, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11566.421875
tensor(11578.6416, grad_fn=<NegBackward0>) tensor(11566.4219, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11548.43359375
tensor(11566.4219, grad_fn=<NegBackward0>) tensor(11548.4336, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11548.40625
tensor(11548.4336, grad_fn=<NegBackward0>) tensor(11548.4062, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11548.40234375
tensor(11548.4062, grad_fn=<NegBackward0>) tensor(11548.4023, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11548.40234375
tensor(11548.4023, grad_fn=<NegBackward0>) tensor(11548.4023, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11548.400390625
tensor(11548.4023, grad_fn=<NegBackward0>) tensor(11548.4004, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11548.400390625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4004, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11548.4013671875
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4014, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11548.400390625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4004, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11548.4150390625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4150, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11548.4013671875
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4014, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11548.4013671875
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4014, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11548.400390625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4004, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11548.400390625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4004, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11548.4013671875
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4014, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11548.40625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4062, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11548.4541015625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.4541, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11548.3994140625
tensor(11548.4004, grad_fn=<NegBackward0>) tensor(11548.3994, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11548.40234375
tensor(11548.3994, grad_fn=<NegBackward0>) tensor(11548.4023, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11548.3994140625
tensor(11548.3994, grad_fn=<NegBackward0>) tensor(11548.3994, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11548.400390625
tensor(11548.3994, grad_fn=<NegBackward0>) tensor(11548.4004, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11548.400390625
tensor(11548.3994, grad_fn=<NegBackward0>) tensor(11548.4004, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7376, 0.2624],
        [0.2433, 0.7567]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4715, 0.5285], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1951, 0.1031],
         [0.5757, 0.3970]],

        [[0.6582, 0.0913],
         [0.6481, 0.7127]],

        [[0.6052, 0.0889],
         [0.5249, 0.5454]],

        [[0.6208, 0.1067],
         [0.6355, 0.7021]],

        [[0.6257, 0.1001],
         [0.7027, 0.5396]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9761598395607173
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22868.03125
inf tensor(22868.0312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12417.75390625
tensor(22868.0312, grad_fn=<NegBackward0>) tensor(12417.7539, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12349.8330078125
tensor(12417.7539, grad_fn=<NegBackward0>) tensor(12349.8330, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11920.4609375
tensor(12349.8330, grad_fn=<NegBackward0>) tensor(11920.4609, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11755.86328125
tensor(11920.4609, grad_fn=<NegBackward0>) tensor(11755.8633, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11688.2060546875
tensor(11755.8633, grad_fn=<NegBackward0>) tensor(11688.2061, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11646.4814453125
tensor(11688.2061, grad_fn=<NegBackward0>) tensor(11646.4814, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11588.1689453125
tensor(11646.4814, grad_fn=<NegBackward0>) tensor(11588.1689, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11576.9013671875
tensor(11588.1689, grad_fn=<NegBackward0>) tensor(11576.9014, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11576.658203125
tensor(11576.9014, grad_fn=<NegBackward0>) tensor(11576.6582, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11566.6064453125
tensor(11576.6582, grad_fn=<NegBackward0>) tensor(11566.6064, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11558.3701171875
tensor(11566.6064, grad_fn=<NegBackward0>) tensor(11558.3701, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11558.1845703125
tensor(11558.3701, grad_fn=<NegBackward0>) tensor(11558.1846, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11552.9150390625
tensor(11558.1846, grad_fn=<NegBackward0>) tensor(11552.9150, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11552.865234375
tensor(11552.9150, grad_fn=<NegBackward0>) tensor(11552.8652, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11552.7080078125
tensor(11552.8652, grad_fn=<NegBackward0>) tensor(11552.7080, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11550.07421875
tensor(11552.7080, grad_fn=<NegBackward0>) tensor(11550.0742, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11540.0322265625
tensor(11550.0742, grad_fn=<NegBackward0>) tensor(11540.0322, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11540.0087890625
tensor(11540.0322, grad_fn=<NegBackward0>) tensor(11540.0088, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11539.9921875
tensor(11540.0088, grad_fn=<NegBackward0>) tensor(11539.9922, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11539.9765625
tensor(11539.9922, grad_fn=<NegBackward0>) tensor(11539.9766, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11539.9638671875
tensor(11539.9766, grad_fn=<NegBackward0>) tensor(11539.9639, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11539.9541015625
tensor(11539.9639, grad_fn=<NegBackward0>) tensor(11539.9541, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11539.9443359375
tensor(11539.9541, grad_fn=<NegBackward0>) tensor(11539.9443, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11539.935546875
tensor(11539.9443, grad_fn=<NegBackward0>) tensor(11539.9355, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11539.9287109375
tensor(11539.9355, grad_fn=<NegBackward0>) tensor(11539.9287, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11539.921875
tensor(11539.9287, grad_fn=<NegBackward0>) tensor(11539.9219, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11539.9169921875
tensor(11539.9219, grad_fn=<NegBackward0>) tensor(11539.9170, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11539.9111328125
tensor(11539.9170, grad_fn=<NegBackward0>) tensor(11539.9111, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11539.90625
tensor(11539.9111, grad_fn=<NegBackward0>) tensor(11539.9062, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11539.9033203125
tensor(11539.9062, grad_fn=<NegBackward0>) tensor(11539.9033, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11539.8994140625
tensor(11539.9033, grad_fn=<NegBackward0>) tensor(11539.8994, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11539.8955078125
tensor(11539.8994, grad_fn=<NegBackward0>) tensor(11539.8955, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11539.8916015625
tensor(11539.8955, grad_fn=<NegBackward0>) tensor(11539.8916, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11539.888671875
tensor(11539.8916, grad_fn=<NegBackward0>) tensor(11539.8887, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11539.8857421875
tensor(11539.8887, grad_fn=<NegBackward0>) tensor(11539.8857, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11539.884765625
tensor(11539.8857, grad_fn=<NegBackward0>) tensor(11539.8848, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11539.8798828125
tensor(11539.8848, grad_fn=<NegBackward0>) tensor(11539.8799, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11539.869140625
tensor(11539.8799, grad_fn=<NegBackward0>) tensor(11539.8691, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11539.86328125
tensor(11539.8691, grad_fn=<NegBackward0>) tensor(11539.8633, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11539.861328125
tensor(11539.8633, grad_fn=<NegBackward0>) tensor(11539.8613, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11539.8583984375
tensor(11539.8613, grad_fn=<NegBackward0>) tensor(11539.8584, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11539.857421875
tensor(11539.8584, grad_fn=<NegBackward0>) tensor(11539.8574, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11539.8544921875
tensor(11539.8574, grad_fn=<NegBackward0>) tensor(11539.8545, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11533.8515625
tensor(11539.8545, grad_fn=<NegBackward0>) tensor(11533.8516, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11533.8212890625
tensor(11533.8516, grad_fn=<NegBackward0>) tensor(11533.8213, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11533.8193359375
tensor(11533.8213, grad_fn=<NegBackward0>) tensor(11533.8193, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11533.8193359375
tensor(11533.8193, grad_fn=<NegBackward0>) tensor(11533.8193, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11533.8173828125
tensor(11533.8193, grad_fn=<NegBackward0>) tensor(11533.8174, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11533.8154296875
tensor(11533.8174, grad_fn=<NegBackward0>) tensor(11533.8154, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11533.8154296875
tensor(11533.8154, grad_fn=<NegBackward0>) tensor(11533.8154, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11533.814453125
tensor(11533.8154, grad_fn=<NegBackward0>) tensor(11533.8145, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11533.8271484375
tensor(11533.8145, grad_fn=<NegBackward0>) tensor(11533.8271, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11533.8125
tensor(11533.8145, grad_fn=<NegBackward0>) tensor(11533.8125, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11533.8115234375
tensor(11533.8125, grad_fn=<NegBackward0>) tensor(11533.8115, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11533.8115234375
tensor(11533.8115, grad_fn=<NegBackward0>) tensor(11533.8115, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11533.810546875
tensor(11533.8115, grad_fn=<NegBackward0>) tensor(11533.8105, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11533.810546875
tensor(11533.8105, grad_fn=<NegBackward0>) tensor(11533.8105, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11533.8095703125
tensor(11533.8105, grad_fn=<NegBackward0>) tensor(11533.8096, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11533.810546875
tensor(11533.8096, grad_fn=<NegBackward0>) tensor(11533.8105, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11533.8076171875
tensor(11533.8096, grad_fn=<NegBackward0>) tensor(11533.8076, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11533.810546875
tensor(11533.8076, grad_fn=<NegBackward0>) tensor(11533.8105, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11533.80859375
tensor(11533.8076, grad_fn=<NegBackward0>) tensor(11533.8086, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11533.8125
tensor(11533.8076, grad_fn=<NegBackward0>) tensor(11533.8125, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11533.806640625
tensor(11533.8076, grad_fn=<NegBackward0>) tensor(11533.8066, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11533.8056640625
tensor(11533.8066, grad_fn=<NegBackward0>) tensor(11533.8057, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11533.8056640625
tensor(11533.8057, grad_fn=<NegBackward0>) tensor(11533.8057, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11533.8056640625
tensor(11533.8057, grad_fn=<NegBackward0>) tensor(11533.8057, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11533.80859375
tensor(11533.8057, grad_fn=<NegBackward0>) tensor(11533.8086, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11533.8046875
tensor(11533.8057, grad_fn=<NegBackward0>) tensor(11533.8047, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11533.806640625
tensor(11533.8047, grad_fn=<NegBackward0>) tensor(11533.8066, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11533.8056640625
tensor(11533.8047, grad_fn=<NegBackward0>) tensor(11533.8057, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11533.8076171875
tensor(11533.8047, grad_fn=<NegBackward0>) tensor(11533.8076, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11533.8037109375
tensor(11533.8047, grad_fn=<NegBackward0>) tensor(11533.8037, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11533.8134765625
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8135, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11533.8056640625
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8057, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11533.8046875
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8047, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11533.8037109375
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8037, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11533.8037109375
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8037, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11533.8037109375
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8037, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11533.857421875
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8574, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11533.80078125
tensor(11533.8037, grad_fn=<NegBackward0>) tensor(11533.8008, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11533.8037109375
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8037, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11533.8037109375
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8037, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11533.80078125
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8008, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11533.8017578125
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8018, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11533.82421875
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8242, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11533.8349609375
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8350, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11533.8134765625
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8135, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11533.80859375
tensor(11533.8008, grad_fn=<NegBackward0>) tensor(11533.8086, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.7427, 0.2573],
        [0.2417, 0.7583]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4812, 0.5188], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.1012],
         [0.6549, 0.3995]],

        [[0.5204, 0.0913],
         [0.5869, 0.5951]],

        [[0.5779, 0.0889],
         [0.5363, 0.6387]],

        [[0.5737, 0.1068],
         [0.7000, 0.6011]],

        [[0.6786, 0.1003],
         [0.6861, 0.6770]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320634059188
Average Adjusted Rand Index: 0.9839985580570193
[0.9760961975210904, 0.9840320634059188] [0.9761598395607173, 0.9839985580570193] [11548.3994140625, 11533.80859375]
-------------------------------------
This iteration is 12
True Objective function: Loss = -11344.34362560901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19674.34375
inf tensor(19674.3438, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11990.7587890625
tensor(19674.3438, grad_fn=<NegBackward0>) tensor(11990.7588, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11962.396484375
tensor(11990.7588, grad_fn=<NegBackward0>) tensor(11962.3965, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11437.1123046875
tensor(11962.3965, grad_fn=<NegBackward0>) tensor(11437.1123, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11370.3740234375
tensor(11437.1123, grad_fn=<NegBackward0>) tensor(11370.3740, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11369.509765625
tensor(11370.3740, grad_fn=<NegBackward0>) tensor(11369.5098, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11362.5166015625
tensor(11369.5098, grad_fn=<NegBackward0>) tensor(11362.5166, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11347.0751953125
tensor(11362.5166, grad_fn=<NegBackward0>) tensor(11347.0752, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11336.203125
tensor(11347.0752, grad_fn=<NegBackward0>) tensor(11336.2031, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11336.1552734375
tensor(11336.2031, grad_fn=<NegBackward0>) tensor(11336.1553, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11336.123046875
tensor(11336.1553, grad_fn=<NegBackward0>) tensor(11336.1230, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11336.09765625
tensor(11336.1230, grad_fn=<NegBackward0>) tensor(11336.0977, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11336.0791015625
tensor(11336.0977, grad_fn=<NegBackward0>) tensor(11336.0791, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11336.0654296875
tensor(11336.0791, grad_fn=<NegBackward0>) tensor(11336.0654, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11336.052734375
tensor(11336.0654, grad_fn=<NegBackward0>) tensor(11336.0527, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11336.052734375
tensor(11336.0527, grad_fn=<NegBackward0>) tensor(11336.0527, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11336.03515625
tensor(11336.0527, grad_fn=<NegBackward0>) tensor(11336.0352, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11336.02734375
tensor(11336.0352, grad_fn=<NegBackward0>) tensor(11336.0273, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11336.0224609375
tensor(11336.0273, grad_fn=<NegBackward0>) tensor(11336.0225, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11336.017578125
tensor(11336.0225, grad_fn=<NegBackward0>) tensor(11336.0176, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11336.013671875
tensor(11336.0176, grad_fn=<NegBackward0>) tensor(11336.0137, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11336.009765625
tensor(11336.0137, grad_fn=<NegBackward0>) tensor(11336.0098, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11336.0068359375
tensor(11336.0098, grad_fn=<NegBackward0>) tensor(11336.0068, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11336.005859375
tensor(11336.0068, grad_fn=<NegBackward0>) tensor(11336.0059, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11336.001953125
tensor(11336.0059, grad_fn=<NegBackward0>) tensor(11336.0020, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11335.9990234375
tensor(11336.0020, grad_fn=<NegBackward0>) tensor(11335.9990, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11335.9970703125
tensor(11335.9990, grad_fn=<NegBackward0>) tensor(11335.9971, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11335.99609375
tensor(11335.9971, grad_fn=<NegBackward0>) tensor(11335.9961, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11335.994140625
tensor(11335.9961, grad_fn=<NegBackward0>) tensor(11335.9941, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11335.9921875
tensor(11335.9941, grad_fn=<NegBackward0>) tensor(11335.9922, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11335.990234375
tensor(11335.9922, grad_fn=<NegBackward0>) tensor(11335.9902, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11335.9892578125
tensor(11335.9902, grad_fn=<NegBackward0>) tensor(11335.9893, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11335.9873046875
tensor(11335.9893, grad_fn=<NegBackward0>) tensor(11335.9873, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11335.9794921875
tensor(11335.9873, grad_fn=<NegBackward0>) tensor(11335.9795, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11335.9775390625
tensor(11335.9795, grad_fn=<NegBackward0>) tensor(11335.9775, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11335.984375
tensor(11335.9775, grad_fn=<NegBackward0>) tensor(11335.9844, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11335.9765625
tensor(11335.9775, grad_fn=<NegBackward0>) tensor(11335.9766, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11335.974609375
tensor(11335.9766, grad_fn=<NegBackward0>) tensor(11335.9746, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11335.9765625
tensor(11335.9746, grad_fn=<NegBackward0>) tensor(11335.9766, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11335.974609375
tensor(11335.9746, grad_fn=<NegBackward0>) tensor(11335.9746, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11335.974609375
tensor(11335.9746, grad_fn=<NegBackward0>) tensor(11335.9746, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11335.974609375
tensor(11335.9746, grad_fn=<NegBackward0>) tensor(11335.9746, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11335.9736328125
tensor(11335.9746, grad_fn=<NegBackward0>) tensor(11335.9736, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11335.97265625
tensor(11335.9736, grad_fn=<NegBackward0>) tensor(11335.9727, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11335.97265625
tensor(11335.9727, grad_fn=<NegBackward0>) tensor(11335.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11335.97265625
tensor(11335.9727, grad_fn=<NegBackward0>) tensor(11335.9727, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11335.9716796875
tensor(11335.9727, grad_fn=<NegBackward0>) tensor(11335.9717, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11335.9736328125
tensor(11335.9717, grad_fn=<NegBackward0>) tensor(11335.9736, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11335.970703125
tensor(11335.9717, grad_fn=<NegBackward0>) tensor(11335.9707, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11335.9697265625
tensor(11335.9707, grad_fn=<NegBackward0>) tensor(11335.9697, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11335.970703125
tensor(11335.9697, grad_fn=<NegBackward0>) tensor(11335.9707, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11335.970703125
tensor(11335.9697, grad_fn=<NegBackward0>) tensor(11335.9707, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11335.9697265625
tensor(11335.9697, grad_fn=<NegBackward0>) tensor(11335.9697, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11335.96875
tensor(11335.9697, grad_fn=<NegBackward0>) tensor(11335.9688, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11335.9697265625
tensor(11335.9688, grad_fn=<NegBackward0>) tensor(11335.9697, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11335.9677734375
tensor(11335.9688, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11335.9775390625
tensor(11335.9678, grad_fn=<NegBackward0>) tensor(11335.9775, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11335.9677734375
tensor(11335.9678, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11335.96875
tensor(11335.9678, grad_fn=<NegBackward0>) tensor(11335.9688, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11335.96875
tensor(11335.9678, grad_fn=<NegBackward0>) tensor(11335.9688, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11335.970703125
tensor(11335.9678, grad_fn=<NegBackward0>) tensor(11335.9707, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -11335.966796875
tensor(11335.9678, grad_fn=<NegBackward0>) tensor(11335.9668, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11335.9677734375
tensor(11335.9668, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11335.9677734375
tensor(11335.9668, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11335.9677734375
tensor(11335.9668, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11335.9677734375
tensor(11335.9668, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11335.9658203125
tensor(11335.9668, grad_fn=<NegBackward0>) tensor(11335.9658, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11335.9677734375
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11335.9697265625
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9697, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11335.966796875
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9668, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11335.9658203125
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9658, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11335.9677734375
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11335.9677734375
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11335.9658203125
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9658, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11335.966796875
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9668, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11335.96875
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9688, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11335.9658203125
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9658, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11335.966796875
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9668, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11335.9677734375
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11335.9716796875
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9717, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11335.96484375
tensor(11335.9658, grad_fn=<NegBackward0>) tensor(11335.9648, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11335.966796875
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11335.9668, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11335.9853515625
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11335.9854, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11335.96484375
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11335.9648, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11335.9658203125
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11335.9658, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11336.0
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11336., grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11335.9677734375
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11335.9678, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11335.9658203125
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11335.9658, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11335.9658203125
tensor(11335.9648, grad_fn=<NegBackward0>) tensor(11335.9658, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.7197, 0.2803],
        [0.2043, 0.7957]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4798, 0.5202], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3937, 0.0959],
         [0.6881, 0.2061]],

        [[0.5124, 0.1001],
         [0.5504, 0.6035]],

        [[0.5578, 0.1059],
         [0.5206, 0.7036]],

        [[0.6913, 0.0922],
         [0.5693, 0.6164]],

        [[0.7269, 0.0951],
         [0.7287, 0.6383]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919990296688364
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21384.0625
inf tensor(21384.0625, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11989.1513671875
tensor(21384.0625, grad_fn=<NegBackward0>) tensor(11989.1514, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11971.61328125
tensor(11989.1514, grad_fn=<NegBackward0>) tensor(11971.6133, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11489.6650390625
tensor(11971.6133, grad_fn=<NegBackward0>) tensor(11489.6650, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11389.4853515625
tensor(11489.6650, grad_fn=<NegBackward0>) tensor(11389.4854, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11368.228515625
tensor(11389.4854, grad_fn=<NegBackward0>) tensor(11368.2285, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11353.5888671875
tensor(11368.2285, grad_fn=<NegBackward0>) tensor(11353.5889, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11341.9267578125
tensor(11353.5889, grad_fn=<NegBackward0>) tensor(11341.9268, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11336.310546875
tensor(11341.9268, grad_fn=<NegBackward0>) tensor(11336.3105, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11336.267578125
tensor(11336.3105, grad_fn=<NegBackward0>) tensor(11336.2676, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11336.23828125
tensor(11336.2676, grad_fn=<NegBackward0>) tensor(11336.2383, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11336.216796875
tensor(11336.2383, grad_fn=<NegBackward0>) tensor(11336.2168, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11336.2001953125
tensor(11336.2168, grad_fn=<NegBackward0>) tensor(11336.2002, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11336.189453125
tensor(11336.2002, grad_fn=<NegBackward0>) tensor(11336.1895, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11336.1787109375
tensor(11336.1895, grad_fn=<NegBackward0>) tensor(11336.1787, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11336.1708984375
tensor(11336.1787, grad_fn=<NegBackward0>) tensor(11336.1709, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11336.1640625
tensor(11336.1709, grad_fn=<NegBackward0>) tensor(11336.1641, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11336.1591796875
tensor(11336.1641, grad_fn=<NegBackward0>) tensor(11336.1592, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11336.1533203125
tensor(11336.1592, grad_fn=<NegBackward0>) tensor(11336.1533, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11336.1494140625
tensor(11336.1533, grad_fn=<NegBackward0>) tensor(11336.1494, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11336.146484375
tensor(11336.1494, grad_fn=<NegBackward0>) tensor(11336.1465, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11336.1435546875
tensor(11336.1465, grad_fn=<NegBackward0>) tensor(11336.1436, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11336.1416015625
tensor(11336.1436, grad_fn=<NegBackward0>) tensor(11336.1416, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11336.138671875
tensor(11336.1416, grad_fn=<NegBackward0>) tensor(11336.1387, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11336.13671875
tensor(11336.1387, grad_fn=<NegBackward0>) tensor(11336.1367, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11336.134765625
tensor(11336.1367, grad_fn=<NegBackward0>) tensor(11336.1348, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11336.1337890625
tensor(11336.1348, grad_fn=<NegBackward0>) tensor(11336.1338, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11336.1328125
tensor(11336.1338, grad_fn=<NegBackward0>) tensor(11336.1328, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11336.130859375
tensor(11336.1328, grad_fn=<NegBackward0>) tensor(11336.1309, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11336.12890625
tensor(11336.1309, grad_fn=<NegBackward0>) tensor(11336.1289, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11336.1279296875
tensor(11336.1289, grad_fn=<NegBackward0>) tensor(11336.1279, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11336.1279296875
tensor(11336.1279, grad_fn=<NegBackward0>) tensor(11336.1279, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11336.126953125
tensor(11336.1279, grad_fn=<NegBackward0>) tensor(11336.1270, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11336.1240234375
tensor(11336.1270, grad_fn=<NegBackward0>) tensor(11336.1240, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11336.123046875
tensor(11336.1240, grad_fn=<NegBackward0>) tensor(11336.1230, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11336.12109375
tensor(11336.1230, grad_fn=<NegBackward0>) tensor(11336.1211, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11336.1220703125
tensor(11336.1211, grad_fn=<NegBackward0>) tensor(11336.1221, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11336.119140625
tensor(11336.1211, grad_fn=<NegBackward0>) tensor(11336.1191, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11336.1142578125
tensor(11336.1191, grad_fn=<NegBackward0>) tensor(11336.1143, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11336.1123046875
tensor(11336.1143, grad_fn=<NegBackward0>) tensor(11336.1123, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11336.111328125
tensor(11336.1123, grad_fn=<NegBackward0>) tensor(11336.1113, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11336.1103515625
tensor(11336.1113, grad_fn=<NegBackward0>) tensor(11336.1104, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11336.1103515625
tensor(11336.1104, grad_fn=<NegBackward0>) tensor(11336.1104, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11336.109375
tensor(11336.1104, grad_fn=<NegBackward0>) tensor(11336.1094, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11336.1083984375
tensor(11336.1094, grad_fn=<NegBackward0>) tensor(11336.1084, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11336.109375
tensor(11336.1084, grad_fn=<NegBackward0>) tensor(11336.1094, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11336.109375
tensor(11336.1084, grad_fn=<NegBackward0>) tensor(11336.1094, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11336.1083984375
tensor(11336.1084, grad_fn=<NegBackward0>) tensor(11336.1084, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11336.107421875
tensor(11336.1084, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11336.107421875
tensor(11336.1074, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11336.107421875
tensor(11336.1074, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11336.107421875
tensor(11336.1074, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11336.107421875
tensor(11336.1074, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11336.1064453125
tensor(11336.1074, grad_fn=<NegBackward0>) tensor(11336.1064, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11336.107421875
tensor(11336.1064, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11336.107421875
tensor(11336.1064, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11336.1064453125
tensor(11336.1064, grad_fn=<NegBackward0>) tensor(11336.1064, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11336.107421875
tensor(11336.1064, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11336.10546875
tensor(11336.1064, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11336.10546875
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11336.10546875
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11336.10546875
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11336.10546875
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11336.10546875
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11336.1064453125
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1064, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11336.1064453125
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1064, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11336.109375
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1094, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11336.1044921875
tensor(11336.1055, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11336.10546875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11336.1044921875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11336.10546875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11336.1103515625
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1104, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11336.1044921875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11336.1044921875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11336.1044921875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11336.1044921875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11336.10546875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11336.10546875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11336.1044921875
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11336.103515625
tensor(11336.1045, grad_fn=<NegBackward0>) tensor(11336.1035, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11336.107421875
tensor(11336.1035, grad_fn=<NegBackward0>) tensor(11336.1074, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11336.14453125
tensor(11336.1035, grad_fn=<NegBackward0>) tensor(11336.1445, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11336.10546875
tensor(11336.1035, grad_fn=<NegBackward0>) tensor(11336.1055, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11336.1044921875
tensor(11336.1035, grad_fn=<NegBackward0>) tensor(11336.1045, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -11336.1298828125
tensor(11336.1035, grad_fn=<NegBackward0>) tensor(11336.1299, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.7199, 0.2801],
        [0.2024, 0.7976]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3947, 0.0958],
         [0.6710, 0.2061]],

        [[0.6516, 0.1001],
         [0.6080, 0.5262]],

        [[0.6652, 0.1059],
         [0.5791, 0.5361]],

        [[0.7244, 0.0922],
         [0.5042, 0.5968]],

        [[0.6324, 0.0951],
         [0.5021, 0.5458]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919990296688364
Average Adjusted Rand Index: 0.9919995611635631
[0.9919990296688364, 0.9919990296688364] [0.9919995611635631, 0.9919995611635631] [11335.9658203125, 11336.1298828125]
-------------------------------------
This iteration is 13
True Objective function: Loss = -11807.492711290199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20222.919921875
inf tensor(20222.9199, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12670.0908203125
tensor(20222.9199, grad_fn=<NegBackward0>) tensor(12670.0908, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12001.7275390625
tensor(12670.0908, grad_fn=<NegBackward0>) tensor(12001.7275, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11863.515625
tensor(12001.7275, grad_fn=<NegBackward0>) tensor(11863.5156, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11858.9208984375
tensor(11863.5156, grad_fn=<NegBackward0>) tensor(11858.9209, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11845.7724609375
tensor(11858.9209, grad_fn=<NegBackward0>) tensor(11845.7725, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11845.5654296875
tensor(11845.7725, grad_fn=<NegBackward0>) tensor(11845.5654, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11845.2587890625
tensor(11845.5654, grad_fn=<NegBackward0>) tensor(11845.2588, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11831.658203125
tensor(11845.2588, grad_fn=<NegBackward0>) tensor(11831.6582, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11826.1748046875
tensor(11831.6582, grad_fn=<NegBackward0>) tensor(11826.1748, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11826.1318359375
tensor(11826.1748, grad_fn=<NegBackward0>) tensor(11826.1318, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11826.09765625
tensor(11826.1318, grad_fn=<NegBackward0>) tensor(11826.0977, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11826.072265625
tensor(11826.0977, grad_fn=<NegBackward0>) tensor(11826.0723, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11826.052734375
tensor(11826.0723, grad_fn=<NegBackward0>) tensor(11826.0527, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11826.0361328125
tensor(11826.0527, grad_fn=<NegBackward0>) tensor(11826.0361, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11826.021484375
tensor(11826.0361, grad_fn=<NegBackward0>) tensor(11826.0215, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11826.0087890625
tensor(11826.0215, grad_fn=<NegBackward0>) tensor(11826.0088, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11825.9951171875
tensor(11826.0088, grad_fn=<NegBackward0>) tensor(11825.9951, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11815.9541015625
tensor(11825.9951, grad_fn=<NegBackward0>) tensor(11815.9541, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11815.9462890625
tensor(11815.9541, grad_fn=<NegBackward0>) tensor(11815.9463, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11815.9404296875
tensor(11815.9463, grad_fn=<NegBackward0>) tensor(11815.9404, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11815.9345703125
tensor(11815.9404, grad_fn=<NegBackward0>) tensor(11815.9346, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11815.9296875
tensor(11815.9346, grad_fn=<NegBackward0>) tensor(11815.9297, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11815.9248046875
tensor(11815.9297, grad_fn=<NegBackward0>) tensor(11815.9248, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11815.921875
tensor(11815.9248, grad_fn=<NegBackward0>) tensor(11815.9219, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11815.91796875
tensor(11815.9219, grad_fn=<NegBackward0>) tensor(11815.9180, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11815.916015625
tensor(11815.9180, grad_fn=<NegBackward0>) tensor(11815.9160, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11815.912109375
tensor(11815.9160, grad_fn=<NegBackward0>) tensor(11815.9121, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11815.91015625
tensor(11815.9121, grad_fn=<NegBackward0>) tensor(11815.9102, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11815.9072265625
tensor(11815.9102, grad_fn=<NegBackward0>) tensor(11815.9072, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11815.9072265625
tensor(11815.9072, grad_fn=<NegBackward0>) tensor(11815.9072, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11815.9052734375
tensor(11815.9072, grad_fn=<NegBackward0>) tensor(11815.9053, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11815.90234375
tensor(11815.9053, grad_fn=<NegBackward0>) tensor(11815.9023, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11815.9013671875
tensor(11815.9023, grad_fn=<NegBackward0>) tensor(11815.9014, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11815.900390625
tensor(11815.9014, grad_fn=<NegBackward0>) tensor(11815.9004, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11815.8984375
tensor(11815.9004, grad_fn=<NegBackward0>) tensor(11815.8984, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11815.8974609375
tensor(11815.8984, grad_fn=<NegBackward0>) tensor(11815.8975, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11815.8984375
tensor(11815.8975, grad_fn=<NegBackward0>) tensor(11815.8984, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11815.8974609375
tensor(11815.8975, grad_fn=<NegBackward0>) tensor(11815.8975, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11815.89453125
tensor(11815.8975, grad_fn=<NegBackward0>) tensor(11815.8945, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11815.8935546875
tensor(11815.8945, grad_fn=<NegBackward0>) tensor(11815.8936, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11815.8916015625
tensor(11815.8936, grad_fn=<NegBackward0>) tensor(11815.8916, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11815.890625
tensor(11815.8916, grad_fn=<NegBackward0>) tensor(11815.8906, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11815.888671875
tensor(11815.8906, grad_fn=<NegBackward0>) tensor(11815.8887, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11814.6591796875
tensor(11815.8887, grad_fn=<NegBackward0>) tensor(11814.6592, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11814.44140625
tensor(11814.6592, grad_fn=<NegBackward0>) tensor(11814.4414, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11814.4365234375
tensor(11814.4414, grad_fn=<NegBackward0>) tensor(11814.4365, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11804.4453125
tensor(11814.4365, grad_fn=<NegBackward0>) tensor(11804.4453, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11804.4462890625
tensor(11804.4453, grad_fn=<NegBackward0>) tensor(11804.4463, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11804.4462890625
tensor(11804.4453, grad_fn=<NegBackward0>) tensor(11804.4463, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11804.4443359375
tensor(11804.4453, grad_fn=<NegBackward0>) tensor(11804.4443, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11804.447265625
tensor(11804.4443, grad_fn=<NegBackward0>) tensor(11804.4473, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11804.4443359375
tensor(11804.4443, grad_fn=<NegBackward0>) tensor(11804.4443, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11804.443359375
tensor(11804.4443, grad_fn=<NegBackward0>) tensor(11804.4434, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11804.443359375
tensor(11804.4434, grad_fn=<NegBackward0>) tensor(11804.4434, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11804.44140625
tensor(11804.4434, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11804.44140625
tensor(11804.4414, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11804.4423828125
tensor(11804.4414, grad_fn=<NegBackward0>) tensor(11804.4424, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11804.4404296875
tensor(11804.4414, grad_fn=<NegBackward0>) tensor(11804.4404, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11804.44140625
tensor(11804.4404, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11804.44140625
tensor(11804.4404, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11804.4404296875
tensor(11804.4404, grad_fn=<NegBackward0>) tensor(11804.4404, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11804.4560546875
tensor(11804.4404, grad_fn=<NegBackward0>) tensor(11804.4561, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11804.44140625
tensor(11804.4404, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11804.4404296875
tensor(11804.4404, grad_fn=<NegBackward0>) tensor(11804.4404, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11804.439453125
tensor(11804.4404, grad_fn=<NegBackward0>) tensor(11804.4395, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11804.4404296875
tensor(11804.4395, grad_fn=<NegBackward0>) tensor(11804.4404, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11804.44140625
tensor(11804.4395, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11804.44140625
tensor(11804.4395, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11804.4384765625
tensor(11804.4395, grad_fn=<NegBackward0>) tensor(11804.4385, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11804.4423828125
tensor(11804.4385, grad_fn=<NegBackward0>) tensor(11804.4424, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11804.4404296875
tensor(11804.4385, grad_fn=<NegBackward0>) tensor(11804.4404, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11804.44140625
tensor(11804.4385, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11804.505859375
tensor(11804.4385, grad_fn=<NegBackward0>) tensor(11804.5059, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11804.44140625
tensor(11804.4385, grad_fn=<NegBackward0>) tensor(11804.4414, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.7416, 0.2584],
        [0.3001, 0.6999]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5302, 0.4698], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4059, 0.1020],
         [0.6802, 0.1982]],

        [[0.5357, 0.1035],
         [0.6081, 0.5032]],

        [[0.5041, 0.0999],
         [0.5134, 0.5400]],

        [[0.5246, 0.1035],
         [0.5351, 0.6306]],

        [[0.6625, 0.1004],
         [0.6149, 0.7112]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21380.197265625
inf tensor(21380.1973, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12711.4443359375
tensor(21380.1973, grad_fn=<NegBackward0>) tensor(12711.4443, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12433.0458984375
tensor(12711.4443, grad_fn=<NegBackward0>) tensor(12433.0459, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12006.3056640625
tensor(12433.0459, grad_fn=<NegBackward0>) tensor(12006.3057, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11955.5810546875
tensor(12006.3057, grad_fn=<NegBackward0>) tensor(11955.5811, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11935.3662109375
tensor(11955.5811, grad_fn=<NegBackward0>) tensor(11935.3662, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11912.8955078125
tensor(11935.3662, grad_fn=<NegBackward0>) tensor(11912.8955, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11896.3330078125
tensor(11912.8955, grad_fn=<NegBackward0>) tensor(11896.3330, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11885.8115234375
tensor(11896.3330, grad_fn=<NegBackward0>) tensor(11885.8115, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11885.7099609375
tensor(11885.8115, grad_fn=<NegBackward0>) tensor(11885.7100, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11869.8134765625
tensor(11885.7100, grad_fn=<NegBackward0>) tensor(11869.8135, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11859.9892578125
tensor(11869.8135, grad_fn=<NegBackward0>) tensor(11859.9893, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11859.947265625
tensor(11859.9893, grad_fn=<NegBackward0>) tensor(11859.9473, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11859.919921875
tensor(11859.9473, grad_fn=<NegBackward0>) tensor(11859.9199, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11859.8974609375
tensor(11859.9199, grad_fn=<NegBackward0>) tensor(11859.8975, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11859.86328125
tensor(11859.8975, grad_fn=<NegBackward0>) tensor(11859.8633, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11846.841796875
tensor(11859.8633, grad_fn=<NegBackward0>) tensor(11846.8418, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11846.8271484375
tensor(11846.8418, grad_fn=<NegBackward0>) tensor(11846.8271, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11835.47265625
tensor(11846.8271, grad_fn=<NegBackward0>) tensor(11835.4727, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11835.4404296875
tensor(11835.4727, grad_fn=<NegBackward0>) tensor(11835.4404, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11835.4306640625
tensor(11835.4404, grad_fn=<NegBackward0>) tensor(11835.4307, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11835.4248046875
tensor(11835.4307, grad_fn=<NegBackward0>) tensor(11835.4248, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11835.41796875
tensor(11835.4248, grad_fn=<NegBackward0>) tensor(11835.4180, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11835.412109375
tensor(11835.4180, grad_fn=<NegBackward0>) tensor(11835.4121, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11835.40625
tensor(11835.4121, grad_fn=<NegBackward0>) tensor(11835.4062, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11835.404296875
tensor(11835.4062, grad_fn=<NegBackward0>) tensor(11835.4043, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11835.3994140625
tensor(11835.4043, grad_fn=<NegBackward0>) tensor(11835.3994, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11835.396484375
tensor(11835.3994, grad_fn=<NegBackward0>) tensor(11835.3965, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11835.3857421875
tensor(11835.3965, grad_fn=<NegBackward0>) tensor(11835.3857, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11821.5
tensor(11835.3857, grad_fn=<NegBackward0>) tensor(11821.5000, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11821.48828125
tensor(11821.5000, grad_fn=<NegBackward0>) tensor(11821.4883, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11820.1171875
tensor(11821.4883, grad_fn=<NegBackward0>) tensor(11820.1172, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11820.11328125
tensor(11820.1172, grad_fn=<NegBackward0>) tensor(11820.1133, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11820.1083984375
tensor(11820.1133, grad_fn=<NegBackward0>) tensor(11820.1084, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11820.091796875
tensor(11820.1084, grad_fn=<NegBackward0>) tensor(11820.0918, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11820.087890625
tensor(11820.0918, grad_fn=<NegBackward0>) tensor(11820.0879, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11820.0869140625
tensor(11820.0879, grad_fn=<NegBackward0>) tensor(11820.0869, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11820.0869140625
tensor(11820.0869, grad_fn=<NegBackward0>) tensor(11820.0869, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11820.0849609375
tensor(11820.0869, grad_fn=<NegBackward0>) tensor(11820.0850, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11820.083984375
tensor(11820.0850, grad_fn=<NegBackward0>) tensor(11820.0840, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11820.0830078125
tensor(11820.0840, grad_fn=<NegBackward0>) tensor(11820.0830, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11820.0810546875
tensor(11820.0830, grad_fn=<NegBackward0>) tensor(11820.0811, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11820.0966796875
tensor(11820.0811, grad_fn=<NegBackward0>) tensor(11820.0967, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11820.0810546875
tensor(11820.0811, grad_fn=<NegBackward0>) tensor(11820.0811, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11820.080078125
tensor(11820.0811, grad_fn=<NegBackward0>) tensor(11820.0801, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11820.0791015625
tensor(11820.0801, grad_fn=<NegBackward0>) tensor(11820.0791, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11820.0771484375
tensor(11820.0791, grad_fn=<NegBackward0>) tensor(11820.0771, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11820.0791015625
tensor(11820.0771, grad_fn=<NegBackward0>) tensor(11820.0791, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11820.083984375
tensor(11820.0771, grad_fn=<NegBackward0>) tensor(11820.0840, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11820.0751953125
tensor(11820.0771, grad_fn=<NegBackward0>) tensor(11820.0752, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11820.0751953125
tensor(11820.0752, grad_fn=<NegBackward0>) tensor(11820.0752, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11820.0751953125
tensor(11820.0752, grad_fn=<NegBackward0>) tensor(11820.0752, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11820.0947265625
tensor(11820.0752, grad_fn=<NegBackward0>) tensor(11820.0947, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11820.0732421875
tensor(11820.0752, grad_fn=<NegBackward0>) tensor(11820.0732, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11820.07421875
tensor(11820.0732, grad_fn=<NegBackward0>) tensor(11820.0742, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11814.4482421875
tensor(11820.0732, grad_fn=<NegBackward0>) tensor(11814.4482, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11814.4462890625
tensor(11814.4482, grad_fn=<NegBackward0>) tensor(11814.4463, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11814.4443359375
tensor(11814.4463, grad_fn=<NegBackward0>) tensor(11814.4443, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11814.443359375
tensor(11814.4443, grad_fn=<NegBackward0>) tensor(11814.4434, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11814.4443359375
tensor(11814.4434, grad_fn=<NegBackward0>) tensor(11814.4443, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11814.4443359375
tensor(11814.4434, grad_fn=<NegBackward0>) tensor(11814.4443, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11814.4443359375
tensor(11814.4434, grad_fn=<NegBackward0>) tensor(11814.4443, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11814.4462890625
tensor(11814.4434, grad_fn=<NegBackward0>) tensor(11814.4463, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -11814.4423828125
tensor(11814.4434, grad_fn=<NegBackward0>) tensor(11814.4424, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11814.443359375
tensor(11814.4424, grad_fn=<NegBackward0>) tensor(11814.4434, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11814.4453125
tensor(11814.4424, grad_fn=<NegBackward0>) tensor(11814.4453, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11814.4462890625
tensor(11814.4424, grad_fn=<NegBackward0>) tensor(11814.4463, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11814.4453125
tensor(11814.4424, grad_fn=<NegBackward0>) tensor(11814.4453, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11814.443359375
tensor(11814.4424, grad_fn=<NegBackward0>) tensor(11814.4434, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.6969, 0.3031],
        [0.2628, 0.7372]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4698, 0.5302], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.1020],
         [0.5330, 0.4067]],

        [[0.5615, 0.1077],
         [0.7004, 0.6938]],

        [[0.7053, 0.0999],
         [0.5064, 0.5153]],

        [[0.5264, 0.1034],
         [0.5810, 0.6518]],

        [[0.6793, 0.1004],
         [0.5076, 0.6137]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919998665493138
Average Adjusted Rand Index: 0.9919996552039955
[1.0, 0.9919998665493138] [1.0, 0.9919996552039955] [11804.44140625, 11814.443359375]
-------------------------------------
This iteration is 14
True Objective function: Loss = -11464.430354957021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23678.521484375
inf tensor(23678.5215, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11886.0986328125
tensor(23678.5215, grad_fn=<NegBackward0>) tensor(11886.0986, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11713.3505859375
tensor(11886.0986, grad_fn=<NegBackward0>) tensor(11713.3506, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11703.7705078125
tensor(11713.3506, grad_fn=<NegBackward0>) tensor(11703.7705, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11695.6923828125
tensor(11703.7705, grad_fn=<NegBackward0>) tensor(11695.6924, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11688.12890625
tensor(11695.6924, grad_fn=<NegBackward0>) tensor(11688.1289, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11687.990234375
tensor(11688.1289, grad_fn=<NegBackward0>) tensor(11687.9902, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11687.9140625
tensor(11687.9902, grad_fn=<NegBackward0>) tensor(11687.9141, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11687.869140625
tensor(11687.9141, grad_fn=<NegBackward0>) tensor(11687.8691, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11687.837890625
tensor(11687.8691, grad_fn=<NegBackward0>) tensor(11687.8379, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11687.814453125
tensor(11687.8379, grad_fn=<NegBackward0>) tensor(11687.8145, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11687.7958984375
tensor(11687.8145, grad_fn=<NegBackward0>) tensor(11687.7959, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11687.78125
tensor(11687.7959, grad_fn=<NegBackward0>) tensor(11687.7812, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11687.7646484375
tensor(11687.7812, grad_fn=<NegBackward0>) tensor(11687.7646, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11687.7568359375
tensor(11687.7646, grad_fn=<NegBackward0>) tensor(11687.7568, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11687.748046875
tensor(11687.7568, grad_fn=<NegBackward0>) tensor(11687.7480, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11687.7421875
tensor(11687.7480, grad_fn=<NegBackward0>) tensor(11687.7422, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11687.7373046875
tensor(11687.7422, grad_fn=<NegBackward0>) tensor(11687.7373, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11687.7333984375
tensor(11687.7373, grad_fn=<NegBackward0>) tensor(11687.7334, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11687.7294921875
tensor(11687.7334, grad_fn=<NegBackward0>) tensor(11687.7295, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11687.7255859375
tensor(11687.7295, grad_fn=<NegBackward0>) tensor(11687.7256, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11687.72265625
tensor(11687.7256, grad_fn=<NegBackward0>) tensor(11687.7227, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11687.7197265625
tensor(11687.7227, grad_fn=<NegBackward0>) tensor(11687.7197, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11687.7177734375
tensor(11687.7197, grad_fn=<NegBackward0>) tensor(11687.7178, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11687.7158203125
tensor(11687.7178, grad_fn=<NegBackward0>) tensor(11687.7158, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11687.7138671875
tensor(11687.7158, grad_fn=<NegBackward0>) tensor(11687.7139, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11687.7119140625
tensor(11687.7139, grad_fn=<NegBackward0>) tensor(11687.7119, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11687.7109375
tensor(11687.7119, grad_fn=<NegBackward0>) tensor(11687.7109, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11687.7080078125
tensor(11687.7109, grad_fn=<NegBackward0>) tensor(11687.7080, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11687.70703125
tensor(11687.7080, grad_fn=<NegBackward0>) tensor(11687.7070, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11687.7060546875
tensor(11687.7070, grad_fn=<NegBackward0>) tensor(11687.7061, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11687.705078125
tensor(11687.7061, grad_fn=<NegBackward0>) tensor(11687.7051, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11687.7041015625
tensor(11687.7051, grad_fn=<NegBackward0>) tensor(11687.7041, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11687.7021484375
tensor(11687.7041, grad_fn=<NegBackward0>) tensor(11687.7021, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11687.701171875
tensor(11687.7021, grad_fn=<NegBackward0>) tensor(11687.7012, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11687.6982421875
tensor(11687.7012, grad_fn=<NegBackward0>) tensor(11687.6982, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11687.6943359375
tensor(11687.6982, grad_fn=<NegBackward0>) tensor(11687.6943, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11687.689453125
tensor(11687.6943, grad_fn=<NegBackward0>) tensor(11687.6895, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11687.6669921875
tensor(11687.6895, grad_fn=<NegBackward0>) tensor(11687.6670, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11684.6923828125
tensor(11687.6670, grad_fn=<NegBackward0>) tensor(11684.6924, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11680.232421875
tensor(11684.6924, grad_fn=<NegBackward0>) tensor(11680.2324, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11679.68359375
tensor(11680.2324, grad_fn=<NegBackward0>) tensor(11679.6836, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11679.62890625
tensor(11679.6836, grad_fn=<NegBackward0>) tensor(11679.6289, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11679.611328125
tensor(11679.6289, grad_fn=<NegBackward0>) tensor(11679.6113, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11679.6015625
tensor(11679.6113, grad_fn=<NegBackward0>) tensor(11679.6016, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11679.599609375
tensor(11679.6016, grad_fn=<NegBackward0>) tensor(11679.5996, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11679.5986328125
tensor(11679.5996, grad_fn=<NegBackward0>) tensor(11679.5986, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11679.5966796875
tensor(11679.5986, grad_fn=<NegBackward0>) tensor(11679.5967, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11679.595703125
tensor(11679.5967, grad_fn=<NegBackward0>) tensor(11679.5957, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11679.521484375
tensor(11679.5957, grad_fn=<NegBackward0>) tensor(11679.5215, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11679.5185546875
tensor(11679.5215, grad_fn=<NegBackward0>) tensor(11679.5186, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11679.51953125
tensor(11679.5186, grad_fn=<NegBackward0>) tensor(11679.5195, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11679.5166015625
tensor(11679.5186, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11679.5185546875
tensor(11679.5166, grad_fn=<NegBackward0>) tensor(11679.5186, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11679.515625
tensor(11679.5166, grad_fn=<NegBackward0>) tensor(11679.5156, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11679.5419921875
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5420, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11679.5166015625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11679.533203125
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5332, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11679.5166015625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11679.515625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5156, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11679.5166015625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11679.517578125
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5176, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11679.5166015625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11679.5166015625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11679.515625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5156, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11679.515625
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5156, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11679.517578125
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5176, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11679.513671875
tensor(11679.5156, grad_fn=<NegBackward0>) tensor(11679.5137, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11679.5166015625
tensor(11679.5137, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11679.5146484375
tensor(11679.5137, grad_fn=<NegBackward0>) tensor(11679.5146, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11679.5166015625
tensor(11679.5137, grad_fn=<NegBackward0>) tensor(11679.5166, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11679.5146484375
tensor(11679.5137, grad_fn=<NegBackward0>) tensor(11679.5146, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11679.515625
tensor(11679.5137, grad_fn=<NegBackward0>) tensor(11679.5156, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.6066, 0.3934],
        [0.3545, 0.6455]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8901, 0.1099], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2072, 0.1047],
         [0.5332, 0.3883]],

        [[0.5941, 0.1075],
         [0.6972, 0.7003]],

        [[0.6987, 0.0936],
         [0.5616, 0.5444]],

        [[0.6816, 0.0934],
         [0.7142, 0.5888]],

        [[0.6134, 0.0988],
         [0.6936, 0.6445]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.006482982171799027
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 24
Adjusted Rand Index: 0.262267233987478
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.22890546210656978
Average Adjusted Rand Index: 0.6511568503631358
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21220.177734375
inf tensor(21220.1777, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12225.865234375
tensor(21220.1777, grad_fn=<NegBackward0>) tensor(12225.8652, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11775.7138671875
tensor(12225.8652, grad_fn=<NegBackward0>) tensor(11775.7139, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11655.8037109375
tensor(11775.7139, grad_fn=<NegBackward0>) tensor(11655.8037, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11616.7099609375
tensor(11655.8037, grad_fn=<NegBackward0>) tensor(11616.7100, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11615.5654296875
tensor(11616.7100, grad_fn=<NegBackward0>) tensor(11615.5654, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11608.8388671875
tensor(11615.5654, grad_fn=<NegBackward0>) tensor(11608.8389, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11605.4013671875
tensor(11608.8389, grad_fn=<NegBackward0>) tensor(11605.4014, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11605.3203125
tensor(11605.4014, grad_fn=<NegBackward0>) tensor(11605.3203, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11605.2666015625
tensor(11605.3203, grad_fn=<NegBackward0>) tensor(11605.2666, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11605.2275390625
tensor(11605.2666, grad_fn=<NegBackward0>) tensor(11605.2275, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11605.1982421875
tensor(11605.2275, grad_fn=<NegBackward0>) tensor(11605.1982, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11605.1748046875
tensor(11605.1982, grad_fn=<NegBackward0>) tensor(11605.1748, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11605.1572265625
tensor(11605.1748, grad_fn=<NegBackward0>) tensor(11605.1572, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11605.142578125
tensor(11605.1572, grad_fn=<NegBackward0>) tensor(11605.1426, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11605.130859375
tensor(11605.1426, grad_fn=<NegBackward0>) tensor(11605.1309, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11605.12109375
tensor(11605.1309, grad_fn=<NegBackward0>) tensor(11605.1211, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11605.1123046875
tensor(11605.1211, grad_fn=<NegBackward0>) tensor(11605.1123, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11605.10546875
tensor(11605.1123, grad_fn=<NegBackward0>) tensor(11605.1055, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11605.0986328125
tensor(11605.1055, grad_fn=<NegBackward0>) tensor(11605.0986, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11605.09375
tensor(11605.0986, grad_fn=<NegBackward0>) tensor(11605.0938, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11605.087890625
tensor(11605.0938, grad_fn=<NegBackward0>) tensor(11605.0879, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11605.083984375
tensor(11605.0879, grad_fn=<NegBackward0>) tensor(11605.0840, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11605.0810546875
tensor(11605.0840, grad_fn=<NegBackward0>) tensor(11605.0811, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11605.078125
tensor(11605.0811, grad_fn=<NegBackward0>) tensor(11605.0781, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11605.0751953125
tensor(11605.0781, grad_fn=<NegBackward0>) tensor(11605.0752, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11605.0732421875
tensor(11605.0752, grad_fn=<NegBackward0>) tensor(11605.0732, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11605.0693359375
tensor(11605.0732, grad_fn=<NegBackward0>) tensor(11605.0693, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11605.068359375
tensor(11605.0693, grad_fn=<NegBackward0>) tensor(11605.0684, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11605.0654296875
tensor(11605.0684, grad_fn=<NegBackward0>) tensor(11605.0654, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11605.064453125
tensor(11605.0654, grad_fn=<NegBackward0>) tensor(11605.0645, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11595.32421875
tensor(11605.0645, grad_fn=<NegBackward0>) tensor(11595.3242, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11595.314453125
tensor(11595.3242, grad_fn=<NegBackward0>) tensor(11595.3145, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11595.3251953125
tensor(11595.3145, grad_fn=<NegBackward0>) tensor(11595.3252, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11595.3125
tensor(11595.3145, grad_fn=<NegBackward0>) tensor(11595.3125, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11595.310546875
tensor(11595.3125, grad_fn=<NegBackward0>) tensor(11595.3105, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11595.3095703125
tensor(11595.3105, grad_fn=<NegBackward0>) tensor(11595.3096, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11595.3076171875
tensor(11595.3096, grad_fn=<NegBackward0>) tensor(11595.3076, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11595.30859375
tensor(11595.3076, grad_fn=<NegBackward0>) tensor(11595.3086, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11595.3076171875
tensor(11595.3076, grad_fn=<NegBackward0>) tensor(11595.3076, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11595.306640625
tensor(11595.3076, grad_fn=<NegBackward0>) tensor(11595.3066, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11595.3056640625
tensor(11595.3066, grad_fn=<NegBackward0>) tensor(11595.3057, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11595.3037109375
tensor(11595.3057, grad_fn=<NegBackward0>) tensor(11595.3037, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11595.3037109375
tensor(11595.3037, grad_fn=<NegBackward0>) tensor(11595.3037, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11595.302734375
tensor(11595.3037, grad_fn=<NegBackward0>) tensor(11595.3027, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11584.439453125
tensor(11595.3027, grad_fn=<NegBackward0>) tensor(11584.4395, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11584.4267578125
tensor(11584.4395, grad_fn=<NegBackward0>) tensor(11584.4268, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11584.427734375
tensor(11584.4268, grad_fn=<NegBackward0>) tensor(11584.4277, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11584.4248046875
tensor(11584.4268, grad_fn=<NegBackward0>) tensor(11584.4248, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11584.4267578125
tensor(11584.4248, grad_fn=<NegBackward0>) tensor(11584.4268, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11584.4248046875
tensor(11584.4248, grad_fn=<NegBackward0>) tensor(11584.4248, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11584.42578125
tensor(11584.4248, grad_fn=<NegBackward0>) tensor(11584.4258, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11584.423828125
tensor(11584.4248, grad_fn=<NegBackward0>) tensor(11584.4238, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11584.423828125
tensor(11584.4238, grad_fn=<NegBackward0>) tensor(11584.4238, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11584.421875
tensor(11584.4238, grad_fn=<NegBackward0>) tensor(11584.4219, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11584.431640625
tensor(11584.4219, grad_fn=<NegBackward0>) tensor(11584.4316, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11584.421875
tensor(11584.4219, grad_fn=<NegBackward0>) tensor(11584.4219, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11584.421875
tensor(11584.4219, grad_fn=<NegBackward0>) tensor(11584.4219, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11584.421875
tensor(11584.4219, grad_fn=<NegBackward0>) tensor(11584.4219, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11584.4208984375
tensor(11584.4219, grad_fn=<NegBackward0>) tensor(11584.4209, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11584.427734375
tensor(11584.4209, grad_fn=<NegBackward0>) tensor(11584.4277, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11579.384765625
tensor(11584.4209, grad_fn=<NegBackward0>) tensor(11579.3848, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11579.3837890625
tensor(11579.3848, grad_fn=<NegBackward0>) tensor(11579.3838, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11579.3837890625
tensor(11579.3838, grad_fn=<NegBackward0>) tensor(11579.3838, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11579.3837890625
tensor(11579.3838, grad_fn=<NegBackward0>) tensor(11579.3838, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11579.3828125
tensor(11579.3838, grad_fn=<NegBackward0>) tensor(11579.3828, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11579.384765625
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3848, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11579.38671875
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3867, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11579.3837890625
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3838, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11579.3876953125
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3877, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11579.3828125
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3828, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11579.38671875
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3867, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11579.3837890625
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3838, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11579.384765625
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3848, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11579.3857421875
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3857, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11579.3828125
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3828, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11579.384765625
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3848, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11579.3818359375
tensor(11579.3828, grad_fn=<NegBackward0>) tensor(11579.3818, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11579.3818359375
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3818, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11579.44140625
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.4414, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11579.3818359375
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3818, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11579.388671875
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3887, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11579.3818359375
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3818, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11579.39453125
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3945, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11579.388671875
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3887, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11579.390625
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3906, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11579.431640625
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.4316, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11579.3818359375
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3818, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11579.380859375
tensor(11579.3818, grad_fn=<NegBackward0>) tensor(11579.3809, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11579.3818359375
tensor(11579.3809, grad_fn=<NegBackward0>) tensor(11579.3818, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11579.4384765625
tensor(11579.3809, grad_fn=<NegBackward0>) tensor(11579.4385, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11579.3828125
tensor(11579.3809, grad_fn=<NegBackward0>) tensor(11579.3828, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11579.408203125
tensor(11579.3809, grad_fn=<NegBackward0>) tensor(11579.4082, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11579.470703125
tensor(11579.3809, grad_fn=<NegBackward0>) tensor(11579.4707, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.6476, 0.3524],
        [0.2243, 0.7757]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 5.4279e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1922, 0.1491],
         [0.5851, 0.3968]],

        [[0.7102, 0.0955],
         [0.7060, 0.7250]],

        [[0.5918, 0.0936],
         [0.6849, 0.5813]],

        [[0.5925, 0.0934],
         [0.5831, 0.6448]],

        [[0.5239, 0.0989],
         [0.5319, 0.5234]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6783525819481542
Average Adjusted Rand Index: 0.8
[0.22890546210656978, 0.6783525819481542] [0.6511568503631358, 0.8] [11679.515625, 11579.470703125]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11501.904779617535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21358.837890625
inf tensor(21358.8379, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12243.880859375
tensor(21358.8379, grad_fn=<NegBackward0>) tensor(12243.8809, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12001.232421875
tensor(12243.8809, grad_fn=<NegBackward0>) tensor(12001.2324, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11571.9169921875
tensor(12001.2324, grad_fn=<NegBackward0>) tensor(11571.9170, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11530.6103515625
tensor(11571.9170, grad_fn=<NegBackward0>) tensor(11530.6104, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11518.9921875
tensor(11530.6104, grad_fn=<NegBackward0>) tensor(11518.9922, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11510.048828125
tensor(11518.9922, grad_fn=<NegBackward0>) tensor(11510.0488, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11496.6513671875
tensor(11510.0488, grad_fn=<NegBackward0>) tensor(11496.6514, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11496.5166015625
tensor(11496.6514, grad_fn=<NegBackward0>) tensor(11496.5166, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11496.4345703125
tensor(11496.5166, grad_fn=<NegBackward0>) tensor(11496.4346, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11496.3759765625
tensor(11496.4346, grad_fn=<NegBackward0>) tensor(11496.3760, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11496.33203125
tensor(11496.3760, grad_fn=<NegBackward0>) tensor(11496.3320, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11496.298828125
tensor(11496.3320, grad_fn=<NegBackward0>) tensor(11496.2988, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11496.271484375
tensor(11496.2988, grad_fn=<NegBackward0>) tensor(11496.2715, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11496.25
tensor(11496.2715, grad_fn=<NegBackward0>) tensor(11496.2500, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11496.232421875
tensor(11496.2500, grad_fn=<NegBackward0>) tensor(11496.2324, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11496.2177734375
tensor(11496.2324, grad_fn=<NegBackward0>) tensor(11496.2178, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11496.2060546875
tensor(11496.2178, grad_fn=<NegBackward0>) tensor(11496.2061, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11496.1962890625
tensor(11496.2061, grad_fn=<NegBackward0>) tensor(11496.1963, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11496.185546875
tensor(11496.1963, grad_fn=<NegBackward0>) tensor(11496.1855, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11496.177734375
tensor(11496.1855, grad_fn=<NegBackward0>) tensor(11496.1777, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11496.171875
tensor(11496.1777, grad_fn=<NegBackward0>) tensor(11496.1719, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11496.1650390625
tensor(11496.1719, grad_fn=<NegBackward0>) tensor(11496.1650, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11496.1611328125
tensor(11496.1650, grad_fn=<NegBackward0>) tensor(11496.1611, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11496.15625
tensor(11496.1611, grad_fn=<NegBackward0>) tensor(11496.1562, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11496.150390625
tensor(11496.1562, grad_fn=<NegBackward0>) tensor(11496.1504, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11496.1484375
tensor(11496.1504, grad_fn=<NegBackward0>) tensor(11496.1484, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11496.14453125
tensor(11496.1484, grad_fn=<NegBackward0>) tensor(11496.1445, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11496.1416015625
tensor(11496.1445, grad_fn=<NegBackward0>) tensor(11496.1416, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11496.1376953125
tensor(11496.1416, grad_fn=<NegBackward0>) tensor(11496.1377, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11496.13671875
tensor(11496.1377, grad_fn=<NegBackward0>) tensor(11496.1367, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11496.1337890625
tensor(11496.1367, grad_fn=<NegBackward0>) tensor(11496.1338, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11496.1318359375
tensor(11496.1338, grad_fn=<NegBackward0>) tensor(11496.1318, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11496.130859375
tensor(11496.1318, grad_fn=<NegBackward0>) tensor(11496.1309, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11496.1279296875
tensor(11496.1309, grad_fn=<NegBackward0>) tensor(11496.1279, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11496.1259765625
tensor(11496.1279, grad_fn=<NegBackward0>) tensor(11496.1260, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11496.1259765625
tensor(11496.1260, grad_fn=<NegBackward0>) tensor(11496.1260, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11496.1240234375
tensor(11496.1260, grad_fn=<NegBackward0>) tensor(11496.1240, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11496.123046875
tensor(11496.1240, grad_fn=<NegBackward0>) tensor(11496.1230, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11496.12109375
tensor(11496.1230, grad_fn=<NegBackward0>) tensor(11496.1211, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11496.1201171875
tensor(11496.1211, grad_fn=<NegBackward0>) tensor(11496.1201, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11496.119140625
tensor(11496.1201, grad_fn=<NegBackward0>) tensor(11496.1191, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11496.130859375
tensor(11496.1191, grad_fn=<NegBackward0>) tensor(11496.1309, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11496.1171875
tensor(11496.1191, grad_fn=<NegBackward0>) tensor(11496.1172, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11496.1171875
tensor(11496.1172, grad_fn=<NegBackward0>) tensor(11496.1172, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11496.1171875
tensor(11496.1172, grad_fn=<NegBackward0>) tensor(11496.1172, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11496.1162109375
tensor(11496.1172, grad_fn=<NegBackward0>) tensor(11496.1162, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11496.1376953125
tensor(11496.1162, grad_fn=<NegBackward0>) tensor(11496.1377, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11496.1142578125
tensor(11496.1162, grad_fn=<NegBackward0>) tensor(11496.1143, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11496.11328125
tensor(11496.1143, grad_fn=<NegBackward0>) tensor(11496.1133, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11496.1123046875
tensor(11496.1133, grad_fn=<NegBackward0>) tensor(11496.1123, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11496.111328125
tensor(11496.1123, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11496.11328125
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1133, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11496.111328125
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11496.111328125
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11496.1142578125
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1143, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11496.1103515625
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1104, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11496.1103515625
tensor(11496.1104, grad_fn=<NegBackward0>) tensor(11496.1104, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11496.1103515625
tensor(11496.1104, grad_fn=<NegBackward0>) tensor(11496.1104, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11496.1083984375
tensor(11496.1104, grad_fn=<NegBackward0>) tensor(11496.1084, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11496.1083984375
tensor(11496.1084, grad_fn=<NegBackward0>) tensor(11496.1084, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11496.107421875
tensor(11496.1084, grad_fn=<NegBackward0>) tensor(11496.1074, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11496.111328125
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11496.111328125
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11496.1064453125
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1064, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11496.107421875
tensor(11496.1064, grad_fn=<NegBackward0>) tensor(11496.1074, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11496.107421875
tensor(11496.1064, grad_fn=<NegBackward0>) tensor(11496.1074, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11496.1083984375
tensor(11496.1064, grad_fn=<NegBackward0>) tensor(11496.1084, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11496.107421875
tensor(11496.1064, grad_fn=<NegBackward0>) tensor(11496.1074, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11496.1123046875
tensor(11496.1064, grad_fn=<NegBackward0>) tensor(11496.1123, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.7477, 0.2523],
        [0.3142, 0.6858]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4597, 0.5403], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.1026],
         [0.7160, 0.4082]],

        [[0.5125, 0.0956],
         [0.6162, 0.6680]],

        [[0.5408, 0.1017],
         [0.6968, 0.6634]],

        [[0.6915, 0.0960],
         [0.5257, 0.6288]],

        [[0.6392, 0.1088],
         [0.7253, 0.6072]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20183.265625
inf tensor(20183.2656, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12244.3388671875
tensor(20183.2656, grad_fn=<NegBackward0>) tensor(12244.3389, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12000.1103515625
tensor(12244.3389, grad_fn=<NegBackward0>) tensor(12000.1104, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11521.3603515625
tensor(12000.1104, grad_fn=<NegBackward0>) tensor(11521.3604, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11497.6748046875
tensor(11521.3604, grad_fn=<NegBackward0>) tensor(11497.6748, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11497.087890625
tensor(11497.6748, grad_fn=<NegBackward0>) tensor(11497.0879, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11496.796875
tensor(11497.0879, grad_fn=<NegBackward0>) tensor(11496.7969, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11496.580078125
tensor(11496.7969, grad_fn=<NegBackward0>) tensor(11496.5801, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11496.4736328125
tensor(11496.5801, grad_fn=<NegBackward0>) tensor(11496.4736, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11496.4033203125
tensor(11496.4736, grad_fn=<NegBackward0>) tensor(11496.4033, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11496.3515625
tensor(11496.4033, grad_fn=<NegBackward0>) tensor(11496.3516, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11496.314453125
tensor(11496.3516, grad_fn=<NegBackward0>) tensor(11496.3145, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11496.28515625
tensor(11496.3145, grad_fn=<NegBackward0>) tensor(11496.2852, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11496.26171875
tensor(11496.2852, grad_fn=<NegBackward0>) tensor(11496.2617, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11496.2412109375
tensor(11496.2617, grad_fn=<NegBackward0>) tensor(11496.2412, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11496.2265625
tensor(11496.2412, grad_fn=<NegBackward0>) tensor(11496.2266, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11496.212890625
tensor(11496.2266, grad_fn=<NegBackward0>) tensor(11496.2129, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11496.203125
tensor(11496.2129, grad_fn=<NegBackward0>) tensor(11496.2031, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11496.19140625
tensor(11496.2031, grad_fn=<NegBackward0>) tensor(11496.1914, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11496.18359375
tensor(11496.1914, grad_fn=<NegBackward0>) tensor(11496.1836, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11496.1748046875
tensor(11496.1836, grad_fn=<NegBackward0>) tensor(11496.1748, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11496.16796875
tensor(11496.1748, grad_fn=<NegBackward0>) tensor(11496.1680, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11496.1611328125
tensor(11496.1680, grad_fn=<NegBackward0>) tensor(11496.1611, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11496.154296875
tensor(11496.1611, grad_fn=<NegBackward0>) tensor(11496.1543, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11496.1513671875
tensor(11496.1543, grad_fn=<NegBackward0>) tensor(11496.1514, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11496.146484375
tensor(11496.1514, grad_fn=<NegBackward0>) tensor(11496.1465, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11496.142578125
tensor(11496.1465, grad_fn=<NegBackward0>) tensor(11496.1426, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11496.140625
tensor(11496.1426, grad_fn=<NegBackward0>) tensor(11496.1406, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11496.13671875
tensor(11496.1406, grad_fn=<NegBackward0>) tensor(11496.1367, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11496.134765625
tensor(11496.1367, grad_fn=<NegBackward0>) tensor(11496.1348, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11496.1318359375
tensor(11496.1348, grad_fn=<NegBackward0>) tensor(11496.1318, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11496.130859375
tensor(11496.1318, grad_fn=<NegBackward0>) tensor(11496.1309, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11496.12890625
tensor(11496.1309, grad_fn=<NegBackward0>) tensor(11496.1289, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11496.1259765625
tensor(11496.1289, grad_fn=<NegBackward0>) tensor(11496.1260, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11496.125
tensor(11496.1260, grad_fn=<NegBackward0>) tensor(11496.1250, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11496.1240234375
tensor(11496.1250, grad_fn=<NegBackward0>) tensor(11496.1240, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11496.123046875
tensor(11496.1240, grad_fn=<NegBackward0>) tensor(11496.1230, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11496.1201171875
tensor(11496.1230, grad_fn=<NegBackward0>) tensor(11496.1201, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11496.1201171875
tensor(11496.1201, grad_fn=<NegBackward0>) tensor(11496.1201, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11496.119140625
tensor(11496.1201, grad_fn=<NegBackward0>) tensor(11496.1191, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11496.1171875
tensor(11496.1191, grad_fn=<NegBackward0>) tensor(11496.1172, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11496.1171875
tensor(11496.1172, grad_fn=<NegBackward0>) tensor(11496.1172, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11496.1162109375
tensor(11496.1172, grad_fn=<NegBackward0>) tensor(11496.1162, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11496.1162109375
tensor(11496.1162, grad_fn=<NegBackward0>) tensor(11496.1162, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11496.1298828125
tensor(11496.1162, grad_fn=<NegBackward0>) tensor(11496.1299, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11496.1142578125
tensor(11496.1162, grad_fn=<NegBackward0>) tensor(11496.1143, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11496.11328125
tensor(11496.1143, grad_fn=<NegBackward0>) tensor(11496.1133, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11496.1123046875
tensor(11496.1133, grad_fn=<NegBackward0>) tensor(11496.1123, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11496.1123046875
tensor(11496.1123, grad_fn=<NegBackward0>) tensor(11496.1123, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11496.111328125
tensor(11496.1123, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11496.111328125
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11496.111328125
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11496.111328125
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1113, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11496.1103515625
tensor(11496.1113, grad_fn=<NegBackward0>) tensor(11496.1104, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11496.1103515625
tensor(11496.1104, grad_fn=<NegBackward0>) tensor(11496.1104, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11496.1103515625
tensor(11496.1104, grad_fn=<NegBackward0>) tensor(11496.1104, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11496.109375
tensor(11496.1104, grad_fn=<NegBackward0>) tensor(11496.1094, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11496.107421875
tensor(11496.1094, grad_fn=<NegBackward0>) tensor(11496.1074, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11496.1171875
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1172, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11496.11328125
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1133, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11496.107421875
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1074, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11496.107421875
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1074, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11496.1162109375
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1162, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11496.1083984375
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1084, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11496.1044921875
tensor(11496.1074, grad_fn=<NegBackward0>) tensor(11496.1045, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11496.10546875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1055, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11496.10546875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1055, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11496.10546875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1055, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11496.10546875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1055, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11496.1044921875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1045, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11496.1044921875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1045, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11496.10546875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1055, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11496.10546875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1055, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11496.1044921875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1045, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11496.1123046875
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1123, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11496.1025390625
tensor(11496.1045, grad_fn=<NegBackward0>) tensor(11496.1025, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11496.1064453125
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1064, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11496.103515625
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1035, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11496.1025390625
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1025, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11496.1044921875
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1045, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11496.103515625
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1035, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11496.103515625
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1035, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11496.10546875
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1055, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11496.193359375
tensor(11496.1025, grad_fn=<NegBackward0>) tensor(11496.1934, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.7498, 0.2502],
        [0.3148, 0.6852]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4610, 0.5390], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.1027],
         [0.5926, 0.4107]],

        [[0.7002, 0.0956],
         [0.7222, 0.6238]],

        [[0.5152, 0.1017],
         [0.7275, 0.6311]],

        [[0.6646, 0.0960],
         [0.5891, 0.5776]],

        [[0.7268, 0.1088],
         [0.5591, 0.6551]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11496.1123046875, 11496.193359375]
-------------------------------------
This iteration is 16
True Objective function: Loss = -11748.912838403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20768.533203125
inf tensor(20768.5332, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12654.720703125
tensor(20768.5332, grad_fn=<NegBackward0>) tensor(12654.7207, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12149.4453125
tensor(12654.7207, grad_fn=<NegBackward0>) tensor(12149.4453, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11975.3701171875
tensor(12149.4453, grad_fn=<NegBackward0>) tensor(11975.3701, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11969.3671875
tensor(11975.3701, grad_fn=<NegBackward0>) tensor(11969.3672, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11969.0771484375
tensor(11969.3672, grad_fn=<NegBackward0>) tensor(11969.0771, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11968.9462890625
tensor(11969.0771, grad_fn=<NegBackward0>) tensor(11968.9463, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11968.8662109375
tensor(11968.9463, grad_fn=<NegBackward0>) tensor(11968.8662, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11968.8125
tensor(11968.8662, grad_fn=<NegBackward0>) tensor(11968.8125, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11968.7744140625
tensor(11968.8125, grad_fn=<NegBackward0>) tensor(11968.7744, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11968.74609375
tensor(11968.7744, grad_fn=<NegBackward0>) tensor(11968.7461, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11968.72265625
tensor(11968.7461, grad_fn=<NegBackward0>) tensor(11968.7227, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11968.7021484375
tensor(11968.7227, grad_fn=<NegBackward0>) tensor(11968.7021, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11966.341796875
tensor(11968.7021, grad_fn=<NegBackward0>) tensor(11966.3418, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11966.255859375
tensor(11966.3418, grad_fn=<NegBackward0>) tensor(11966.2559, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11966.2421875
tensor(11966.2559, grad_fn=<NegBackward0>) tensor(11966.2422, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11966.2333984375
tensor(11966.2422, grad_fn=<NegBackward0>) tensor(11966.2334, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11966.1748046875
tensor(11966.2334, grad_fn=<NegBackward0>) tensor(11966.1748, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11966.162109375
tensor(11966.1748, grad_fn=<NegBackward0>) tensor(11966.1621, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11966.154296875
tensor(11966.1621, grad_fn=<NegBackward0>) tensor(11966.1543, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11966.1494140625
tensor(11966.1543, grad_fn=<NegBackward0>) tensor(11966.1494, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11966.146484375
tensor(11966.1494, grad_fn=<NegBackward0>) tensor(11966.1465, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11966.1435546875
tensor(11966.1465, grad_fn=<NegBackward0>) tensor(11966.1436, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11966.1396484375
tensor(11966.1436, grad_fn=<NegBackward0>) tensor(11966.1396, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11966.138671875
tensor(11966.1396, grad_fn=<NegBackward0>) tensor(11966.1387, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11966.14453125
tensor(11966.1387, grad_fn=<NegBackward0>) tensor(11966.1445, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11966.1337890625
tensor(11966.1387, grad_fn=<NegBackward0>) tensor(11966.1338, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11966.1357421875
tensor(11966.1338, grad_fn=<NegBackward0>) tensor(11966.1357, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11966.130859375
tensor(11966.1338, grad_fn=<NegBackward0>) tensor(11966.1309, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11966.14453125
tensor(11966.1309, grad_fn=<NegBackward0>) tensor(11966.1445, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11966.1279296875
tensor(11966.1309, grad_fn=<NegBackward0>) tensor(11966.1279, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11966.126953125
tensor(11966.1279, grad_fn=<NegBackward0>) tensor(11966.1270, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11966.1259765625
tensor(11966.1270, grad_fn=<NegBackward0>) tensor(11966.1260, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11966.1240234375
tensor(11966.1260, grad_fn=<NegBackward0>) tensor(11966.1240, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11966.1318359375
tensor(11966.1240, grad_fn=<NegBackward0>) tensor(11966.1318, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11966.1240234375
tensor(11966.1240, grad_fn=<NegBackward0>) tensor(11966.1240, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11966.123046875
tensor(11966.1240, grad_fn=<NegBackward0>) tensor(11966.1230, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11966.12109375
tensor(11966.1230, grad_fn=<NegBackward0>) tensor(11966.1211, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11966.1201171875
tensor(11966.1211, grad_fn=<NegBackward0>) tensor(11966.1201, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11966.1181640625
tensor(11966.1201, grad_fn=<NegBackward0>) tensor(11966.1182, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11965.8720703125
tensor(11966.1182, grad_fn=<NegBackward0>) tensor(11965.8721, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11965.87109375
tensor(11965.8721, grad_fn=<NegBackward0>) tensor(11965.8711, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11965.87109375
tensor(11965.8711, grad_fn=<NegBackward0>) tensor(11965.8711, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11965.8701171875
tensor(11965.8711, grad_fn=<NegBackward0>) tensor(11965.8701, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11965.869140625
tensor(11965.8701, grad_fn=<NegBackward0>) tensor(11965.8691, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11965.87890625
tensor(11965.8691, grad_fn=<NegBackward0>) tensor(11965.8789, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11965.869140625
tensor(11965.8691, grad_fn=<NegBackward0>) tensor(11965.8691, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11965.869140625
tensor(11965.8691, grad_fn=<NegBackward0>) tensor(11965.8691, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11965.869140625
tensor(11965.8691, grad_fn=<NegBackward0>) tensor(11965.8691, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11965.8671875
tensor(11965.8691, grad_fn=<NegBackward0>) tensor(11965.8672, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11965.8681640625
tensor(11965.8672, grad_fn=<NegBackward0>) tensor(11965.8682, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11965.8671875
tensor(11965.8672, grad_fn=<NegBackward0>) tensor(11965.8672, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11965.8662109375
tensor(11965.8672, grad_fn=<NegBackward0>) tensor(11965.8662, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11965.8720703125
tensor(11965.8662, grad_fn=<NegBackward0>) tensor(11965.8721, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11965.8681640625
tensor(11965.8662, grad_fn=<NegBackward0>) tensor(11965.8682, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11965.8662109375
tensor(11965.8662, grad_fn=<NegBackward0>) tensor(11965.8662, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11965.8662109375
tensor(11965.8662, grad_fn=<NegBackward0>) tensor(11965.8662, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11965.8662109375
tensor(11965.8662, grad_fn=<NegBackward0>) tensor(11965.8662, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11965.865234375
tensor(11965.8662, grad_fn=<NegBackward0>) tensor(11965.8652, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11965.865234375
tensor(11965.8652, grad_fn=<NegBackward0>) tensor(11965.8652, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11965.865234375
tensor(11965.8652, grad_fn=<NegBackward0>) tensor(11965.8652, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11965.8642578125
tensor(11965.8652, grad_fn=<NegBackward0>) tensor(11965.8643, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11965.865234375
tensor(11965.8643, grad_fn=<NegBackward0>) tensor(11965.8652, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11965.87109375
tensor(11965.8643, grad_fn=<NegBackward0>) tensor(11965.8711, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11965.8603515625
tensor(11965.8643, grad_fn=<NegBackward0>) tensor(11965.8604, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11965.849609375
tensor(11965.8604, grad_fn=<NegBackward0>) tensor(11965.8496, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11965.8486328125
tensor(11965.8496, grad_fn=<NegBackward0>) tensor(11965.8486, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11965.849609375
tensor(11965.8486, grad_fn=<NegBackward0>) tensor(11965.8496, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11965.8701171875
tensor(11965.8486, grad_fn=<NegBackward0>) tensor(11965.8701, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11965.84765625
tensor(11965.8486, grad_fn=<NegBackward0>) tensor(11965.8477, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11965.853515625
tensor(11965.8477, grad_fn=<NegBackward0>) tensor(11965.8535, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11965.84765625
tensor(11965.8477, grad_fn=<NegBackward0>) tensor(11965.8477, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11965.84765625
tensor(11965.8477, grad_fn=<NegBackward0>) tensor(11965.8477, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11965.900390625
tensor(11965.8477, grad_fn=<NegBackward0>) tensor(11965.9004, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11965.84375
tensor(11965.8477, grad_fn=<NegBackward0>) tensor(11965.8438, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11965.857421875
tensor(11965.8438, grad_fn=<NegBackward0>) tensor(11965.8574, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11964.5458984375
tensor(11965.8438, grad_fn=<NegBackward0>) tensor(11964.5459, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11964.5361328125
tensor(11964.5459, grad_fn=<NegBackward0>) tensor(11964.5361, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11964.537109375
tensor(11964.5361, grad_fn=<NegBackward0>) tensor(11964.5371, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11964.53515625
tensor(11964.5361, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11964.5537109375
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5537, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11964.5400390625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5400, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11964.5361328125
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5361, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11964.53515625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11964.537109375
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5371, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11964.53515625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11964.5390625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5391, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11964.53515625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11964.5390625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5391, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11964.53515625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11964.5361328125
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5361, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11964.5673828125
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5674, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11964.5361328125
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5361, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11964.5361328125
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5361, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11964.53515625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11964.53515625
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11964.5341796875
tensor(11964.5352, grad_fn=<NegBackward0>) tensor(11964.5342, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11964.53515625
tensor(11964.5342, grad_fn=<NegBackward0>) tensor(11964.5352, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11964.5361328125
tensor(11964.5342, grad_fn=<NegBackward0>) tensor(11964.5361, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11964.5341796875
tensor(11964.5342, grad_fn=<NegBackward0>) tensor(11964.5342, grad_fn=<NegBackward0>)
pi: tensor([[0.5274, 0.4726],
        [0.5320, 0.4680]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4657, 0.5343], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2479, 0.0968],
         [0.5499, 0.3865]],

        [[0.5809, 0.1044],
         [0.6579, 0.7007]],

        [[0.5318, 0.0911],
         [0.7134, 0.5816]],

        [[0.5341, 0.0980],
         [0.6717, 0.6754]],

        [[0.5603, 0.1017],
         [0.5836, 0.6974]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6677239700512669
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.39317360047145195
Average Adjusted Rand Index: 0.9177059214793838
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22641.3984375
inf tensor(22641.3984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12636.3642578125
tensor(22641.3984, grad_fn=<NegBackward0>) tensor(12636.3643, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12560.1005859375
tensor(12636.3643, grad_fn=<NegBackward0>) tensor(12560.1006, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12515.08984375
tensor(12560.1006, grad_fn=<NegBackward0>) tensor(12515.0898, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11950.41796875
tensor(12515.0898, grad_fn=<NegBackward0>) tensor(11950.4180, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11821.595703125
tensor(11950.4180, grad_fn=<NegBackward0>) tensor(11821.5957, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11794.2578125
tensor(11821.5957, grad_fn=<NegBackward0>) tensor(11794.2578, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11779.294921875
tensor(11794.2578, grad_fn=<NegBackward0>) tensor(11779.2949, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11779.21875
tensor(11779.2949, grad_fn=<NegBackward0>) tensor(11779.2188, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11766.4130859375
tensor(11779.2188, grad_fn=<NegBackward0>) tensor(11766.4131, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11766.3623046875
tensor(11766.4131, grad_fn=<NegBackward0>) tensor(11766.3623, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11766.3408203125
tensor(11766.3623, grad_fn=<NegBackward0>) tensor(11766.3408, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11766.3232421875
tensor(11766.3408, grad_fn=<NegBackward0>) tensor(11766.3232, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11766.3125
tensor(11766.3232, grad_fn=<NegBackward0>) tensor(11766.3125, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11766.298828125
tensor(11766.3125, grad_fn=<NegBackward0>) tensor(11766.2988, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11766.2900390625
tensor(11766.2988, grad_fn=<NegBackward0>) tensor(11766.2900, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11764.3466796875
tensor(11766.2900, grad_fn=<NegBackward0>) tensor(11764.3467, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11761.2060546875
tensor(11764.3467, grad_fn=<NegBackward0>) tensor(11761.2061, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11761.2021484375
tensor(11761.2061, grad_fn=<NegBackward0>) tensor(11761.2021, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11761.197265625
tensor(11761.2021, grad_fn=<NegBackward0>) tensor(11761.1973, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11761.1923828125
tensor(11761.1973, grad_fn=<NegBackward0>) tensor(11761.1924, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11761.189453125
tensor(11761.1924, grad_fn=<NegBackward0>) tensor(11761.1895, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11761.1875
tensor(11761.1895, grad_fn=<NegBackward0>) tensor(11761.1875, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11761.1845703125
tensor(11761.1875, grad_fn=<NegBackward0>) tensor(11761.1846, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11761.18359375
tensor(11761.1846, grad_fn=<NegBackward0>) tensor(11761.1836, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11761.1806640625
tensor(11761.1836, grad_fn=<NegBackward0>) tensor(11761.1807, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11761.1787109375
tensor(11761.1807, grad_fn=<NegBackward0>) tensor(11761.1787, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11761.1767578125
tensor(11761.1787, grad_fn=<NegBackward0>) tensor(11761.1768, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11761.17578125
tensor(11761.1768, grad_fn=<NegBackward0>) tensor(11761.1758, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11761.1728515625
tensor(11761.1758, grad_fn=<NegBackward0>) tensor(11761.1729, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11761.1728515625
tensor(11761.1729, grad_fn=<NegBackward0>) tensor(11761.1729, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11761.1787109375
tensor(11761.1729, grad_fn=<NegBackward0>) tensor(11761.1787, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11761.1708984375
tensor(11761.1729, grad_fn=<NegBackward0>) tensor(11761.1709, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11761.1708984375
tensor(11761.1709, grad_fn=<NegBackward0>) tensor(11761.1709, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11761.1708984375
tensor(11761.1709, grad_fn=<NegBackward0>) tensor(11761.1709, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11761.1669921875
tensor(11761.1709, grad_fn=<NegBackward0>) tensor(11761.1670, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11761.1669921875
tensor(11761.1670, grad_fn=<NegBackward0>) tensor(11761.1670, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11761.1669921875
tensor(11761.1670, grad_fn=<NegBackward0>) tensor(11761.1670, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11761.166015625
tensor(11761.1670, grad_fn=<NegBackward0>) tensor(11761.1660, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11761.1669921875
tensor(11761.1660, grad_fn=<NegBackward0>) tensor(11761.1670, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11761.1650390625
tensor(11761.1660, grad_fn=<NegBackward0>) tensor(11761.1650, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11761.166015625
tensor(11761.1650, grad_fn=<NegBackward0>) tensor(11761.1660, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11761.1650390625
tensor(11761.1650, grad_fn=<NegBackward0>) tensor(11761.1650, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11761.1640625
tensor(11761.1650, grad_fn=<NegBackward0>) tensor(11761.1641, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11761.162109375
tensor(11761.1641, grad_fn=<NegBackward0>) tensor(11761.1621, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11761.1630859375
tensor(11761.1621, grad_fn=<NegBackward0>) tensor(11761.1631, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11761.162109375
tensor(11761.1621, grad_fn=<NegBackward0>) tensor(11761.1621, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11761.166015625
tensor(11761.1621, grad_fn=<NegBackward0>) tensor(11761.1660, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11761.162109375
tensor(11761.1621, grad_fn=<NegBackward0>) tensor(11761.1621, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11761.1748046875
tensor(11761.1621, grad_fn=<NegBackward0>) tensor(11761.1748, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11761.16015625
tensor(11761.1621, grad_fn=<NegBackward0>) tensor(11761.1602, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11761.1611328125
tensor(11761.1602, grad_fn=<NegBackward0>) tensor(11761.1611, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11761.1669921875
tensor(11761.1602, grad_fn=<NegBackward0>) tensor(11761.1670, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11761.16015625
tensor(11761.1602, grad_fn=<NegBackward0>) tensor(11761.1602, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11761.1611328125
tensor(11761.1602, grad_fn=<NegBackward0>) tensor(11761.1611, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11761.1591796875
tensor(11761.1602, grad_fn=<NegBackward0>) tensor(11761.1592, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11761.1728515625
tensor(11761.1592, grad_fn=<NegBackward0>) tensor(11761.1729, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11761.1591796875
tensor(11761.1592, grad_fn=<NegBackward0>) tensor(11761.1592, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11761.1591796875
tensor(11761.1592, grad_fn=<NegBackward0>) tensor(11761.1592, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11758.2763671875
tensor(11761.1592, grad_fn=<NegBackward0>) tensor(11758.2764, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11758.267578125
tensor(11758.2764, grad_fn=<NegBackward0>) tensor(11758.2676, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11758.267578125
tensor(11758.2676, grad_fn=<NegBackward0>) tensor(11758.2676, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11758.267578125
tensor(11758.2676, grad_fn=<NegBackward0>) tensor(11758.2676, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11758.267578125
tensor(11758.2676, grad_fn=<NegBackward0>) tensor(11758.2676, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11758.267578125
tensor(11758.2676, grad_fn=<NegBackward0>) tensor(11758.2676, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11758.2666015625
tensor(11758.2676, grad_fn=<NegBackward0>) tensor(11758.2666, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11758.2666015625
tensor(11758.2666, grad_fn=<NegBackward0>) tensor(11758.2666, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11758.265625
tensor(11758.2666, grad_fn=<NegBackward0>) tensor(11758.2656, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11758.2666015625
tensor(11758.2656, grad_fn=<NegBackward0>) tensor(11758.2666, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11750.298828125
tensor(11758.2656, grad_fn=<NegBackward0>) tensor(11750.2988, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11750.298828125
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.2988, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11750.2998046875
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.2998, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11750.298828125
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.2988, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11750.30078125
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.3008, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11750.298828125
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.2988, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11750.298828125
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.2988, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11750.306640625
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.3066, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11750.2978515625
tensor(11750.2988, grad_fn=<NegBackward0>) tensor(11750.2979, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11747.4072265625
tensor(11750.2979, grad_fn=<NegBackward0>) tensor(11747.4072, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11747.408203125
tensor(11747.4072, grad_fn=<NegBackward0>) tensor(11747.4082, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11747.40625
tensor(11747.4072, grad_fn=<NegBackward0>) tensor(11747.4062, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11747.4052734375
tensor(11747.4062, grad_fn=<NegBackward0>) tensor(11747.4053, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11747.404296875
tensor(11747.4053, grad_fn=<NegBackward0>) tensor(11747.4043, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11746.3505859375
tensor(11747.4043, grad_fn=<NegBackward0>) tensor(11746.3506, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11746.353515625
tensor(11746.3506, grad_fn=<NegBackward0>) tensor(11746.3535, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11746.3525390625
tensor(11746.3506, grad_fn=<NegBackward0>) tensor(11746.3525, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11746.3603515625
tensor(11746.3506, grad_fn=<NegBackward0>) tensor(11746.3604, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11746.3515625
tensor(11746.3506, grad_fn=<NegBackward0>) tensor(11746.3516, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11746.35546875
tensor(11746.3506, grad_fn=<NegBackward0>) tensor(11746.3555, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.7676, 0.2324],
        [0.2732, 0.7268]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5237, 0.4763], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4004, 0.0987],
         [0.7201, 0.2033]],

        [[0.7245, 0.1043],
         [0.7282, 0.6400]],

        [[0.5400, 0.0917],
         [0.6740, 0.5707]],

        [[0.6626, 0.0981],
         [0.5133, 0.5896]],

        [[0.5462, 0.1020],
         [0.6248, 0.7040]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[0.39317360047145195, 1.0] [0.9177059214793838, 1.0] [11964.541015625, 11746.35546875]
-------------------------------------
This iteration is 17
True Objective function: Loss = -11876.476520204336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20523.6875
inf tensor(20523.6875, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12772.0849609375
tensor(20523.6875, grad_fn=<NegBackward0>) tensor(12772.0850, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12098.66015625
tensor(12772.0850, grad_fn=<NegBackward0>) tensor(12098.6602, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11965.291015625
tensor(12098.6602, grad_fn=<NegBackward0>) tensor(11965.2910, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11913.029296875
tensor(11965.2910, grad_fn=<NegBackward0>) tensor(11913.0293, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11899.81640625
tensor(11913.0293, grad_fn=<NegBackward0>) tensor(11899.8164, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11899.58203125
tensor(11899.8164, grad_fn=<NegBackward0>) tensor(11899.5820, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11898.798828125
tensor(11899.5820, grad_fn=<NegBackward0>) tensor(11898.7988, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11898.6748046875
tensor(11898.7988, grad_fn=<NegBackward0>) tensor(11898.6748, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11898.6171875
tensor(11898.6748, grad_fn=<NegBackward0>) tensor(11898.6172, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11898.5732421875
tensor(11898.6172, grad_fn=<NegBackward0>) tensor(11898.5732, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11898.5341796875
tensor(11898.5732, grad_fn=<NegBackward0>) tensor(11898.5342, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11891.619140625
tensor(11898.5342, grad_fn=<NegBackward0>) tensor(11891.6191, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11891.5947265625
tensor(11891.6191, grad_fn=<NegBackward0>) tensor(11891.5947, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11891.5791015625
tensor(11891.5947, grad_fn=<NegBackward0>) tensor(11891.5791, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11891.5654296875
tensor(11891.5791, grad_fn=<NegBackward0>) tensor(11891.5654, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11891.5546875
tensor(11891.5654, grad_fn=<NegBackward0>) tensor(11891.5547, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11891.5439453125
tensor(11891.5547, grad_fn=<NegBackward0>) tensor(11891.5439, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11891.5341796875
tensor(11891.5439, grad_fn=<NegBackward0>) tensor(11891.5342, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11891.5283203125
tensor(11891.5342, grad_fn=<NegBackward0>) tensor(11891.5283, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11891.521484375
tensor(11891.5283, grad_fn=<NegBackward0>) tensor(11891.5215, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11891.517578125
tensor(11891.5215, grad_fn=<NegBackward0>) tensor(11891.5176, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11891.513671875
tensor(11891.5176, grad_fn=<NegBackward0>) tensor(11891.5137, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11891.509765625
tensor(11891.5137, grad_fn=<NegBackward0>) tensor(11891.5098, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11891.50390625
tensor(11891.5098, grad_fn=<NegBackward0>) tensor(11891.5039, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11891.5029296875
tensor(11891.5039, grad_fn=<NegBackward0>) tensor(11891.5029, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11891.5009765625
tensor(11891.5029, grad_fn=<NegBackward0>) tensor(11891.5010, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11891.4970703125
tensor(11891.5010, grad_fn=<NegBackward0>) tensor(11891.4971, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11891.5029296875
tensor(11891.4971, grad_fn=<NegBackward0>) tensor(11891.5029, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11891.494140625
tensor(11891.4971, grad_fn=<NegBackward0>) tensor(11891.4941, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11891.4912109375
tensor(11891.4941, grad_fn=<NegBackward0>) tensor(11891.4912, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11891.490234375
tensor(11891.4912, grad_fn=<NegBackward0>) tensor(11891.4902, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11891.4873046875
tensor(11891.4902, grad_fn=<NegBackward0>) tensor(11891.4873, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11891.4931640625
tensor(11891.4873, grad_fn=<NegBackward0>) tensor(11891.4932, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11891.4853515625
tensor(11891.4873, grad_fn=<NegBackward0>) tensor(11891.4854, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11891.484375
tensor(11891.4854, grad_fn=<NegBackward0>) tensor(11891.4844, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11891.4853515625
tensor(11891.4844, grad_fn=<NegBackward0>) tensor(11891.4854, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11877.5576171875
tensor(11891.4844, grad_fn=<NegBackward0>) tensor(11877.5576, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11877.5400390625
tensor(11877.5576, grad_fn=<NegBackward0>) tensor(11877.5400, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11877.5390625
tensor(11877.5400, grad_fn=<NegBackward0>) tensor(11877.5391, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11877.537109375
tensor(11877.5391, grad_fn=<NegBackward0>) tensor(11877.5371, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11877.5380859375
tensor(11877.5371, grad_fn=<NegBackward0>) tensor(11877.5381, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11877.5361328125
tensor(11877.5371, grad_fn=<NegBackward0>) tensor(11877.5361, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11877.537109375
tensor(11877.5361, grad_fn=<NegBackward0>) tensor(11877.5371, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11877.5341796875
tensor(11877.5361, grad_fn=<NegBackward0>) tensor(11877.5342, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11877.5341796875
tensor(11877.5342, grad_fn=<NegBackward0>) tensor(11877.5342, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11877.533203125
tensor(11877.5342, grad_fn=<NegBackward0>) tensor(11877.5332, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11873.3046875
tensor(11877.5332, grad_fn=<NegBackward0>) tensor(11873.3047, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11873.181640625
tensor(11873.3047, grad_fn=<NegBackward0>) tensor(11873.1816, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11873.1845703125
tensor(11873.1816, grad_fn=<NegBackward0>) tensor(11873.1846, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11873.1904296875
tensor(11873.1816, grad_fn=<NegBackward0>) tensor(11873.1904, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11873.18359375
tensor(11873.1816, grad_fn=<NegBackward0>) tensor(11873.1836, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11873.1806640625
tensor(11873.1816, grad_fn=<NegBackward0>) tensor(11873.1807, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11873.1806640625
tensor(11873.1807, grad_fn=<NegBackward0>) tensor(11873.1807, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11873.1796875
tensor(11873.1807, grad_fn=<NegBackward0>) tensor(11873.1797, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11873.1787109375
tensor(11873.1797, grad_fn=<NegBackward0>) tensor(11873.1787, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11873.1806640625
tensor(11873.1787, grad_fn=<NegBackward0>) tensor(11873.1807, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11873.1796875
tensor(11873.1787, grad_fn=<NegBackward0>) tensor(11873.1797, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11873.177734375
tensor(11873.1787, grad_fn=<NegBackward0>) tensor(11873.1777, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11873.1796875
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1797, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11873.1845703125
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1846, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11873.1796875
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1797, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11873.177734375
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1777, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11873.1787109375
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1787, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11873.177734375
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1777, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11873.1787109375
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1787, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11873.177734375
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1777, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11873.177734375
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11873.1777, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11872.419921875
tensor(11873.1777, grad_fn=<NegBackward0>) tensor(11872.4199, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11868.5546875
tensor(11872.4199, grad_fn=<NegBackward0>) tensor(11868.5547, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11868.556640625
tensor(11868.5547, grad_fn=<NegBackward0>) tensor(11868.5566, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11868.5537109375
tensor(11868.5547, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11868.5537109375
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11868.5546875
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5547, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11868.5556640625
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5557, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11868.5537109375
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11868.5546875
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5547, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11868.5537109375
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11868.5576171875
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5576, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11868.5537109375
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11868.5537109375
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11868.552734375
tensor(11868.5537, grad_fn=<NegBackward0>) tensor(11868.5527, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11868.5537109375
tensor(11868.5527, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11868.5537109375
tensor(11868.5527, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11868.5537109375
tensor(11868.5527, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11868.5537109375
tensor(11868.5527, grad_fn=<NegBackward0>) tensor(11868.5537, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11868.6455078125
tensor(11868.5527, grad_fn=<NegBackward0>) tensor(11868.6455, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.8129, 0.1871],
        [0.2706, 0.7294]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5307, 0.4693], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3958, 0.1126],
         [0.5766, 0.1931]],

        [[0.5110, 0.0979],
         [0.6384, 0.6393]],

        [[0.5989, 0.0981],
         [0.6712, 0.5946]],

        [[0.6514, 0.1080],
         [0.7309, 0.6930]],

        [[0.6187, 0.0946],
         [0.6653, 0.5239]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23659.443359375
inf tensor(23659.4434, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12747.4091796875
tensor(23659.4434, grad_fn=<NegBackward0>) tensor(12747.4092, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12323.8359375
tensor(12747.4092, grad_fn=<NegBackward0>) tensor(12323.8359, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12216.0771484375
tensor(12323.8359, grad_fn=<NegBackward0>) tensor(12216.0771, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12147.6328125
tensor(12216.0771, grad_fn=<NegBackward0>) tensor(12147.6328, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12108.0029296875
tensor(12147.6328, grad_fn=<NegBackward0>) tensor(12108.0029, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12100.6953125
tensor(12108.0029, grad_fn=<NegBackward0>) tensor(12100.6953, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12100.4326171875
tensor(12100.6953, grad_fn=<NegBackward0>) tensor(12100.4326, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12096.478515625
tensor(12100.4326, grad_fn=<NegBackward0>) tensor(12096.4785, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12096.4130859375
tensor(12096.4785, grad_fn=<NegBackward0>) tensor(12096.4131, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12096.365234375
tensor(12096.4131, grad_fn=<NegBackward0>) tensor(12096.3652, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12096.32421875
tensor(12096.3652, grad_fn=<NegBackward0>) tensor(12096.3242, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12086.8876953125
tensor(12096.3242, grad_fn=<NegBackward0>) tensor(12086.8877, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12086.84765625
tensor(12086.8877, grad_fn=<NegBackward0>) tensor(12086.8477, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12086.8291015625
tensor(12086.8477, grad_fn=<NegBackward0>) tensor(12086.8291, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12086.814453125
tensor(12086.8291, grad_fn=<NegBackward0>) tensor(12086.8145, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12086.80078125
tensor(12086.8145, grad_fn=<NegBackward0>) tensor(12086.8008, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12086.68359375
tensor(12086.8008, grad_fn=<NegBackward0>) tensor(12086.6836, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12086.638671875
tensor(12086.6836, grad_fn=<NegBackward0>) tensor(12086.6387, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12086.6220703125
tensor(12086.6387, grad_fn=<NegBackward0>) tensor(12086.6221, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12086.615234375
tensor(12086.6221, grad_fn=<NegBackward0>) tensor(12086.6152, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12086.609375
tensor(12086.6152, grad_fn=<NegBackward0>) tensor(12086.6094, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12086.6005859375
tensor(12086.6094, grad_fn=<NegBackward0>) tensor(12086.6006, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12084.5576171875
tensor(12086.6006, grad_fn=<NegBackward0>) tensor(12084.5576, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12084.501953125
tensor(12084.5576, grad_fn=<NegBackward0>) tensor(12084.5020, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12084.458984375
tensor(12084.5020, grad_fn=<NegBackward0>) tensor(12084.4590, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12084.4560546875
tensor(12084.4590, grad_fn=<NegBackward0>) tensor(12084.4561, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12084.4482421875
tensor(12084.4561, grad_fn=<NegBackward0>) tensor(12084.4482, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12084.3759765625
tensor(12084.4482, grad_fn=<NegBackward0>) tensor(12084.3760, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12084.373046875
tensor(12084.3760, grad_fn=<NegBackward0>) tensor(12084.3730, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12084.37109375
tensor(12084.3730, grad_fn=<NegBackward0>) tensor(12084.3711, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12084.3681640625
tensor(12084.3711, grad_fn=<NegBackward0>) tensor(12084.3682, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12084.369140625
tensor(12084.3682, grad_fn=<NegBackward0>) tensor(12084.3691, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12084.3671875
tensor(12084.3682, grad_fn=<NegBackward0>) tensor(12084.3672, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12084.365234375
tensor(12084.3672, grad_fn=<NegBackward0>) tensor(12084.3652, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12084.3642578125
tensor(12084.3652, grad_fn=<NegBackward0>) tensor(12084.3643, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12084.3623046875
tensor(12084.3643, grad_fn=<NegBackward0>) tensor(12084.3623, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12084.3623046875
tensor(12084.3623, grad_fn=<NegBackward0>) tensor(12084.3623, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12084.359375
tensor(12084.3623, grad_fn=<NegBackward0>) tensor(12084.3594, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12084.3564453125
tensor(12084.3594, grad_fn=<NegBackward0>) tensor(12084.3564, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12084.326171875
tensor(12084.3564, grad_fn=<NegBackward0>) tensor(12084.3262, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12084.3271484375
tensor(12084.3262, grad_fn=<NegBackward0>) tensor(12084.3271, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12084.3251953125
tensor(12084.3262, grad_fn=<NegBackward0>) tensor(12084.3252, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12084.326171875
tensor(12084.3252, grad_fn=<NegBackward0>) tensor(12084.3262, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12084.3310546875
tensor(12084.3252, grad_fn=<NegBackward0>) tensor(12084.3311, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -12084.3232421875
tensor(12084.3252, grad_fn=<NegBackward0>) tensor(12084.3232, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12084.32421875
tensor(12084.3232, grad_fn=<NegBackward0>) tensor(12084.3242, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12084.322265625
tensor(12084.3232, grad_fn=<NegBackward0>) tensor(12084.3223, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12084.322265625
tensor(12084.3223, grad_fn=<NegBackward0>) tensor(12084.3223, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12084.337890625
tensor(12084.3223, grad_fn=<NegBackward0>) tensor(12084.3379, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12084.3212890625
tensor(12084.3223, grad_fn=<NegBackward0>) tensor(12084.3213, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12084.326171875
tensor(12084.3213, grad_fn=<NegBackward0>) tensor(12084.3262, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12084.3212890625
tensor(12084.3213, grad_fn=<NegBackward0>) tensor(12084.3213, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12084.3203125
tensor(12084.3213, grad_fn=<NegBackward0>) tensor(12084.3203, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12084.3212890625
tensor(12084.3203, grad_fn=<NegBackward0>) tensor(12084.3213, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12084.3212890625
tensor(12084.3203, grad_fn=<NegBackward0>) tensor(12084.3213, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12084.3193359375
tensor(12084.3203, grad_fn=<NegBackward0>) tensor(12084.3193, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12084.3203125
tensor(12084.3193, grad_fn=<NegBackward0>) tensor(12084.3203, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12084.3193359375
tensor(12084.3193, grad_fn=<NegBackward0>) tensor(12084.3193, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12084.318359375
tensor(12084.3193, grad_fn=<NegBackward0>) tensor(12084.3184, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12084.318359375
tensor(12084.3184, grad_fn=<NegBackward0>) tensor(12084.3184, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12084.3212890625
tensor(12084.3184, grad_fn=<NegBackward0>) tensor(12084.3213, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12084.318359375
tensor(12084.3184, grad_fn=<NegBackward0>) tensor(12084.3184, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12084.318359375
tensor(12084.3184, grad_fn=<NegBackward0>) tensor(12084.3184, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12084.328125
tensor(12084.3184, grad_fn=<NegBackward0>) tensor(12084.3281, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12084.3173828125
tensor(12084.3184, grad_fn=<NegBackward0>) tensor(12084.3174, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12084.3173828125
tensor(12084.3174, grad_fn=<NegBackward0>) tensor(12084.3174, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12084.3193359375
tensor(12084.3174, grad_fn=<NegBackward0>) tensor(12084.3193, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12084.32421875
tensor(12084.3174, grad_fn=<NegBackward0>) tensor(12084.3242, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12084.322265625
tensor(12084.3174, grad_fn=<NegBackward0>) tensor(12084.3223, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12084.31640625
tensor(12084.3174, grad_fn=<NegBackward0>) tensor(12084.3164, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12084.3349609375
tensor(12084.3164, grad_fn=<NegBackward0>) tensor(12084.3350, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12084.31640625
tensor(12084.3164, grad_fn=<NegBackward0>) tensor(12084.3164, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12084.31640625
tensor(12084.3164, grad_fn=<NegBackward0>) tensor(12084.3164, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12084.3154296875
tensor(12084.3164, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12084.31640625
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3164, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12084.3154296875
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12084.3154296875
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12084.3154296875
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12084.3173828125
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3174, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12084.328125
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3281, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12084.3154296875
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12084.3154296875
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12084.3642578125
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3643, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12084.314453125
tensor(12084.3154, grad_fn=<NegBackward0>) tensor(12084.3145, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12084.318359375
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3184, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12084.3154296875
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12084.396484375
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3965, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12084.314453125
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3145, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12084.3154296875
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12084.3681640625
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3682, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12084.3154296875
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12084.314453125
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3145, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12084.3154296875
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12084.314453125
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3145, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12084.314453125
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3145, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12084.326171875
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3262, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12084.314453125
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3145, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12084.3154296875
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3154, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12084.314453125
tensor(12084.3145, grad_fn=<NegBackward0>) tensor(12084.3145, grad_fn=<NegBackward0>)
pi: tensor([[0.5384, 0.4616],
        [0.4399, 0.5601]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5298, 0.4702], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3929, 0.1118],
         [0.7217, 0.2204]],

        [[0.6681, 0.0978],
         [0.6184, 0.6008]],

        [[0.6274, 0.0978],
         [0.5141, 0.6924]],

        [[0.5064, 0.0975],
         [0.5123, 0.5858]],

        [[0.6926, 0.0946],
         [0.7079, 0.6305]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 68
Adjusted Rand Index: 0.12445796817869018
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5290538227175603
Average Adjusted Rand Index: 0.8248915936357382
[1.0, 0.5290538227175603] [1.0, 0.8248915936357382] [11868.6455078125, 12084.3154296875]
-------------------------------------
This iteration is 18
True Objective function: Loss = -11586.285803183191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21386.40234375
inf tensor(21386.4023, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12300.947265625
tensor(21386.4023, grad_fn=<NegBackward0>) tensor(12300.9473, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12292.255859375
tensor(12300.9473, grad_fn=<NegBackward0>) tensor(12292.2559, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12272.4931640625
tensor(12292.2559, grad_fn=<NegBackward0>) tensor(12272.4932, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12108.01953125
tensor(12272.4932, grad_fn=<NegBackward0>) tensor(12108.0195, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11986.056640625
tensor(12108.0195, grad_fn=<NegBackward0>) tensor(11986.0566, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11955.9951171875
tensor(11986.0566, grad_fn=<NegBackward0>) tensor(11955.9951, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11942.712890625
tensor(11955.9951, grad_fn=<NegBackward0>) tensor(11942.7129, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11932.6513671875
tensor(11942.7129, grad_fn=<NegBackward0>) tensor(11932.6514, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11915.841796875
tensor(11932.6514, grad_fn=<NegBackward0>) tensor(11915.8418, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11906.01171875
tensor(11915.8418, grad_fn=<NegBackward0>) tensor(11906.0117, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11887.8154296875
tensor(11906.0117, grad_fn=<NegBackward0>) tensor(11887.8154, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11872.3701171875
tensor(11887.8154, grad_fn=<NegBackward0>) tensor(11872.3701, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11872.1318359375
tensor(11872.3701, grad_fn=<NegBackward0>) tensor(11872.1318, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11867.4501953125
tensor(11872.1318, grad_fn=<NegBackward0>) tensor(11867.4502, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11867.1875
tensor(11867.4502, grad_fn=<NegBackward0>) tensor(11867.1875, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11863.177734375
tensor(11867.1875, grad_fn=<NegBackward0>) tensor(11863.1777, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11831.8642578125
tensor(11863.1777, grad_fn=<NegBackward0>) tensor(11831.8643, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11831.7197265625
tensor(11831.8643, grad_fn=<NegBackward0>) tensor(11831.7197, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11826.169921875
tensor(11831.7197, grad_fn=<NegBackward0>) tensor(11826.1699, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11826.126953125
tensor(11826.1699, grad_fn=<NegBackward0>) tensor(11826.1270, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11826.0966796875
tensor(11826.1270, grad_fn=<NegBackward0>) tensor(11826.0967, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11822.4501953125
tensor(11826.0967, grad_fn=<NegBackward0>) tensor(11822.4502, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11822.435546875
tensor(11822.4502, grad_fn=<NegBackward0>) tensor(11822.4355, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11822.427734375
tensor(11822.4355, grad_fn=<NegBackward0>) tensor(11822.4277, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11822.4140625
tensor(11822.4277, grad_fn=<NegBackward0>) tensor(11822.4141, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11822.4033203125
tensor(11822.4141, grad_fn=<NegBackward0>) tensor(11822.4033, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11822.3583984375
tensor(11822.4033, grad_fn=<NegBackward0>) tensor(11822.3584, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11821.9443359375
tensor(11822.3584, grad_fn=<NegBackward0>) tensor(11821.9443, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11821.626953125
tensor(11821.9443, grad_fn=<NegBackward0>) tensor(11821.6270, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11821.4423828125
tensor(11821.6270, grad_fn=<NegBackward0>) tensor(11821.4424, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11820.970703125
tensor(11821.4424, grad_fn=<NegBackward0>) tensor(11820.9707, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11819.669921875
tensor(11820.9707, grad_fn=<NegBackward0>) tensor(11819.6699, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11819.5908203125
tensor(11819.6699, grad_fn=<NegBackward0>) tensor(11819.5908, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11819.5888671875
tensor(11819.5908, grad_fn=<NegBackward0>) tensor(11819.5889, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11819.5390625
tensor(11819.5889, grad_fn=<NegBackward0>) tensor(11819.5391, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11819.0517578125
tensor(11819.5391, grad_fn=<NegBackward0>) tensor(11819.0518, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11819.0458984375
tensor(11819.0518, grad_fn=<NegBackward0>) tensor(11819.0459, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11819.0419921875
tensor(11819.0459, grad_fn=<NegBackward0>) tensor(11819.0420, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11817.5068359375
tensor(11819.0420, grad_fn=<NegBackward0>) tensor(11817.5068, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11817.4521484375
tensor(11817.5068, grad_fn=<NegBackward0>) tensor(11817.4521, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11810.7080078125
tensor(11817.4521, grad_fn=<NegBackward0>) tensor(11810.7080, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11810.40625
tensor(11810.7080, grad_fn=<NegBackward0>) tensor(11810.4062, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11810.3720703125
tensor(11810.4062, grad_fn=<NegBackward0>) tensor(11810.3721, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11809.5185546875
tensor(11810.3721, grad_fn=<NegBackward0>) tensor(11809.5186, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11809.4814453125
tensor(11809.5186, grad_fn=<NegBackward0>) tensor(11809.4814, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11809.4794921875
tensor(11809.4814, grad_fn=<NegBackward0>) tensor(11809.4795, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11809.4794921875
tensor(11809.4795, grad_fn=<NegBackward0>) tensor(11809.4795, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11809.46484375
tensor(11809.4795, grad_fn=<NegBackward0>) tensor(11809.4648, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11809.4521484375
tensor(11809.4648, grad_fn=<NegBackward0>) tensor(11809.4521, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11809.435546875
tensor(11809.4521, grad_fn=<NegBackward0>) tensor(11809.4355, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11809.4365234375
tensor(11809.4355, grad_fn=<NegBackward0>) tensor(11809.4365, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11809.4345703125
tensor(11809.4355, grad_fn=<NegBackward0>) tensor(11809.4346, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11809.4345703125
tensor(11809.4346, grad_fn=<NegBackward0>) tensor(11809.4346, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11809.4404296875
tensor(11809.4346, grad_fn=<NegBackward0>) tensor(11809.4404, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11809.43359375
tensor(11809.4346, grad_fn=<NegBackward0>) tensor(11809.4336, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11809.43359375
tensor(11809.4336, grad_fn=<NegBackward0>) tensor(11809.4336, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11809.4482421875
tensor(11809.4336, grad_fn=<NegBackward0>) tensor(11809.4482, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11809.4326171875
tensor(11809.4336, grad_fn=<NegBackward0>) tensor(11809.4326, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11809.4306640625
tensor(11809.4326, grad_fn=<NegBackward0>) tensor(11809.4307, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11809.4296875
tensor(11809.4307, grad_fn=<NegBackward0>) tensor(11809.4297, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11809.40625
tensor(11809.4297, grad_fn=<NegBackward0>) tensor(11809.4062, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11809.40625
tensor(11809.4062, grad_fn=<NegBackward0>) tensor(11809.4062, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11809.4091796875
tensor(11809.4062, grad_fn=<NegBackward0>) tensor(11809.4092, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11809.4052734375
tensor(11809.4062, grad_fn=<NegBackward0>) tensor(11809.4053, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11809.404296875
tensor(11809.4053, grad_fn=<NegBackward0>) tensor(11809.4043, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11809.40625
tensor(11809.4043, grad_fn=<NegBackward0>) tensor(11809.4062, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11809.404296875
tensor(11809.4043, grad_fn=<NegBackward0>) tensor(11809.4043, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11809.408203125
tensor(11809.4043, grad_fn=<NegBackward0>) tensor(11809.4082, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11809.419921875
tensor(11809.4043, grad_fn=<NegBackward0>) tensor(11809.4199, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11809.4033203125
tensor(11809.4043, grad_fn=<NegBackward0>) tensor(11809.4033, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11809.423828125
tensor(11809.4033, grad_fn=<NegBackward0>) tensor(11809.4238, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11809.416015625
tensor(11809.4033, grad_fn=<NegBackward0>) tensor(11809.4160, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11809.4091796875
tensor(11809.4033, grad_fn=<NegBackward0>) tensor(11809.4092, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11809.412109375
tensor(11809.4033, grad_fn=<NegBackward0>) tensor(11809.4121, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11809.40234375
tensor(11809.4033, grad_fn=<NegBackward0>) tensor(11809.4023, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11809.40234375
tensor(11809.4023, grad_fn=<NegBackward0>) tensor(11809.4023, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11809.4013671875
tensor(11809.4023, grad_fn=<NegBackward0>) tensor(11809.4014, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11809.4013671875
tensor(11809.4014, grad_fn=<NegBackward0>) tensor(11809.4014, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11809.4013671875
tensor(11809.4014, grad_fn=<NegBackward0>) tensor(11809.4014, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11809.3896484375
tensor(11809.4014, grad_fn=<NegBackward0>) tensor(11809.3896, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11809.3876953125
tensor(11809.3896, grad_fn=<NegBackward0>) tensor(11809.3877, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11809.3896484375
tensor(11809.3877, grad_fn=<NegBackward0>) tensor(11809.3896, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11809.384765625
tensor(11809.3877, grad_fn=<NegBackward0>) tensor(11809.3848, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11808.20703125
tensor(11809.3848, grad_fn=<NegBackward0>) tensor(11808.2070, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11807.9716796875
tensor(11808.2070, grad_fn=<NegBackward0>) tensor(11807.9717, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11808.0107421875
tensor(11807.9717, grad_fn=<NegBackward0>) tensor(11808.0107, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11807.966796875
tensor(11807.9717, grad_fn=<NegBackward0>) tensor(11807.9668, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11807.9072265625
tensor(11807.9668, grad_fn=<NegBackward0>) tensor(11807.9072, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11807.748046875
tensor(11807.9072, grad_fn=<NegBackward0>) tensor(11807.7480, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11807.74609375
tensor(11807.7480, grad_fn=<NegBackward0>) tensor(11807.7461, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11807.7470703125
tensor(11807.7461, grad_fn=<NegBackward0>) tensor(11807.7471, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11807.7451171875
tensor(11807.7461, grad_fn=<NegBackward0>) tensor(11807.7451, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11807.7919921875
tensor(11807.7451, grad_fn=<NegBackward0>) tensor(11807.7920, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11807.744140625
tensor(11807.7451, grad_fn=<NegBackward0>) tensor(11807.7441, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11807.7080078125
tensor(11807.7441, grad_fn=<NegBackward0>) tensor(11807.7080, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11807.70703125
tensor(11807.7080, grad_fn=<NegBackward0>) tensor(11807.7070, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11807.705078125
tensor(11807.7070, grad_fn=<NegBackward0>) tensor(11807.7051, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11807.8193359375
tensor(11807.7051, grad_fn=<NegBackward0>) tensor(11807.8193, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11807.7060546875
tensor(11807.7051, grad_fn=<NegBackward0>) tensor(11807.7061, grad_fn=<NegBackward0>)
2
pi: tensor([[0.6371, 0.3629],
        [0.5759, 0.4241]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5114, 0.4886], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2465, 0.1061],
         [0.5735, 0.3717]],

        [[0.6186, 0.0945],
         [0.5248, 0.7138]],

        [[0.7289, 0.1031],
         [0.6877, 0.7240]],

        [[0.6188, 0.0944],
         [0.5005, 0.5471]],

        [[0.6096, 0.1017],
         [0.7216, 0.5436]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 11
Adjusted Rand Index: 0.6044444444444445
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 18
Adjusted Rand Index: 0.40440205429200293
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.09810261605438353
Average Adjusted Rand Index: 0.8017692997472896
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21118.478515625
inf tensor(21118.4785, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12301.9541015625
tensor(21118.4785, grad_fn=<NegBackward0>) tensor(12301.9541, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12298.8642578125
tensor(12301.9541, grad_fn=<NegBackward0>) tensor(12298.8643, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12129.72265625
tensor(12298.8643, grad_fn=<NegBackward0>) tensor(12129.7227, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11754.232421875
tensor(12129.7227, grad_fn=<NegBackward0>) tensor(11754.2324, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11730.216796875
tensor(11754.2324, grad_fn=<NegBackward0>) tensor(11730.2168, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11728.998046875
tensor(11730.2168, grad_fn=<NegBackward0>) tensor(11728.9980, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11728.7900390625
tensor(11728.9980, grad_fn=<NegBackward0>) tensor(11728.7900, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11728.6708984375
tensor(11728.7900, grad_fn=<NegBackward0>) tensor(11728.6709, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11728.5361328125
tensor(11728.6709, grad_fn=<NegBackward0>) tensor(11728.5361, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11719.076171875
tensor(11728.5361, grad_fn=<NegBackward0>) tensor(11719.0762, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11718.9453125
tensor(11719.0762, grad_fn=<NegBackward0>) tensor(11718.9453, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11718.912109375
tensor(11718.9453, grad_fn=<NegBackward0>) tensor(11718.9121, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11718.8876953125
tensor(11718.9121, grad_fn=<NegBackward0>) tensor(11718.8877, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11718.8662109375
tensor(11718.8877, grad_fn=<NegBackward0>) tensor(11718.8662, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11718.8525390625
tensor(11718.8662, grad_fn=<NegBackward0>) tensor(11718.8525, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11718.841796875
tensor(11718.8525, grad_fn=<NegBackward0>) tensor(11718.8418, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11718.83203125
tensor(11718.8418, grad_fn=<NegBackward0>) tensor(11718.8320, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11718.82421875
tensor(11718.8320, grad_fn=<NegBackward0>) tensor(11718.8242, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11718.8173828125
tensor(11718.8242, grad_fn=<NegBackward0>) tensor(11718.8174, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11718.8125
tensor(11718.8174, grad_fn=<NegBackward0>) tensor(11718.8125, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11718.806640625
tensor(11718.8125, grad_fn=<NegBackward0>) tensor(11718.8066, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11718.802734375
tensor(11718.8066, grad_fn=<NegBackward0>) tensor(11718.8027, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11718.796875
tensor(11718.8027, grad_fn=<NegBackward0>) tensor(11718.7969, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11718.7373046875
tensor(11718.7969, grad_fn=<NegBackward0>) tensor(11718.7373, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11718.7333984375
tensor(11718.7373, grad_fn=<NegBackward0>) tensor(11718.7334, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11718.7294921875
tensor(11718.7334, grad_fn=<NegBackward0>) tensor(11718.7295, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11718.7275390625
tensor(11718.7295, grad_fn=<NegBackward0>) tensor(11718.7275, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11718.7255859375
tensor(11718.7275, grad_fn=<NegBackward0>) tensor(11718.7256, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11718.724609375
tensor(11718.7256, grad_fn=<NegBackward0>) tensor(11718.7246, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11718.72265625
tensor(11718.7246, grad_fn=<NegBackward0>) tensor(11718.7227, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11718.7216796875
tensor(11718.7227, grad_fn=<NegBackward0>) tensor(11718.7217, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11718.720703125
tensor(11718.7217, grad_fn=<NegBackward0>) tensor(11718.7207, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11718.7177734375
tensor(11718.7207, grad_fn=<NegBackward0>) tensor(11718.7178, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11718.7177734375
tensor(11718.7178, grad_fn=<NegBackward0>) tensor(11718.7178, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11718.7158203125
tensor(11718.7178, grad_fn=<NegBackward0>) tensor(11718.7158, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11718.71484375
tensor(11718.7158, grad_fn=<NegBackward0>) tensor(11718.7148, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11718.71484375
tensor(11718.7148, grad_fn=<NegBackward0>) tensor(11718.7148, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11718.712890625
tensor(11718.7148, grad_fn=<NegBackward0>) tensor(11718.7129, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11718.712890625
tensor(11718.7129, grad_fn=<NegBackward0>) tensor(11718.7129, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11718.7119140625
tensor(11718.7129, grad_fn=<NegBackward0>) tensor(11718.7119, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11718.7109375
tensor(11718.7119, grad_fn=<NegBackward0>) tensor(11718.7109, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11718.7109375
tensor(11718.7109, grad_fn=<NegBackward0>) tensor(11718.7109, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11718.7099609375
tensor(11718.7109, grad_fn=<NegBackward0>) tensor(11718.7100, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11718.7119140625
tensor(11718.7100, grad_fn=<NegBackward0>) tensor(11718.7119, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11718.708984375
tensor(11718.7100, grad_fn=<NegBackward0>) tensor(11718.7090, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11718.7099609375
tensor(11718.7090, grad_fn=<NegBackward0>) tensor(11718.7100, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11718.70703125
tensor(11718.7090, grad_fn=<NegBackward0>) tensor(11718.7070, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11718.708984375
tensor(11718.7070, grad_fn=<NegBackward0>) tensor(11718.7090, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11718.7099609375
tensor(11718.7070, grad_fn=<NegBackward0>) tensor(11718.7100, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11718.70703125
tensor(11718.7070, grad_fn=<NegBackward0>) tensor(11718.7070, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11718.72265625
tensor(11718.7070, grad_fn=<NegBackward0>) tensor(11718.7227, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11718.70703125
tensor(11718.7070, grad_fn=<NegBackward0>) tensor(11718.7070, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11718.7060546875
tensor(11718.7070, grad_fn=<NegBackward0>) tensor(11718.7061, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11718.7060546875
tensor(11718.7061, grad_fn=<NegBackward0>) tensor(11718.7061, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11718.705078125
tensor(11718.7061, grad_fn=<NegBackward0>) tensor(11718.7051, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11718.7041015625
tensor(11718.7051, grad_fn=<NegBackward0>) tensor(11718.7041, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11718.7041015625
tensor(11718.7041, grad_fn=<NegBackward0>) tensor(11718.7041, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11718.70703125
tensor(11718.7041, grad_fn=<NegBackward0>) tensor(11718.7070, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11718.7041015625
tensor(11718.7041, grad_fn=<NegBackward0>) tensor(11718.7041, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11718.705078125
tensor(11718.7041, grad_fn=<NegBackward0>) tensor(11718.7051, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11718.703125
tensor(11718.7041, grad_fn=<NegBackward0>) tensor(11718.7031, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11718.703125
tensor(11718.7031, grad_fn=<NegBackward0>) tensor(11718.7031, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11718.7041015625
tensor(11718.7031, grad_fn=<NegBackward0>) tensor(11718.7041, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11718.703125
tensor(11718.7031, grad_fn=<NegBackward0>) tensor(11718.7031, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11718.701171875
tensor(11718.7031, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11718.7021484375
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7021, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11718.7041015625
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7041, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11718.7021484375
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7021, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11718.7021484375
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7021, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11718.701171875
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11718.703125
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7031, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11718.701171875
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11718.701171875
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11718.7099609375
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7100, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11718.701171875
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11718.701171875
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11718.7021484375
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7021, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11718.7275390625
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7275, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11718.701171875
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11718.7021484375
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7021, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11718.701171875
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11718.7001953125
tensor(11718.7012, grad_fn=<NegBackward0>) tensor(11718.7002, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11718.703125
tensor(11718.7002, grad_fn=<NegBackward0>) tensor(11718.7031, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11718.701171875
tensor(11718.7002, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11718.701171875
tensor(11718.7002, grad_fn=<NegBackward0>) tensor(11718.7012, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11718.7021484375
tensor(11718.7002, grad_fn=<NegBackward0>) tensor(11718.7021, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11718.7138671875
tensor(11718.7002, grad_fn=<NegBackward0>) tensor(11718.7139, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.4704, 0.5296],
        [0.3784, 0.6216]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4828, 0.5172], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3951, 0.1068],
         [0.5907, 0.2193]],

        [[0.5193, 0.0985],
         [0.6275, 0.6359]],

        [[0.5653, 0.1021],
         [0.6547, 0.6085]],

        [[0.6323, 0.0949],
         [0.5789, 0.6074]],

        [[0.7288, 0.1020],
         [0.5129, 0.6015]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 76
Adjusted Rand Index: 0.2646713890322278
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4833955656512149
Average Adjusted Rand Index: 0.8529342778064455
[0.09810261605438353, 0.4833955656512149] [0.8017692997472896, 0.8529342778064455] [11807.75, 11718.7138671875]
-------------------------------------
This iteration is 19
True Objective function: Loss = -11704.979080462863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23271.12890625
inf tensor(23271.1289, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12147.390625
tensor(23271.1289, grad_fn=<NegBackward0>) tensor(12147.3906, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12145.849609375
tensor(12147.3906, grad_fn=<NegBackward0>) tensor(12145.8496, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12145.44921875
tensor(12145.8496, grad_fn=<NegBackward0>) tensor(12145.4492, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12073.068359375
tensor(12145.4492, grad_fn=<NegBackward0>) tensor(12073.0684, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11986.2470703125
tensor(12073.0684, grad_fn=<NegBackward0>) tensor(11986.2471, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11978.30859375
tensor(11986.2471, grad_fn=<NegBackward0>) tensor(11978.3086, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11974.48828125
tensor(11978.3086, grad_fn=<NegBackward0>) tensor(11974.4883, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11972.8603515625
tensor(11974.4883, grad_fn=<NegBackward0>) tensor(11972.8604, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11971.876953125
tensor(11972.8604, grad_fn=<NegBackward0>) tensor(11971.8770, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11970.3623046875
tensor(11971.8770, grad_fn=<NegBackward0>) tensor(11970.3623, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11970.33984375
tensor(11970.3623, grad_fn=<NegBackward0>) tensor(11970.3398, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11970.2958984375
tensor(11970.3398, grad_fn=<NegBackward0>) tensor(11970.2959, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11970.2685546875
tensor(11970.2959, grad_fn=<NegBackward0>) tensor(11970.2686, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11970.263671875
tensor(11970.2686, grad_fn=<NegBackward0>) tensor(11970.2637, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11970.2607421875
tensor(11970.2637, grad_fn=<NegBackward0>) tensor(11970.2607, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11970.2578125
tensor(11970.2607, grad_fn=<NegBackward0>) tensor(11970.2578, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11970.2548828125
tensor(11970.2578, grad_fn=<NegBackward0>) tensor(11970.2549, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11970.2529296875
tensor(11970.2549, grad_fn=<NegBackward0>) tensor(11970.2529, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11970.251953125
tensor(11970.2529, grad_fn=<NegBackward0>) tensor(11970.2520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11970.240234375
tensor(11970.2520, grad_fn=<NegBackward0>) tensor(11970.2402, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11970.1513671875
tensor(11970.2402, grad_fn=<NegBackward0>) tensor(11970.1514, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11970.1513671875
tensor(11970.1514, grad_fn=<NegBackward0>) tensor(11970.1514, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11970.1494140625
tensor(11970.1514, grad_fn=<NegBackward0>) tensor(11970.1494, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11970.1484375
tensor(11970.1494, grad_fn=<NegBackward0>) tensor(11970.1484, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11970.1484375
tensor(11970.1484, grad_fn=<NegBackward0>) tensor(11970.1484, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11970.146484375
tensor(11970.1484, grad_fn=<NegBackward0>) tensor(11970.1465, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11970.14453125
tensor(11970.1465, grad_fn=<NegBackward0>) tensor(11970.1445, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11970.0888671875
tensor(11970.1445, grad_fn=<NegBackward0>) tensor(11970.0889, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11970.0830078125
tensor(11970.0889, grad_fn=<NegBackward0>) tensor(11970.0830, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11970.080078125
tensor(11970.0830, grad_fn=<NegBackward0>) tensor(11970.0801, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11970.078125
tensor(11970.0801, grad_fn=<NegBackward0>) tensor(11970.0781, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11970.080078125
tensor(11970.0781, grad_fn=<NegBackward0>) tensor(11970.0801, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11970.080078125
tensor(11970.0781, grad_fn=<NegBackward0>) tensor(11970.0801, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -11970.080078125
tensor(11970.0781, grad_fn=<NegBackward0>) tensor(11970.0801, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -11970.0791015625
tensor(11970.0781, grad_fn=<NegBackward0>) tensor(11970.0791, grad_fn=<NegBackward0>)
4
Iteration 3600: Loss = -11970.0771484375
tensor(11970.0781, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11970.0791015625
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0791, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11970.0859375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0859, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11970.078125
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0781, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11970.0810546875
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0811, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11970.0869140625
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0869, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11970.078125
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0781, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11970.0771484375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11970.08984375
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0898, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11970.076171875
tensor(11970.0771, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11970.0771484375
tensor(11970.0762, grad_fn=<NegBackward0>) tensor(11970.0771, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11970.076171875
tensor(11970.0762, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11970.076171875
tensor(11970.0762, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11970.076171875
tensor(11970.0762, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11970.0751953125
tensor(11970.0762, grad_fn=<NegBackward0>) tensor(11970.0752, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11970.1025390625
tensor(11970.0752, grad_fn=<NegBackward0>) tensor(11970.1025, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11970.0751953125
tensor(11970.0752, grad_fn=<NegBackward0>) tensor(11970.0752, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11970.0927734375
tensor(11970.0752, grad_fn=<NegBackward0>) tensor(11970.0928, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11970.0751953125
tensor(11970.0752, grad_fn=<NegBackward0>) tensor(11970.0752, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11970.076171875
tensor(11970.0752, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11970.07421875
tensor(11970.0752, grad_fn=<NegBackward0>) tensor(11970.0742, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11970.0751953125
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0752, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11970.076171875
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11970.07421875
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0742, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11970.0751953125
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0752, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11970.07421875
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0742, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11970.0751953125
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0752, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11970.076171875
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11970.076171875
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0762, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11970.0751953125
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0752, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11970.078125
tensor(11970.0742, grad_fn=<NegBackward0>) tensor(11970.0781, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.5868, 0.4132],
        [0.5827, 0.4173]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5425, 0.4575], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2386, 0.1080],
         [0.7271, 0.3853]],

        [[0.7131, 0.1023],
         [0.7189, 0.5796]],

        [[0.6008, 0.0976],
         [0.5138, 0.5783]],

        [[0.6548, 0.1018],
         [0.5286, 0.6717]],

        [[0.5210, 0.1182],
         [0.5782, 0.5390]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 20
Adjusted Rand Index: 0.3545232273838631
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 22
Adjusted Rand Index: 0.3069711157192286
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.13078527259622996
Average Adjusted Rand Index: 0.7242988686206184
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22994.11328125
inf tensor(22994.1133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12462.8388671875
tensor(22994.1133, grad_fn=<NegBackward0>) tensor(12462.8389, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12188.501953125
tensor(12462.8389, grad_fn=<NegBackward0>) tensor(12188.5020, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11972.03125
tensor(12188.5020, grad_fn=<NegBackward0>) tensor(11972.0312, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11887.048828125
tensor(11972.0312, grad_fn=<NegBackward0>) tensor(11887.0488, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11865.6884765625
tensor(11887.0488, grad_fn=<NegBackward0>) tensor(11865.6885, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11850.6162109375
tensor(11865.6885, grad_fn=<NegBackward0>) tensor(11850.6162, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11850.4375
tensor(11850.6162, grad_fn=<NegBackward0>) tensor(11850.4375, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11850.28515625
tensor(11850.4375, grad_fn=<NegBackward0>) tensor(11850.2852, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11840.001953125
tensor(11850.2852, grad_fn=<NegBackward0>) tensor(11840.0020, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11839.9462890625
tensor(11840.0020, grad_fn=<NegBackward0>) tensor(11839.9463, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11839.904296875
tensor(11839.9463, grad_fn=<NegBackward0>) tensor(11839.9043, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11839.87109375
tensor(11839.9043, grad_fn=<NegBackward0>) tensor(11839.8711, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11839.8447265625
tensor(11839.8711, grad_fn=<NegBackward0>) tensor(11839.8447, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11839.82421875
tensor(11839.8447, grad_fn=<NegBackward0>) tensor(11839.8242, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11839.8076171875
tensor(11839.8242, grad_fn=<NegBackward0>) tensor(11839.8076, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11839.771484375
tensor(11839.8076, grad_fn=<NegBackward0>) tensor(11839.7715, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11836.7685546875
tensor(11839.7715, grad_fn=<NegBackward0>) tensor(11836.7686, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11836.7578125
tensor(11836.7686, grad_fn=<NegBackward0>) tensor(11836.7578, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11836.748046875
tensor(11836.7578, grad_fn=<NegBackward0>) tensor(11836.7480, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11836.740234375
tensor(11836.7480, grad_fn=<NegBackward0>) tensor(11836.7402, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11836.7333984375
tensor(11836.7402, grad_fn=<NegBackward0>) tensor(11836.7334, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11836.7236328125
tensor(11836.7334, grad_fn=<NegBackward0>) tensor(11836.7236, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11836.71484375
tensor(11836.7236, grad_fn=<NegBackward0>) tensor(11836.7148, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11836.689453125
tensor(11836.7148, grad_fn=<NegBackward0>) tensor(11836.6895, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11836.267578125
tensor(11836.6895, grad_fn=<NegBackward0>) tensor(11836.2676, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11836.1943359375
tensor(11836.2676, grad_fn=<NegBackward0>) tensor(11836.1943, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11836.1806640625
tensor(11836.1943, grad_fn=<NegBackward0>) tensor(11836.1807, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11836.1650390625
tensor(11836.1807, grad_fn=<NegBackward0>) tensor(11836.1650, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11836.1572265625
tensor(11836.1650, grad_fn=<NegBackward0>) tensor(11836.1572, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11836.1513671875
tensor(11836.1572, grad_fn=<NegBackward0>) tensor(11836.1514, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11836.115234375
tensor(11836.1514, grad_fn=<NegBackward0>) tensor(11836.1152, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11836.111328125
tensor(11836.1152, grad_fn=<NegBackward0>) tensor(11836.1113, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11836.109375
tensor(11836.1113, grad_fn=<NegBackward0>) tensor(11836.1094, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11836.1083984375
tensor(11836.1094, grad_fn=<NegBackward0>) tensor(11836.1084, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11836.107421875
tensor(11836.1084, grad_fn=<NegBackward0>) tensor(11836.1074, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11836.1064453125
tensor(11836.1074, grad_fn=<NegBackward0>) tensor(11836.1064, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11836.1044921875
tensor(11836.1064, grad_fn=<NegBackward0>) tensor(11836.1045, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11836.103515625
tensor(11836.1045, grad_fn=<NegBackward0>) tensor(11836.1035, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11836.103515625
tensor(11836.1035, grad_fn=<NegBackward0>) tensor(11836.1035, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11836.1015625
tensor(11836.1035, grad_fn=<NegBackward0>) tensor(11836.1016, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11836.1005859375
tensor(11836.1016, grad_fn=<NegBackward0>) tensor(11836.1006, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11836.099609375
tensor(11836.1006, grad_fn=<NegBackward0>) tensor(11836.0996, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11836.099609375
tensor(11836.0996, grad_fn=<NegBackward0>) tensor(11836.0996, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11836.0986328125
tensor(11836.0996, grad_fn=<NegBackward0>) tensor(11836.0986, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11836.09765625
tensor(11836.0986, grad_fn=<NegBackward0>) tensor(11836.0977, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11836.0966796875
tensor(11836.0977, grad_fn=<NegBackward0>) tensor(11836.0967, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11836.09765625
tensor(11836.0967, grad_fn=<NegBackward0>) tensor(11836.0977, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11836.10546875
tensor(11836.0967, grad_fn=<NegBackward0>) tensor(11836.1055, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11834.62109375
tensor(11836.0967, grad_fn=<NegBackward0>) tensor(11834.6211, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11834.5888671875
tensor(11834.6211, grad_fn=<NegBackward0>) tensor(11834.5889, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11834.5869140625
tensor(11834.5889, grad_fn=<NegBackward0>) tensor(11834.5869, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11834.5849609375
tensor(11834.5869, grad_fn=<NegBackward0>) tensor(11834.5850, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11834.5751953125
tensor(11834.5850, grad_fn=<NegBackward0>) tensor(11834.5752, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11834.5498046875
tensor(11834.5752, grad_fn=<NegBackward0>) tensor(11834.5498, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11834.5478515625
tensor(11834.5498, grad_fn=<NegBackward0>) tensor(11834.5479, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11834.5458984375
tensor(11834.5479, grad_fn=<NegBackward0>) tensor(11834.5459, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11834.4443359375
tensor(11834.5459, grad_fn=<NegBackward0>) tensor(11834.4443, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11833.455078125
tensor(11834.4443, grad_fn=<NegBackward0>) tensor(11833.4551, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11833.4541015625
tensor(11833.4551, grad_fn=<NegBackward0>) tensor(11833.4541, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11833.431640625
tensor(11833.4541, grad_fn=<NegBackward0>) tensor(11833.4316, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11833.4248046875
tensor(11833.4316, grad_fn=<NegBackward0>) tensor(11833.4248, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11833.42578125
tensor(11833.4248, grad_fn=<NegBackward0>) tensor(11833.4258, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11833.423828125
tensor(11833.4248, grad_fn=<NegBackward0>) tensor(11833.4238, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11833.42578125
tensor(11833.4238, grad_fn=<NegBackward0>) tensor(11833.4258, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11833.443359375
tensor(11833.4238, grad_fn=<NegBackward0>) tensor(11833.4434, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11833.4228515625
tensor(11833.4238, grad_fn=<NegBackward0>) tensor(11833.4229, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11833.423828125
tensor(11833.4229, grad_fn=<NegBackward0>) tensor(11833.4238, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11833.4228515625
tensor(11833.4229, grad_fn=<NegBackward0>) tensor(11833.4229, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11833.421875
tensor(11833.4229, grad_fn=<NegBackward0>) tensor(11833.4219, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11833.421875
tensor(11833.4219, grad_fn=<NegBackward0>) tensor(11833.4219, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11828.8876953125
tensor(11833.4219, grad_fn=<NegBackward0>) tensor(11828.8877, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11828.880859375
tensor(11828.8877, grad_fn=<NegBackward0>) tensor(11828.8809, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11828.8818359375
tensor(11828.8809, grad_fn=<NegBackward0>) tensor(11828.8818, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11828.8798828125
tensor(11828.8809, grad_fn=<NegBackward0>) tensor(11828.8799, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11828.8798828125
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8799, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11828.880859375
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8809, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11828.8837890625
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8838, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11828.8876953125
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8877, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11828.880859375
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8809, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11828.8798828125
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8799, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11828.8798828125
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8799, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11828.91796875
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.9180, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11828.8798828125
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8799, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11828.8857421875
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8857, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11828.8798828125
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8799, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11828.8935546875
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8936, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11828.8779296875
tensor(11828.8799, grad_fn=<NegBackward0>) tensor(11828.8779, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11828.8798828125
tensor(11828.8779, grad_fn=<NegBackward0>) tensor(11828.8799, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11828.8310546875
tensor(11828.8779, grad_fn=<NegBackward0>) tensor(11828.8311, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11828.8310546875
tensor(11828.8311, grad_fn=<NegBackward0>) tensor(11828.8311, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11828.83203125
tensor(11828.8311, grad_fn=<NegBackward0>) tensor(11828.8320, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11828.833984375
tensor(11828.8311, grad_fn=<NegBackward0>) tensor(11828.8340, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11828.8349609375
tensor(11828.8311, grad_fn=<NegBackward0>) tensor(11828.8350, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11828.830078125
tensor(11828.8311, grad_fn=<NegBackward0>) tensor(11828.8301, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11828.83203125
tensor(11828.8301, grad_fn=<NegBackward0>) tensor(11828.8320, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11828.9091796875
tensor(11828.8301, grad_fn=<NegBackward0>) tensor(11828.9092, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11828.8212890625
tensor(11828.8301, grad_fn=<NegBackward0>) tensor(11828.8213, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11828.943359375
tensor(11828.8213, grad_fn=<NegBackward0>) tensor(11828.9434, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11828.8203125
tensor(11828.8213, grad_fn=<NegBackward0>) tensor(11828.8203, grad_fn=<NegBackward0>)
pi: tensor([[0.6487, 0.3513],
        [0.2768, 0.7232]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9134, 0.0866], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2068, 0.1018],
         [0.6680, 0.4083]],

        [[0.5406, 0.1010],
         [0.6697, 0.5166]],

        [[0.5836, 0.1023],
         [0.6922, 0.6583]],

        [[0.5340, 0.0962],
         [0.5443, 0.6895]],

        [[0.5462, 0.1184],
         [0.5928, 0.5034]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6138982435699294
Average Adjusted Rand Index: 0.7907062870368
[0.13078527259622996, 0.6138982435699294] [0.7242988686206184, 0.7907062870368] [11970.078125, 11828.8369140625]
-------------------------------------
This iteration is 20
True Objective function: Loss = -11554.688311123846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21659.892578125
inf tensor(21659.8926, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12319.7197265625
tensor(21659.8926, grad_fn=<NegBackward0>) tensor(12319.7197, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11627.8486328125
tensor(12319.7197, grad_fn=<NegBackward0>) tensor(11627.8486, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11572.71875
tensor(11627.8486, grad_fn=<NegBackward0>) tensor(11572.7188, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11572.25390625
tensor(11572.7188, grad_fn=<NegBackward0>) tensor(11572.2539, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11566.0224609375
tensor(11572.2539, grad_fn=<NegBackward0>) tensor(11566.0225, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11565.908203125
tensor(11566.0225, grad_fn=<NegBackward0>) tensor(11565.9082, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11555.515625
tensor(11565.9082, grad_fn=<NegBackward0>) tensor(11555.5156, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11555.4052734375
tensor(11555.5156, grad_fn=<NegBackward0>) tensor(11555.4053, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11550.0986328125
tensor(11555.4053, grad_fn=<NegBackward0>) tensor(11550.0986, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11550.0732421875
tensor(11550.0986, grad_fn=<NegBackward0>) tensor(11550.0732, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11550.0556640625
tensor(11550.0732, grad_fn=<NegBackward0>) tensor(11550.0557, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11550.0380859375
tensor(11550.0557, grad_fn=<NegBackward0>) tensor(11550.0381, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11550.02734375
tensor(11550.0381, grad_fn=<NegBackward0>) tensor(11550.0273, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11550.0185546875
tensor(11550.0273, grad_fn=<NegBackward0>) tensor(11550.0186, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11550.0078125
tensor(11550.0186, grad_fn=<NegBackward0>) tensor(11550.0078, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11550.0029296875
tensor(11550.0078, grad_fn=<NegBackward0>) tensor(11550.0029, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11549.998046875
tensor(11550.0029, grad_fn=<NegBackward0>) tensor(11549.9980, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11549.9912109375
tensor(11549.9980, grad_fn=<NegBackward0>) tensor(11549.9912, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11549.986328125
tensor(11549.9912, grad_fn=<NegBackward0>) tensor(11549.9863, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11549.982421875
tensor(11549.9863, grad_fn=<NegBackward0>) tensor(11549.9824, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11549.98046875
tensor(11549.9824, grad_fn=<NegBackward0>) tensor(11549.9805, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11549.978515625
tensor(11549.9805, grad_fn=<NegBackward0>) tensor(11549.9785, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11549.9755859375
tensor(11549.9785, grad_fn=<NegBackward0>) tensor(11549.9756, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11549.97265625
tensor(11549.9756, grad_fn=<NegBackward0>) tensor(11549.9727, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11549.9716796875
tensor(11549.9727, grad_fn=<NegBackward0>) tensor(11549.9717, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11549.96875
tensor(11549.9717, grad_fn=<NegBackward0>) tensor(11549.9688, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11549.9697265625
tensor(11549.9688, grad_fn=<NegBackward0>) tensor(11549.9697, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11549.9658203125
tensor(11549.9688, grad_fn=<NegBackward0>) tensor(11549.9658, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11549.962890625
tensor(11549.9658, grad_fn=<NegBackward0>) tensor(11549.9629, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11549.9619140625
tensor(11549.9629, grad_fn=<NegBackward0>) tensor(11549.9619, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11549.958984375
tensor(11549.9619, grad_fn=<NegBackward0>) tensor(11549.9590, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11549.958984375
tensor(11549.9590, grad_fn=<NegBackward0>) tensor(11549.9590, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11549.9580078125
tensor(11549.9590, grad_fn=<NegBackward0>) tensor(11549.9580, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11549.9560546875
tensor(11549.9580, grad_fn=<NegBackward0>) tensor(11549.9561, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11549.955078125
tensor(11549.9561, grad_fn=<NegBackward0>) tensor(11549.9551, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11549.955078125
tensor(11549.9551, grad_fn=<NegBackward0>) tensor(11549.9551, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11549.9658203125
tensor(11549.9551, grad_fn=<NegBackward0>) tensor(11549.9658, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11549.9541015625
tensor(11549.9551, grad_fn=<NegBackward0>) tensor(11549.9541, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11549.953125
tensor(11549.9541, grad_fn=<NegBackward0>) tensor(11549.9531, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11549.9541015625
tensor(11549.9531, grad_fn=<NegBackward0>) tensor(11549.9541, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11549.9521484375
tensor(11549.9531, grad_fn=<NegBackward0>) tensor(11549.9521, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11549.9521484375
tensor(11549.9521, grad_fn=<NegBackward0>) tensor(11549.9521, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11549.951171875
tensor(11549.9521, grad_fn=<NegBackward0>) tensor(11549.9512, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11549.9501953125
tensor(11549.9512, grad_fn=<NegBackward0>) tensor(11549.9502, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11549.94921875
tensor(11549.9502, grad_fn=<NegBackward0>) tensor(11549.9492, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11549.94921875
tensor(11549.9492, grad_fn=<NegBackward0>) tensor(11549.9492, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11549.94921875
tensor(11549.9492, grad_fn=<NegBackward0>) tensor(11549.9492, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11549.94921875
tensor(11549.9492, grad_fn=<NegBackward0>) tensor(11549.9492, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11549.9521484375
tensor(11549.9492, grad_fn=<NegBackward0>) tensor(11549.9521, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11549.9482421875
tensor(11549.9492, grad_fn=<NegBackward0>) tensor(11549.9482, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11549.94921875
tensor(11549.9482, grad_fn=<NegBackward0>) tensor(11549.9492, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11549.947265625
tensor(11549.9482, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11549.958984375
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9590, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11549.947265625
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11549.962890625
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9629, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11549.9482421875
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9482, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11549.94921875
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9492, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11549.9462890625
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9463, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11549.947265625
tensor(11549.9463, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11549.9453125
tensor(11549.9463, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11549.9482421875
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9482, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11549.9453125
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11549.9462890625
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9463, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11549.9453125
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11549.947265625
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11549.9482421875
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9482, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11549.9443359375
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9443, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11549.9453125
tensor(11549.9443, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11549.9462890625
tensor(11549.9443, grad_fn=<NegBackward0>) tensor(11549.9463, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11549.95703125
tensor(11549.9443, grad_fn=<NegBackward0>) tensor(11549.9570, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11549.943359375
tensor(11549.9443, grad_fn=<NegBackward0>) tensor(11549.9434, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11549.9453125
tensor(11549.9434, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11549.9443359375
tensor(11549.9434, grad_fn=<NegBackward0>) tensor(11549.9443, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11549.9453125
tensor(11549.9434, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11549.9443359375
tensor(11549.9434, grad_fn=<NegBackward0>) tensor(11549.9443, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11549.9453125
tensor(11549.9434, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.7484, 0.2516],
        [0.2272, 0.7728]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5502, 0.4498], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3954, 0.0929],
         [0.5447, 0.1994]],

        [[0.6378, 0.0935],
         [0.6236, 0.7169]],

        [[0.6716, 0.1091],
         [0.7170, 0.6468]],

        [[0.5920, 0.1049],
         [0.5815, 0.5743]],

        [[0.6261, 0.0983],
         [0.6888, 0.5425]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21287.875
inf tensor(21287.8750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12344.6044921875
tensor(21287.8750, grad_fn=<NegBackward0>) tensor(12344.6045, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12284.4990234375
tensor(12344.6045, grad_fn=<NegBackward0>) tensor(12284.4990, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11729.4931640625
tensor(12284.4990, grad_fn=<NegBackward0>) tensor(11729.4932, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11556.94921875
tensor(11729.4932, grad_fn=<NegBackward0>) tensor(11556.9492, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11551.759765625
tensor(11556.9492, grad_fn=<NegBackward0>) tensor(11551.7598, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11551.291015625
tensor(11551.7598, grad_fn=<NegBackward0>) tensor(11551.2910, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11551.033203125
tensor(11551.2910, grad_fn=<NegBackward0>) tensor(11551.0332, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11550.6513671875
tensor(11551.0332, grad_fn=<NegBackward0>) tensor(11550.6514, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11550.5087890625
tensor(11550.6514, grad_fn=<NegBackward0>) tensor(11550.5088, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11550.4287109375
tensor(11550.5088, grad_fn=<NegBackward0>) tensor(11550.4287, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11550.294921875
tensor(11550.4287, grad_fn=<NegBackward0>) tensor(11550.2949, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11550.2490234375
tensor(11550.2949, grad_fn=<NegBackward0>) tensor(11550.2490, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11550.216796875
tensor(11550.2490, grad_fn=<NegBackward0>) tensor(11550.2168, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11550.1904296875
tensor(11550.2168, grad_fn=<NegBackward0>) tensor(11550.1904, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11550.1689453125
tensor(11550.1904, grad_fn=<NegBackward0>) tensor(11550.1689, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11550.150390625
tensor(11550.1689, grad_fn=<NegBackward0>) tensor(11550.1504, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11550.1357421875
tensor(11550.1504, grad_fn=<NegBackward0>) tensor(11550.1357, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11550.1201171875
tensor(11550.1357, grad_fn=<NegBackward0>) tensor(11550.1201, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11550.1064453125
tensor(11550.1201, grad_fn=<NegBackward0>) tensor(11550.1064, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11550.09375
tensor(11550.1064, grad_fn=<NegBackward0>) tensor(11550.0938, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11550.08203125
tensor(11550.0938, grad_fn=<NegBackward0>) tensor(11550.0820, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11550.0732421875
tensor(11550.0820, grad_fn=<NegBackward0>) tensor(11550.0732, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11550.0673828125
tensor(11550.0732, grad_fn=<NegBackward0>) tensor(11550.0674, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11550.0615234375
tensor(11550.0674, grad_fn=<NegBackward0>) tensor(11550.0615, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11550.0556640625
tensor(11550.0615, grad_fn=<NegBackward0>) tensor(11550.0557, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11550.0517578125
tensor(11550.0557, grad_fn=<NegBackward0>) tensor(11550.0518, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11550.0458984375
tensor(11550.0518, grad_fn=<NegBackward0>) tensor(11550.0459, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11550.0419921875
tensor(11550.0459, grad_fn=<NegBackward0>) tensor(11550.0420, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11550.0380859375
tensor(11550.0420, grad_fn=<NegBackward0>) tensor(11550.0381, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11550.03515625
tensor(11550.0381, grad_fn=<NegBackward0>) tensor(11550.0352, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11550.0322265625
tensor(11550.0352, grad_fn=<NegBackward0>) tensor(11550.0322, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11550.029296875
tensor(11550.0322, grad_fn=<NegBackward0>) tensor(11550.0293, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11550.0263671875
tensor(11550.0293, grad_fn=<NegBackward0>) tensor(11550.0264, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11550.025390625
tensor(11550.0264, grad_fn=<NegBackward0>) tensor(11550.0254, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11550.0166015625
tensor(11550.0254, grad_fn=<NegBackward0>) tensor(11550.0166, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11549.984375
tensor(11550.0166, grad_fn=<NegBackward0>) tensor(11549.9844, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11549.982421875
tensor(11549.9844, grad_fn=<NegBackward0>) tensor(11549.9824, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11549.98046875
tensor(11549.9824, grad_fn=<NegBackward0>) tensor(11549.9805, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11549.978515625
tensor(11549.9805, grad_fn=<NegBackward0>) tensor(11549.9785, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11549.9775390625
tensor(11549.9785, grad_fn=<NegBackward0>) tensor(11549.9775, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11549.982421875
tensor(11549.9775, grad_fn=<NegBackward0>) tensor(11549.9824, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11549.9765625
tensor(11549.9775, grad_fn=<NegBackward0>) tensor(11549.9766, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11549.974609375
tensor(11549.9766, grad_fn=<NegBackward0>) tensor(11549.9746, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11549.9736328125
tensor(11549.9746, grad_fn=<NegBackward0>) tensor(11549.9736, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11549.9736328125
tensor(11549.9736, grad_fn=<NegBackward0>) tensor(11549.9736, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11549.98046875
tensor(11549.9736, grad_fn=<NegBackward0>) tensor(11549.9805, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11549.9716796875
tensor(11549.9736, grad_fn=<NegBackward0>) tensor(11549.9717, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11549.970703125
tensor(11549.9717, grad_fn=<NegBackward0>) tensor(11549.9707, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11549.9697265625
tensor(11549.9707, grad_fn=<NegBackward0>) tensor(11549.9697, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11549.9638671875
tensor(11549.9697, grad_fn=<NegBackward0>) tensor(11549.9639, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11549.962890625
tensor(11549.9639, grad_fn=<NegBackward0>) tensor(11549.9629, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11549.9609375
tensor(11549.9629, grad_fn=<NegBackward0>) tensor(11549.9609, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11549.9599609375
tensor(11549.9609, grad_fn=<NegBackward0>) tensor(11549.9600, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11549.9609375
tensor(11549.9600, grad_fn=<NegBackward0>) tensor(11549.9609, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11549.9599609375
tensor(11549.9600, grad_fn=<NegBackward0>) tensor(11549.9600, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11549.958984375
tensor(11549.9600, grad_fn=<NegBackward0>) tensor(11549.9590, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11549.958984375
tensor(11549.9590, grad_fn=<NegBackward0>) tensor(11549.9590, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11549.9580078125
tensor(11549.9590, grad_fn=<NegBackward0>) tensor(11549.9580, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11549.974609375
tensor(11549.9580, grad_fn=<NegBackward0>) tensor(11549.9746, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11549.966796875
tensor(11549.9580, grad_fn=<NegBackward0>) tensor(11549.9668, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11549.95703125
tensor(11549.9580, grad_fn=<NegBackward0>) tensor(11549.9570, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11549.95703125
tensor(11549.9570, grad_fn=<NegBackward0>) tensor(11549.9570, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11549.95703125
tensor(11549.9570, grad_fn=<NegBackward0>) tensor(11549.9570, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11549.955078125
tensor(11549.9570, grad_fn=<NegBackward0>) tensor(11549.9551, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11549.95703125
tensor(11549.9551, grad_fn=<NegBackward0>) tensor(11549.9570, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11549.947265625
tensor(11549.9551, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11549.9501953125
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9502, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11549.947265625
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11549.947265625
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11549.947265625
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11549.9580078125
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9580, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11549.9453125
tensor(11549.9473, grad_fn=<NegBackward0>) tensor(11549.9453, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11549.9462890625
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9463, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11549.9609375
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9609, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11549.947265625
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9473, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11549.9482421875
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9482, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11549.9462890625
tensor(11549.9453, grad_fn=<NegBackward0>) tensor(11549.9463, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7478, 0.2522],
        [0.2274, 0.7726]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5502, 0.4498], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3955, 0.0929],
         [0.6424, 0.1994]],

        [[0.6701, 0.0935],
         [0.6427, 0.6104]],

        [[0.6400, 0.1091],
         [0.5553, 0.5344]],

        [[0.7272, 0.1049],
         [0.5686, 0.5894]],

        [[0.6522, 0.0983],
         [0.7261, 0.6159]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11549.9453125, 11549.9462890625]
-------------------------------------
This iteration is 21
True Objective function: Loss = -11599.887283587863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19630.8359375
inf tensor(19630.8359, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12267.1162109375
tensor(19630.8359, grad_fn=<NegBackward0>) tensor(12267.1162, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11739.59765625
tensor(12267.1162, grad_fn=<NegBackward0>) tensor(11739.5977, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11712.6591796875
tensor(11739.5977, grad_fn=<NegBackward0>) tensor(11712.6592, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11712.201171875
tensor(11712.6592, grad_fn=<NegBackward0>) tensor(11712.2012, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11709.9443359375
tensor(11712.2012, grad_fn=<NegBackward0>) tensor(11709.9443, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11709.533203125
tensor(11709.9443, grad_fn=<NegBackward0>) tensor(11709.5332, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11709.3974609375
tensor(11709.5332, grad_fn=<NegBackward0>) tensor(11709.3975, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11709.349609375
tensor(11709.3975, grad_fn=<NegBackward0>) tensor(11709.3496, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11709.3173828125
tensor(11709.3496, grad_fn=<NegBackward0>) tensor(11709.3174, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11706.9716796875
tensor(11709.3174, grad_fn=<NegBackward0>) tensor(11706.9717, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11705.8115234375
tensor(11706.9717, grad_fn=<NegBackward0>) tensor(11705.8115, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11705.794921875
tensor(11705.8115, grad_fn=<NegBackward0>) tensor(11705.7949, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11705.76953125
tensor(11705.7949, grad_fn=<NegBackward0>) tensor(11705.7695, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11705.75
tensor(11705.7695, grad_fn=<NegBackward0>) tensor(11705.7500, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11705.7294921875
tensor(11705.7500, grad_fn=<NegBackward0>) tensor(11705.7295, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11705.7041015625
tensor(11705.7295, grad_fn=<NegBackward0>) tensor(11705.7041, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11705.6728515625
tensor(11705.7041, grad_fn=<NegBackward0>) tensor(11705.6729, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11705.61328125
tensor(11705.6729, grad_fn=<NegBackward0>) tensor(11705.6133, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11704.0234375
tensor(11705.6133, grad_fn=<NegBackward0>) tensor(11704.0234, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11589.9228515625
tensor(11704.0234, grad_fn=<NegBackward0>) tensor(11589.9229, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11589.798828125
tensor(11589.9229, grad_fn=<NegBackward0>) tensor(11589.7988, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11589.7529296875
tensor(11589.7988, grad_fn=<NegBackward0>) tensor(11589.7529, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11589.72265625
tensor(11589.7529, grad_fn=<NegBackward0>) tensor(11589.7227, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11589.7060546875
tensor(11589.7227, grad_fn=<NegBackward0>) tensor(11589.7061, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11589.6962890625
tensor(11589.7061, grad_fn=<NegBackward0>) tensor(11589.6963, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11589.6865234375
tensor(11589.6963, grad_fn=<NegBackward0>) tensor(11589.6865, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11589.6806640625
tensor(11589.6865, grad_fn=<NegBackward0>) tensor(11589.6807, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11589.6953125
tensor(11589.6807, grad_fn=<NegBackward0>) tensor(11589.6953, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11589.6728515625
tensor(11589.6807, grad_fn=<NegBackward0>) tensor(11589.6729, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11589.6689453125
tensor(11589.6729, grad_fn=<NegBackward0>) tensor(11589.6689, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11589.6669921875
tensor(11589.6689, grad_fn=<NegBackward0>) tensor(11589.6670, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11589.6650390625
tensor(11589.6670, grad_fn=<NegBackward0>) tensor(11589.6650, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11589.6708984375
tensor(11589.6650, grad_fn=<NegBackward0>) tensor(11589.6709, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11589.6611328125
tensor(11589.6650, grad_fn=<NegBackward0>) tensor(11589.6611, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11589.6591796875
tensor(11589.6611, grad_fn=<NegBackward0>) tensor(11589.6592, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11589.658203125
tensor(11589.6592, grad_fn=<NegBackward0>) tensor(11589.6582, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11589.6572265625
tensor(11589.6582, grad_fn=<NegBackward0>) tensor(11589.6572, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11589.6708984375
tensor(11589.6572, grad_fn=<NegBackward0>) tensor(11589.6709, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11589.654296875
tensor(11589.6572, grad_fn=<NegBackward0>) tensor(11589.6543, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11589.6533203125
tensor(11589.6543, grad_fn=<NegBackward0>) tensor(11589.6533, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11589.65234375
tensor(11589.6533, grad_fn=<NegBackward0>) tensor(11589.6523, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11589.65234375
tensor(11589.6523, grad_fn=<NegBackward0>) tensor(11589.6523, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11589.6630859375
tensor(11589.6523, grad_fn=<NegBackward0>) tensor(11589.6631, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11589.650390625
tensor(11589.6523, grad_fn=<NegBackward0>) tensor(11589.6504, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11589.6494140625
tensor(11589.6504, grad_fn=<NegBackward0>) tensor(11589.6494, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11589.650390625
tensor(11589.6494, grad_fn=<NegBackward0>) tensor(11589.6504, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11589.650390625
tensor(11589.6494, grad_fn=<NegBackward0>) tensor(11589.6504, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11589.6533203125
tensor(11589.6494, grad_fn=<NegBackward0>) tensor(11589.6533, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11589.6494140625
tensor(11589.6494, grad_fn=<NegBackward0>) tensor(11589.6494, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11589.6474609375
tensor(11589.6494, grad_fn=<NegBackward0>) tensor(11589.6475, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11589.6484375
tensor(11589.6475, grad_fn=<NegBackward0>) tensor(11589.6484, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11589.6474609375
tensor(11589.6475, grad_fn=<NegBackward0>) tensor(11589.6475, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11589.646484375
tensor(11589.6475, grad_fn=<NegBackward0>) tensor(11589.6465, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11589.646484375
tensor(11589.6465, grad_fn=<NegBackward0>) tensor(11589.6465, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11589.646484375
tensor(11589.6465, grad_fn=<NegBackward0>) tensor(11589.6465, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11589.6484375
tensor(11589.6465, grad_fn=<NegBackward0>) tensor(11589.6484, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11589.646484375
tensor(11589.6465, grad_fn=<NegBackward0>) tensor(11589.6465, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11589.6455078125
tensor(11589.6465, grad_fn=<NegBackward0>) tensor(11589.6455, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11589.6513671875
tensor(11589.6455, grad_fn=<NegBackward0>) tensor(11589.6514, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11589.6455078125
tensor(11589.6455, grad_fn=<NegBackward0>) tensor(11589.6455, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11589.64453125
tensor(11589.6455, grad_fn=<NegBackward0>) tensor(11589.6445, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11589.6455078125
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6455, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11589.6455078125
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6455, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11589.64453125
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6445, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11589.64453125
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6445, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11589.64453125
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6445, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11589.64453125
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6445, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11589.6455078125
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6455, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11589.6435546875
tensor(11589.6445, grad_fn=<NegBackward0>) tensor(11589.6436, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11589.6435546875
tensor(11589.6436, grad_fn=<NegBackward0>) tensor(11589.6436, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11589.642578125
tensor(11589.6436, grad_fn=<NegBackward0>) tensor(11589.6426, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11589.6455078125
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6455, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11589.6435546875
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6436, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11589.646484375
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6465, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11589.6435546875
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6436, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11589.642578125
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6426, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11589.6513671875
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6514, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11589.646484375
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6465, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11589.646484375
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6465, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11589.642578125
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6426, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11589.64453125
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6445, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11589.6416015625
tensor(11589.6426, grad_fn=<NegBackward0>) tensor(11589.6416, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11589.6435546875
tensor(11589.6416, grad_fn=<NegBackward0>) tensor(11589.6436, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11589.640625
tensor(11589.6416, grad_fn=<NegBackward0>) tensor(11589.6406, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11589.64453125
tensor(11589.6406, grad_fn=<NegBackward0>) tensor(11589.6445, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11589.6416015625
tensor(11589.6406, grad_fn=<NegBackward0>) tensor(11589.6416, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11589.6455078125
tensor(11589.6406, grad_fn=<NegBackward0>) tensor(11589.6455, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11589.6416015625
tensor(11589.6406, grad_fn=<NegBackward0>) tensor(11589.6416, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11589.650390625
tensor(11589.6406, grad_fn=<NegBackward0>) tensor(11589.6504, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.7231, 0.2769],
        [0.2700, 0.7300]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4378, 0.5622], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3982, 0.0954],
         [0.6379, 0.2043]],

        [[0.5999, 0.0988],
         [0.6035, 0.5755]],

        [[0.6144, 0.0955],
         [0.7091, 0.6690]],

        [[0.6783, 0.1220],
         [0.6497, 0.5952]],

        [[0.7196, 0.1054],
         [0.6387, 0.5093]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22184.677734375
inf tensor(22184.6777, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12278.9501953125
tensor(22184.6777, grad_fn=<NegBackward0>) tensor(12278.9502, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12206.23828125
tensor(12278.9502, grad_fn=<NegBackward0>) tensor(12206.2383, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12003.32421875
tensor(12206.2383, grad_fn=<NegBackward0>) tensor(12003.3242, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11990.677734375
tensor(12003.3242, grad_fn=<NegBackward0>) tensor(11990.6777, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11981.9873046875
tensor(11990.6777, grad_fn=<NegBackward0>) tensor(11981.9873, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11968.525390625
tensor(11981.9873, grad_fn=<NegBackward0>) tensor(11968.5254, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11959.99609375
tensor(11968.5254, grad_fn=<NegBackward0>) tensor(11959.9961, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11943.47265625
tensor(11959.9961, grad_fn=<NegBackward0>) tensor(11943.4727, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11930.25
tensor(11943.4727, grad_fn=<NegBackward0>) tensor(11930.2500, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11929.3310546875
tensor(11930.2500, grad_fn=<NegBackward0>) tensor(11929.3311, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11926.041015625
tensor(11929.3311, grad_fn=<NegBackward0>) tensor(11926.0410, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11893.76171875
tensor(11926.0410, grad_fn=<NegBackward0>) tensor(11893.7617, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11871.2841796875
tensor(11893.7617, grad_fn=<NegBackward0>) tensor(11871.2842, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11845.453125
tensor(11871.2842, grad_fn=<NegBackward0>) tensor(11845.4531, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11828.2568359375
tensor(11845.4531, grad_fn=<NegBackward0>) tensor(11828.2568, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11828.1728515625
tensor(11828.2568, grad_fn=<NegBackward0>) tensor(11828.1729, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11828.0966796875
tensor(11828.1729, grad_fn=<NegBackward0>) tensor(11828.0967, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11820.0244140625
tensor(11828.0967, grad_fn=<NegBackward0>) tensor(11820.0244, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11819.9248046875
tensor(11820.0244, grad_fn=<NegBackward0>) tensor(11819.9248, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11810.20703125
tensor(11819.9248, grad_fn=<NegBackward0>) tensor(11810.2070, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11810.1435546875
tensor(11810.2070, grad_fn=<NegBackward0>) tensor(11810.1436, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11783.2265625
tensor(11810.1436, grad_fn=<NegBackward0>) tensor(11783.2266, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11765.1318359375
tensor(11783.2266, grad_fn=<NegBackward0>) tensor(11765.1318, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11749.92578125
tensor(11765.1318, grad_fn=<NegBackward0>) tensor(11749.9258, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11749.796875
tensor(11749.9258, grad_fn=<NegBackward0>) tensor(11749.7969, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11746.228515625
tensor(11749.7969, grad_fn=<NegBackward0>) tensor(11746.2285, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11746.080078125
tensor(11746.2285, grad_fn=<NegBackward0>) tensor(11746.0801, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11738.15625
tensor(11746.0801, grad_fn=<NegBackward0>) tensor(11738.1562, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11738.1416015625
tensor(11738.1562, grad_fn=<NegBackward0>) tensor(11738.1416, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11737.4150390625
tensor(11738.1416, grad_fn=<NegBackward0>) tensor(11737.4150, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11736.296875
tensor(11737.4150, grad_fn=<NegBackward0>) tensor(11736.2969, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11736.2900390625
tensor(11736.2969, grad_fn=<NegBackward0>) tensor(11736.2900, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11736.2861328125
tensor(11736.2900, grad_fn=<NegBackward0>) tensor(11736.2861, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11736.2744140625
tensor(11736.2861, grad_fn=<NegBackward0>) tensor(11736.2744, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11736.259765625
tensor(11736.2744, grad_fn=<NegBackward0>) tensor(11736.2598, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11736.2548828125
tensor(11736.2598, grad_fn=<NegBackward0>) tensor(11736.2549, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11736.2470703125
tensor(11736.2549, grad_fn=<NegBackward0>) tensor(11736.2471, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11736.228515625
tensor(11736.2471, grad_fn=<NegBackward0>) tensor(11736.2285, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11726.5439453125
tensor(11736.2285, grad_fn=<NegBackward0>) tensor(11726.5439, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11726.4990234375
tensor(11726.5439, grad_fn=<NegBackward0>) tensor(11726.4990, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11726.49609375
tensor(11726.4990, grad_fn=<NegBackward0>) tensor(11726.4961, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11726.494140625
tensor(11726.4961, grad_fn=<NegBackward0>) tensor(11726.4941, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11726.4912109375
tensor(11726.4941, grad_fn=<NegBackward0>) tensor(11726.4912, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11726.4892578125
tensor(11726.4912, grad_fn=<NegBackward0>) tensor(11726.4893, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11726.4873046875
tensor(11726.4893, grad_fn=<NegBackward0>) tensor(11726.4873, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11726.486328125
tensor(11726.4873, grad_fn=<NegBackward0>) tensor(11726.4863, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11726.486328125
tensor(11726.4863, grad_fn=<NegBackward0>) tensor(11726.4863, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11726.484375
tensor(11726.4863, grad_fn=<NegBackward0>) tensor(11726.4844, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11726.482421875
tensor(11726.4844, grad_fn=<NegBackward0>) tensor(11726.4824, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11726.48046875
tensor(11726.4824, grad_fn=<NegBackward0>) tensor(11726.4805, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11726.4775390625
tensor(11726.4805, grad_fn=<NegBackward0>) tensor(11726.4775, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11726.4716796875
tensor(11726.4775, grad_fn=<NegBackward0>) tensor(11726.4717, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11726.470703125
tensor(11726.4717, grad_fn=<NegBackward0>) tensor(11726.4707, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11726.455078125
tensor(11726.4707, grad_fn=<NegBackward0>) tensor(11726.4551, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11726.4560546875
tensor(11726.4551, grad_fn=<NegBackward0>) tensor(11726.4561, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11726.4541015625
tensor(11726.4551, grad_fn=<NegBackward0>) tensor(11726.4541, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11726.4541015625
tensor(11726.4541, grad_fn=<NegBackward0>) tensor(11726.4541, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11726.4541015625
tensor(11726.4541, grad_fn=<NegBackward0>) tensor(11726.4541, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11726.4521484375
tensor(11726.4541, grad_fn=<NegBackward0>) tensor(11726.4521, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11726.451171875
tensor(11726.4521, grad_fn=<NegBackward0>) tensor(11726.4512, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11726.4521484375
tensor(11726.4512, grad_fn=<NegBackward0>) tensor(11726.4521, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11726.4541015625
tensor(11726.4512, grad_fn=<NegBackward0>) tensor(11726.4541, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11726.455078125
tensor(11726.4512, grad_fn=<NegBackward0>) tensor(11726.4551, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11726.46484375
tensor(11726.4512, grad_fn=<NegBackward0>) tensor(11726.4648, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11726.44921875
tensor(11726.4512, grad_fn=<NegBackward0>) tensor(11726.4492, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11726.4482421875
tensor(11726.4492, grad_fn=<NegBackward0>) tensor(11726.4482, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11717.22265625
tensor(11726.4482, grad_fn=<NegBackward0>) tensor(11717.2227, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11717.1875
tensor(11717.2227, grad_fn=<NegBackward0>) tensor(11717.1875, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11717.1875
tensor(11717.1875, grad_fn=<NegBackward0>) tensor(11717.1875, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11717.185546875
tensor(11717.1875, grad_fn=<NegBackward0>) tensor(11717.1855, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11717.18359375
tensor(11717.1855, grad_fn=<NegBackward0>) tensor(11717.1836, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11717.1767578125
tensor(11717.1836, grad_fn=<NegBackward0>) tensor(11717.1768, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11717.17578125
tensor(11717.1768, grad_fn=<NegBackward0>) tensor(11717.1758, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11717.177734375
tensor(11717.1758, grad_fn=<NegBackward0>) tensor(11717.1777, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11717.1748046875
tensor(11717.1758, grad_fn=<NegBackward0>) tensor(11717.1748, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11717.1748046875
tensor(11717.1748, grad_fn=<NegBackward0>) tensor(11717.1748, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11717.173828125
tensor(11717.1748, grad_fn=<NegBackward0>) tensor(11717.1738, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11717.1748046875
tensor(11717.1738, grad_fn=<NegBackward0>) tensor(11717.1748, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11717.177734375
tensor(11717.1738, grad_fn=<NegBackward0>) tensor(11717.1777, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11717.1728515625
tensor(11717.1738, grad_fn=<NegBackward0>) tensor(11717.1729, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11717.173828125
tensor(11717.1729, grad_fn=<NegBackward0>) tensor(11717.1738, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11717.18359375
tensor(11717.1729, grad_fn=<NegBackward0>) tensor(11717.1836, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11717.171875
tensor(11717.1729, grad_fn=<NegBackward0>) tensor(11717.1719, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11717.1767578125
tensor(11717.1719, grad_fn=<NegBackward0>) tensor(11717.1768, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11717.1708984375
tensor(11717.1719, grad_fn=<NegBackward0>) tensor(11717.1709, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11717.1708984375
tensor(11717.1709, grad_fn=<NegBackward0>) tensor(11717.1709, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11717.23828125
tensor(11717.1709, grad_fn=<NegBackward0>) tensor(11717.2383, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11717.1708984375
tensor(11717.1709, grad_fn=<NegBackward0>) tensor(11717.1709, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11717.1748046875
tensor(11717.1709, grad_fn=<NegBackward0>) tensor(11717.1748, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11717.16796875
tensor(11717.1709, grad_fn=<NegBackward0>) tensor(11717.1680, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11717.166015625
tensor(11717.1680, grad_fn=<NegBackward0>) tensor(11717.1660, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11717.166015625
tensor(11717.1660, grad_fn=<NegBackward0>) tensor(11717.1660, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11717.1533203125
tensor(11717.1660, grad_fn=<NegBackward0>) tensor(11717.1533, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11717.1484375
tensor(11717.1533, grad_fn=<NegBackward0>) tensor(11717.1484, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11717.1533203125
tensor(11717.1484, grad_fn=<NegBackward0>) tensor(11717.1533, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11705.7548828125
tensor(11717.1484, grad_fn=<NegBackward0>) tensor(11705.7549, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11705.740234375
tensor(11705.7549, grad_fn=<NegBackward0>) tensor(11705.7402, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11705.685546875
tensor(11705.7402, grad_fn=<NegBackward0>) tensor(11705.6855, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11705.701171875
tensor(11705.6855, grad_fn=<NegBackward0>) tensor(11705.7012, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7347, 0.2653],
        [0.3412, 0.6588]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0079, 0.9921], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4042, 0.1234],
         [0.7178, 0.1939]],

        [[0.6505, 0.0991],
         [0.6938, 0.6453]],

        [[0.6347, 0.0955],
         [0.5206, 0.6396]],

        [[0.5951, 0.1219],
         [0.5498, 0.5528]],

        [[0.6197, 0.1052],
         [0.6819, 0.5591]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6783213004497619
Average Adjusted Rand Index: 0.8
[1.0, 0.6783213004497619] [1.0, 0.8] [11589.650390625, 11705.6826171875]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11719.08571048985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19349.83984375
inf tensor(19349.8398, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12260.61328125
tensor(19349.8398, grad_fn=<NegBackward0>) tensor(12260.6133, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11726.4521484375
tensor(12260.6133, grad_fn=<NegBackward0>) tensor(11726.4521, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11712.8369140625
tensor(11726.4521, grad_fn=<NegBackward0>) tensor(11712.8369, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11712.4365234375
tensor(11712.8369, grad_fn=<NegBackward0>) tensor(11712.4365, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11712.2548828125
tensor(11712.4365, grad_fn=<NegBackward0>) tensor(11712.2549, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11712.1533203125
tensor(11712.2549, grad_fn=<NegBackward0>) tensor(11712.1533, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11712.0888671875
tensor(11712.1533, grad_fn=<NegBackward0>) tensor(11712.0889, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11711.955078125
tensor(11712.0889, grad_fn=<NegBackward0>) tensor(11711.9551, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11711.923828125
tensor(11711.9551, grad_fn=<NegBackward0>) tensor(11711.9238, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11711.900390625
tensor(11711.9238, grad_fn=<NegBackward0>) tensor(11711.9004, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11711.8857421875
tensor(11711.9004, grad_fn=<NegBackward0>) tensor(11711.8857, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11711.8701171875
tensor(11711.8857, grad_fn=<NegBackward0>) tensor(11711.8701, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11711.857421875
tensor(11711.8701, grad_fn=<NegBackward0>) tensor(11711.8574, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11711.8486328125
tensor(11711.8574, grad_fn=<NegBackward0>) tensor(11711.8486, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11711.841796875
tensor(11711.8486, grad_fn=<NegBackward0>) tensor(11711.8418, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11711.8349609375
tensor(11711.8418, grad_fn=<NegBackward0>) tensor(11711.8350, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11711.830078125
tensor(11711.8350, grad_fn=<NegBackward0>) tensor(11711.8301, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11711.82421875
tensor(11711.8301, grad_fn=<NegBackward0>) tensor(11711.8242, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11711.8203125
tensor(11711.8242, grad_fn=<NegBackward0>) tensor(11711.8203, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11711.81640625
tensor(11711.8203, grad_fn=<NegBackward0>) tensor(11711.8164, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11711.814453125
tensor(11711.8164, grad_fn=<NegBackward0>) tensor(11711.8145, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11711.8115234375
tensor(11711.8145, grad_fn=<NegBackward0>) tensor(11711.8115, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11711.8095703125
tensor(11711.8115, grad_fn=<NegBackward0>) tensor(11711.8096, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11711.8076171875
tensor(11711.8096, grad_fn=<NegBackward0>) tensor(11711.8076, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11711.8046875
tensor(11711.8076, grad_fn=<NegBackward0>) tensor(11711.8047, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11711.8046875
tensor(11711.8047, grad_fn=<NegBackward0>) tensor(11711.8047, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11711.802734375
tensor(11711.8047, grad_fn=<NegBackward0>) tensor(11711.8027, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11711.7998046875
tensor(11711.8027, grad_fn=<NegBackward0>) tensor(11711.7998, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11711.802734375
tensor(11711.7998, grad_fn=<NegBackward0>) tensor(11711.8027, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11711.798828125
tensor(11711.7998, grad_fn=<NegBackward0>) tensor(11711.7988, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11711.796875
tensor(11711.7988, grad_fn=<NegBackward0>) tensor(11711.7969, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11711.7958984375
tensor(11711.7969, grad_fn=<NegBackward0>) tensor(11711.7959, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11711.7958984375
tensor(11711.7959, grad_fn=<NegBackward0>) tensor(11711.7959, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11711.7978515625
tensor(11711.7959, grad_fn=<NegBackward0>) tensor(11711.7979, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11711.7939453125
tensor(11711.7959, grad_fn=<NegBackward0>) tensor(11711.7939, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11711.7939453125
tensor(11711.7939, grad_fn=<NegBackward0>) tensor(11711.7939, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11711.79296875
tensor(11711.7939, grad_fn=<NegBackward0>) tensor(11711.7930, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11711.79296875
tensor(11711.7930, grad_fn=<NegBackward0>) tensor(11711.7930, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11711.7919921875
tensor(11711.7930, grad_fn=<NegBackward0>) tensor(11711.7920, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11711.791015625
tensor(11711.7920, grad_fn=<NegBackward0>) tensor(11711.7910, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11711.7958984375
tensor(11711.7910, grad_fn=<NegBackward0>) tensor(11711.7959, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11711.7900390625
tensor(11711.7910, grad_fn=<NegBackward0>) tensor(11711.7900, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11711.7890625
tensor(11711.7900, grad_fn=<NegBackward0>) tensor(11711.7891, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11711.7900390625
tensor(11711.7891, grad_fn=<NegBackward0>) tensor(11711.7900, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11711.796875
tensor(11711.7891, grad_fn=<NegBackward0>) tensor(11711.7969, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11711.7880859375
tensor(11711.7891, grad_fn=<NegBackward0>) tensor(11711.7881, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11711.7939453125
tensor(11711.7881, grad_fn=<NegBackward0>) tensor(11711.7939, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11711.787109375
tensor(11711.7881, grad_fn=<NegBackward0>) tensor(11711.7871, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11711.7900390625
tensor(11711.7871, grad_fn=<NegBackward0>) tensor(11711.7900, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11711.7880859375
tensor(11711.7871, grad_fn=<NegBackward0>) tensor(11711.7881, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11711.7890625
tensor(11711.7871, grad_fn=<NegBackward0>) tensor(11711.7891, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11711.791015625
tensor(11711.7871, grad_fn=<NegBackward0>) tensor(11711.7910, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -11711.7861328125
tensor(11711.7871, grad_fn=<NegBackward0>) tensor(11711.7861, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11711.787109375
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7871, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11711.79296875
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7930, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11711.787109375
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7871, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11711.787109375
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7871, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -11711.7861328125
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7861, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11711.7861328125
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7861, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11711.787109375
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7871, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11711.7890625
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7891, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11711.7861328125
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7861, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11711.791015625
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7910, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11711.78515625
tensor(11711.7861, grad_fn=<NegBackward0>) tensor(11711.7852, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11711.78515625
tensor(11711.7852, grad_fn=<NegBackward0>) tensor(11711.7852, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11711.787109375
tensor(11711.7852, grad_fn=<NegBackward0>) tensor(11711.7871, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11711.8095703125
tensor(11711.7852, grad_fn=<NegBackward0>) tensor(11711.8096, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11711.7841796875
tensor(11711.7852, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11711.7841796875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11711.7861328125
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7861, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11711.7958984375
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7959, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11711.8203125
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.8203, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11711.78515625
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7852, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11711.7841796875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11711.79296875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7930, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11711.7841796875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11711.830078125
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.8301, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11711.7841796875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11711.78515625
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7852, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11711.7841796875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11711.78515625
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7852, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11711.7841796875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11711.7900390625
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7900, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11711.7880859375
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7881, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11711.791015625
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7910, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11711.826171875
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.8262, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11711.783203125
tensor(11711.7842, grad_fn=<NegBackward0>) tensor(11711.7832, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11711.787109375
tensor(11711.7832, grad_fn=<NegBackward0>) tensor(11711.7871, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11711.791015625
tensor(11711.7832, grad_fn=<NegBackward0>) tensor(11711.7910, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11711.7841796875
tensor(11711.7832, grad_fn=<NegBackward0>) tensor(11711.7842, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11711.78515625
tensor(11711.7832, grad_fn=<NegBackward0>) tensor(11711.7852, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11711.8544921875
tensor(11711.7832, grad_fn=<NegBackward0>) tensor(11711.8545, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.7527, 0.2473],
        [0.2480, 0.7520]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4599, 0.5401], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.1006],
         [0.6130, 0.4091]],

        [[0.6185, 0.0901],
         [0.6221, 0.5876]],

        [[0.7102, 0.1163],
         [0.5451, 0.5222]],

        [[0.6402, 0.1062],
         [0.6568, 0.6610]],

        [[0.5465, 0.1067],
         [0.5627, 0.6476]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22220.90625
inf tensor(22220.9062, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12164.712890625
tensor(22220.9062, grad_fn=<NegBackward0>) tensor(12164.7129, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11961.384765625
tensor(12164.7129, grad_fn=<NegBackward0>) tensor(11961.3848, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11931.6669921875
tensor(11961.3848, grad_fn=<NegBackward0>) tensor(11931.6670, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11926.3603515625
tensor(11931.6670, grad_fn=<NegBackward0>) tensor(11926.3604, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11926.2353515625
tensor(11926.3604, grad_fn=<NegBackward0>) tensor(11926.2354, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11926.16796875
tensor(11926.2354, grad_fn=<NegBackward0>) tensor(11926.1680, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11926.1259765625
tensor(11926.1680, grad_fn=<NegBackward0>) tensor(11926.1260, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11926.0986328125
tensor(11926.1260, grad_fn=<NegBackward0>) tensor(11926.0986, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11926.0791015625
tensor(11926.0986, grad_fn=<NegBackward0>) tensor(11926.0791, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11926.0634765625
tensor(11926.0791, grad_fn=<NegBackward0>) tensor(11926.0635, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11926.0517578125
tensor(11926.0635, grad_fn=<NegBackward0>) tensor(11926.0518, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11926.0419921875
tensor(11926.0518, grad_fn=<NegBackward0>) tensor(11926.0420, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11926.03515625
tensor(11926.0420, grad_fn=<NegBackward0>) tensor(11926.0352, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11926.029296875
tensor(11926.0352, grad_fn=<NegBackward0>) tensor(11926.0293, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11926.0244140625
tensor(11926.0293, grad_fn=<NegBackward0>) tensor(11926.0244, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11926.01953125
tensor(11926.0244, grad_fn=<NegBackward0>) tensor(11926.0195, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11926.0185546875
tensor(11926.0195, grad_fn=<NegBackward0>) tensor(11926.0186, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11926.0146484375
tensor(11926.0186, grad_fn=<NegBackward0>) tensor(11926.0146, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11926.0107421875
tensor(11926.0146, grad_fn=<NegBackward0>) tensor(11926.0107, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11926.0087890625
tensor(11926.0107, grad_fn=<NegBackward0>) tensor(11926.0088, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11926.0068359375
tensor(11926.0088, grad_fn=<NegBackward0>) tensor(11926.0068, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11926.0048828125
tensor(11926.0068, grad_fn=<NegBackward0>) tensor(11926.0049, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11926.0048828125
tensor(11926.0049, grad_fn=<NegBackward0>) tensor(11926.0049, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11926.0029296875
tensor(11926.0049, grad_fn=<NegBackward0>) tensor(11926.0029, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11926.0009765625
tensor(11926.0029, grad_fn=<NegBackward0>) tensor(11926.0010, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11926.001953125
tensor(11926.0010, grad_fn=<NegBackward0>) tensor(11926.0020, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11925.9990234375
tensor(11926.0010, grad_fn=<NegBackward0>) tensor(11925.9990, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11925.9990234375
tensor(11925.9990, grad_fn=<NegBackward0>) tensor(11925.9990, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11925.998046875
tensor(11925.9990, grad_fn=<NegBackward0>) tensor(11925.9980, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11925.9951171875
tensor(11925.9980, grad_fn=<NegBackward0>) tensor(11925.9951, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11925.9951171875
tensor(11925.9951, grad_fn=<NegBackward0>) tensor(11925.9951, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11925.99609375
tensor(11925.9951, grad_fn=<NegBackward0>) tensor(11925.9961, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11925.9951171875
tensor(11925.9951, grad_fn=<NegBackward0>) tensor(11925.9951, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11925.994140625
tensor(11925.9951, grad_fn=<NegBackward0>) tensor(11925.9941, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11925.9931640625
tensor(11925.9941, grad_fn=<NegBackward0>) tensor(11925.9932, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11925.9931640625
tensor(11925.9932, grad_fn=<NegBackward0>) tensor(11925.9932, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11926.0009765625
tensor(11925.9932, grad_fn=<NegBackward0>) tensor(11926.0010, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11925.9931640625
tensor(11925.9932, grad_fn=<NegBackward0>) tensor(11925.9932, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11925.9921875
tensor(11925.9932, grad_fn=<NegBackward0>) tensor(11925.9922, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11925.9921875
tensor(11925.9922, grad_fn=<NegBackward0>) tensor(11925.9922, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11925.9912109375
tensor(11925.9922, grad_fn=<NegBackward0>) tensor(11925.9912, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11925.9921875
tensor(11925.9912, grad_fn=<NegBackward0>) tensor(11925.9922, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11926.005859375
tensor(11925.9912, grad_fn=<NegBackward0>) tensor(11926.0059, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11925.9912109375
tensor(11925.9912, grad_fn=<NegBackward0>) tensor(11925.9912, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11925.9912109375
tensor(11925.9912, grad_fn=<NegBackward0>) tensor(11925.9912, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11925.9912109375
tensor(11925.9912, grad_fn=<NegBackward0>) tensor(11925.9912, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11925.9892578125
tensor(11925.9912, grad_fn=<NegBackward0>) tensor(11925.9893, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11925.9912109375
tensor(11925.9893, grad_fn=<NegBackward0>) tensor(11925.9912, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11925.69921875
tensor(11925.9893, grad_fn=<NegBackward0>) tensor(11925.6992, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11925.697265625
tensor(11925.6992, grad_fn=<NegBackward0>) tensor(11925.6973, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11925.69921875
tensor(11925.6973, grad_fn=<NegBackward0>) tensor(11925.6992, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11916.384765625
tensor(11925.6973, grad_fn=<NegBackward0>) tensor(11916.3848, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11916.294921875
tensor(11916.3848, grad_fn=<NegBackward0>) tensor(11916.2949, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11916.2880859375
tensor(11916.2949, grad_fn=<NegBackward0>) tensor(11916.2881, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11916.287109375
tensor(11916.2881, grad_fn=<NegBackward0>) tensor(11916.2871, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11916.2861328125
tensor(11916.2871, grad_fn=<NegBackward0>) tensor(11916.2861, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11916.28515625
tensor(11916.2861, grad_fn=<NegBackward0>) tensor(11916.2852, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11916.28515625
tensor(11916.2852, grad_fn=<NegBackward0>) tensor(11916.2852, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11916.283203125
tensor(11916.2852, grad_fn=<NegBackward0>) tensor(11916.2832, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11916.283203125
tensor(11916.2832, grad_fn=<NegBackward0>) tensor(11916.2832, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11916.2841796875
tensor(11916.2832, grad_fn=<NegBackward0>) tensor(11916.2842, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11916.28125
tensor(11916.2832, grad_fn=<NegBackward0>) tensor(11916.2812, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11916.2822265625
tensor(11916.2812, grad_fn=<NegBackward0>) tensor(11916.2822, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11916.2822265625
tensor(11916.2812, grad_fn=<NegBackward0>) tensor(11916.2822, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11916.2822265625
tensor(11916.2812, grad_fn=<NegBackward0>) tensor(11916.2822, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11916.2822265625
tensor(11916.2812, grad_fn=<NegBackward0>) tensor(11916.2822, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11916.2841796875
tensor(11916.2812, grad_fn=<NegBackward0>) tensor(11916.2842, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.5108, 0.4892],
        [0.3758, 0.6242]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5413, 0.4587], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4063, 0.1006],
         [0.7294, 0.2126]],

        [[0.5704, 0.1029],
         [0.6581, 0.6887]],

        [[0.7121, 0.1159],
         [0.5771, 0.6447]],

        [[0.5896, 0.1060],
         [0.6310, 0.5794]],

        [[0.5403, 0.1062],
         [0.7039, 0.5502]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 71
Adjusted Rand Index: 0.1705473823297185
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5116838645433265
Average Adjusted Rand Index: 0.8341094764659438
[1.0, 0.5116838645433265] [1.0, 0.8341094764659438] [11711.8544921875, 11916.2841796875]
-------------------------------------
This iteration is 23
True Objective function: Loss = -11644.980118596859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22329.52734375
inf tensor(22329.5273, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12399.392578125
tensor(22329.5273, grad_fn=<NegBackward0>) tensor(12399.3926, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12375.3232421875
tensor(12399.3926, grad_fn=<NegBackward0>) tensor(12375.3232, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11942.7890625
tensor(12375.3232, grad_fn=<NegBackward0>) tensor(11942.7891, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11809.8505859375
tensor(11942.7891, grad_fn=<NegBackward0>) tensor(11809.8506, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11737.232421875
tensor(11809.8506, grad_fn=<NegBackward0>) tensor(11737.2324, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11683.6201171875
tensor(11737.2324, grad_fn=<NegBackward0>) tensor(11683.6201, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11675.4140625
tensor(11683.6201, grad_fn=<NegBackward0>) tensor(11675.4141, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11675.1044921875
tensor(11675.4141, grad_fn=<NegBackward0>) tensor(11675.1045, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11674.880859375
tensor(11675.1045, grad_fn=<NegBackward0>) tensor(11674.8809, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11663.3349609375
tensor(11674.8809, grad_fn=<NegBackward0>) tensor(11663.3350, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11662.3466796875
tensor(11663.3350, grad_fn=<NegBackward0>) tensor(11662.3467, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11656.408203125
tensor(11662.3467, grad_fn=<NegBackward0>) tensor(11656.4082, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11656.353515625
tensor(11656.4082, grad_fn=<NegBackward0>) tensor(11656.3535, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11656.3095703125
tensor(11656.3535, grad_fn=<NegBackward0>) tensor(11656.3096, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11656.2734375
tensor(11656.3096, grad_fn=<NegBackward0>) tensor(11656.2734, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11648.5703125
tensor(11656.2734, grad_fn=<NegBackward0>) tensor(11648.5703, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11648.5400390625
tensor(11648.5703, grad_fn=<NegBackward0>) tensor(11648.5400, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11648.5185546875
tensor(11648.5400, grad_fn=<NegBackward0>) tensor(11648.5186, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11648.501953125
tensor(11648.5186, grad_fn=<NegBackward0>) tensor(11648.5020, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11648.486328125
tensor(11648.5020, grad_fn=<NegBackward0>) tensor(11648.4863, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11648.4755859375
tensor(11648.4863, grad_fn=<NegBackward0>) tensor(11648.4756, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11648.462890625
tensor(11648.4756, grad_fn=<NegBackward0>) tensor(11648.4629, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11648.455078125
tensor(11648.4629, grad_fn=<NegBackward0>) tensor(11648.4551, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11648.4453125
tensor(11648.4551, grad_fn=<NegBackward0>) tensor(11648.4453, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11648.4384765625
tensor(11648.4453, grad_fn=<NegBackward0>) tensor(11648.4385, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11648.4306640625
tensor(11648.4385, grad_fn=<NegBackward0>) tensor(11648.4307, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11648.42578125
tensor(11648.4307, grad_fn=<NegBackward0>) tensor(11648.4258, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11648.421875
tensor(11648.4258, grad_fn=<NegBackward0>) tensor(11648.4219, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11648.4169921875
tensor(11648.4219, grad_fn=<NegBackward0>) tensor(11648.4170, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11648.412109375
tensor(11648.4170, grad_fn=<NegBackward0>) tensor(11648.4121, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11648.408203125
tensor(11648.4121, grad_fn=<NegBackward0>) tensor(11648.4082, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11648.4052734375
tensor(11648.4082, grad_fn=<NegBackward0>) tensor(11648.4053, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11648.4013671875
tensor(11648.4053, grad_fn=<NegBackward0>) tensor(11648.4014, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11648.3984375
tensor(11648.4014, grad_fn=<NegBackward0>) tensor(11648.3984, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11648.396484375
tensor(11648.3984, grad_fn=<NegBackward0>) tensor(11648.3965, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11648.3935546875
tensor(11648.3965, grad_fn=<NegBackward0>) tensor(11648.3936, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11648.3916015625
tensor(11648.3936, grad_fn=<NegBackward0>) tensor(11648.3916, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11648.3876953125
tensor(11648.3916, grad_fn=<NegBackward0>) tensor(11648.3877, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11648.384765625
tensor(11648.3877, grad_fn=<NegBackward0>) tensor(11648.3848, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11648.380859375
tensor(11648.3848, grad_fn=<NegBackward0>) tensor(11648.3809, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11648.37890625
tensor(11648.3809, grad_fn=<NegBackward0>) tensor(11648.3789, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11648.380859375
tensor(11648.3789, grad_fn=<NegBackward0>) tensor(11648.3809, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11648.34765625
tensor(11648.3789, grad_fn=<NegBackward0>) tensor(11648.3477, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11641.7919921875
tensor(11648.3477, grad_fn=<NegBackward0>) tensor(11641.7920, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11641.7900390625
tensor(11641.7920, grad_fn=<NegBackward0>) tensor(11641.7900, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11641.7890625
tensor(11641.7900, grad_fn=<NegBackward0>) tensor(11641.7891, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11641.802734375
tensor(11641.7891, grad_fn=<NegBackward0>) tensor(11641.8027, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11641.7880859375
tensor(11641.7891, grad_fn=<NegBackward0>) tensor(11641.7881, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11641.787109375
tensor(11641.7881, grad_fn=<NegBackward0>) tensor(11641.7871, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11641.7841796875
tensor(11641.7871, grad_fn=<NegBackward0>) tensor(11641.7842, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11641.7841796875
tensor(11641.7842, grad_fn=<NegBackward0>) tensor(11641.7842, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11641.783203125
tensor(11641.7842, grad_fn=<NegBackward0>) tensor(11641.7832, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11641.7841796875
tensor(11641.7832, grad_fn=<NegBackward0>) tensor(11641.7842, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11641.791015625
tensor(11641.7832, grad_fn=<NegBackward0>) tensor(11641.7910, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11641.7822265625
tensor(11641.7832, grad_fn=<NegBackward0>) tensor(11641.7822, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11641.7861328125
tensor(11641.7822, grad_fn=<NegBackward0>) tensor(11641.7861, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11641.7802734375
tensor(11641.7822, grad_fn=<NegBackward0>) tensor(11641.7803, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11641.79296875
tensor(11641.7803, grad_fn=<NegBackward0>) tensor(11641.7930, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11641.779296875
tensor(11641.7803, grad_fn=<NegBackward0>) tensor(11641.7793, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11641.779296875
tensor(11641.7793, grad_fn=<NegBackward0>) tensor(11641.7793, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11641.7802734375
tensor(11641.7793, grad_fn=<NegBackward0>) tensor(11641.7803, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11641.779296875
tensor(11641.7793, grad_fn=<NegBackward0>) tensor(11641.7793, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11641.7783203125
tensor(11641.7793, grad_fn=<NegBackward0>) tensor(11641.7783, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11641.7783203125
tensor(11641.7783, grad_fn=<NegBackward0>) tensor(11641.7783, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11641.7763671875
tensor(11641.7783, grad_fn=<NegBackward0>) tensor(11641.7764, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11641.775390625
tensor(11641.7764, grad_fn=<NegBackward0>) tensor(11641.7754, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11641.775390625
tensor(11641.7754, grad_fn=<NegBackward0>) tensor(11641.7754, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11641.7763671875
tensor(11641.7754, grad_fn=<NegBackward0>) tensor(11641.7764, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11641.7744140625
tensor(11641.7754, grad_fn=<NegBackward0>) tensor(11641.7744, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11641.775390625
tensor(11641.7744, grad_fn=<NegBackward0>) tensor(11641.7754, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11641.7763671875
tensor(11641.7744, grad_fn=<NegBackward0>) tensor(11641.7764, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11641.7734375
tensor(11641.7744, grad_fn=<NegBackward0>) tensor(11641.7734, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11641.7724609375
tensor(11641.7734, grad_fn=<NegBackward0>) tensor(11641.7725, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11638.0234375
tensor(11641.7725, grad_fn=<NegBackward0>) tensor(11638.0234, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11638.0234375
tensor(11638.0234, grad_fn=<NegBackward0>) tensor(11638.0234, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11638.0234375
tensor(11638.0234, grad_fn=<NegBackward0>) tensor(11638.0234, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11638.048828125
tensor(11638.0234, grad_fn=<NegBackward0>) tensor(11638.0488, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11638.0224609375
tensor(11638.0234, grad_fn=<NegBackward0>) tensor(11638.0225, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11638.0224609375
tensor(11638.0225, grad_fn=<NegBackward0>) tensor(11638.0225, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11638.0302734375
tensor(11638.0225, grad_fn=<NegBackward0>) tensor(11638.0303, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11638.0205078125
tensor(11638.0225, grad_fn=<NegBackward0>) tensor(11638.0205, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11638.046875
tensor(11638.0205, grad_fn=<NegBackward0>) tensor(11638.0469, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11638.0224609375
tensor(11638.0205, grad_fn=<NegBackward0>) tensor(11638.0225, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11638.0185546875
tensor(11638.0205, grad_fn=<NegBackward0>) tensor(11638.0186, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11638.0166015625
tensor(11638.0186, grad_fn=<NegBackward0>) tensor(11638.0166, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11638.015625
tensor(11638.0166, grad_fn=<NegBackward0>) tensor(11638.0156, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11638.025390625
tensor(11638.0156, grad_fn=<NegBackward0>) tensor(11638.0254, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11638.0224609375
tensor(11638.0156, grad_fn=<NegBackward0>) tensor(11638.0225, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11638.0166015625
tensor(11638.0156, grad_fn=<NegBackward0>) tensor(11638.0166, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11638.013671875
tensor(11638.0156, grad_fn=<NegBackward0>) tensor(11638.0137, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11638.0146484375
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0146, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11638.015625
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0156, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11638.017578125
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0176, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11638.013671875
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0137, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11638.017578125
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0176, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11638.0302734375
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0303, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11638.0205078125
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0205, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11638.0166015625
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0166, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11638.015625
tensor(11638.0137, grad_fn=<NegBackward0>) tensor(11638.0156, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7576, 0.2424],
        [0.2851, 0.7149]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4709, 0.5291], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2068, 0.1010],
         [0.6892, 0.4085]],

        [[0.7277, 0.1048],
         [0.6926, 0.6788]],

        [[0.5583, 0.1164],
         [0.5850, 0.6104]],

        [[0.7199, 0.0970],
         [0.6769, 0.5872]],

        [[0.6916, 0.0960],
         [0.7136, 0.6075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21215.283203125
inf tensor(21215.2832, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12241.681640625
tensor(21215.2832, grad_fn=<NegBackward0>) tensor(12241.6816, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11889.3369140625
tensor(12241.6816, grad_fn=<NegBackward0>) tensor(11889.3369, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11847.0625
tensor(11889.3369, grad_fn=<NegBackward0>) tensor(11847.0625, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11845.013671875
tensor(11847.0625, grad_fn=<NegBackward0>) tensor(11845.0137, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11844.8193359375
tensor(11845.0137, grad_fn=<NegBackward0>) tensor(11844.8193, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11844.7333984375
tensor(11844.8193, grad_fn=<NegBackward0>) tensor(11844.7334, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11844.6787109375
tensor(11844.7334, grad_fn=<NegBackward0>) tensor(11844.6787, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11844.6435546875
tensor(11844.6787, grad_fn=<NegBackward0>) tensor(11844.6436, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11844.6171875
tensor(11844.6436, grad_fn=<NegBackward0>) tensor(11844.6172, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11844.6015625
tensor(11844.6172, grad_fn=<NegBackward0>) tensor(11844.6016, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11844.587890625
tensor(11844.6016, grad_fn=<NegBackward0>) tensor(11844.5879, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11844.578125
tensor(11844.5879, grad_fn=<NegBackward0>) tensor(11844.5781, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11844.568359375
tensor(11844.5781, grad_fn=<NegBackward0>) tensor(11844.5684, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11844.560546875
tensor(11844.5684, grad_fn=<NegBackward0>) tensor(11844.5605, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11844.4951171875
tensor(11844.5605, grad_fn=<NegBackward0>) tensor(11844.4951, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11844.48046875
tensor(11844.4951, grad_fn=<NegBackward0>) tensor(11844.4805, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11844.4736328125
tensor(11844.4805, grad_fn=<NegBackward0>) tensor(11844.4736, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11844.4716796875
tensor(11844.4736, grad_fn=<NegBackward0>) tensor(11844.4717, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11844.46875
tensor(11844.4717, grad_fn=<NegBackward0>) tensor(11844.4688, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11844.46484375
tensor(11844.4688, grad_fn=<NegBackward0>) tensor(11844.4648, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11844.462890625
tensor(11844.4648, grad_fn=<NegBackward0>) tensor(11844.4629, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11844.4599609375
tensor(11844.4629, grad_fn=<NegBackward0>) tensor(11844.4600, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11844.458984375
tensor(11844.4600, grad_fn=<NegBackward0>) tensor(11844.4590, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11844.4580078125
tensor(11844.4590, grad_fn=<NegBackward0>) tensor(11844.4580, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11844.4560546875
tensor(11844.4580, grad_fn=<NegBackward0>) tensor(11844.4561, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11844.4541015625
tensor(11844.4561, grad_fn=<NegBackward0>) tensor(11844.4541, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11844.453125
tensor(11844.4541, grad_fn=<NegBackward0>) tensor(11844.4531, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11844.451171875
tensor(11844.4531, grad_fn=<NegBackward0>) tensor(11844.4512, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11844.4501953125
tensor(11844.4512, grad_fn=<NegBackward0>) tensor(11844.4502, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11844.4443359375
tensor(11844.4502, grad_fn=<NegBackward0>) tensor(11844.4443, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11844.439453125
tensor(11844.4443, grad_fn=<NegBackward0>) tensor(11844.4395, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11844.4150390625
tensor(11844.4395, grad_fn=<NegBackward0>) tensor(11844.4150, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11844.4140625
tensor(11844.4150, grad_fn=<NegBackward0>) tensor(11844.4141, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11844.4130859375
tensor(11844.4141, grad_fn=<NegBackward0>) tensor(11844.4131, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11844.412109375
tensor(11844.4131, grad_fn=<NegBackward0>) tensor(11844.4121, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11844.416015625
tensor(11844.4121, grad_fn=<NegBackward0>) tensor(11844.4160, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11844.4091796875
tensor(11844.4121, grad_fn=<NegBackward0>) tensor(11844.4092, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11843.158203125
tensor(11844.4092, grad_fn=<NegBackward0>) tensor(11843.1582, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11843.1748046875
tensor(11843.1582, grad_fn=<NegBackward0>) tensor(11843.1748, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11843.1513671875
tensor(11843.1582, grad_fn=<NegBackward0>) tensor(11843.1514, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11843.1513671875
tensor(11843.1514, grad_fn=<NegBackward0>) tensor(11843.1514, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11843.1513671875
tensor(11843.1514, grad_fn=<NegBackward0>) tensor(11843.1514, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11843.150390625
tensor(11843.1514, grad_fn=<NegBackward0>) tensor(11843.1504, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11843.1513671875
tensor(11843.1504, grad_fn=<NegBackward0>) tensor(11843.1514, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11843.150390625
tensor(11843.1504, grad_fn=<NegBackward0>) tensor(11843.1504, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11843.1494140625
tensor(11843.1504, grad_fn=<NegBackward0>) tensor(11843.1494, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11843.1494140625
tensor(11843.1494, grad_fn=<NegBackward0>) tensor(11843.1494, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11843.1494140625
tensor(11843.1494, grad_fn=<NegBackward0>) tensor(11843.1494, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11843.1513671875
tensor(11843.1494, grad_fn=<NegBackward0>) tensor(11843.1514, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11843.1484375
tensor(11843.1494, grad_fn=<NegBackward0>) tensor(11843.1484, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11843.1484375
tensor(11843.1484, grad_fn=<NegBackward0>) tensor(11843.1484, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11843.1474609375
tensor(11843.1484, grad_fn=<NegBackward0>) tensor(11843.1475, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11843.146484375
tensor(11843.1475, grad_fn=<NegBackward0>) tensor(11843.1465, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11842.9873046875
tensor(11843.1465, grad_fn=<NegBackward0>) tensor(11842.9873, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11842.986328125
tensor(11842.9873, grad_fn=<NegBackward0>) tensor(11842.9863, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11842.990234375
tensor(11842.9863, grad_fn=<NegBackward0>) tensor(11842.9902, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11842.984375
tensor(11842.9863, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11842.9873046875
tensor(11842.9844, grad_fn=<NegBackward0>) tensor(11842.9873, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11842.984375
tensor(11842.9844, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11842.9853515625
tensor(11842.9844, grad_fn=<NegBackward0>) tensor(11842.9854, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11842.984375
tensor(11842.9844, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11843.0185546875
tensor(11842.9844, grad_fn=<NegBackward0>) tensor(11843.0186, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11842.984375
tensor(11842.9844, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11842.9833984375
tensor(11842.9844, grad_fn=<NegBackward0>) tensor(11842.9834, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11842.984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11842.9833984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9834, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11842.994140625
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9941, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11842.984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11842.984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11842.9833984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9834, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11842.984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11842.9892578125
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9893, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11842.984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11842.984375
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11842.9844, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11843.0048828125
tensor(11842.9834, grad_fn=<NegBackward0>) tensor(11843.0049, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.6271, 0.3729],
        [0.5542, 0.4458]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4704, 0.5296], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2193, 0.1011],
         [0.6238, 0.4008]],

        [[0.5207, 0.1049],
         [0.7148, 0.5923]],

        [[0.6735, 0.1167],
         [0.6791, 0.6867]],

        [[0.5582, 0.1133],
         [0.6308, 0.7207]],

        [[0.6466, 0.0960],
         [0.5755, 0.6413]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 28
Adjusted Rand Index: 0.18726837735140967
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5059564705469372
Average Adjusted Rand Index: 0.837453675470282
[1.0, 0.5059564705469372] [1.0, 0.837453675470282] [11638.015625, 11843.0048828125]
-------------------------------------
This iteration is 24
True Objective function: Loss = -11573.362951620691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22349.96875
inf tensor(22349.9688, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12353.7666015625
tensor(22349.9688, grad_fn=<NegBackward0>) tensor(12353.7666, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12316.5634765625
tensor(12353.7666, grad_fn=<NegBackward0>) tensor(12316.5635, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11952.11328125
tensor(12316.5635, grad_fn=<NegBackward0>) tensor(11952.1133, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11729.400390625
tensor(11952.1133, grad_fn=<NegBackward0>) tensor(11729.4004, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11638.5693359375
tensor(11729.4004, grad_fn=<NegBackward0>) tensor(11638.5693, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11611.7080078125
tensor(11638.5693, grad_fn=<NegBackward0>) tensor(11611.7080, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11599.6982421875
tensor(11611.7080, grad_fn=<NegBackward0>) tensor(11599.6982, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11599.38671875
tensor(11599.6982, grad_fn=<NegBackward0>) tensor(11599.3867, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11589.572265625
tensor(11599.3867, grad_fn=<NegBackward0>) tensor(11589.5723, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11578.234375
tensor(11589.5723, grad_fn=<NegBackward0>) tensor(11578.2344, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11578.1005859375
tensor(11578.2344, grad_fn=<NegBackward0>) tensor(11578.1006, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11578.0146484375
tensor(11578.1006, grad_fn=<NegBackward0>) tensor(11578.0146, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11577.9560546875
tensor(11578.0146, grad_fn=<NegBackward0>) tensor(11577.9561, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11570.8955078125
tensor(11577.9561, grad_fn=<NegBackward0>) tensor(11570.8955, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11570.845703125
tensor(11570.8955, grad_fn=<NegBackward0>) tensor(11570.8457, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11570.818359375
tensor(11570.8457, grad_fn=<NegBackward0>) tensor(11570.8184, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11570.798828125
tensor(11570.8184, grad_fn=<NegBackward0>) tensor(11570.7988, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11570.779296875
tensor(11570.7988, grad_fn=<NegBackward0>) tensor(11570.7793, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11570.76171875
tensor(11570.7793, grad_fn=<NegBackward0>) tensor(11570.7617, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11570.748046875
tensor(11570.7617, grad_fn=<NegBackward0>) tensor(11570.7480, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11570.73828125
tensor(11570.7480, grad_fn=<NegBackward0>) tensor(11570.7383, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11570.7275390625
tensor(11570.7383, grad_fn=<NegBackward0>) tensor(11570.7275, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11570.71875
tensor(11570.7275, grad_fn=<NegBackward0>) tensor(11570.7188, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11570.716796875
tensor(11570.7188, grad_fn=<NegBackward0>) tensor(11570.7168, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11570.705078125
tensor(11570.7168, grad_fn=<NegBackward0>) tensor(11570.7051, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11570.6982421875
tensor(11570.7051, grad_fn=<NegBackward0>) tensor(11570.6982, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11570.693359375
tensor(11570.6982, grad_fn=<NegBackward0>) tensor(11570.6934, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11570.6884765625
tensor(11570.6934, grad_fn=<NegBackward0>) tensor(11570.6885, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11570.6865234375
tensor(11570.6885, grad_fn=<NegBackward0>) tensor(11570.6865, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11570.6806640625
tensor(11570.6865, grad_fn=<NegBackward0>) tensor(11570.6807, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11570.6767578125
tensor(11570.6807, grad_fn=<NegBackward0>) tensor(11570.6768, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11570.673828125
tensor(11570.6768, grad_fn=<NegBackward0>) tensor(11570.6738, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11570.6708984375
tensor(11570.6738, grad_fn=<NegBackward0>) tensor(11570.6709, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11570.66796875
tensor(11570.6709, grad_fn=<NegBackward0>) tensor(11570.6680, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11570.6650390625
tensor(11570.6680, grad_fn=<NegBackward0>) tensor(11570.6650, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11570.6640625
tensor(11570.6650, grad_fn=<NegBackward0>) tensor(11570.6641, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11570.66015625
tensor(11570.6641, grad_fn=<NegBackward0>) tensor(11570.6602, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11570.662109375
tensor(11570.6602, grad_fn=<NegBackward0>) tensor(11570.6621, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11570.654296875
tensor(11570.6602, grad_fn=<NegBackward0>) tensor(11570.6543, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11570.4912109375
tensor(11570.6543, grad_fn=<NegBackward0>) tensor(11570.4912, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11570.2509765625
tensor(11570.4912, grad_fn=<NegBackward0>) tensor(11570.2510, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11570.2451171875
tensor(11570.2510, grad_fn=<NegBackward0>) tensor(11570.2451, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11570.2568359375
tensor(11570.2451, grad_fn=<NegBackward0>) tensor(11570.2568, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11570.2412109375
tensor(11570.2451, grad_fn=<NegBackward0>) tensor(11570.2412, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11570.2421875
tensor(11570.2412, grad_fn=<NegBackward0>) tensor(11570.2422, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11570.240234375
tensor(11570.2412, grad_fn=<NegBackward0>) tensor(11570.2402, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11570.23828125
tensor(11570.2402, grad_fn=<NegBackward0>) tensor(11570.2383, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11570.24609375
tensor(11570.2383, grad_fn=<NegBackward0>) tensor(11570.2461, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11570.236328125
tensor(11570.2383, grad_fn=<NegBackward0>) tensor(11570.2363, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11570.236328125
tensor(11570.2363, grad_fn=<NegBackward0>) tensor(11570.2363, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11570.236328125
tensor(11570.2363, grad_fn=<NegBackward0>) tensor(11570.2363, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11570.2333984375
tensor(11570.2363, grad_fn=<NegBackward0>) tensor(11570.2334, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11570.234375
tensor(11570.2334, grad_fn=<NegBackward0>) tensor(11570.2344, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11570.232421875
tensor(11570.2334, grad_fn=<NegBackward0>) tensor(11570.2324, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11570.232421875
tensor(11570.2324, grad_fn=<NegBackward0>) tensor(11570.2324, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11570.2353515625
tensor(11570.2324, grad_fn=<NegBackward0>) tensor(11570.2354, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11570.2314453125
tensor(11570.2324, grad_fn=<NegBackward0>) tensor(11570.2314, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11570.2451171875
tensor(11570.2314, grad_fn=<NegBackward0>) tensor(11570.2451, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11570.23046875
tensor(11570.2314, grad_fn=<NegBackward0>) tensor(11570.2305, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11570.232421875
tensor(11570.2305, grad_fn=<NegBackward0>) tensor(11570.2324, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11570.2294921875
tensor(11570.2305, grad_fn=<NegBackward0>) tensor(11570.2295, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11570.2294921875
tensor(11570.2295, grad_fn=<NegBackward0>) tensor(11570.2295, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11570.2294921875
tensor(11570.2295, grad_fn=<NegBackward0>) tensor(11570.2295, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11570.228515625
tensor(11570.2295, grad_fn=<NegBackward0>) tensor(11570.2285, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11570.2333984375
tensor(11570.2285, grad_fn=<NegBackward0>) tensor(11570.2334, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11570.2265625
tensor(11570.2285, grad_fn=<NegBackward0>) tensor(11570.2266, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11570.2294921875
tensor(11570.2266, grad_fn=<NegBackward0>) tensor(11570.2295, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11570.2275390625
tensor(11570.2266, grad_fn=<NegBackward0>) tensor(11570.2275, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11570.2294921875
tensor(11570.2266, grad_fn=<NegBackward0>) tensor(11570.2295, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11570.2255859375
tensor(11570.2266, grad_fn=<NegBackward0>) tensor(11570.2256, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11570.2314453125
tensor(11570.2256, grad_fn=<NegBackward0>) tensor(11570.2314, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11570.224609375
tensor(11570.2256, grad_fn=<NegBackward0>) tensor(11570.2246, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11570.2333984375
tensor(11570.2246, grad_fn=<NegBackward0>) tensor(11570.2334, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11569.736328125
tensor(11570.2246, grad_fn=<NegBackward0>) tensor(11569.7363, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11569.73828125
tensor(11569.7363, grad_fn=<NegBackward0>) tensor(11569.7383, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11569.736328125
tensor(11569.7363, grad_fn=<NegBackward0>) tensor(11569.7363, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11569.734375
tensor(11569.7363, grad_fn=<NegBackward0>) tensor(11569.7344, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11569.736328125
tensor(11569.7344, grad_fn=<NegBackward0>) tensor(11569.7363, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11569.7353515625
tensor(11569.7344, grad_fn=<NegBackward0>) tensor(11569.7354, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11569.736328125
tensor(11569.7344, grad_fn=<NegBackward0>) tensor(11569.7363, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11569.7275390625
tensor(11569.7344, grad_fn=<NegBackward0>) tensor(11569.7275, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11569.720703125
tensor(11569.7275, grad_fn=<NegBackward0>) tensor(11569.7207, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11569.7314453125
tensor(11569.7207, grad_fn=<NegBackward0>) tensor(11569.7314, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11569.716796875
tensor(11569.7207, grad_fn=<NegBackward0>) tensor(11569.7168, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11569.720703125
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7207, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11569.716796875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7168, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11569.7421875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7422, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11569.716796875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7168, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11569.7216796875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7217, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11569.716796875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7168, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11569.716796875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7168, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11569.71875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7188, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11569.716796875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7168, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11569.71875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7188, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11569.716796875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7168, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11569.75
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7500, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11569.73046875
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7305, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11569.7314453125
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7314, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11569.7177734375
tensor(11569.7168, grad_fn=<NegBackward0>) tensor(11569.7178, grad_fn=<NegBackward0>)
4
pi: tensor([[0.7393, 0.2607],
        [0.2601, 0.7399]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5204, 0.4796], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1975, 0.1047],
         [0.6099, 0.4056]],

        [[0.5610, 0.1082],
         [0.6610, 0.6615]],

        [[0.6754, 0.0978],
         [0.5987, 0.6397]],

        [[0.5785, 0.0978],
         [0.5608, 0.7178]],

        [[0.5025, 0.1038],
         [0.5254, 0.5730]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320243133913
Average Adjusted Rand Index: 0.9839993730966994
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23685.046875
inf tensor(23685.0469, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12345.1357421875
tensor(23685.0469, grad_fn=<NegBackward0>) tensor(12345.1357, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12258.33203125
tensor(12345.1357, grad_fn=<NegBackward0>) tensor(12258.3320, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11645.23046875
tensor(12258.3320, grad_fn=<NegBackward0>) tensor(11645.2305, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11603.0966796875
tensor(11645.2305, grad_fn=<NegBackward0>) tensor(11603.0967, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11592.8876953125
tensor(11603.0967, grad_fn=<NegBackward0>) tensor(11592.8877, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11592.6083984375
tensor(11592.8877, grad_fn=<NegBackward0>) tensor(11592.6084, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11586.669921875
tensor(11592.6084, grad_fn=<NegBackward0>) tensor(11586.6699, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11578.396484375
tensor(11586.6699, grad_fn=<NegBackward0>) tensor(11578.3965, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11570.45703125
tensor(11578.3965, grad_fn=<NegBackward0>) tensor(11570.4570, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11570.3671875
tensor(11570.4570, grad_fn=<NegBackward0>) tensor(11570.3672, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11569.8427734375
tensor(11570.3672, grad_fn=<NegBackward0>) tensor(11569.8428, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11569.8125
tensor(11569.8428, grad_fn=<NegBackward0>) tensor(11569.8125, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11569.78515625
tensor(11569.8125, grad_fn=<NegBackward0>) tensor(11569.7852, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11569.765625
tensor(11569.7852, grad_fn=<NegBackward0>) tensor(11569.7656, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11569.748046875
tensor(11569.7656, grad_fn=<NegBackward0>) tensor(11569.7480, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11569.7373046875
tensor(11569.7480, grad_fn=<NegBackward0>) tensor(11569.7373, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11569.7265625
tensor(11569.7373, grad_fn=<NegBackward0>) tensor(11569.7266, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11569.7177734375
tensor(11569.7266, grad_fn=<NegBackward0>) tensor(11569.7178, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11569.7099609375
tensor(11569.7178, grad_fn=<NegBackward0>) tensor(11569.7100, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11569.703125
tensor(11569.7100, grad_fn=<NegBackward0>) tensor(11569.7031, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11569.6982421875
tensor(11569.7031, grad_fn=<NegBackward0>) tensor(11569.6982, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11569.693359375
tensor(11569.6982, grad_fn=<NegBackward0>) tensor(11569.6934, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11569.6884765625
tensor(11569.6934, grad_fn=<NegBackward0>) tensor(11569.6885, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11569.685546875
tensor(11569.6885, grad_fn=<NegBackward0>) tensor(11569.6855, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11569.681640625
tensor(11569.6855, grad_fn=<NegBackward0>) tensor(11569.6816, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11569.6787109375
tensor(11569.6816, grad_fn=<NegBackward0>) tensor(11569.6787, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11569.6767578125
tensor(11569.6787, grad_fn=<NegBackward0>) tensor(11569.6768, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11569.673828125
tensor(11569.6768, grad_fn=<NegBackward0>) tensor(11569.6738, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11569.671875
tensor(11569.6738, grad_fn=<NegBackward0>) tensor(11569.6719, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11569.66796875
tensor(11569.6719, grad_fn=<NegBackward0>) tensor(11569.6680, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11569.666015625
tensor(11569.6680, grad_fn=<NegBackward0>) tensor(11569.6660, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11569.6650390625
tensor(11569.6660, grad_fn=<NegBackward0>) tensor(11569.6650, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11569.662109375
tensor(11569.6650, grad_fn=<NegBackward0>) tensor(11569.6621, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11569.6611328125
tensor(11569.6621, grad_fn=<NegBackward0>) tensor(11569.6611, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11569.6611328125
tensor(11569.6611, grad_fn=<NegBackward0>) tensor(11569.6611, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11569.6591796875
tensor(11569.6611, grad_fn=<NegBackward0>) tensor(11569.6592, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11569.658203125
tensor(11569.6592, grad_fn=<NegBackward0>) tensor(11569.6582, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11569.65625
tensor(11569.6582, grad_fn=<NegBackward0>) tensor(11569.6562, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11569.65625
tensor(11569.6562, grad_fn=<NegBackward0>) tensor(11569.6562, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11569.654296875
tensor(11569.6562, grad_fn=<NegBackward0>) tensor(11569.6543, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11569.6552734375
tensor(11569.6543, grad_fn=<NegBackward0>) tensor(11569.6553, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11569.6552734375
tensor(11569.6543, grad_fn=<NegBackward0>) tensor(11569.6553, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11569.6533203125
tensor(11569.6543, grad_fn=<NegBackward0>) tensor(11569.6533, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11569.65234375
tensor(11569.6533, grad_fn=<NegBackward0>) tensor(11569.6523, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11569.65234375
tensor(11569.6523, grad_fn=<NegBackward0>) tensor(11569.6523, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11569.6513671875
tensor(11569.6523, grad_fn=<NegBackward0>) tensor(11569.6514, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11569.6513671875
tensor(11569.6514, grad_fn=<NegBackward0>) tensor(11569.6514, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11569.650390625
tensor(11569.6514, grad_fn=<NegBackward0>) tensor(11569.6504, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11569.654296875
tensor(11569.6504, grad_fn=<NegBackward0>) tensor(11569.6543, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11569.6533203125
tensor(11569.6504, grad_fn=<NegBackward0>) tensor(11569.6533, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11569.6474609375
tensor(11569.6504, grad_fn=<NegBackward0>) tensor(11569.6475, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11569.6484375
tensor(11569.6475, grad_fn=<NegBackward0>) tensor(11569.6484, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11569.6474609375
tensor(11569.6475, grad_fn=<NegBackward0>) tensor(11569.6475, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11569.646484375
tensor(11569.6475, grad_fn=<NegBackward0>) tensor(11569.6465, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11569.646484375
tensor(11569.6465, grad_fn=<NegBackward0>) tensor(11569.6465, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11569.6494140625
tensor(11569.6465, grad_fn=<NegBackward0>) tensor(11569.6494, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11569.646484375
tensor(11569.6465, grad_fn=<NegBackward0>) tensor(11569.6465, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11569.646484375
tensor(11569.6465, grad_fn=<NegBackward0>) tensor(11569.6465, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11569.6455078125
tensor(11569.6465, grad_fn=<NegBackward0>) tensor(11569.6455, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11569.64453125
tensor(11569.6455, grad_fn=<NegBackward0>) tensor(11569.6445, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11569.64453125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6445, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11569.6455078125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6455, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11569.6455078125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6455, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11569.64453125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6445, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11569.6455078125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6455, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11569.6455078125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6455, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11569.64453125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6445, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11569.64453125
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6445, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11569.6494140625
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6494, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11569.6435546875
tensor(11569.6445, grad_fn=<NegBackward0>) tensor(11569.6436, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11569.6416015625
tensor(11569.6436, grad_fn=<NegBackward0>) tensor(11569.6416, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11569.642578125
tensor(11569.6416, grad_fn=<NegBackward0>) tensor(11569.6426, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11569.642578125
tensor(11569.6416, grad_fn=<NegBackward0>) tensor(11569.6426, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11569.6953125
tensor(11569.6416, grad_fn=<NegBackward0>) tensor(11569.6953, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11569.642578125
tensor(11569.6416, grad_fn=<NegBackward0>) tensor(11569.6426, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11569.642578125
tensor(11569.6416, grad_fn=<NegBackward0>) tensor(11569.6426, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.7399, 0.2601],
        [0.2606, 0.7394]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4802, 0.5198], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4057, 0.1046],
         [0.7180, 0.1976]],

        [[0.7033, 0.1083],
         [0.5211, 0.5967]],

        [[0.5942, 0.0978],
         [0.6288, 0.5177]],

        [[0.7011, 0.0978],
         [0.6150, 0.5465]],

        [[0.5897, 0.1039],
         [0.6696, 0.5998]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320243133913
Average Adjusted Rand Index: 0.9839993730966994
[0.9840320243133913, 0.9840320243133913] [0.9839993730966994, 0.9839993730966994] [11569.7158203125, 11569.642578125]
-------------------------------------
This iteration is 25
True Objective function: Loss = -11500.599948450712
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22394.68359375
inf tensor(22394.6836, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12124.0087890625
tensor(22394.6836, grad_fn=<NegBackward0>) tensor(12124.0088, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12100.107421875
tensor(12124.0088, grad_fn=<NegBackward0>) tensor(12100.1074, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11647.046875
tensor(12100.1074, grad_fn=<NegBackward0>) tensor(11647.0469, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11514.9951171875
tensor(11647.0469, grad_fn=<NegBackward0>) tensor(11514.9951, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11506.1953125
tensor(11514.9951, grad_fn=<NegBackward0>) tensor(11506.1953, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11505.7900390625
tensor(11506.1953, grad_fn=<NegBackward0>) tensor(11505.7900, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11505.580078125
tensor(11505.7900, grad_fn=<NegBackward0>) tensor(11505.5801, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11505.4482421875
tensor(11505.5801, grad_fn=<NegBackward0>) tensor(11505.4482, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11498.958984375
tensor(11505.4482, grad_fn=<NegBackward0>) tensor(11498.9590, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11497.845703125
tensor(11498.9590, grad_fn=<NegBackward0>) tensor(11497.8457, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11497.7998046875
tensor(11497.8457, grad_fn=<NegBackward0>) tensor(11497.7998, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11497.7646484375
tensor(11497.7998, grad_fn=<NegBackward0>) tensor(11497.7646, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11497.7373046875
tensor(11497.7646, grad_fn=<NegBackward0>) tensor(11497.7373, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11497.712890625
tensor(11497.7373, grad_fn=<NegBackward0>) tensor(11497.7129, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11497.6533203125
tensor(11497.7129, grad_fn=<NegBackward0>) tensor(11497.6533, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11496.009765625
tensor(11497.6533, grad_fn=<NegBackward0>) tensor(11496.0098, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11495.9931640625
tensor(11496.0098, grad_fn=<NegBackward0>) tensor(11495.9932, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11495.9716796875
tensor(11495.9932, grad_fn=<NegBackward0>) tensor(11495.9717, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11495.4951171875
tensor(11495.9717, grad_fn=<NegBackward0>) tensor(11495.4951, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11495.4873046875
tensor(11495.4951, grad_fn=<NegBackward0>) tensor(11495.4873, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11495.4814453125
tensor(11495.4873, grad_fn=<NegBackward0>) tensor(11495.4814, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11495.474609375
tensor(11495.4814, grad_fn=<NegBackward0>) tensor(11495.4746, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11489.8837890625
tensor(11495.4746, grad_fn=<NegBackward0>) tensor(11489.8838, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11489.8603515625
tensor(11489.8838, grad_fn=<NegBackward0>) tensor(11489.8604, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11489.861328125
tensor(11489.8604, grad_fn=<NegBackward0>) tensor(11489.8613, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11489.853515625
tensor(11489.8604, grad_fn=<NegBackward0>) tensor(11489.8535, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11489.849609375
tensor(11489.8535, grad_fn=<NegBackward0>) tensor(11489.8496, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11489.8466796875
tensor(11489.8496, grad_fn=<NegBackward0>) tensor(11489.8467, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11489.84375
tensor(11489.8467, grad_fn=<NegBackward0>) tensor(11489.8438, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11489.841796875
tensor(11489.8438, grad_fn=<NegBackward0>) tensor(11489.8418, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11489.8388671875
tensor(11489.8418, grad_fn=<NegBackward0>) tensor(11489.8389, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11489.8359375
tensor(11489.8389, grad_fn=<NegBackward0>) tensor(11489.8359, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11489.8359375
tensor(11489.8359, grad_fn=<NegBackward0>) tensor(11489.8359, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11489.8291015625
tensor(11489.8359, grad_fn=<NegBackward0>) tensor(11489.8291, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11489.8154296875
tensor(11489.8291, grad_fn=<NegBackward0>) tensor(11489.8154, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11489.814453125
tensor(11489.8154, grad_fn=<NegBackward0>) tensor(11489.8145, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11489.8134765625
tensor(11489.8145, grad_fn=<NegBackward0>) tensor(11489.8135, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11489.8115234375
tensor(11489.8135, grad_fn=<NegBackward0>) tensor(11489.8115, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11489.8095703125
tensor(11489.8115, grad_fn=<NegBackward0>) tensor(11489.8096, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11489.810546875
tensor(11489.8096, grad_fn=<NegBackward0>) tensor(11489.8105, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11489.8173828125
tensor(11489.8096, grad_fn=<NegBackward0>) tensor(11489.8174, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11489.806640625
tensor(11489.8096, grad_fn=<NegBackward0>) tensor(11489.8066, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11489.806640625
tensor(11489.8066, grad_fn=<NegBackward0>) tensor(11489.8066, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11489.8056640625
tensor(11489.8066, grad_fn=<NegBackward0>) tensor(11489.8057, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11489.8056640625
tensor(11489.8057, grad_fn=<NegBackward0>) tensor(11489.8057, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11489.8037109375
tensor(11489.8057, grad_fn=<NegBackward0>) tensor(11489.8037, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11489.8046875
tensor(11489.8037, grad_fn=<NegBackward0>) tensor(11489.8047, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11489.802734375
tensor(11489.8037, grad_fn=<NegBackward0>) tensor(11489.8027, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11489.802734375
tensor(11489.8027, grad_fn=<NegBackward0>) tensor(11489.8027, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11489.802734375
tensor(11489.8027, grad_fn=<NegBackward0>) tensor(11489.8027, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11489.8017578125
tensor(11489.8027, grad_fn=<NegBackward0>) tensor(11489.8018, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11489.80078125
tensor(11489.8018, grad_fn=<NegBackward0>) tensor(11489.8008, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11489.80078125
tensor(11489.8008, grad_fn=<NegBackward0>) tensor(11489.8008, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11489.80078125
tensor(11489.8008, grad_fn=<NegBackward0>) tensor(11489.8008, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11489.8095703125
tensor(11489.8008, grad_fn=<NegBackward0>) tensor(11489.8096, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11489.798828125
tensor(11489.8008, grad_fn=<NegBackward0>) tensor(11489.7988, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11489.798828125
tensor(11489.7988, grad_fn=<NegBackward0>) tensor(11489.7988, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11489.798828125
tensor(11489.7988, grad_fn=<NegBackward0>) tensor(11489.7988, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11489.7978515625
tensor(11489.7988, grad_fn=<NegBackward0>) tensor(11489.7979, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11489.8056640625
tensor(11489.7979, grad_fn=<NegBackward0>) tensor(11489.8057, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11489.796875
tensor(11489.7979, grad_fn=<NegBackward0>) tensor(11489.7969, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11489.7998046875
tensor(11489.7969, grad_fn=<NegBackward0>) tensor(11489.7998, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11489.7978515625
tensor(11489.7969, grad_fn=<NegBackward0>) tensor(11489.7979, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11489.7978515625
tensor(11489.7969, grad_fn=<NegBackward0>) tensor(11489.7979, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11489.796875
tensor(11489.7969, grad_fn=<NegBackward0>) tensor(11489.7969, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11489.7958984375
tensor(11489.7969, grad_fn=<NegBackward0>) tensor(11489.7959, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11489.7998046875
tensor(11489.7959, grad_fn=<NegBackward0>) tensor(11489.7998, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11489.796875
tensor(11489.7959, grad_fn=<NegBackward0>) tensor(11489.7969, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11489.7998046875
tensor(11489.7959, grad_fn=<NegBackward0>) tensor(11489.7998, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11489.796875
tensor(11489.7959, grad_fn=<NegBackward0>) tensor(11489.7969, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11489.7939453125
tensor(11489.7959, grad_fn=<NegBackward0>) tensor(11489.7939, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11489.7978515625
tensor(11489.7939, grad_fn=<NegBackward0>) tensor(11489.7979, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11489.7939453125
tensor(11489.7939, grad_fn=<NegBackward0>) tensor(11489.7939, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11489.79296875
tensor(11489.7939, grad_fn=<NegBackward0>) tensor(11489.7930, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11489.7978515625
tensor(11489.7930, grad_fn=<NegBackward0>) tensor(11489.7979, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11489.861328125
tensor(11489.7930, grad_fn=<NegBackward0>) tensor(11489.8613, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11489.79296875
tensor(11489.7930, grad_fn=<NegBackward0>) tensor(11489.7930, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11489.7919921875
tensor(11489.7930, grad_fn=<NegBackward0>) tensor(11489.7920, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11489.7919921875
tensor(11489.7920, grad_fn=<NegBackward0>) tensor(11489.7920, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11489.794921875
tensor(11489.7920, grad_fn=<NegBackward0>) tensor(11489.7949, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11489.791015625
tensor(11489.7920, grad_fn=<NegBackward0>) tensor(11489.7910, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11489.7900390625
tensor(11489.7910, grad_fn=<NegBackward0>) tensor(11489.7900, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11489.7890625
tensor(11489.7900, grad_fn=<NegBackward0>) tensor(11489.7891, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11489.7890625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7891, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11489.7890625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7891, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11489.787109375
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7871, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11489.7880859375
tensor(11489.7871, grad_fn=<NegBackward0>) tensor(11489.7881, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11489.7880859375
tensor(11489.7871, grad_fn=<NegBackward0>) tensor(11489.7881, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11489.7890625
tensor(11489.7871, grad_fn=<NegBackward0>) tensor(11489.7891, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11489.7890625
tensor(11489.7871, grad_fn=<NegBackward0>) tensor(11489.7891, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11489.7880859375
tensor(11489.7871, grad_fn=<NegBackward0>) tensor(11489.7881, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.6693, 0.3307],
        [0.2709, 0.7291]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5061, 0.4939], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3879, 0.0978],
         [0.7200, 0.2067]],

        [[0.6439, 0.1065],
         [0.5601, 0.5413]],

        [[0.5462, 0.0932],
         [0.5282, 0.5500]],

        [[0.7180, 0.1050],
         [0.5501, 0.6758]],

        [[0.6623, 0.1000],
         [0.7223, 0.6815]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999328927508
Average Adjusted Rand Index: 0.9919996552039955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21381.220703125
inf tensor(21381.2207, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12126.53515625
tensor(21381.2207, grad_fn=<NegBackward0>) tensor(12126.5352, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12115.5068359375
tensor(12126.5352, grad_fn=<NegBackward0>) tensor(12115.5068, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12022.1748046875
tensor(12115.5068, grad_fn=<NegBackward0>) tensor(12022.1748, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11544.423828125
tensor(12022.1748, grad_fn=<NegBackward0>) tensor(11544.4238, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11526.8740234375
tensor(11544.4238, grad_fn=<NegBackward0>) tensor(11526.8740, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11513.9189453125
tensor(11526.8740, grad_fn=<NegBackward0>) tensor(11513.9189, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11507.96875
tensor(11513.9189, grad_fn=<NegBackward0>) tensor(11507.9688, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11498.708984375
tensor(11507.9688, grad_fn=<NegBackward0>) tensor(11498.7090, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11498.5654296875
tensor(11498.7090, grad_fn=<NegBackward0>) tensor(11498.5654, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11498.4765625
tensor(11498.5654, grad_fn=<NegBackward0>) tensor(11498.4766, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11498.39453125
tensor(11498.4766, grad_fn=<NegBackward0>) tensor(11498.3945, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11496.8291015625
tensor(11498.3945, grad_fn=<NegBackward0>) tensor(11496.8291, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11496.677734375
tensor(11496.8291, grad_fn=<NegBackward0>) tensor(11496.6777, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11496.6455078125
tensor(11496.6777, grad_fn=<NegBackward0>) tensor(11496.6455, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11496.62109375
tensor(11496.6455, grad_fn=<NegBackward0>) tensor(11496.6211, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11496.6005859375
tensor(11496.6211, grad_fn=<NegBackward0>) tensor(11496.6006, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11496.5830078125
tensor(11496.6006, grad_fn=<NegBackward0>) tensor(11496.5830, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11496.5693359375
tensor(11496.5830, grad_fn=<NegBackward0>) tensor(11496.5693, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11496.5576171875
tensor(11496.5693, grad_fn=<NegBackward0>) tensor(11496.5576, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11496.5478515625
tensor(11496.5576, grad_fn=<NegBackward0>) tensor(11496.5479, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11496.5390625
tensor(11496.5479, grad_fn=<NegBackward0>) tensor(11496.5391, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11496.53125
tensor(11496.5391, grad_fn=<NegBackward0>) tensor(11496.5312, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11496.533203125
tensor(11496.5312, grad_fn=<NegBackward0>) tensor(11496.5332, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -11496.515625
tensor(11496.5312, grad_fn=<NegBackward0>) tensor(11496.5156, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11496.41796875
tensor(11496.5156, grad_fn=<NegBackward0>) tensor(11496.4180, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11495.4677734375
tensor(11496.4180, grad_fn=<NegBackward0>) tensor(11495.4678, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11495.462890625
tensor(11495.4678, grad_fn=<NegBackward0>) tensor(11495.4629, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11495.4599609375
tensor(11495.4629, grad_fn=<NegBackward0>) tensor(11495.4600, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11495.4560546875
tensor(11495.4600, grad_fn=<NegBackward0>) tensor(11495.4561, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11495.453125
tensor(11495.4561, grad_fn=<NegBackward0>) tensor(11495.4531, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11495.4501953125
tensor(11495.4531, grad_fn=<NegBackward0>) tensor(11495.4502, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11495.4482421875
tensor(11495.4502, grad_fn=<NegBackward0>) tensor(11495.4482, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11495.4453125
tensor(11495.4482, grad_fn=<NegBackward0>) tensor(11495.4453, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11495.44140625
tensor(11495.4453, grad_fn=<NegBackward0>) tensor(11495.4414, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11495.4384765625
tensor(11495.4414, grad_fn=<NegBackward0>) tensor(11495.4385, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11489.8359375
tensor(11495.4385, grad_fn=<NegBackward0>) tensor(11489.8359, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11489.837890625
tensor(11489.8359, grad_fn=<NegBackward0>) tensor(11489.8379, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11489.8330078125
tensor(11489.8359, grad_fn=<NegBackward0>) tensor(11489.8330, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11489.8310546875
tensor(11489.8330, grad_fn=<NegBackward0>) tensor(11489.8311, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11489.830078125
tensor(11489.8311, grad_fn=<NegBackward0>) tensor(11489.8301, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11489.828125
tensor(11489.8301, grad_fn=<NegBackward0>) tensor(11489.8281, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11489.826171875
tensor(11489.8281, grad_fn=<NegBackward0>) tensor(11489.8262, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11489.828125
tensor(11489.8262, grad_fn=<NegBackward0>) tensor(11489.8281, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11489.8251953125
tensor(11489.8262, grad_fn=<NegBackward0>) tensor(11489.8252, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11489.8232421875
tensor(11489.8252, grad_fn=<NegBackward0>) tensor(11489.8232, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11489.8232421875
tensor(11489.8232, grad_fn=<NegBackward0>) tensor(11489.8232, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11489.822265625
tensor(11489.8232, grad_fn=<NegBackward0>) tensor(11489.8223, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11489.8330078125
tensor(11489.8223, grad_fn=<NegBackward0>) tensor(11489.8330, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11489.8212890625
tensor(11489.8223, grad_fn=<NegBackward0>) tensor(11489.8213, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11489.8203125
tensor(11489.8213, grad_fn=<NegBackward0>) tensor(11489.8203, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11489.8203125
tensor(11489.8203, grad_fn=<NegBackward0>) tensor(11489.8203, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11489.8193359375
tensor(11489.8203, grad_fn=<NegBackward0>) tensor(11489.8193, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11489.818359375
tensor(11489.8193, grad_fn=<NegBackward0>) tensor(11489.8184, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11489.818359375
tensor(11489.8184, grad_fn=<NegBackward0>) tensor(11489.8184, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11489.8193359375
tensor(11489.8184, grad_fn=<NegBackward0>) tensor(11489.8193, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11489.818359375
tensor(11489.8184, grad_fn=<NegBackward0>) tensor(11489.8184, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11489.818359375
tensor(11489.8184, grad_fn=<NegBackward0>) tensor(11489.8184, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11489.81640625
tensor(11489.8184, grad_fn=<NegBackward0>) tensor(11489.8164, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11489.818359375
tensor(11489.8164, grad_fn=<NegBackward0>) tensor(11489.8184, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11489.8154296875
tensor(11489.8164, grad_fn=<NegBackward0>) tensor(11489.8154, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11489.8154296875
tensor(11489.8154, grad_fn=<NegBackward0>) tensor(11489.8154, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11489.8154296875
tensor(11489.8154, grad_fn=<NegBackward0>) tensor(11489.8154, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11489.8154296875
tensor(11489.8154, grad_fn=<NegBackward0>) tensor(11489.8154, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11489.81640625
tensor(11489.8154, grad_fn=<NegBackward0>) tensor(11489.8164, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11489.8134765625
tensor(11489.8154, grad_fn=<NegBackward0>) tensor(11489.8135, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11489.8125
tensor(11489.8135, grad_fn=<NegBackward0>) tensor(11489.8125, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11489.8115234375
tensor(11489.8125, grad_fn=<NegBackward0>) tensor(11489.8115, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11489.8125
tensor(11489.8115, grad_fn=<NegBackward0>) tensor(11489.8125, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11489.8134765625
tensor(11489.8115, grad_fn=<NegBackward0>) tensor(11489.8135, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11489.810546875
tensor(11489.8115, grad_fn=<NegBackward0>) tensor(11489.8105, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11489.810546875
tensor(11489.8105, grad_fn=<NegBackward0>) tensor(11489.8105, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11489.810546875
tensor(11489.8105, grad_fn=<NegBackward0>) tensor(11489.8105, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11489.8115234375
tensor(11489.8105, grad_fn=<NegBackward0>) tensor(11489.8115, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11489.810546875
tensor(11489.8105, grad_fn=<NegBackward0>) tensor(11489.8105, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11489.8095703125
tensor(11489.8105, grad_fn=<NegBackward0>) tensor(11489.8096, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11489.80859375
tensor(11489.8096, grad_fn=<NegBackward0>) tensor(11489.8086, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11489.8173828125
tensor(11489.8086, grad_fn=<NegBackward0>) tensor(11489.8174, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11489.80859375
tensor(11489.8086, grad_fn=<NegBackward0>) tensor(11489.8086, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11489.828125
tensor(11489.8086, grad_fn=<NegBackward0>) tensor(11489.8281, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11489.7900390625
tensor(11489.8086, grad_fn=<NegBackward0>) tensor(11489.7900, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11489.791015625
tensor(11489.7900, grad_fn=<NegBackward0>) tensor(11489.7910, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11489.7890625
tensor(11489.7900, grad_fn=<NegBackward0>) tensor(11489.7891, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11489.791015625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7910, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11489.7900390625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7900, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11489.7900390625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7900, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11489.7890625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7891, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11489.7900390625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7900, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11489.7900390625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7900, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11489.7958984375
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7959, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11489.806640625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.8066, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11489.7900390625
tensor(11489.7891, grad_fn=<NegBackward0>) tensor(11489.7900, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.6696, 0.3304],
        [0.2709, 0.7291]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5064, 0.4936], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3877, 0.0978],
         [0.6592, 0.2067]],

        [[0.5579, 0.1065],
         [0.5208, 0.7126]],

        [[0.6934, 0.0932],
         [0.7073, 0.6502]],

        [[0.5428, 0.1050],
         [0.6031, 0.7191]],

        [[0.6497, 0.1000],
         [0.7264, 0.5076]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999328927508
Average Adjusted Rand Index: 0.9919996552039955
[0.9919999328927508, 0.9919999328927508] [0.9919996552039955, 0.9919996552039955] [11489.7880859375, 11489.7900390625]
-------------------------------------
This iteration is 26
True Objective function: Loss = -11537.294428055035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21319.861328125
inf tensor(21319.8613, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12007.62109375
tensor(21319.8613, grad_fn=<NegBackward0>) tensor(12007.6211, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11652.7353515625
tensor(12007.6211, grad_fn=<NegBackward0>) tensor(11652.7354, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11651.7734375
tensor(11652.7354, grad_fn=<NegBackward0>) tensor(11651.7734, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11651.470703125
tensor(11651.7734, grad_fn=<NegBackward0>) tensor(11651.4707, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11651.3330078125
tensor(11651.4707, grad_fn=<NegBackward0>) tensor(11651.3330, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11651.1982421875
tensor(11651.3330, grad_fn=<NegBackward0>) tensor(11651.1982, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11650.6279296875
tensor(11651.1982, grad_fn=<NegBackward0>) tensor(11650.6279, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11650.6025390625
tensor(11650.6279, grad_fn=<NegBackward0>) tensor(11650.6025, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11650.5693359375
tensor(11650.6025, grad_fn=<NegBackward0>) tensor(11650.5693, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11650.5517578125
tensor(11650.5693, grad_fn=<NegBackward0>) tensor(11650.5518, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11650.5380859375
tensor(11650.5518, grad_fn=<NegBackward0>) tensor(11650.5381, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11650.5283203125
tensor(11650.5381, grad_fn=<NegBackward0>) tensor(11650.5283, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11650.521484375
tensor(11650.5283, grad_fn=<NegBackward0>) tensor(11650.5215, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11650.5146484375
tensor(11650.5215, grad_fn=<NegBackward0>) tensor(11650.5146, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11650.5078125
tensor(11650.5146, grad_fn=<NegBackward0>) tensor(11650.5078, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11650.5
tensor(11650.5078, grad_fn=<NegBackward0>) tensor(11650.5000, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11650.4951171875
tensor(11650.5000, grad_fn=<NegBackward0>) tensor(11650.4951, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11650.4931640625
tensor(11650.4951, grad_fn=<NegBackward0>) tensor(11650.4932, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11650.4892578125
tensor(11650.4932, grad_fn=<NegBackward0>) tensor(11650.4893, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11650.4853515625
tensor(11650.4893, grad_fn=<NegBackward0>) tensor(11650.4854, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11650.486328125
tensor(11650.4854, grad_fn=<NegBackward0>) tensor(11650.4863, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -11650.48046875
tensor(11650.4854, grad_fn=<NegBackward0>) tensor(11650.4805, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11650.474609375
tensor(11650.4805, grad_fn=<NegBackward0>) tensor(11650.4746, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11650.466796875
tensor(11650.4746, grad_fn=<NegBackward0>) tensor(11650.4668, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11650.46484375
tensor(11650.4668, grad_fn=<NegBackward0>) tensor(11650.4648, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11650.46484375
tensor(11650.4648, grad_fn=<NegBackward0>) tensor(11650.4648, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11650.46484375
tensor(11650.4648, grad_fn=<NegBackward0>) tensor(11650.4648, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11650.4609375
tensor(11650.4648, grad_fn=<NegBackward0>) tensor(11650.4609, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11650.4677734375
tensor(11650.4609, grad_fn=<NegBackward0>) tensor(11650.4678, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11650.4697265625
tensor(11650.4609, grad_fn=<NegBackward0>) tensor(11650.4697, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -11650.458984375
tensor(11650.4609, grad_fn=<NegBackward0>) tensor(11650.4590, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11650.458984375
tensor(11650.4590, grad_fn=<NegBackward0>) tensor(11650.4590, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11650.4580078125
tensor(11650.4590, grad_fn=<NegBackward0>) tensor(11650.4580, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11650.458984375
tensor(11650.4580, grad_fn=<NegBackward0>) tensor(11650.4590, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11650.4580078125
tensor(11650.4580, grad_fn=<NegBackward0>) tensor(11650.4580, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11650.45703125
tensor(11650.4580, grad_fn=<NegBackward0>) tensor(11650.4570, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11650.4580078125
tensor(11650.4570, grad_fn=<NegBackward0>) tensor(11650.4580, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11650.4560546875
tensor(11650.4570, grad_fn=<NegBackward0>) tensor(11650.4561, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11650.45703125
tensor(11650.4561, grad_fn=<NegBackward0>) tensor(11650.4570, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11650.455078125
tensor(11650.4561, grad_fn=<NegBackward0>) tensor(11650.4551, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11650.45703125
tensor(11650.4551, grad_fn=<NegBackward0>) tensor(11650.4570, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11650.4599609375
tensor(11650.4551, grad_fn=<NegBackward0>) tensor(11650.4600, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11650.458984375
tensor(11650.4551, grad_fn=<NegBackward0>) tensor(11650.4590, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -11650.4541015625
tensor(11650.4551, grad_fn=<NegBackward0>) tensor(11650.4541, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11650.453125
tensor(11650.4541, grad_fn=<NegBackward0>) tensor(11650.4531, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11650.4521484375
tensor(11650.4531, grad_fn=<NegBackward0>) tensor(11650.4521, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11650.4521484375
tensor(11650.4521, grad_fn=<NegBackward0>) tensor(11650.4521, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11650.4521484375
tensor(11650.4521, grad_fn=<NegBackward0>) tensor(11650.4521, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11650.451171875
tensor(11650.4521, grad_fn=<NegBackward0>) tensor(11650.4512, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11650.451171875
tensor(11650.4512, grad_fn=<NegBackward0>) tensor(11650.4512, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11650.451171875
tensor(11650.4512, grad_fn=<NegBackward0>) tensor(11650.4512, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11650.451171875
tensor(11650.4512, grad_fn=<NegBackward0>) tensor(11650.4512, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11650.4501953125
tensor(11650.4512, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11650.4501953125
tensor(11650.4502, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11650.453125
tensor(11650.4502, grad_fn=<NegBackward0>) tensor(11650.4531, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11650.4501953125
tensor(11650.4502, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11650.4501953125
tensor(11650.4502, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11650.4501953125
tensor(11650.4502, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11650.4501953125
tensor(11650.4502, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11650.44921875
tensor(11650.4502, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11650.4501953125
tensor(11650.4492, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11650.44921875
tensor(11650.4492, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11650.451171875
tensor(11650.4492, grad_fn=<NegBackward0>) tensor(11650.4512, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11650.44921875
tensor(11650.4492, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11650.4501953125
tensor(11650.4492, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11650.4501953125
tensor(11650.4492, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11650.4482421875
tensor(11650.4492, grad_fn=<NegBackward0>) tensor(11650.4482, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11650.451171875
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4512, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11650.4501953125
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4502, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11650.4482421875
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4482, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11650.4560546875
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4561, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11650.4482421875
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4482, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11650.4658203125
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4658, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11650.4482421875
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4482, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11650.44921875
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11650.5361328125
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.5361, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11650.447265625
tensor(11650.4482, grad_fn=<NegBackward0>) tensor(11650.4473, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11650.4892578125
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4893, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11650.44921875
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11650.447265625
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4473, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11650.447265625
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4473, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11650.447265625
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4473, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11650.44921875
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11650.447265625
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4473, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11650.470703125
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4707, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11650.44921875
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11650.44921875
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4492, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11650.4482421875
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4482, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11650.4482421875
tensor(11650.4473, grad_fn=<NegBackward0>) tensor(11650.4482, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.4733, 0.5267],
        [0.4349, 0.5651]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5370, 0.4630], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3777, 0.0999],
         [0.6018, 0.2162]],

        [[0.5449, 0.1025],
         [0.6523, 0.7304]],

        [[0.5398, 0.0986],
         [0.5300, 0.6736]],

        [[0.6857, 0.0966],
         [0.7254, 0.5498]],

        [[0.7227, 0.1027],
         [0.6929, 0.5436]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.24196687024629002
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.4558891679098334
Average Adjusted Rand Index: 0.801036857458579
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20909.16796875
inf tensor(20909.1680, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12220.7880859375
tensor(20909.1680, grad_fn=<NegBackward0>) tensor(12220.7881, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12205.107421875
tensor(12220.7881, grad_fn=<NegBackward0>) tensor(12205.1074, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12044.3544921875
tensor(12205.1074, grad_fn=<NegBackward0>) tensor(12044.3545, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11709.92578125
tensor(12044.3545, grad_fn=<NegBackward0>) tensor(11709.9258, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11662.4638671875
tensor(11709.9258, grad_fn=<NegBackward0>) tensor(11662.4639, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11661.8603515625
tensor(11662.4639, grad_fn=<NegBackward0>) tensor(11661.8604, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11661.6259765625
tensor(11661.8604, grad_fn=<NegBackward0>) tensor(11661.6260, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11661.4970703125
tensor(11661.6260, grad_fn=<NegBackward0>) tensor(11661.4971, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11661.419921875
tensor(11661.4971, grad_fn=<NegBackward0>) tensor(11661.4199, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11661.3623046875
tensor(11661.4199, grad_fn=<NegBackward0>) tensor(11661.3623, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11661.3203125
tensor(11661.3623, grad_fn=<NegBackward0>) tensor(11661.3203, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11661.2880859375
tensor(11661.3203, grad_fn=<NegBackward0>) tensor(11661.2881, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11660.7080078125
tensor(11661.2881, grad_fn=<NegBackward0>) tensor(11660.7080, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11660.654296875
tensor(11660.7080, grad_fn=<NegBackward0>) tensor(11660.6543, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11657.7978515625
tensor(11660.6543, grad_fn=<NegBackward0>) tensor(11657.7979, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11657.7783203125
tensor(11657.7979, grad_fn=<NegBackward0>) tensor(11657.7783, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11657.765625
tensor(11657.7783, grad_fn=<NegBackward0>) tensor(11657.7656, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11657.7548828125
tensor(11657.7656, grad_fn=<NegBackward0>) tensor(11657.7549, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11656.5458984375
tensor(11657.7549, grad_fn=<NegBackward0>) tensor(11656.5459, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11656.42578125
tensor(11656.5459, grad_fn=<NegBackward0>) tensor(11656.4258, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11656.3828125
tensor(11656.4258, grad_fn=<NegBackward0>) tensor(11656.3828, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11655.8037109375
tensor(11656.3828, grad_fn=<NegBackward0>) tensor(11655.8037, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11655.796875
tensor(11655.8037, grad_fn=<NegBackward0>) tensor(11655.7969, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11655.76171875
tensor(11655.7969, grad_fn=<NegBackward0>) tensor(11655.7617, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11650.8203125
tensor(11655.7617, grad_fn=<NegBackward0>) tensor(11650.8203, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11650.814453125
tensor(11650.8203, grad_fn=<NegBackward0>) tensor(11650.8145, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11650.8134765625
tensor(11650.8145, grad_fn=<NegBackward0>) tensor(11650.8135, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11650.810546875
tensor(11650.8135, grad_fn=<NegBackward0>) tensor(11650.8105, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11650.8076171875
tensor(11650.8105, grad_fn=<NegBackward0>) tensor(11650.8076, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11650.8037109375
tensor(11650.8076, grad_fn=<NegBackward0>) tensor(11650.8037, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11650.8017578125
tensor(11650.8037, grad_fn=<NegBackward0>) tensor(11650.8018, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11650.8017578125
tensor(11650.8018, grad_fn=<NegBackward0>) tensor(11650.8018, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11650.798828125
tensor(11650.8018, grad_fn=<NegBackward0>) tensor(11650.7988, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11650.796875
tensor(11650.7988, grad_fn=<NegBackward0>) tensor(11650.7969, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11650.7958984375
tensor(11650.7969, grad_fn=<NegBackward0>) tensor(11650.7959, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11650.7939453125
tensor(11650.7959, grad_fn=<NegBackward0>) tensor(11650.7939, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11650.79296875
tensor(11650.7939, grad_fn=<NegBackward0>) tensor(11650.7930, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11650.7919921875
tensor(11650.7930, grad_fn=<NegBackward0>) tensor(11650.7920, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11650.7890625
tensor(11650.7920, grad_fn=<NegBackward0>) tensor(11650.7891, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11650.7939453125
tensor(11650.7891, grad_fn=<NegBackward0>) tensor(11650.7939, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11650.787109375
tensor(11650.7891, grad_fn=<NegBackward0>) tensor(11650.7871, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11650.787109375
tensor(11650.7871, grad_fn=<NegBackward0>) tensor(11650.7871, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11650.7861328125
tensor(11650.7871, grad_fn=<NegBackward0>) tensor(11650.7861, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11650.7861328125
tensor(11650.7861, grad_fn=<NegBackward0>) tensor(11650.7861, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11650.7841796875
tensor(11650.7861, grad_fn=<NegBackward0>) tensor(11650.7842, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11650.7880859375
tensor(11650.7842, grad_fn=<NegBackward0>) tensor(11650.7881, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11650.7919921875
tensor(11650.7842, grad_fn=<NegBackward0>) tensor(11650.7920, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11650.7841796875
tensor(11650.7842, grad_fn=<NegBackward0>) tensor(11650.7842, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11650.78515625
tensor(11650.7842, grad_fn=<NegBackward0>) tensor(11650.7852, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11650.7783203125
tensor(11650.7842, grad_fn=<NegBackward0>) tensor(11650.7783, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11650.767578125
tensor(11650.7783, grad_fn=<NegBackward0>) tensor(11650.7676, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11650.767578125
tensor(11650.7676, grad_fn=<NegBackward0>) tensor(11650.7676, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11650.771484375
tensor(11650.7676, grad_fn=<NegBackward0>) tensor(11650.7715, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11650.767578125
tensor(11650.7676, grad_fn=<NegBackward0>) tensor(11650.7676, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11650.767578125
tensor(11650.7676, grad_fn=<NegBackward0>) tensor(11650.7676, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11650.76953125
tensor(11650.7676, grad_fn=<NegBackward0>) tensor(11650.7695, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11650.7666015625
tensor(11650.7676, grad_fn=<NegBackward0>) tensor(11650.7666, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11650.7607421875
tensor(11650.7666, grad_fn=<NegBackward0>) tensor(11650.7607, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11650.759765625
tensor(11650.7607, grad_fn=<NegBackward0>) tensor(11650.7598, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11650.7607421875
tensor(11650.7598, grad_fn=<NegBackward0>) tensor(11650.7607, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11650.765625
tensor(11650.7598, grad_fn=<NegBackward0>) tensor(11650.7656, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11650.759765625
tensor(11650.7598, grad_fn=<NegBackward0>) tensor(11650.7598, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11650.759765625
tensor(11650.7598, grad_fn=<NegBackward0>) tensor(11650.7598, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11650.759765625
tensor(11650.7598, grad_fn=<NegBackward0>) tensor(11650.7598, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11650.7578125
tensor(11650.7598, grad_fn=<NegBackward0>) tensor(11650.7578, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11650.751953125
tensor(11650.7578, grad_fn=<NegBackward0>) tensor(11650.7520, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11650.7607421875
tensor(11650.7520, grad_fn=<NegBackward0>) tensor(11650.7607, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11650.748046875
tensor(11650.7520, grad_fn=<NegBackward0>) tensor(11650.7480, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11650.7470703125
tensor(11650.7480, grad_fn=<NegBackward0>) tensor(11650.7471, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11650.7490234375
tensor(11650.7471, grad_fn=<NegBackward0>) tensor(11650.7490, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11650.7470703125
tensor(11650.7471, grad_fn=<NegBackward0>) tensor(11650.7471, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11650.7744140625
tensor(11650.7471, grad_fn=<NegBackward0>) tensor(11650.7744, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11650.7470703125
tensor(11650.7471, grad_fn=<NegBackward0>) tensor(11650.7471, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11650.74609375
tensor(11650.7471, grad_fn=<NegBackward0>) tensor(11650.7461, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11650.74609375
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.7461, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11650.74609375
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.7461, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11650.7470703125
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.7471, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11650.7470703125
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.7471, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11650.74609375
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.7461, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11650.7470703125
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.7471, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11650.74609375
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.7461, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11650.6435546875
tensor(11650.7461, grad_fn=<NegBackward0>) tensor(11650.6436, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11650.6435546875
tensor(11650.6436, grad_fn=<NegBackward0>) tensor(11650.6436, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11650.7001953125
tensor(11650.6436, grad_fn=<NegBackward0>) tensor(11650.7002, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11650.6435546875
tensor(11650.6436, grad_fn=<NegBackward0>) tensor(11650.6436, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11650.6767578125
tensor(11650.6436, grad_fn=<NegBackward0>) tensor(11650.6768, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11650.642578125
tensor(11650.6436, grad_fn=<NegBackward0>) tensor(11650.6426, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11650.65234375
tensor(11650.6426, grad_fn=<NegBackward0>) tensor(11650.6523, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11650.6416015625
tensor(11650.6426, grad_fn=<NegBackward0>) tensor(11650.6416, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11650.6806640625
tensor(11650.6416, grad_fn=<NegBackward0>) tensor(11650.6807, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11650.642578125
tensor(11650.6416, grad_fn=<NegBackward0>) tensor(11650.6426, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11650.642578125
tensor(11650.6416, grad_fn=<NegBackward0>) tensor(11650.6426, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11650.642578125
tensor(11650.6416, grad_fn=<NegBackward0>) tensor(11650.6426, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11650.642578125
tensor(11650.6416, grad_fn=<NegBackward0>) tensor(11650.6426, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.5649, 0.4351],
        [0.5287, 0.4713]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4631, 0.5369], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2161, 0.0999],
         [0.6773, 0.3781]],

        [[0.6341, 0.1025],
         [0.6808, 0.6613]],

        [[0.5795, 0.0983],
         [0.6918, 0.7052]],

        [[0.6289, 0.0966],
         [0.6454, 0.7226]],

        [[0.5579, 0.1028],
         [0.7265, 0.5071]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 25
Adjusted Rand Index: 0.24196687024629002
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.4558891679098334
Average Adjusted Rand Index: 0.801036857458579
[0.4558891679098334, 0.4558891679098334] [0.801036857458579, 0.801036857458579] [11650.4482421875, 11650.642578125]
-------------------------------------
This iteration is 27
True Objective function: Loss = -11419.135412558191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19960.27734375
inf tensor(19960.2773, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11620.087890625
tensor(19960.2773, grad_fn=<NegBackward0>) tensor(11620.0879, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11513.5205078125
tensor(11620.0879, grad_fn=<NegBackward0>) tensor(11513.5205, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11445.6875
tensor(11513.5205, grad_fn=<NegBackward0>) tensor(11445.6875, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11402.87890625
tensor(11445.6875, grad_fn=<NegBackward0>) tensor(11402.8789, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11402.5537109375
tensor(11402.8789, grad_fn=<NegBackward0>) tensor(11402.5537, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11402.4072265625
tensor(11402.5537, grad_fn=<NegBackward0>) tensor(11402.4072, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11402.3212890625
tensor(11402.4072, grad_fn=<NegBackward0>) tensor(11402.3213, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11402.2685546875
tensor(11402.3213, grad_fn=<NegBackward0>) tensor(11402.2686, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11402.23046875
tensor(11402.2686, grad_fn=<NegBackward0>) tensor(11402.2305, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11402.203125
tensor(11402.2305, grad_fn=<NegBackward0>) tensor(11402.2031, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11402.181640625
tensor(11402.2031, grad_fn=<NegBackward0>) tensor(11402.1816, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11402.166015625
tensor(11402.1816, grad_fn=<NegBackward0>) tensor(11402.1660, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11402.1533203125
tensor(11402.1660, grad_fn=<NegBackward0>) tensor(11402.1533, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11402.1416015625
tensor(11402.1533, grad_fn=<NegBackward0>) tensor(11402.1416, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11402.1328125
tensor(11402.1416, grad_fn=<NegBackward0>) tensor(11402.1328, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11400.2421875
tensor(11402.1328, grad_fn=<NegBackward0>) tensor(11400.2422, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11400.234375
tensor(11400.2422, grad_fn=<NegBackward0>) tensor(11400.2344, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11400.212890625
tensor(11400.2344, grad_fn=<NegBackward0>) tensor(11400.2129, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11400.095703125
tensor(11400.2129, grad_fn=<NegBackward0>) tensor(11400.0957, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11400.0908203125
tensor(11400.0957, grad_fn=<NegBackward0>) tensor(11400.0908, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11400.0888671875
tensor(11400.0908, grad_fn=<NegBackward0>) tensor(11400.0889, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11400.0849609375
tensor(11400.0889, grad_fn=<NegBackward0>) tensor(11400.0850, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11400.0830078125
tensor(11400.0850, grad_fn=<NegBackward0>) tensor(11400.0830, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11400.0810546875
tensor(11400.0830, grad_fn=<NegBackward0>) tensor(11400.0811, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11400.078125
tensor(11400.0811, grad_fn=<NegBackward0>) tensor(11400.0781, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11400.0771484375
tensor(11400.0781, grad_fn=<NegBackward0>) tensor(11400.0771, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11400.07421875
tensor(11400.0771, grad_fn=<NegBackward0>) tensor(11400.0742, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11400.072265625
tensor(11400.0742, grad_fn=<NegBackward0>) tensor(11400.0723, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11400.072265625
tensor(11400.0723, grad_fn=<NegBackward0>) tensor(11400.0723, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11400.0712890625
tensor(11400.0723, grad_fn=<NegBackward0>) tensor(11400.0713, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11400.0703125
tensor(11400.0713, grad_fn=<NegBackward0>) tensor(11400.0703, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11400.0693359375
tensor(11400.0703, grad_fn=<NegBackward0>) tensor(11400.0693, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11400.0673828125
tensor(11400.0693, grad_fn=<NegBackward0>) tensor(11400.0674, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11400.068359375
tensor(11400.0674, grad_fn=<NegBackward0>) tensor(11400.0684, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11400.06640625
tensor(11400.0674, grad_fn=<NegBackward0>) tensor(11400.0664, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11400.0654296875
tensor(11400.0664, grad_fn=<NegBackward0>) tensor(11400.0654, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11400.0654296875
tensor(11400.0654, grad_fn=<NegBackward0>) tensor(11400.0654, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11400.064453125
tensor(11400.0654, grad_fn=<NegBackward0>) tensor(11400.0645, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11400.064453125
tensor(11400.0645, grad_fn=<NegBackward0>) tensor(11400.0645, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11400.064453125
tensor(11400.0645, grad_fn=<NegBackward0>) tensor(11400.0645, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11400.0634765625
tensor(11400.0645, grad_fn=<NegBackward0>) tensor(11400.0635, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11400.064453125
tensor(11400.0635, grad_fn=<NegBackward0>) tensor(11400.0645, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11400.0625
tensor(11400.0635, grad_fn=<NegBackward0>) tensor(11400.0625, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11400.060546875
tensor(11400.0625, grad_fn=<NegBackward0>) tensor(11400.0605, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11400.07421875
tensor(11400.0605, grad_fn=<NegBackward0>) tensor(11400.0742, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11400.0615234375
tensor(11400.0605, grad_fn=<NegBackward0>) tensor(11400.0615, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11400.060546875
tensor(11400.0605, grad_fn=<NegBackward0>) tensor(11400.0605, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11400.068359375
tensor(11400.0605, grad_fn=<NegBackward0>) tensor(11400.0684, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11400.05859375
tensor(11400.0605, grad_fn=<NegBackward0>) tensor(11400.0586, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11399.4462890625
tensor(11400.0586, grad_fn=<NegBackward0>) tensor(11399.4463, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11399.4482421875
tensor(11399.4463, grad_fn=<NegBackward0>) tensor(11399.4482, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11399.44140625
tensor(11399.4463, grad_fn=<NegBackward0>) tensor(11399.4414, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11399.44140625
tensor(11399.4414, grad_fn=<NegBackward0>) tensor(11399.4414, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11399.4404296875
tensor(11399.4414, grad_fn=<NegBackward0>) tensor(11399.4404, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11399.44140625
tensor(11399.4404, grad_fn=<NegBackward0>) tensor(11399.4414, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11399.4404296875
tensor(11399.4404, grad_fn=<NegBackward0>) tensor(11399.4404, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11399.4404296875
tensor(11399.4404, grad_fn=<NegBackward0>) tensor(11399.4404, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11399.439453125
tensor(11399.4404, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11399.4404296875
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4404, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11399.44140625
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4414, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11399.439453125
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11399.439453125
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11399.4404296875
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4404, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11399.439453125
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11399.4443359375
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4443, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11399.4384765625
tensor(11399.4395, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11399.439453125
tensor(11399.4385, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11399.4384765625
tensor(11399.4385, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11399.439453125
tensor(11399.4385, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11399.46875
tensor(11399.4385, grad_fn=<NegBackward0>) tensor(11399.4688, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11399.4384765625
tensor(11399.4385, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11399.439453125
tensor(11399.4385, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11399.4375
tensor(11399.4385, grad_fn=<NegBackward0>) tensor(11399.4375, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11399.4384765625
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11399.4423828125
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4424, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11399.4375
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4375, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11399.4404296875
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4404, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11399.4423828125
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4424, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11399.439453125
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11399.439453125
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4395, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11399.4375
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4375, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11399.4384765625
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11399.4384765625
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11399.4384765625
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11399.4384765625
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11399.4384765625
tensor(11399.4375, grad_fn=<NegBackward0>) tensor(11399.4385, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7284, 0.2716],
        [0.2428, 0.7572]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5429, 0.4571], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1923, 0.0967],
         [0.6330, 0.3873]],

        [[0.6630, 0.1001],
         [0.5662, 0.5567]],

        [[0.6517, 0.0951],
         [0.6450, 0.7208]],

        [[0.6774, 0.1024],
         [0.5845, 0.6628]],

        [[0.6580, 0.0966],
         [0.5847, 0.7187]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999681285151
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22143.994140625
inf tensor(22143.9941, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12137.2490234375
tensor(22143.9941, grad_fn=<NegBackward0>) tensor(12137.2490, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12005.50390625
tensor(12137.2490, grad_fn=<NegBackward0>) tensor(12005.5039, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11723.626953125
tensor(12005.5039, grad_fn=<NegBackward0>) tensor(11723.6270, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11674.796875
tensor(11723.6270, grad_fn=<NegBackward0>) tensor(11674.7969, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11662.595703125
tensor(11674.7969, grad_fn=<NegBackward0>) tensor(11662.5957, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11662.1015625
tensor(11662.5957, grad_fn=<NegBackward0>) tensor(11662.1016, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11661.8974609375
tensor(11662.1016, grad_fn=<NegBackward0>) tensor(11661.8975, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11661.7724609375
tensor(11661.8975, grad_fn=<NegBackward0>) tensor(11661.7725, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11661.685546875
tensor(11661.7725, grad_fn=<NegBackward0>) tensor(11661.6855, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11656.5654296875
tensor(11661.6855, grad_fn=<NegBackward0>) tensor(11656.5654, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11656.4267578125
tensor(11656.5654, grad_fn=<NegBackward0>) tensor(11656.4268, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11656.3916015625
tensor(11656.4268, grad_fn=<NegBackward0>) tensor(11656.3916, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11651.302734375
tensor(11656.3916, grad_fn=<NegBackward0>) tensor(11651.3027, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11651.2666015625
tensor(11651.3027, grad_fn=<NegBackward0>) tensor(11651.2666, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11651.25
tensor(11651.2666, grad_fn=<NegBackward0>) tensor(11651.2500, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11651.234375
tensor(11651.2500, grad_fn=<NegBackward0>) tensor(11651.2344, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11651.22265625
tensor(11651.2344, grad_fn=<NegBackward0>) tensor(11651.2227, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11651.2138671875
tensor(11651.2227, grad_fn=<NegBackward0>) tensor(11651.2139, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11651.2001953125
tensor(11651.2139, grad_fn=<NegBackward0>) tensor(11651.2002, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11651.1884765625
tensor(11651.2002, grad_fn=<NegBackward0>) tensor(11651.1885, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11651.1806640625
tensor(11651.1885, grad_fn=<NegBackward0>) tensor(11651.1807, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11651.1748046875
tensor(11651.1807, grad_fn=<NegBackward0>) tensor(11651.1748, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11651.169921875
tensor(11651.1748, grad_fn=<NegBackward0>) tensor(11651.1699, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11651.1669921875
tensor(11651.1699, grad_fn=<NegBackward0>) tensor(11651.1670, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11651.162109375
tensor(11651.1670, grad_fn=<NegBackward0>) tensor(11651.1621, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11651.16015625
tensor(11651.1621, grad_fn=<NegBackward0>) tensor(11651.1602, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11651.1533203125
tensor(11651.1602, grad_fn=<NegBackward0>) tensor(11651.1533, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11651.1298828125
tensor(11651.1533, grad_fn=<NegBackward0>) tensor(11651.1299, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11651.1240234375
tensor(11651.1299, grad_fn=<NegBackward0>) tensor(11651.1240, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11651.125
tensor(11651.1240, grad_fn=<NegBackward0>) tensor(11651.1250, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11651.119140625
tensor(11651.1240, grad_fn=<NegBackward0>) tensor(11651.1191, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11651.1162109375
tensor(11651.1191, grad_fn=<NegBackward0>) tensor(11651.1162, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11651.1123046875
tensor(11651.1162, grad_fn=<NegBackward0>) tensor(11651.1123, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11651.109375
tensor(11651.1123, grad_fn=<NegBackward0>) tensor(11651.1094, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11651.1357421875
tensor(11651.1094, grad_fn=<NegBackward0>) tensor(11651.1357, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11650.4296875
tensor(11651.1094, grad_fn=<NegBackward0>) tensor(11650.4297, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11650.4228515625
tensor(11650.4297, grad_fn=<NegBackward0>) tensor(11650.4229, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11650.4208984375
tensor(11650.4229, grad_fn=<NegBackward0>) tensor(11650.4209, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11650.419921875
tensor(11650.4209, grad_fn=<NegBackward0>) tensor(11650.4199, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11650.4169921875
tensor(11650.4199, grad_fn=<NegBackward0>) tensor(11650.4170, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11648.79296875
tensor(11650.4170, grad_fn=<NegBackward0>) tensor(11648.7930, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11648.8212890625
tensor(11648.7930, grad_fn=<NegBackward0>) tensor(11648.8213, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11648.8212890625
tensor(11648.7930, grad_fn=<NegBackward0>) tensor(11648.8213, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11648.814453125
tensor(11648.7930, grad_fn=<NegBackward0>) tensor(11648.8145, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11648.6513671875
tensor(11648.7930, grad_fn=<NegBackward0>) tensor(11648.6514, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11648.6494140625
tensor(11648.6514, grad_fn=<NegBackward0>) tensor(11648.6494, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11648.6494140625
tensor(11648.6494, grad_fn=<NegBackward0>) tensor(11648.6494, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11648.6455078125
tensor(11648.6494, grad_fn=<NegBackward0>) tensor(11648.6455, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11648.638671875
tensor(11648.6455, grad_fn=<NegBackward0>) tensor(11648.6387, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11648.6396484375
tensor(11648.6387, grad_fn=<NegBackward0>) tensor(11648.6396, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11648.638671875
tensor(11648.6387, grad_fn=<NegBackward0>) tensor(11648.6387, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11648.638671875
tensor(11648.6387, grad_fn=<NegBackward0>) tensor(11648.6387, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11648.63671875
tensor(11648.6387, grad_fn=<NegBackward0>) tensor(11648.6367, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11648.6376953125
tensor(11648.6367, grad_fn=<NegBackward0>) tensor(11648.6377, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11648.6357421875
tensor(11648.6367, grad_fn=<NegBackward0>) tensor(11648.6357, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11648.6357421875
tensor(11648.6357, grad_fn=<NegBackward0>) tensor(11648.6357, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11648.6552734375
tensor(11648.6357, grad_fn=<NegBackward0>) tensor(11648.6553, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11648.6357421875
tensor(11648.6357, grad_fn=<NegBackward0>) tensor(11648.6357, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11648.642578125
tensor(11648.6357, grad_fn=<NegBackward0>) tensor(11648.6426, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11648.6357421875
tensor(11648.6357, grad_fn=<NegBackward0>) tensor(11648.6357, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11648.6357421875
tensor(11648.6357, grad_fn=<NegBackward0>) tensor(11648.6357, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11648.634765625
tensor(11648.6357, grad_fn=<NegBackward0>) tensor(11648.6348, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11648.6337890625
tensor(11648.6348, grad_fn=<NegBackward0>) tensor(11648.6338, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11648.6337890625
tensor(11648.6338, grad_fn=<NegBackward0>) tensor(11648.6338, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11648.6328125
tensor(11648.6338, grad_fn=<NegBackward0>) tensor(11648.6328, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11648.6328125
tensor(11648.6328, grad_fn=<NegBackward0>) tensor(11648.6328, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11648.6318359375
tensor(11648.6328, grad_fn=<NegBackward0>) tensor(11648.6318, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11648.630859375
tensor(11648.6318, grad_fn=<NegBackward0>) tensor(11648.6309, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11648.630859375
tensor(11648.6309, grad_fn=<NegBackward0>) tensor(11648.6309, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11648.630859375
tensor(11648.6309, grad_fn=<NegBackward0>) tensor(11648.6309, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11648.6318359375
tensor(11648.6309, grad_fn=<NegBackward0>) tensor(11648.6318, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11648.6298828125
tensor(11648.6309, grad_fn=<NegBackward0>) tensor(11648.6299, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11648.630859375
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6309, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11648.630859375
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6309, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11648.6318359375
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6318, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11648.6298828125
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6299, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11648.640625
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6406, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11648.6298828125
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6299, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11648.6513671875
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6514, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11648.62890625
tensor(11648.6299, grad_fn=<NegBackward0>) tensor(11648.6289, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11648.6318359375
tensor(11648.6289, grad_fn=<NegBackward0>) tensor(11648.6318, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11648.6328125
tensor(11648.6289, grad_fn=<NegBackward0>) tensor(11648.6328, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11648.671875
tensor(11648.6289, grad_fn=<NegBackward0>) tensor(11648.6719, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11648.69140625
tensor(11648.6289, grad_fn=<NegBackward0>) tensor(11648.6914, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11648.6298828125
tensor(11648.6289, grad_fn=<NegBackward0>) tensor(11648.6299, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.6782, 0.3218],
        [0.3062, 0.6938]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9706, 0.0294], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1922, 0.3008],
         [0.7162, 0.3906]],

        [[0.5224, 0.1020],
         [0.6384, 0.6050]],

        [[0.7219, 0.0951],
         [0.5542, 0.5342]],

        [[0.5544, 0.1025],
         [0.6704, 0.5281]],

        [[0.6377, 0.0965],
         [0.6936, 0.5725]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.07167087033623441
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.31233850343427383
Average Adjusted Rand Index: 0.606973445603696
[0.9919999681285151, 0.31233850343427383] [0.9919995611635631, 0.606973445603696] [11399.4384765625, 11648.6298828125]
-------------------------------------
This iteration is 28
True Objective function: Loss = -11541.031896933191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21980.251953125
inf tensor(21980.2520, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12199.943359375
tensor(21980.2520, grad_fn=<NegBackward0>) tensor(12199.9434, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12171.71484375
tensor(12199.9434, grad_fn=<NegBackward0>) tensor(12171.7148, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11964.736328125
tensor(12171.7148, grad_fn=<NegBackward0>) tensor(11964.7363, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11823.599609375
tensor(11964.7363, grad_fn=<NegBackward0>) tensor(11823.5996, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11786.4501953125
tensor(11823.5996, grad_fn=<NegBackward0>) tensor(11786.4502, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11761.736328125
tensor(11786.4502, grad_fn=<NegBackward0>) tensor(11761.7363, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11744.1318359375
tensor(11761.7363, grad_fn=<NegBackward0>) tensor(11744.1318, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11717.5166015625
tensor(11744.1318, grad_fn=<NegBackward0>) tensor(11717.5166, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11706.84375
tensor(11717.5166, grad_fn=<NegBackward0>) tensor(11706.8438, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11706.685546875
tensor(11706.8438, grad_fn=<NegBackward0>) tensor(11706.6855, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11706.5517578125
tensor(11706.6855, grad_fn=<NegBackward0>) tensor(11706.5518, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11706.2890625
tensor(11706.5518, grad_fn=<NegBackward0>) tensor(11706.2891, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11706.2021484375
tensor(11706.2891, grad_fn=<NegBackward0>) tensor(11706.2021, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11706.0986328125
tensor(11706.2021, grad_fn=<NegBackward0>) tensor(11706.0986, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11705.919921875
tensor(11706.0986, grad_fn=<NegBackward0>) tensor(11705.9199, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11705.6650390625
tensor(11705.9199, grad_fn=<NegBackward0>) tensor(11705.6650, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11705.4970703125
tensor(11705.6650, grad_fn=<NegBackward0>) tensor(11705.4971, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11702.6337890625
tensor(11705.4971, grad_fn=<NegBackward0>) tensor(11702.6338, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11699.65234375
tensor(11702.6338, grad_fn=<NegBackward0>) tensor(11699.6523, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11699.435546875
tensor(11699.6523, grad_fn=<NegBackward0>) tensor(11699.4355, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11699.283203125
tensor(11699.4355, grad_fn=<NegBackward0>) tensor(11699.2832, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11699.15234375
tensor(11699.2832, grad_fn=<NegBackward0>) tensor(11699.1523, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11699.134765625
tensor(11699.1523, grad_fn=<NegBackward0>) tensor(11699.1348, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11699.1220703125
tensor(11699.1348, grad_fn=<NegBackward0>) tensor(11699.1221, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11699.1162109375
tensor(11699.1221, grad_fn=<NegBackward0>) tensor(11699.1162, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11699.107421875
tensor(11699.1162, grad_fn=<NegBackward0>) tensor(11699.1074, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11699.1025390625
tensor(11699.1074, grad_fn=<NegBackward0>) tensor(11699.1025, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11699.0986328125
tensor(11699.1025, grad_fn=<NegBackward0>) tensor(11699.0986, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11699.09375
tensor(11699.0986, grad_fn=<NegBackward0>) tensor(11699.0938, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11699.087890625
tensor(11699.0938, grad_fn=<NegBackward0>) tensor(11699.0879, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11699.0859375
tensor(11699.0879, grad_fn=<NegBackward0>) tensor(11699.0859, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11699.08203125
tensor(11699.0859, grad_fn=<NegBackward0>) tensor(11699.0820, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11699.0791015625
tensor(11699.0820, grad_fn=<NegBackward0>) tensor(11699.0791, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11699.0771484375
tensor(11699.0791, grad_fn=<NegBackward0>) tensor(11699.0771, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11699.0732421875
tensor(11699.0771, grad_fn=<NegBackward0>) tensor(11699.0732, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11699.05859375
tensor(11699.0732, grad_fn=<NegBackward0>) tensor(11699.0586, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11693.5498046875
tensor(11699.0586, grad_fn=<NegBackward0>) tensor(11693.5498, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11693.5302734375
tensor(11693.5498, grad_fn=<NegBackward0>) tensor(11693.5303, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11693.525390625
tensor(11693.5303, grad_fn=<NegBackward0>) tensor(11693.5254, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11693.5244140625
tensor(11693.5254, grad_fn=<NegBackward0>) tensor(11693.5244, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11693.521484375
tensor(11693.5244, grad_fn=<NegBackward0>) tensor(11693.5215, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11693.5205078125
tensor(11693.5215, grad_fn=<NegBackward0>) tensor(11693.5205, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11693.517578125
tensor(11693.5205, grad_fn=<NegBackward0>) tensor(11693.5176, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11693.5146484375
tensor(11693.5176, grad_fn=<NegBackward0>) tensor(11693.5146, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11684.8837890625
tensor(11693.5146, grad_fn=<NegBackward0>) tensor(11684.8838, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11676.4462890625
tensor(11684.8838, grad_fn=<NegBackward0>) tensor(11676.4463, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11676.3525390625
tensor(11676.4463, grad_fn=<NegBackward0>) tensor(11676.3525, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11676.3486328125
tensor(11676.3525, grad_fn=<NegBackward0>) tensor(11676.3486, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11676.349609375
tensor(11676.3486, grad_fn=<NegBackward0>) tensor(11676.3496, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11676.34765625
tensor(11676.3486, grad_fn=<NegBackward0>) tensor(11676.3477, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11676.3447265625
tensor(11676.3477, grad_fn=<NegBackward0>) tensor(11676.3447, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11676.3408203125
tensor(11676.3447, grad_fn=<NegBackward0>) tensor(11676.3408, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11676.2978515625
tensor(11676.3408, grad_fn=<NegBackward0>) tensor(11676.2979, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11676.2958984375
tensor(11676.2979, grad_fn=<NegBackward0>) tensor(11676.2959, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11676.2978515625
tensor(11676.2959, grad_fn=<NegBackward0>) tensor(11676.2979, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11676.2978515625
tensor(11676.2959, grad_fn=<NegBackward0>) tensor(11676.2979, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11676.2939453125
tensor(11676.2959, grad_fn=<NegBackward0>) tensor(11676.2939, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11676.2919921875
tensor(11676.2939, grad_fn=<NegBackward0>) tensor(11676.2920, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11676.29296875
tensor(11676.2920, grad_fn=<NegBackward0>) tensor(11676.2930, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11676.2939453125
tensor(11676.2920, grad_fn=<NegBackward0>) tensor(11676.2939, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11676.29296875
tensor(11676.2920, grad_fn=<NegBackward0>) tensor(11676.2930, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11676.29296875
tensor(11676.2920, grad_fn=<NegBackward0>) tensor(11676.2930, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -11676.2919921875
tensor(11676.2920, grad_fn=<NegBackward0>) tensor(11676.2920, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11676.291015625
tensor(11676.2920, grad_fn=<NegBackward0>) tensor(11676.2910, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11676.291015625
tensor(11676.2910, grad_fn=<NegBackward0>) tensor(11676.2910, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11676.2919921875
tensor(11676.2910, grad_fn=<NegBackward0>) tensor(11676.2920, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11676.2900390625
tensor(11676.2910, grad_fn=<NegBackward0>) tensor(11676.2900, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11676.2900390625
tensor(11676.2900, grad_fn=<NegBackward0>) tensor(11676.2900, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11676.2900390625
tensor(11676.2900, grad_fn=<NegBackward0>) tensor(11676.2900, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11676.2900390625
tensor(11676.2900, grad_fn=<NegBackward0>) tensor(11676.2900, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11676.2880859375
tensor(11676.2900, grad_fn=<NegBackward0>) tensor(11676.2881, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11676.23828125
tensor(11676.2881, grad_fn=<NegBackward0>) tensor(11676.2383, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11675.3740234375
tensor(11676.2383, grad_fn=<NegBackward0>) tensor(11675.3740, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11675.3740234375
tensor(11675.3740, grad_fn=<NegBackward0>) tensor(11675.3740, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11675.37109375
tensor(11675.3740, grad_fn=<NegBackward0>) tensor(11675.3711, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11675.3720703125
tensor(11675.3711, grad_fn=<NegBackward0>) tensor(11675.3721, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11675.37109375
tensor(11675.3711, grad_fn=<NegBackward0>) tensor(11675.3711, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11675.3701171875
tensor(11675.3711, grad_fn=<NegBackward0>) tensor(11675.3701, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11675.380859375
tensor(11675.3701, grad_fn=<NegBackward0>) tensor(11675.3809, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11675.3798828125
tensor(11675.3701, grad_fn=<NegBackward0>) tensor(11675.3799, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11675.373046875
tensor(11675.3701, grad_fn=<NegBackward0>) tensor(11675.3730, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11675.3701171875
tensor(11675.3701, grad_fn=<NegBackward0>) tensor(11675.3701, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11675.447265625
tensor(11675.3701, grad_fn=<NegBackward0>) tensor(11675.4473, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11675.3681640625
tensor(11675.3701, grad_fn=<NegBackward0>) tensor(11675.3682, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11674.8896484375
tensor(11675.3682, grad_fn=<NegBackward0>) tensor(11674.8896, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11674.935546875
tensor(11674.8896, grad_fn=<NegBackward0>) tensor(11674.9355, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11674.884765625
tensor(11674.8896, grad_fn=<NegBackward0>) tensor(11674.8848, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11674.8896484375
tensor(11674.8848, grad_fn=<NegBackward0>) tensor(11674.8896, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11674.880859375
tensor(11674.8848, grad_fn=<NegBackward0>) tensor(11674.8809, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11674.8310546875
tensor(11674.8809, grad_fn=<NegBackward0>) tensor(11674.8311, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11674.8330078125
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8330, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11674.8330078125
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8330, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11674.8447265625
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8447, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11674.8564453125
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8564, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11674.8310546875
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8311, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11674.8359375
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8359, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11674.845703125
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8457, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11674.8330078125
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8330, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11674.8330078125
tensor(11674.8311, grad_fn=<NegBackward0>) tensor(11674.8330, grad_fn=<NegBackward0>)
4
pi: tensor([[0.6369, 0.3631],
        [0.3501, 0.6499]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7725, 0.2275], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2159, 0.0894],
         [0.7298, 0.3826]],

        [[0.5361, 0.0971],
         [0.6681, 0.7123]],

        [[0.6675, 0.1073],
         [0.5513, 0.6072]],

        [[0.6291, 0.1068],
         [0.6101, 0.6727]],

        [[0.5609, 0.1066],
         [0.5384, 0.6792]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 34
Adjusted Rand Index: 0.0920958936624733
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5232168857052437
Average Adjusted Rand Index: 0.7945793054514809
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20095.939453125
inf tensor(20095.9395, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12196.3115234375
tensor(20095.9395, grad_fn=<NegBackward0>) tensor(12196.3115, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12018.75390625
tensor(12196.3115, grad_fn=<NegBackward0>) tensor(12018.7539, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11770.255859375
tensor(12018.7539, grad_fn=<NegBackward0>) tensor(11770.2559, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11715.23046875
tensor(11770.2559, grad_fn=<NegBackward0>) tensor(11715.2305, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11709.13671875
tensor(11715.2305, grad_fn=<NegBackward0>) tensor(11709.1367, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11708.6416015625
tensor(11709.1367, grad_fn=<NegBackward0>) tensor(11708.6416, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11708.1943359375
tensor(11708.6416, grad_fn=<NegBackward0>) tensor(11708.1943, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11705.3330078125
tensor(11708.1943, grad_fn=<NegBackward0>) tensor(11705.3330, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11694.53125
tensor(11705.3330, grad_fn=<NegBackward0>) tensor(11694.5312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11691.552734375
tensor(11694.5312, grad_fn=<NegBackward0>) tensor(11691.5527, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11691.4208984375
tensor(11691.5527, grad_fn=<NegBackward0>) tensor(11691.4209, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11691.3583984375
tensor(11691.4209, grad_fn=<NegBackward0>) tensor(11691.3584, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11690.44921875
tensor(11691.3584, grad_fn=<NegBackward0>) tensor(11690.4492, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11690.3369140625
tensor(11690.4492, grad_fn=<NegBackward0>) tensor(11690.3369, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11690.2939453125
tensor(11690.3369, grad_fn=<NegBackward0>) tensor(11690.2939, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11684.796875
tensor(11690.2939, grad_fn=<NegBackward0>) tensor(11684.7969, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11684.2880859375
tensor(11684.7969, grad_fn=<NegBackward0>) tensor(11684.2881, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11684.2607421875
tensor(11684.2881, grad_fn=<NegBackward0>) tensor(11684.2607, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11684.2373046875
tensor(11684.2607, grad_fn=<NegBackward0>) tensor(11684.2373, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11684.2255859375
tensor(11684.2373, grad_fn=<NegBackward0>) tensor(11684.2256, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11684.2138671875
tensor(11684.2256, grad_fn=<NegBackward0>) tensor(11684.2139, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11684.203125
tensor(11684.2139, grad_fn=<NegBackward0>) tensor(11684.2031, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11684.1923828125
tensor(11684.2031, grad_fn=<NegBackward0>) tensor(11684.1924, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11683.984375
tensor(11684.1924, grad_fn=<NegBackward0>) tensor(11683.9844, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11680.6318359375
tensor(11683.9844, grad_fn=<NegBackward0>) tensor(11680.6318, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11680.470703125
tensor(11680.6318, grad_fn=<NegBackward0>) tensor(11680.4707, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11680.44921875
tensor(11680.4707, grad_fn=<NegBackward0>) tensor(11680.4492, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11677.728515625
tensor(11680.4492, grad_fn=<NegBackward0>) tensor(11677.7285, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11676.6328125
tensor(11677.7285, grad_fn=<NegBackward0>) tensor(11676.6328, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11674.921875
tensor(11676.6328, grad_fn=<NegBackward0>) tensor(11674.9219, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11674.89453125
tensor(11674.9219, grad_fn=<NegBackward0>) tensor(11674.8945, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11674.8671875
tensor(11674.8945, grad_fn=<NegBackward0>) tensor(11674.8672, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11674.8486328125
tensor(11674.8672, grad_fn=<NegBackward0>) tensor(11674.8486, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11674.8466796875
tensor(11674.8486, grad_fn=<NegBackward0>) tensor(11674.8467, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11674.84375
tensor(11674.8467, grad_fn=<NegBackward0>) tensor(11674.8438, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11674.84375
tensor(11674.8438, grad_fn=<NegBackward0>) tensor(11674.8438, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11674.84375
tensor(11674.8438, grad_fn=<NegBackward0>) tensor(11674.8438, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11674.841796875
tensor(11674.8438, grad_fn=<NegBackward0>) tensor(11674.8418, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11674.8388671875
tensor(11674.8418, grad_fn=<NegBackward0>) tensor(11674.8389, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11674.8369140625
tensor(11674.8389, grad_fn=<NegBackward0>) tensor(11674.8369, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11674.837890625
tensor(11674.8369, grad_fn=<NegBackward0>) tensor(11674.8379, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11674.8369140625
tensor(11674.8369, grad_fn=<NegBackward0>) tensor(11674.8369, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11674.8369140625
tensor(11674.8369, grad_fn=<NegBackward0>) tensor(11674.8369, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11674.8359375
tensor(11674.8369, grad_fn=<NegBackward0>) tensor(11674.8359, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11674.833984375
tensor(11674.8359, grad_fn=<NegBackward0>) tensor(11674.8340, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11674.833984375
tensor(11674.8340, grad_fn=<NegBackward0>) tensor(11674.8340, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11674.833984375
tensor(11674.8340, grad_fn=<NegBackward0>) tensor(11674.8340, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11674.833984375
tensor(11674.8340, grad_fn=<NegBackward0>) tensor(11674.8340, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11674.83203125
tensor(11674.8340, grad_fn=<NegBackward0>) tensor(11674.8320, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11674.83203125
tensor(11674.8320, grad_fn=<NegBackward0>) tensor(11674.8320, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11674.833984375
tensor(11674.8320, grad_fn=<NegBackward0>) tensor(11674.8340, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11674.830078125
tensor(11674.8320, grad_fn=<NegBackward0>) tensor(11674.8301, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11674.83203125
tensor(11674.8301, grad_fn=<NegBackward0>) tensor(11674.8320, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11674.8310546875
tensor(11674.8301, grad_fn=<NegBackward0>) tensor(11674.8311, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11674.83203125
tensor(11674.8301, grad_fn=<NegBackward0>) tensor(11674.8320, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11674.830078125
tensor(11674.8301, grad_fn=<NegBackward0>) tensor(11674.8301, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11674.828125
tensor(11674.8301, grad_fn=<NegBackward0>) tensor(11674.8281, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11674.830078125
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11674.8301, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11674.8291015625
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11674.8291, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11674.828125
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11674.8281, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11674.828125
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11674.8281, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11674.8291015625
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11674.8291, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11674.8291015625
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11674.8291, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11674.8271484375
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11674.8271, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11674.828125
tensor(11674.8271, grad_fn=<NegBackward0>) tensor(11674.8281, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11674.8291015625
tensor(11674.8271, grad_fn=<NegBackward0>) tensor(11674.8291, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11674.830078125
tensor(11674.8271, grad_fn=<NegBackward0>) tensor(11674.8301, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11674.8271484375
tensor(11674.8271, grad_fn=<NegBackward0>) tensor(11674.8271, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11674.8330078125
tensor(11674.8271, grad_fn=<NegBackward0>) tensor(11674.8330, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11674.8251953125
tensor(11674.8271, grad_fn=<NegBackward0>) tensor(11674.8252, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11674.833984375
tensor(11674.8252, grad_fn=<NegBackward0>) tensor(11674.8340, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11674.8251953125
tensor(11674.8252, grad_fn=<NegBackward0>) tensor(11674.8252, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11674.82421875
tensor(11674.8252, grad_fn=<NegBackward0>) tensor(11674.8242, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11674.8291015625
tensor(11674.8242, grad_fn=<NegBackward0>) tensor(11674.8291, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11674.8271484375
tensor(11674.8242, grad_fn=<NegBackward0>) tensor(11674.8271, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11674.83203125
tensor(11674.8242, grad_fn=<NegBackward0>) tensor(11674.8320, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11674.826171875
tensor(11674.8242, grad_fn=<NegBackward0>) tensor(11674.8262, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11674.828125
tensor(11674.8242, grad_fn=<NegBackward0>) tensor(11674.8281, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.6458, 0.3542],
        [0.3641, 0.6359]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2294, 0.7706], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3813, 0.0895],
         [0.5150, 0.2165]],

        [[0.5653, 0.0971],
         [0.5382, 0.6276]],

        [[0.6881, 0.1079],
         [0.6531, 0.5809]],

        [[0.6015, 0.1066],
         [0.5374, 0.6251]],

        [[0.6075, 0.1061],
         [0.5350, 0.6029]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 66
Adjusted Rand Index: 0.0920958936624733
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5232168857052437
Average Adjusted Rand Index: 0.7945793054514809
[0.5232168857052437, 0.5232168857052437] [0.7945793054514809, 0.7945793054514809] [11674.857421875, 11674.828125]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11749.670486712863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23154.376953125
inf tensor(23154.3770, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12573.556640625
tensor(23154.3770, grad_fn=<NegBackward0>) tensor(12573.5566, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12404.3974609375
tensor(12573.5566, grad_fn=<NegBackward0>) tensor(12404.3975, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12132.4365234375
tensor(12404.3975, grad_fn=<NegBackward0>) tensor(12132.4365, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12115.708984375
tensor(12132.4365, grad_fn=<NegBackward0>) tensor(12115.7090, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12105.521484375
tensor(12115.7090, grad_fn=<NegBackward0>) tensor(12105.5215, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12094.4765625
tensor(12105.5215, grad_fn=<NegBackward0>) tensor(12094.4766, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12094.259765625
tensor(12094.4766, grad_fn=<NegBackward0>) tensor(12094.2598, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12080.2197265625
tensor(12094.2598, grad_fn=<NegBackward0>) tensor(12080.2197, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12080.1240234375
tensor(12080.2197, grad_fn=<NegBackward0>) tensor(12080.1240, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12080.0625
tensor(12080.1240, grad_fn=<NegBackward0>) tensor(12080.0625, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12080.0166015625
tensor(12080.0625, grad_fn=<NegBackward0>) tensor(12080.0166, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12079.9765625
tensor(12080.0166, grad_fn=<NegBackward0>) tensor(12079.9766, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12070.0390625
tensor(12079.9766, grad_fn=<NegBackward0>) tensor(12070.0391, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12069.9873046875
tensor(12070.0391, grad_fn=<NegBackward0>) tensor(12069.9873, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12069.9560546875
tensor(12069.9873, grad_fn=<NegBackward0>) tensor(12069.9561, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12069.896484375
tensor(12069.9561, grad_fn=<NegBackward0>) tensor(12069.8965, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12061.03515625
tensor(12069.8965, grad_fn=<NegBackward0>) tensor(12061.0352, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12051.1865234375
tensor(12061.0352, grad_fn=<NegBackward0>) tensor(12051.1865, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12050.845703125
tensor(12051.1865, grad_fn=<NegBackward0>) tensor(12050.8457, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12050.671875
tensor(12050.8457, grad_fn=<NegBackward0>) tensor(12050.6719, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12050.64453125
tensor(12050.6719, grad_fn=<NegBackward0>) tensor(12050.6445, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12050.634765625
tensor(12050.6445, grad_fn=<NegBackward0>) tensor(12050.6348, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12050.62890625
tensor(12050.6348, grad_fn=<NegBackward0>) tensor(12050.6289, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12050.623046875
tensor(12050.6289, grad_fn=<NegBackward0>) tensor(12050.6230, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12050.6171875
tensor(12050.6230, grad_fn=<NegBackward0>) tensor(12050.6172, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12050.6123046875
tensor(12050.6172, grad_fn=<NegBackward0>) tensor(12050.6123, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12050.609375
tensor(12050.6123, grad_fn=<NegBackward0>) tensor(12050.6094, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12050.6064453125
tensor(12050.6094, grad_fn=<NegBackward0>) tensor(12050.6064, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12050.6044921875
tensor(12050.6064, grad_fn=<NegBackward0>) tensor(12050.6045, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12050.599609375
tensor(12050.6045, grad_fn=<NegBackward0>) tensor(12050.5996, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12050.5986328125
tensor(12050.5996, grad_fn=<NegBackward0>) tensor(12050.5986, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12050.5947265625
tensor(12050.5986, grad_fn=<NegBackward0>) tensor(12050.5947, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12050.59375
tensor(12050.5947, grad_fn=<NegBackward0>) tensor(12050.5938, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12050.5927734375
tensor(12050.5938, grad_fn=<NegBackward0>) tensor(12050.5928, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12050.5927734375
tensor(12050.5928, grad_fn=<NegBackward0>) tensor(12050.5928, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12050.5888671875
tensor(12050.5928, grad_fn=<NegBackward0>) tensor(12050.5889, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12050.5859375
tensor(12050.5889, grad_fn=<NegBackward0>) tensor(12050.5859, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12050.578125
tensor(12050.5859, grad_fn=<NegBackward0>) tensor(12050.5781, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12050.5615234375
tensor(12050.5781, grad_fn=<NegBackward0>) tensor(12050.5615, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12050.5556640625
tensor(12050.5615, grad_fn=<NegBackward0>) tensor(12050.5557, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12050.5185546875
tensor(12050.5557, grad_fn=<NegBackward0>) tensor(12050.5186, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12050.517578125
tensor(12050.5186, grad_fn=<NegBackward0>) tensor(12050.5176, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12050.498046875
tensor(12050.5176, grad_fn=<NegBackward0>) tensor(12050.4980, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12050.486328125
tensor(12050.4980, grad_fn=<NegBackward0>) tensor(12050.4863, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12050.4755859375
tensor(12050.4863, grad_fn=<NegBackward0>) tensor(12050.4756, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12050.4716796875
tensor(12050.4756, grad_fn=<NegBackward0>) tensor(12050.4717, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12050.166015625
tensor(12050.4717, grad_fn=<NegBackward0>) tensor(12050.1660, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12050.1611328125
tensor(12050.1660, grad_fn=<NegBackward0>) tensor(12050.1611, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12050.1005859375
tensor(12050.1611, grad_fn=<NegBackward0>) tensor(12050.1006, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12050.09765625
tensor(12050.1006, grad_fn=<NegBackward0>) tensor(12050.0977, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12050.099609375
tensor(12050.0977, grad_fn=<NegBackward0>) tensor(12050.0996, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12050.095703125
tensor(12050.0977, grad_fn=<NegBackward0>) tensor(12050.0957, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12050.091796875
tensor(12050.0957, grad_fn=<NegBackward0>) tensor(12050.0918, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12050.091796875
tensor(12050.0918, grad_fn=<NegBackward0>) tensor(12050.0918, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12050.091796875
tensor(12050.0918, grad_fn=<NegBackward0>) tensor(12050.0918, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12050.0908203125
tensor(12050.0918, grad_fn=<NegBackward0>) tensor(12050.0908, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12050.08984375
tensor(12050.0908, grad_fn=<NegBackward0>) tensor(12050.0898, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12050.091796875
tensor(12050.0898, grad_fn=<NegBackward0>) tensor(12050.0918, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12050.0966796875
tensor(12050.0898, grad_fn=<NegBackward0>) tensor(12050.0967, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12050.087890625
tensor(12050.0898, grad_fn=<NegBackward0>) tensor(12050.0879, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12050.080078125
tensor(12050.0879, grad_fn=<NegBackward0>) tensor(12050.0801, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12050.080078125
tensor(12050.0801, grad_fn=<NegBackward0>) tensor(12050.0801, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12050.080078125
tensor(12050.0801, grad_fn=<NegBackward0>) tensor(12050.0801, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12050.0791015625
tensor(12050.0801, grad_fn=<NegBackward0>) tensor(12050.0791, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12050.078125
tensor(12050.0791, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12050.078125
tensor(12050.0781, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12050.078125
tensor(12050.0781, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12050.078125
tensor(12050.0781, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12050.078125
tensor(12050.0781, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12050.078125
tensor(12050.0781, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12050.076171875
tensor(12050.0781, grad_fn=<NegBackward0>) tensor(12050.0762, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12050.078125
tensor(12050.0762, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12050.0771484375
tensor(12050.0762, grad_fn=<NegBackward0>) tensor(12050.0771, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12050.0771484375
tensor(12050.0762, grad_fn=<NegBackward0>) tensor(12050.0771, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12050.0771484375
tensor(12050.0762, grad_fn=<NegBackward0>) tensor(12050.0771, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -12050.078125
tensor(12050.0762, grad_fn=<NegBackward0>) tensor(12050.0781, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.5614, 0.4386],
        [0.5009, 0.4991]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8885, 0.1115], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2249, 0.0950],
         [0.5035, 0.3966]],

        [[0.5141, 0.1004],
         [0.5172, 0.6996]],

        [[0.6121, 0.1139],
         [0.6045, 0.5942]],

        [[0.5071, 0.0992],
         [0.6967, 0.6206]],

        [[0.7301, 0.0906],
         [0.5186, 0.5187]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.027613264228709862
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 27
Adjusted Rand Index: 0.20591790142799588
Global Adjusted Rand Index: 0.2174724137110556
Average Adjusted Rand Index: 0.638705574858631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22979.275390625
inf tensor(22979.2754, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12512.373046875
tensor(22979.2754, grad_fn=<NegBackward0>) tensor(12512.3730, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12046.5771484375
tensor(12512.3730, grad_fn=<NegBackward0>) tensor(12046.5771, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12045.029296875
tensor(12046.5771, grad_fn=<NegBackward0>) tensor(12045.0293, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12044.6806640625
tensor(12045.0293, grad_fn=<NegBackward0>) tensor(12044.6807, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12044.318359375
tensor(12044.6807, grad_fn=<NegBackward0>) tensor(12044.3184, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12044.2021484375
tensor(12044.3184, grad_fn=<NegBackward0>) tensor(12044.2021, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12044.134765625
tensor(12044.2021, grad_fn=<NegBackward0>) tensor(12044.1348, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12044.060546875
tensor(12044.1348, grad_fn=<NegBackward0>) tensor(12044.0605, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12043.2646484375
tensor(12044.0605, grad_fn=<NegBackward0>) tensor(12043.2646, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12043.107421875
tensor(12043.2646, grad_fn=<NegBackward0>) tensor(12043.1074, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12043.0908203125
tensor(12043.1074, grad_fn=<NegBackward0>) tensor(12043.0908, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12043.078125
tensor(12043.0908, grad_fn=<NegBackward0>) tensor(12043.0781, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12043.0673828125
tensor(12043.0781, grad_fn=<NegBackward0>) tensor(12043.0674, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12043.0576171875
tensor(12043.0674, grad_fn=<NegBackward0>) tensor(12043.0576, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12043.0498046875
tensor(12043.0576, grad_fn=<NegBackward0>) tensor(12043.0498, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12043.04296875
tensor(12043.0498, grad_fn=<NegBackward0>) tensor(12043.0430, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12043.0380859375
tensor(12043.0430, grad_fn=<NegBackward0>) tensor(12043.0381, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12043.03515625
tensor(12043.0381, grad_fn=<NegBackward0>) tensor(12043.0352, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12043.0302734375
tensor(12043.0352, grad_fn=<NegBackward0>) tensor(12043.0303, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12043.02734375
tensor(12043.0303, grad_fn=<NegBackward0>) tensor(12043.0273, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12043.0234375
tensor(12043.0273, grad_fn=<NegBackward0>) tensor(12043.0234, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12043.0224609375
tensor(12043.0234, grad_fn=<NegBackward0>) tensor(12043.0225, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12043.0185546875
tensor(12043.0225, grad_fn=<NegBackward0>) tensor(12043.0186, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12043.0185546875
tensor(12043.0186, grad_fn=<NegBackward0>) tensor(12043.0186, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12043.015625
tensor(12043.0186, grad_fn=<NegBackward0>) tensor(12043.0156, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12043.015625
tensor(12043.0156, grad_fn=<NegBackward0>) tensor(12043.0156, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12043.013671875
tensor(12043.0156, grad_fn=<NegBackward0>) tensor(12043.0137, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12043.0126953125
tensor(12043.0137, grad_fn=<NegBackward0>) tensor(12043.0127, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12043.0107421875
tensor(12043.0127, grad_fn=<NegBackward0>) tensor(12043.0107, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12043.009765625
tensor(12043.0107, grad_fn=<NegBackward0>) tensor(12043.0098, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12043.0087890625
tensor(12043.0098, grad_fn=<NegBackward0>) tensor(12043.0088, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12043.0087890625
tensor(12043.0088, grad_fn=<NegBackward0>) tensor(12043.0088, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12043.0078125
tensor(12043.0088, grad_fn=<NegBackward0>) tensor(12043.0078, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12043.005859375
tensor(12043.0078, grad_fn=<NegBackward0>) tensor(12043.0059, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12043.005859375
tensor(12043.0059, grad_fn=<NegBackward0>) tensor(12043.0059, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12042.998046875
tensor(12043.0059, grad_fn=<NegBackward0>) tensor(12042.9980, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12039.501953125
tensor(12042.9980, grad_fn=<NegBackward0>) tensor(12039.5020, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12036.4384765625
tensor(12039.5020, grad_fn=<NegBackward0>) tensor(12036.4385, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12036.4248046875
tensor(12036.4385, grad_fn=<NegBackward0>) tensor(12036.4248, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12036.4189453125
tensor(12036.4248, grad_fn=<NegBackward0>) tensor(12036.4189, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12036.412109375
tensor(12036.4189, grad_fn=<NegBackward0>) tensor(12036.4121, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12036.412109375
tensor(12036.4121, grad_fn=<NegBackward0>) tensor(12036.4121, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12036.4111328125
tensor(12036.4121, grad_fn=<NegBackward0>) tensor(12036.4111, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12036.4111328125
tensor(12036.4111, grad_fn=<NegBackward0>) tensor(12036.4111, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12036.419921875
tensor(12036.4111, grad_fn=<NegBackward0>) tensor(12036.4199, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12036.4033203125
tensor(12036.4111, grad_fn=<NegBackward0>) tensor(12036.4033, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12036.3984375
tensor(12036.4033, grad_fn=<NegBackward0>) tensor(12036.3984, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12036.3984375
tensor(12036.3984, grad_fn=<NegBackward0>) tensor(12036.3984, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12036.396484375
tensor(12036.3984, grad_fn=<NegBackward0>) tensor(12036.3965, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12036.404296875
tensor(12036.3965, grad_fn=<NegBackward0>) tensor(12036.4043, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12036.37890625
tensor(12036.3965, grad_fn=<NegBackward0>) tensor(12036.3789, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12036.3818359375
tensor(12036.3789, grad_fn=<NegBackward0>) tensor(12036.3818, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12036.37890625
tensor(12036.3789, grad_fn=<NegBackward0>) tensor(12036.3789, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12036.3779296875
tensor(12036.3789, grad_fn=<NegBackward0>) tensor(12036.3779, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12036.3779296875
tensor(12036.3779, grad_fn=<NegBackward0>) tensor(12036.3779, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12036.37890625
tensor(12036.3779, grad_fn=<NegBackward0>) tensor(12036.3789, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12036.3798828125
tensor(12036.3779, grad_fn=<NegBackward0>) tensor(12036.3799, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12036.3779296875
tensor(12036.3779, grad_fn=<NegBackward0>) tensor(12036.3779, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12036.376953125
tensor(12036.3779, grad_fn=<NegBackward0>) tensor(12036.3770, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12036.3876953125
tensor(12036.3770, grad_fn=<NegBackward0>) tensor(12036.3877, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12036.3798828125
tensor(12036.3770, grad_fn=<NegBackward0>) tensor(12036.3799, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -12036.392578125
tensor(12036.3770, grad_fn=<NegBackward0>) tensor(12036.3926, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -12036.3779296875
tensor(12036.3770, grad_fn=<NegBackward0>) tensor(12036.3779, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -12036.3759765625
tensor(12036.3770, grad_fn=<NegBackward0>) tensor(12036.3760, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12036.376953125
tensor(12036.3760, grad_fn=<NegBackward0>) tensor(12036.3770, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12036.376953125
tensor(12036.3760, grad_fn=<NegBackward0>) tensor(12036.3770, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12036.376953125
tensor(12036.3760, grad_fn=<NegBackward0>) tensor(12036.3770, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -12036.376953125
tensor(12036.3760, grad_fn=<NegBackward0>) tensor(12036.3770, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -12036.376953125
tensor(12036.3760, grad_fn=<NegBackward0>) tensor(12036.3770, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.4909, 0.5091],
        [0.4545, 0.5455]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2695, 0.7305], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3868, 0.1054],
         [0.6430, 0.2377]],

        [[0.5888, 0.1001],
         [0.5001, 0.5841]],

        [[0.6329, 0.1136],
         [0.6585, 0.7138]],

        [[0.7018, 0.0991],
         [0.5446, 0.6243]],

        [[0.6841, 0.0910],
         [0.6266, 0.5956]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 74
Adjusted Rand Index: 0.22409191428789305
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 76
Adjusted Rand Index: 0.2646838453269049
Global Adjusted Rand Index: 0.1519735539392429
Average Adjusted Rand Index: 0.6817544936502495
[0.2174724137110556, 0.1519735539392429] [0.638705574858631, 0.6817544936502495] [12050.078125, 12036.376953125]
-------------------------------------
This iteration is 30
True Objective function: Loss = -11597.829727971859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22469.21484375
inf tensor(22469.2148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12238.927734375
tensor(22469.2148, grad_fn=<NegBackward0>) tensor(12238.9277, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12102.4306640625
tensor(12238.9277, grad_fn=<NegBackward0>) tensor(12102.4307, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12094.3193359375
tensor(12102.4307, grad_fn=<NegBackward0>) tensor(12094.3193, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11631.7353515625
tensor(12094.3193, grad_fn=<NegBackward0>) tensor(11631.7354, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11600.2685546875
tensor(11631.7354, grad_fn=<NegBackward0>) tensor(11600.2686, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11592.6259765625
tensor(11600.2686, grad_fn=<NegBackward0>) tensor(11592.6260, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11592.3447265625
tensor(11592.6260, grad_fn=<NegBackward0>) tensor(11592.3447, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11592.2216796875
tensor(11592.3447, grad_fn=<NegBackward0>) tensor(11592.2217, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11592.1416015625
tensor(11592.2217, grad_fn=<NegBackward0>) tensor(11592.1416, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11585.751953125
tensor(11592.1416, grad_fn=<NegBackward0>) tensor(11585.7520, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11585.68359375
tensor(11585.7520, grad_fn=<NegBackward0>) tensor(11585.6836, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11585.6474609375
tensor(11585.6836, grad_fn=<NegBackward0>) tensor(11585.6475, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11585.6240234375
tensor(11585.6475, grad_fn=<NegBackward0>) tensor(11585.6240, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11585.6083984375
tensor(11585.6240, grad_fn=<NegBackward0>) tensor(11585.6084, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11585.5888671875
tensor(11585.6084, grad_fn=<NegBackward0>) tensor(11585.5889, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11585.57421875
tensor(11585.5889, grad_fn=<NegBackward0>) tensor(11585.5742, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11585.5634765625
tensor(11585.5742, grad_fn=<NegBackward0>) tensor(11585.5635, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11585.5546875
tensor(11585.5635, grad_fn=<NegBackward0>) tensor(11585.5547, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11585.546875
tensor(11585.5547, grad_fn=<NegBackward0>) tensor(11585.5469, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11585.5400390625
tensor(11585.5469, grad_fn=<NegBackward0>) tensor(11585.5400, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11585.5341796875
tensor(11585.5400, grad_fn=<NegBackward0>) tensor(11585.5342, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11585.5283203125
tensor(11585.5342, grad_fn=<NegBackward0>) tensor(11585.5283, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11585.5244140625
tensor(11585.5283, grad_fn=<NegBackward0>) tensor(11585.5244, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11585.515625
tensor(11585.5244, grad_fn=<NegBackward0>) tensor(11585.5156, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11585.45703125
tensor(11585.5156, grad_fn=<NegBackward0>) tensor(11585.4570, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11585.453125
tensor(11585.4570, grad_fn=<NegBackward0>) tensor(11585.4531, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11585.44921875
tensor(11585.4531, grad_fn=<NegBackward0>) tensor(11585.4492, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11585.4482421875
tensor(11585.4492, grad_fn=<NegBackward0>) tensor(11585.4482, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11585.4453125
tensor(11585.4482, grad_fn=<NegBackward0>) tensor(11585.4453, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11585.443359375
tensor(11585.4453, grad_fn=<NegBackward0>) tensor(11585.4434, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11585.4423828125
tensor(11585.4434, grad_fn=<NegBackward0>) tensor(11585.4424, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11585.4375
tensor(11585.4424, grad_fn=<NegBackward0>) tensor(11585.4375, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11585.4384765625
tensor(11585.4375, grad_fn=<NegBackward0>) tensor(11585.4385, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11585.4365234375
tensor(11585.4375, grad_fn=<NegBackward0>) tensor(11585.4365, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11585.435546875
tensor(11585.4365, grad_fn=<NegBackward0>) tensor(11585.4355, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11585.43359375
tensor(11585.4355, grad_fn=<NegBackward0>) tensor(11585.4336, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11585.43359375
tensor(11585.4336, grad_fn=<NegBackward0>) tensor(11585.4336, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11585.4326171875
tensor(11585.4336, grad_fn=<NegBackward0>) tensor(11585.4326, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11585.431640625
tensor(11585.4326, grad_fn=<NegBackward0>) tensor(11585.4316, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11585.4296875
tensor(11585.4316, grad_fn=<NegBackward0>) tensor(11585.4297, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11585.4287109375
tensor(11585.4297, grad_fn=<NegBackward0>) tensor(11585.4287, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11585.4306640625
tensor(11585.4287, grad_fn=<NegBackward0>) tensor(11585.4307, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11585.427734375
tensor(11585.4287, grad_fn=<NegBackward0>) tensor(11585.4277, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11585.427734375
tensor(11585.4277, grad_fn=<NegBackward0>) tensor(11585.4277, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11585.42578125
tensor(11585.4277, grad_fn=<NegBackward0>) tensor(11585.4258, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11585.423828125
tensor(11585.4258, grad_fn=<NegBackward0>) tensor(11585.4238, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11585.4150390625
tensor(11585.4238, grad_fn=<NegBackward0>) tensor(11585.4150, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11585.4150390625
tensor(11585.4150, grad_fn=<NegBackward0>) tensor(11585.4150, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11585.4150390625
tensor(11585.4150, grad_fn=<NegBackward0>) tensor(11585.4150, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11585.4130859375
tensor(11585.4150, grad_fn=<NegBackward0>) tensor(11585.4131, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11585.4130859375
tensor(11585.4131, grad_fn=<NegBackward0>) tensor(11585.4131, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11585.4111328125
tensor(11585.4131, grad_fn=<NegBackward0>) tensor(11585.4111, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11585.4111328125
tensor(11585.4111, grad_fn=<NegBackward0>) tensor(11585.4111, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11585.4091796875
tensor(11585.4111, grad_fn=<NegBackward0>) tensor(11585.4092, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11585.416015625
tensor(11585.4092, grad_fn=<NegBackward0>) tensor(11585.4160, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11585.404296875
tensor(11585.4092, grad_fn=<NegBackward0>) tensor(11585.4043, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11585.4033203125
tensor(11585.4043, grad_fn=<NegBackward0>) tensor(11585.4033, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11585.4033203125
tensor(11585.4033, grad_fn=<NegBackward0>) tensor(11585.4033, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11585.4072265625
tensor(11585.4033, grad_fn=<NegBackward0>) tensor(11585.4072, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11585.4033203125
tensor(11585.4033, grad_fn=<NegBackward0>) tensor(11585.4033, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11585.40625
tensor(11585.4033, grad_fn=<NegBackward0>) tensor(11585.4062, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11585.40234375
tensor(11585.4033, grad_fn=<NegBackward0>) tensor(11585.4023, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11585.400390625
tensor(11585.4023, grad_fn=<NegBackward0>) tensor(11585.4004, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11585.400390625
tensor(11585.4004, grad_fn=<NegBackward0>) tensor(11585.4004, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11585.4052734375
tensor(11585.4004, grad_fn=<NegBackward0>) tensor(11585.4053, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11585.4013671875
tensor(11585.4004, grad_fn=<NegBackward0>) tensor(11585.4014, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11585.4013671875
tensor(11585.4004, grad_fn=<NegBackward0>) tensor(11585.4014, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11585.400390625
tensor(11585.4004, grad_fn=<NegBackward0>) tensor(11585.4004, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11585.3994140625
tensor(11585.4004, grad_fn=<NegBackward0>) tensor(11585.3994, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11585.400390625
tensor(11585.3994, grad_fn=<NegBackward0>) tensor(11585.4004, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11585.400390625
tensor(11585.3994, grad_fn=<NegBackward0>) tensor(11585.4004, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11585.3994140625
tensor(11585.3994, grad_fn=<NegBackward0>) tensor(11585.3994, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11585.4033203125
tensor(11585.3994, grad_fn=<NegBackward0>) tensor(11585.4033, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11585.3994140625
tensor(11585.3994, grad_fn=<NegBackward0>) tensor(11585.3994, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11585.3984375
tensor(11585.3994, grad_fn=<NegBackward0>) tensor(11585.3984, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11585.40234375
tensor(11585.3984, grad_fn=<NegBackward0>) tensor(11585.4023, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11585.4033203125
tensor(11585.3984, grad_fn=<NegBackward0>) tensor(11585.4033, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11585.4150390625
tensor(11585.3984, grad_fn=<NegBackward0>) tensor(11585.4150, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11585.400390625
tensor(11585.3984, grad_fn=<NegBackward0>) tensor(11585.4004, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11585.400390625
tensor(11585.3984, grad_fn=<NegBackward0>) tensor(11585.4004, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.7617, 0.2383],
        [0.2929, 0.7071]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4588, 0.5412], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2097, 0.1078],
         [0.6827, 0.3889]],

        [[0.5679, 0.0979],
         [0.5762, 0.5408]],

        [[0.6297, 0.1030],
         [0.6301, 0.6946]],

        [[0.5781, 0.0979],
         [0.5731, 0.5447]],

        [[0.5244, 0.1015],
         [0.7056, 0.5936]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320660777474
Average Adjusted Rand Index: 0.9839998119331363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22723.65625
inf tensor(22723.6562, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12097.1416015625
tensor(22723.6562, grad_fn=<NegBackward0>) tensor(12097.1416, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11593.0771484375
tensor(12097.1416, grad_fn=<NegBackward0>) tensor(11593.0771, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11592.29296875
tensor(11593.0771, grad_fn=<NegBackward0>) tensor(11592.2930, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11592.033203125
tensor(11592.2930, grad_fn=<NegBackward0>) tensor(11592.0332, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11591.91796875
tensor(11592.0332, grad_fn=<NegBackward0>) tensor(11591.9180, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11591.8525390625
tensor(11591.9180, grad_fn=<NegBackward0>) tensor(11591.8525, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11591.810546875
tensor(11591.8525, grad_fn=<NegBackward0>) tensor(11591.8105, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11591.783203125
tensor(11591.8105, grad_fn=<NegBackward0>) tensor(11591.7832, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11591.7607421875
tensor(11591.7832, grad_fn=<NegBackward0>) tensor(11591.7607, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11591.7470703125
tensor(11591.7607, grad_fn=<NegBackward0>) tensor(11591.7471, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11591.736328125
tensor(11591.7471, grad_fn=<NegBackward0>) tensor(11591.7363, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11591.7275390625
tensor(11591.7363, grad_fn=<NegBackward0>) tensor(11591.7275, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11591.71875
tensor(11591.7275, grad_fn=<NegBackward0>) tensor(11591.7188, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11591.7060546875
tensor(11591.7188, grad_fn=<NegBackward0>) tensor(11591.7061, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11587.0546875
tensor(11591.7061, grad_fn=<NegBackward0>) tensor(11587.0547, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11587.0498046875
tensor(11587.0547, grad_fn=<NegBackward0>) tensor(11587.0498, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11587.0478515625
tensor(11587.0498, grad_fn=<NegBackward0>) tensor(11587.0479, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11587.04296875
tensor(11587.0479, grad_fn=<NegBackward0>) tensor(11587.0430, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11587.041015625
tensor(11587.0430, grad_fn=<NegBackward0>) tensor(11587.0410, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11587.0390625
tensor(11587.0410, grad_fn=<NegBackward0>) tensor(11587.0391, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11587.0361328125
tensor(11587.0391, grad_fn=<NegBackward0>) tensor(11587.0361, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11587.0341796875
tensor(11587.0361, grad_fn=<NegBackward0>) tensor(11587.0342, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11587.0341796875
tensor(11587.0342, grad_fn=<NegBackward0>) tensor(11587.0342, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11587.0322265625
tensor(11587.0342, grad_fn=<NegBackward0>) tensor(11587.0322, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11587.0302734375
tensor(11587.0322, grad_fn=<NegBackward0>) tensor(11587.0303, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11587.03125
tensor(11587.0303, grad_fn=<NegBackward0>) tensor(11587.0312, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11587.0283203125
tensor(11587.0303, grad_fn=<NegBackward0>) tensor(11587.0283, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11587.0283203125
tensor(11587.0283, grad_fn=<NegBackward0>) tensor(11587.0283, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11587.02734375
tensor(11587.0283, grad_fn=<NegBackward0>) tensor(11587.0273, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11587.0263671875
tensor(11587.0273, grad_fn=<NegBackward0>) tensor(11587.0264, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11587.0263671875
tensor(11587.0264, grad_fn=<NegBackward0>) tensor(11587.0264, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11587.025390625
tensor(11587.0264, grad_fn=<NegBackward0>) tensor(11587.0254, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11587.0283203125
tensor(11587.0254, grad_fn=<NegBackward0>) tensor(11587.0283, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11587.0244140625
tensor(11587.0254, grad_fn=<NegBackward0>) tensor(11587.0244, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11587.02734375
tensor(11587.0244, grad_fn=<NegBackward0>) tensor(11587.0273, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11587.0234375
tensor(11587.0244, grad_fn=<NegBackward0>) tensor(11587.0234, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11587.0244140625
tensor(11587.0234, grad_fn=<NegBackward0>) tensor(11587.0244, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11587.0244140625
tensor(11587.0234, grad_fn=<NegBackward0>) tensor(11587.0244, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11587.02734375
tensor(11587.0234, grad_fn=<NegBackward0>) tensor(11587.0273, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -11587.021484375
tensor(11587.0234, grad_fn=<NegBackward0>) tensor(11587.0215, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11587.0224609375
tensor(11587.0215, grad_fn=<NegBackward0>) tensor(11587.0225, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11587.021484375
tensor(11587.0215, grad_fn=<NegBackward0>) tensor(11587.0215, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11587.021484375
tensor(11587.0215, grad_fn=<NegBackward0>) tensor(11587.0215, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11587.0302734375
tensor(11587.0215, grad_fn=<NegBackward0>) tensor(11587.0303, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11587.01953125
tensor(11587.0215, grad_fn=<NegBackward0>) tensor(11587.0195, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11587.01953125
tensor(11587.0195, grad_fn=<NegBackward0>) tensor(11587.0195, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11587.01953125
tensor(11587.0195, grad_fn=<NegBackward0>) tensor(11587.0195, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11587.021484375
tensor(11587.0195, grad_fn=<NegBackward0>) tensor(11587.0215, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11587.021484375
tensor(11587.0195, grad_fn=<NegBackward0>) tensor(11587.0215, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11587.033203125
tensor(11587.0195, grad_fn=<NegBackward0>) tensor(11587.0332, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11587.0244140625
tensor(11587.0195, grad_fn=<NegBackward0>) tensor(11587.0244, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -11587.015625
tensor(11587.0195, grad_fn=<NegBackward0>) tensor(11587.0156, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11587.01171875
tensor(11587.0156, grad_fn=<NegBackward0>) tensor(11587.0117, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11587.0107421875
tensor(11587.0117, grad_fn=<NegBackward0>) tensor(11587.0107, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11587.0263671875
tensor(11587.0107, grad_fn=<NegBackward0>) tensor(11587.0264, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11587.0107421875
tensor(11587.0107, grad_fn=<NegBackward0>) tensor(11587.0107, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11587.009765625
tensor(11587.0107, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11587.0107421875
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0107, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11587.0205078125
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0205, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11587.0107421875
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0107, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11587.01171875
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0117, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11587.017578125
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0176, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11587.01171875
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0117, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11587.009765625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11587.0087890625
tensor(11587.0098, grad_fn=<NegBackward0>) tensor(11587.0088, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11587.009765625
tensor(11587.0088, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11587.009765625
tensor(11587.0088, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11587.009765625
tensor(11587.0088, grad_fn=<NegBackward0>) tensor(11587.0098, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11587.01171875
tensor(11587.0088, grad_fn=<NegBackward0>) tensor(11587.0117, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11587.013671875
tensor(11587.0088, grad_fn=<NegBackward0>) tensor(11587.0137, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7585, 0.2415],
        [0.2936, 0.7064]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4586, 0.5414], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2100, 0.1078],
         [0.6331, 0.3875]],

        [[0.5418, 0.0979],
         [0.6486, 0.6725]],

        [[0.6723, 0.1030],
         [0.6701, 0.6221]],

        [[0.5905, 0.0972],
         [0.5908, 0.6325]],

        [[0.5927, 0.1015],
         [0.6662, 0.6301]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9761614280947526
[0.9840320660777474, 0.9760961975210904] [0.9839998119331363, 0.9761614280947526] [11585.400390625, 11587.013671875]
-------------------------------------
This iteration is 31
True Objective function: Loss = -11560.614010278518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20837.37890625
inf tensor(20837.3789, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12397.4111328125
tensor(20837.3789, grad_fn=<NegBackward0>) tensor(12397.4111, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11682.05859375
tensor(12397.4111, grad_fn=<NegBackward0>) tensor(11682.0586, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11577.6259765625
tensor(11682.0586, grad_fn=<NegBackward0>) tensor(11577.6260, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11566.2607421875
tensor(11577.6260, grad_fn=<NegBackward0>) tensor(11566.2607, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11562.1044921875
tensor(11566.2607, grad_fn=<NegBackward0>) tensor(11562.1045, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11561.017578125
tensor(11562.1045, grad_fn=<NegBackward0>) tensor(11561.0176, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11560.87109375
tensor(11561.0176, grad_fn=<NegBackward0>) tensor(11560.8711, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11560.7861328125
tensor(11560.8711, grad_fn=<NegBackward0>) tensor(11560.7861, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11560.728515625
tensor(11560.7861, grad_fn=<NegBackward0>) tensor(11560.7285, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11560.6552734375
tensor(11560.7285, grad_fn=<NegBackward0>) tensor(11560.6553, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11556.40234375
tensor(11560.6553, grad_fn=<NegBackward0>) tensor(11556.4023, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11556.3798828125
tensor(11556.4023, grad_fn=<NegBackward0>) tensor(11556.3799, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11556.35546875
tensor(11556.3799, grad_fn=<NegBackward0>) tensor(11556.3555, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11556.3388671875
tensor(11556.3555, grad_fn=<NegBackward0>) tensor(11556.3389, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11556.32421875
tensor(11556.3389, grad_fn=<NegBackward0>) tensor(11556.3242, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11556.314453125
tensor(11556.3242, grad_fn=<NegBackward0>) tensor(11556.3145, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11556.306640625
tensor(11556.3145, grad_fn=<NegBackward0>) tensor(11556.3066, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11556.2978515625
tensor(11556.3066, grad_fn=<NegBackward0>) tensor(11556.2979, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11556.29296875
tensor(11556.2979, grad_fn=<NegBackward0>) tensor(11556.2930, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11556.28515625
tensor(11556.2930, grad_fn=<NegBackward0>) tensor(11556.2852, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11556.2802734375
tensor(11556.2852, grad_fn=<NegBackward0>) tensor(11556.2803, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11556.2744140625
tensor(11556.2803, grad_fn=<NegBackward0>) tensor(11556.2744, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11556.271484375
tensor(11556.2744, grad_fn=<NegBackward0>) tensor(11556.2715, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11556.267578125
tensor(11556.2715, grad_fn=<NegBackward0>) tensor(11556.2676, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11556.265625
tensor(11556.2676, grad_fn=<NegBackward0>) tensor(11556.2656, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11556.2607421875
tensor(11556.2656, grad_fn=<NegBackward0>) tensor(11556.2607, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11556.2646484375
tensor(11556.2607, grad_fn=<NegBackward0>) tensor(11556.2646, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11556.2568359375
tensor(11556.2607, grad_fn=<NegBackward0>) tensor(11556.2568, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11556.2548828125
tensor(11556.2568, grad_fn=<NegBackward0>) tensor(11556.2549, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11556.25390625
tensor(11556.2549, grad_fn=<NegBackward0>) tensor(11556.2539, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11556.2490234375
tensor(11556.2539, grad_fn=<NegBackward0>) tensor(11556.2490, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11556.2412109375
tensor(11556.2490, grad_fn=<NegBackward0>) tensor(11556.2412, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11556.23828125
tensor(11556.2412, grad_fn=<NegBackward0>) tensor(11556.2383, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11556.2373046875
tensor(11556.2383, grad_fn=<NegBackward0>) tensor(11556.2373, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11556.2353515625
tensor(11556.2373, grad_fn=<NegBackward0>) tensor(11556.2354, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11556.234375
tensor(11556.2354, grad_fn=<NegBackward0>) tensor(11556.2344, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11556.23828125
tensor(11556.2344, grad_fn=<NegBackward0>) tensor(11556.2383, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11556.234375
tensor(11556.2344, grad_fn=<NegBackward0>) tensor(11556.2344, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11556.2333984375
tensor(11556.2344, grad_fn=<NegBackward0>) tensor(11556.2334, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11556.2314453125
tensor(11556.2334, grad_fn=<NegBackward0>) tensor(11556.2314, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11556.2314453125
tensor(11556.2314, grad_fn=<NegBackward0>) tensor(11556.2314, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11556.2294921875
tensor(11556.2314, grad_fn=<NegBackward0>) tensor(11556.2295, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11556.23046875
tensor(11556.2295, grad_fn=<NegBackward0>) tensor(11556.2305, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11556.2294921875
tensor(11556.2295, grad_fn=<NegBackward0>) tensor(11556.2295, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11556.2294921875
tensor(11556.2295, grad_fn=<NegBackward0>) tensor(11556.2295, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11556.228515625
tensor(11556.2295, grad_fn=<NegBackward0>) tensor(11556.2285, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11556.2275390625
tensor(11556.2285, grad_fn=<NegBackward0>) tensor(11556.2275, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11556.2265625
tensor(11556.2275, grad_fn=<NegBackward0>) tensor(11556.2266, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11556.2265625
tensor(11556.2266, grad_fn=<NegBackward0>) tensor(11556.2266, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11556.2275390625
tensor(11556.2266, grad_fn=<NegBackward0>) tensor(11556.2275, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11556.2333984375
tensor(11556.2266, grad_fn=<NegBackward0>) tensor(11556.2334, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11556.2265625
tensor(11556.2266, grad_fn=<NegBackward0>) tensor(11556.2266, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11556.224609375
tensor(11556.2266, grad_fn=<NegBackward0>) tensor(11556.2246, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11556.228515625
tensor(11556.2246, grad_fn=<NegBackward0>) tensor(11556.2285, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11556.224609375
tensor(11556.2246, grad_fn=<NegBackward0>) tensor(11556.2246, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11556.2255859375
tensor(11556.2246, grad_fn=<NegBackward0>) tensor(11556.2256, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11556.224609375
tensor(11556.2246, grad_fn=<NegBackward0>) tensor(11556.2246, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11556.2236328125
tensor(11556.2246, grad_fn=<NegBackward0>) tensor(11556.2236, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11556.22265625
tensor(11556.2236, grad_fn=<NegBackward0>) tensor(11556.2227, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11556.2294921875
tensor(11556.2227, grad_fn=<NegBackward0>) tensor(11556.2295, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11556.228515625
tensor(11556.2227, grad_fn=<NegBackward0>) tensor(11556.2285, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11556.22265625
tensor(11556.2227, grad_fn=<NegBackward0>) tensor(11556.2227, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11556.2216796875
tensor(11556.2227, grad_fn=<NegBackward0>) tensor(11556.2217, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11556.22265625
tensor(11556.2217, grad_fn=<NegBackward0>) tensor(11556.2227, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11556.2216796875
tensor(11556.2217, grad_fn=<NegBackward0>) tensor(11556.2217, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11556.22265625
tensor(11556.2217, grad_fn=<NegBackward0>) tensor(11556.2227, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11556.2236328125
tensor(11556.2217, grad_fn=<NegBackward0>) tensor(11556.2236, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11556.220703125
tensor(11556.2217, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11556.220703125
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11556.244140625
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2441, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11556.2900390625
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2900, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11556.220703125
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11556.220703125
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11556.224609375
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2246, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11556.220703125
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11556.220703125
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11556.220703125
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11556.2197265625
tensor(11556.2207, grad_fn=<NegBackward0>) tensor(11556.2197, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11556.2236328125
tensor(11556.2197, grad_fn=<NegBackward0>) tensor(11556.2236, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11556.2197265625
tensor(11556.2197, grad_fn=<NegBackward0>) tensor(11556.2197, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11556.220703125
tensor(11556.2197, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11556.21875
tensor(11556.2197, grad_fn=<NegBackward0>) tensor(11556.2188, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11556.2197265625
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2197, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11556.2197265625
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2197, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11556.2216796875
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2217, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11556.21875
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2188, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11556.2236328125
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2236, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11556.21875
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2188, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11556.283203125
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2832, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11556.2216796875
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2217, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11556.21875
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2188, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11556.220703125
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2207, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11556.2177734375
tensor(11556.2188, grad_fn=<NegBackward0>) tensor(11556.2178, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11556.2197265625
tensor(11556.2178, grad_fn=<NegBackward0>) tensor(11556.2197, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11556.2314453125
tensor(11556.2178, grad_fn=<NegBackward0>) tensor(11556.2314, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11556.2197265625
tensor(11556.2178, grad_fn=<NegBackward0>) tensor(11556.2197, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11556.2314453125
tensor(11556.2178, grad_fn=<NegBackward0>) tensor(11556.2314, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11556.228515625
tensor(11556.2178, grad_fn=<NegBackward0>) tensor(11556.2285, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7534, 0.2466],
        [0.2625, 0.7375]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5447, 0.4553], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4002, 0.1030],
         [0.5546, 0.1929]],

        [[0.7186, 0.0941],
         [0.6239, 0.6883]],

        [[0.7064, 0.0954],
         [0.5522, 0.5947]],

        [[0.7191, 0.1024],
         [0.6228, 0.6419]],

        [[0.7240, 0.0958],
         [0.5767, 0.6430]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22564.345703125
inf tensor(22564.3457, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12327.8212890625
tensor(22564.3457, grad_fn=<NegBackward0>) tensor(12327.8213, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11850.8681640625
tensor(12327.8213, grad_fn=<NegBackward0>) tensor(11850.8682, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11763.322265625
tensor(11850.8682, grad_fn=<NegBackward0>) tensor(11763.3223, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11762.4775390625
tensor(11763.3223, grad_fn=<NegBackward0>) tensor(11762.4775, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11762.1689453125
tensor(11762.4775, grad_fn=<NegBackward0>) tensor(11762.1689, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11761.6962890625
tensor(11762.1689, grad_fn=<NegBackward0>) tensor(11761.6963, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11754.974609375
tensor(11761.6963, grad_fn=<NegBackward0>) tensor(11754.9746, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11744.3095703125
tensor(11754.9746, grad_fn=<NegBackward0>) tensor(11744.3096, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11744.099609375
tensor(11744.3096, grad_fn=<NegBackward0>) tensor(11744.0996, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11744.0048828125
tensor(11744.0996, grad_fn=<NegBackward0>) tensor(11744.0049, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11743.9384765625
tensor(11744.0049, grad_fn=<NegBackward0>) tensor(11743.9385, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11743.9130859375
tensor(11743.9385, grad_fn=<NegBackward0>) tensor(11743.9131, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11743.8876953125
tensor(11743.9131, grad_fn=<NegBackward0>) tensor(11743.8877, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11743.873046875
tensor(11743.8877, grad_fn=<NegBackward0>) tensor(11743.8730, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11743.859375
tensor(11743.8730, grad_fn=<NegBackward0>) tensor(11743.8594, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11743.84765625
tensor(11743.8594, grad_fn=<NegBackward0>) tensor(11743.8477, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11743.8388671875
tensor(11743.8477, grad_fn=<NegBackward0>) tensor(11743.8389, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11743.8330078125
tensor(11743.8389, grad_fn=<NegBackward0>) tensor(11743.8330, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11743.828125
tensor(11743.8330, grad_fn=<NegBackward0>) tensor(11743.8281, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11743.8212890625
tensor(11743.8281, grad_fn=<NegBackward0>) tensor(11743.8213, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11743.8173828125
tensor(11743.8213, grad_fn=<NegBackward0>) tensor(11743.8174, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11743.8134765625
tensor(11743.8174, grad_fn=<NegBackward0>) tensor(11743.8135, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11743.8095703125
tensor(11743.8135, grad_fn=<NegBackward0>) tensor(11743.8096, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11743.806640625
tensor(11743.8096, grad_fn=<NegBackward0>) tensor(11743.8066, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11743.8037109375
tensor(11743.8066, grad_fn=<NegBackward0>) tensor(11743.8037, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11743.8017578125
tensor(11743.8037, grad_fn=<NegBackward0>) tensor(11743.8018, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11743.7998046875
tensor(11743.8018, grad_fn=<NegBackward0>) tensor(11743.7998, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11743.796875
tensor(11743.7998, grad_fn=<NegBackward0>) tensor(11743.7969, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11743.7958984375
tensor(11743.7969, grad_fn=<NegBackward0>) tensor(11743.7959, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11743.79296875
tensor(11743.7959, grad_fn=<NegBackward0>) tensor(11743.7930, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11743.7919921875
tensor(11743.7930, grad_fn=<NegBackward0>) tensor(11743.7920, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11743.791015625
tensor(11743.7920, grad_fn=<NegBackward0>) tensor(11743.7910, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11743.7890625
tensor(11743.7910, grad_fn=<NegBackward0>) tensor(11743.7891, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11743.787109375
tensor(11743.7891, grad_fn=<NegBackward0>) tensor(11743.7871, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11743.7861328125
tensor(11743.7871, grad_fn=<NegBackward0>) tensor(11743.7861, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11743.7861328125
tensor(11743.7861, grad_fn=<NegBackward0>) tensor(11743.7861, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11743.78515625
tensor(11743.7861, grad_fn=<NegBackward0>) tensor(11743.7852, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11743.783203125
tensor(11743.7852, grad_fn=<NegBackward0>) tensor(11743.7832, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11743.7822265625
tensor(11743.7832, grad_fn=<NegBackward0>) tensor(11743.7822, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11743.7822265625
tensor(11743.7822, grad_fn=<NegBackward0>) tensor(11743.7822, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11743.78125
tensor(11743.7822, grad_fn=<NegBackward0>) tensor(11743.7812, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11743.78125
tensor(11743.7812, grad_fn=<NegBackward0>) tensor(11743.7812, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11743.779296875
tensor(11743.7812, grad_fn=<NegBackward0>) tensor(11743.7793, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11743.77734375
tensor(11743.7793, grad_fn=<NegBackward0>) tensor(11743.7773, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11743.77734375
tensor(11743.7773, grad_fn=<NegBackward0>) tensor(11743.7773, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11743.7783203125
tensor(11743.7773, grad_fn=<NegBackward0>) tensor(11743.7783, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11743.7802734375
tensor(11743.7773, grad_fn=<NegBackward0>) tensor(11743.7803, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11743.7763671875
tensor(11743.7773, grad_fn=<NegBackward0>) tensor(11743.7764, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11743.78125
tensor(11743.7764, grad_fn=<NegBackward0>) tensor(11743.7812, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11743.7744140625
tensor(11743.7764, grad_fn=<NegBackward0>) tensor(11743.7744, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11743.775390625
tensor(11743.7744, grad_fn=<NegBackward0>) tensor(11743.7754, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11743.7734375
tensor(11743.7744, grad_fn=<NegBackward0>) tensor(11743.7734, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11743.78125
tensor(11743.7734, grad_fn=<NegBackward0>) tensor(11743.7812, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11743.7734375
tensor(11743.7734, grad_fn=<NegBackward0>) tensor(11743.7734, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11743.7734375
tensor(11743.7734, grad_fn=<NegBackward0>) tensor(11743.7734, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11743.7734375
tensor(11743.7734, grad_fn=<NegBackward0>) tensor(11743.7734, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11743.771484375
tensor(11743.7734, grad_fn=<NegBackward0>) tensor(11743.7715, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11743.068359375
tensor(11743.7715, grad_fn=<NegBackward0>) tensor(11743.0684, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11743.0693359375
tensor(11743.0684, grad_fn=<NegBackward0>) tensor(11743.0693, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11743.0966796875
tensor(11743.0684, grad_fn=<NegBackward0>) tensor(11743.0967, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11743.068359375
tensor(11743.0684, grad_fn=<NegBackward0>) tensor(11743.0684, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11743.07421875
tensor(11743.0684, grad_fn=<NegBackward0>) tensor(11743.0742, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11743.0673828125
tensor(11743.0684, grad_fn=<NegBackward0>) tensor(11743.0674, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11743.0673828125
tensor(11743.0674, grad_fn=<NegBackward0>) tensor(11743.0674, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11743.068359375
tensor(11743.0674, grad_fn=<NegBackward0>) tensor(11743.0684, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11743.0673828125
tensor(11743.0674, grad_fn=<NegBackward0>) tensor(11743.0674, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11743.06640625
tensor(11743.0674, grad_fn=<NegBackward0>) tensor(11743.0664, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11743.0693359375
tensor(11743.0664, grad_fn=<NegBackward0>) tensor(11743.0693, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11743.0654296875
tensor(11743.0664, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11743.0654296875
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11743.06640625
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0664, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11743.080078125
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0801, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11743.0654296875
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11743.0654296875
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11743.080078125
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0801, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11743.06640625
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0664, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11743.076171875
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0762, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11743.0654296875
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11743.06640625
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0664, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11743.0654296875
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11743.0654296875
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11743.06640625
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0664, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11743.109375
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.1094, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11743.064453125
tensor(11743.0654, grad_fn=<NegBackward0>) tensor(11743.0645, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11743.0673828125
tensor(11743.0645, grad_fn=<NegBackward0>) tensor(11743.0674, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11743.0654296875
tensor(11743.0645, grad_fn=<NegBackward0>) tensor(11743.0654, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11743.0634765625
tensor(11743.0645, grad_fn=<NegBackward0>) tensor(11743.0635, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11743.064453125
tensor(11743.0635, grad_fn=<NegBackward0>) tensor(11743.0645, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11743.0791015625
tensor(11743.0635, grad_fn=<NegBackward0>) tensor(11743.0791, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11743.064453125
tensor(11743.0635, grad_fn=<NegBackward0>) tensor(11743.0645, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11743.08203125
tensor(11743.0635, grad_fn=<NegBackward0>) tensor(11743.0820, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11743.072265625
tensor(11743.0635, grad_fn=<NegBackward0>) tensor(11743.0723, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.6211, 0.3789],
        [0.4849, 0.5151]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4565, 0.5435], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2243, 0.1022],
         [0.5908, 0.3974]],

        [[0.6332, 0.0942],
         [0.5985, 0.5899]],

        [[0.7177, 0.0953],
         [0.5859, 0.7077]],

        [[0.7158, 0.0965],
         [0.7263, 0.6877]],

        [[0.5942, 0.0970],
         [0.6194, 0.7163]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 21
Adjusted Rand Index: 0.32884850162989443
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.4558855664466331
Average Adjusted Rand Index: 0.8499308277951092
[1.0, 0.4558855664466331] [1.0, 0.8499308277951092] [11556.228515625, 11743.072265625]
-------------------------------------
This iteration is 32
True Objective function: Loss = -11425.967320665199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23179.65625
inf tensor(23179.6562, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12228.626953125
tensor(23179.6562, grad_fn=<NegBackward0>) tensor(12228.6270, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11772.412109375
tensor(12228.6270, grad_fn=<NegBackward0>) tensor(11772.4121, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11582.0361328125
tensor(11772.4121, grad_fn=<NegBackward0>) tensor(11582.0361, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11538.59375
tensor(11582.0361, grad_fn=<NegBackward0>) tensor(11538.5938, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11486.498046875
tensor(11538.5938, grad_fn=<NegBackward0>) tensor(11486.4980, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11445.9287109375
tensor(11486.4980, grad_fn=<NegBackward0>) tensor(11445.9287, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11437.8701171875
tensor(11445.9287, grad_fn=<NegBackward0>) tensor(11437.8701, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11437.6650390625
tensor(11437.8701, grad_fn=<NegBackward0>) tensor(11437.6650, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11437.2314453125
tensor(11437.6650, grad_fn=<NegBackward0>) tensor(11437.2314, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11437.1591796875
tensor(11437.2314, grad_fn=<NegBackward0>) tensor(11437.1592, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11437.1064453125
tensor(11437.1592, grad_fn=<NegBackward0>) tensor(11437.1064, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11432.72265625
tensor(11437.1064, grad_fn=<NegBackward0>) tensor(11432.7227, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11432.330078125
tensor(11432.7227, grad_fn=<NegBackward0>) tensor(11432.3301, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11432.306640625
tensor(11432.3301, grad_fn=<NegBackward0>) tensor(11432.3066, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11432.2841796875
tensor(11432.3066, grad_fn=<NegBackward0>) tensor(11432.2842, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11420.068359375
tensor(11432.2842, grad_fn=<NegBackward0>) tensor(11420.0684, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11420.052734375
tensor(11420.0684, grad_fn=<NegBackward0>) tensor(11420.0527, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11420.04296875
tensor(11420.0527, grad_fn=<NegBackward0>) tensor(11420.0430, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11420.03125
tensor(11420.0430, grad_fn=<NegBackward0>) tensor(11420.0312, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11420.0244140625
tensor(11420.0312, grad_fn=<NegBackward0>) tensor(11420.0244, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11420.015625
tensor(11420.0244, grad_fn=<NegBackward0>) tensor(11420.0156, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11420.0107421875
tensor(11420.0156, grad_fn=<NegBackward0>) tensor(11420.0107, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11420.0078125
tensor(11420.0107, grad_fn=<NegBackward0>) tensor(11420.0078, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11419.9990234375
tensor(11420.0078, grad_fn=<NegBackward0>) tensor(11419.9990, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11419.9951171875
tensor(11419.9990, grad_fn=<NegBackward0>) tensor(11419.9951, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11419.9912109375
tensor(11419.9951, grad_fn=<NegBackward0>) tensor(11419.9912, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11419.9892578125
tensor(11419.9912, grad_fn=<NegBackward0>) tensor(11419.9893, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11419.9921875
tensor(11419.9893, grad_fn=<NegBackward0>) tensor(11419.9922, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11419.9814453125
tensor(11419.9893, grad_fn=<NegBackward0>) tensor(11419.9814, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11419.9794921875
tensor(11419.9814, grad_fn=<NegBackward0>) tensor(11419.9795, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11419.9765625
tensor(11419.9795, grad_fn=<NegBackward0>) tensor(11419.9766, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11419.9755859375
tensor(11419.9766, grad_fn=<NegBackward0>) tensor(11419.9756, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11419.9794921875
tensor(11419.9756, grad_fn=<NegBackward0>) tensor(11419.9795, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11419.97265625
tensor(11419.9756, grad_fn=<NegBackward0>) tensor(11419.9727, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11419.9716796875
tensor(11419.9727, grad_fn=<NegBackward0>) tensor(11419.9717, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11419.970703125
tensor(11419.9717, grad_fn=<NegBackward0>) tensor(11419.9707, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11419.96875
tensor(11419.9707, grad_fn=<NegBackward0>) tensor(11419.9688, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11419.9658203125
tensor(11419.9688, grad_fn=<NegBackward0>) tensor(11419.9658, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11419.96484375
tensor(11419.9658, grad_fn=<NegBackward0>) tensor(11419.9648, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11419.96484375
tensor(11419.9648, grad_fn=<NegBackward0>) tensor(11419.9648, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11419.9638671875
tensor(11419.9648, grad_fn=<NegBackward0>) tensor(11419.9639, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11419.9638671875
tensor(11419.9639, grad_fn=<NegBackward0>) tensor(11419.9639, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11419.9609375
tensor(11419.9639, grad_fn=<NegBackward0>) tensor(11419.9609, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11419.9599609375
tensor(11419.9609, grad_fn=<NegBackward0>) tensor(11419.9600, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11419.9580078125
tensor(11419.9600, grad_fn=<NegBackward0>) tensor(11419.9580, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11419.9580078125
tensor(11419.9580, grad_fn=<NegBackward0>) tensor(11419.9580, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11419.962890625
tensor(11419.9580, grad_fn=<NegBackward0>) tensor(11419.9629, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11419.9580078125
tensor(11419.9580, grad_fn=<NegBackward0>) tensor(11419.9580, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11419.955078125
tensor(11419.9580, grad_fn=<NegBackward0>) tensor(11419.9551, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11419.95703125
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9570, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11419.958984375
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9590, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11419.9560546875
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9561, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11419.95703125
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9570, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -11419.955078125
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9551, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11419.9560546875
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9561, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11419.95703125
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9570, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11419.9619140625
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9619, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11419.9521484375
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11419.955078125
tensor(11419.9521, grad_fn=<NegBackward0>) tensor(11419.9551, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11419.9521484375
tensor(11419.9521, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11419.951171875
tensor(11419.9521, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11419.9521484375
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11419.9541015625
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9541, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11419.953125
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9531, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11419.951171875
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11419.9521484375
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11419.94921875
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11419.94921875
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11419.94921875
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11419.9482421875
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11419.947265625
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11419.947265625
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11419.947265625
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11419.947265625
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11419.9482421875
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11419.9482421875
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11419.9482421875
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11419.9482421875
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11419.947265625
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11419.9462890625
tensor(11419.9473, grad_fn=<NegBackward0>) tensor(11419.9463, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11420.0732421875
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11420.0732, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11419.947265625
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11420.060546875
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11420.0605, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11419.9462890625
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9463, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11420.0126953125
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11420.0127, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11419.9453125
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9453, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11419.9501953125
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9502, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11419.94921875
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11419.9560546875
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9561, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11419.9453125
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9453, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11419.951171875
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11419.9453125
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9453, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11419.9453125
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9453, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11419.9501953125
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9502, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11419.9580078125
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9580, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11419.9541015625
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9541, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11419.9453125
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9453, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11419.9462890625
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9463, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11419.958984375
tensor(11419.9453, grad_fn=<NegBackward0>) tensor(11419.9590, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7276, 0.2724],
        [0.2707, 0.7293]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4727, 0.5273], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3996, 0.0972],
         [0.5404, 0.1993]],

        [[0.6959, 0.0991],
         [0.6691, 0.6350]],

        [[0.6122, 0.0996],
         [0.5798, 0.6055]],

        [[0.6464, 0.0989],
         [0.5938, 0.6517]],

        [[0.7253, 0.0847],
         [0.7062, 0.7298]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9841602586080228
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19721.32421875
inf tensor(19721.3242, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12235.1767578125
tensor(19721.3242, grad_fn=<NegBackward0>) tensor(12235.1768, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12192.1044921875
tensor(12235.1768, grad_fn=<NegBackward0>) tensor(12192.1045, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11510.171875
tensor(12192.1045, grad_fn=<NegBackward0>) tensor(11510.1719, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11459.978515625
tensor(11510.1719, grad_fn=<NegBackward0>) tensor(11459.9785, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11445.443359375
tensor(11459.9785, grad_fn=<NegBackward0>) tensor(11445.4434, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11440.763671875
tensor(11445.4434, grad_fn=<NegBackward0>) tensor(11440.7637, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11435.1650390625
tensor(11440.7637, grad_fn=<NegBackward0>) tensor(11435.1650, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11431.0703125
tensor(11435.1650, grad_fn=<NegBackward0>) tensor(11431.0703, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11430.974609375
tensor(11431.0703, grad_fn=<NegBackward0>) tensor(11430.9746, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11430.912109375
tensor(11430.9746, grad_fn=<NegBackward0>) tensor(11430.9121, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11430.8359375
tensor(11430.9121, grad_fn=<NegBackward0>) tensor(11430.8359, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11420.1474609375
tensor(11430.8359, grad_fn=<NegBackward0>) tensor(11420.1475, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11420.12109375
tensor(11420.1475, grad_fn=<NegBackward0>) tensor(11420.1211, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11420.09765625
tensor(11420.1211, grad_fn=<NegBackward0>) tensor(11420.0977, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11420.080078125
tensor(11420.0977, grad_fn=<NegBackward0>) tensor(11420.0801, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11420.0654296875
tensor(11420.0801, grad_fn=<NegBackward0>) tensor(11420.0654, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11420.052734375
tensor(11420.0654, grad_fn=<NegBackward0>) tensor(11420.0527, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11420.046875
tensor(11420.0527, grad_fn=<NegBackward0>) tensor(11420.0469, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11420.03125
tensor(11420.0469, grad_fn=<NegBackward0>) tensor(11420.0312, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11420.0234375
tensor(11420.0312, grad_fn=<NegBackward0>) tensor(11420.0234, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11420.0166015625
tensor(11420.0234, grad_fn=<NegBackward0>) tensor(11420.0166, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11420.0087890625
tensor(11420.0166, grad_fn=<NegBackward0>) tensor(11420.0088, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11420.001953125
tensor(11420.0088, grad_fn=<NegBackward0>) tensor(11420.0020, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11419.9970703125
tensor(11420.0020, grad_fn=<NegBackward0>) tensor(11419.9971, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11419.9931640625
tensor(11419.9971, grad_fn=<NegBackward0>) tensor(11419.9932, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11419.9892578125
tensor(11419.9932, grad_fn=<NegBackward0>) tensor(11419.9893, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11419.9853515625
tensor(11419.9893, grad_fn=<NegBackward0>) tensor(11419.9854, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11419.9853515625
tensor(11419.9854, grad_fn=<NegBackward0>) tensor(11419.9854, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11419.98046875
tensor(11419.9854, grad_fn=<NegBackward0>) tensor(11419.9805, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11419.9775390625
tensor(11419.9805, grad_fn=<NegBackward0>) tensor(11419.9775, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11419.974609375
tensor(11419.9775, grad_fn=<NegBackward0>) tensor(11419.9746, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11419.97265625
tensor(11419.9746, grad_fn=<NegBackward0>) tensor(11419.9727, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11419.984375
tensor(11419.9727, grad_fn=<NegBackward0>) tensor(11419.9844, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11419.96875
tensor(11419.9727, grad_fn=<NegBackward0>) tensor(11419.9688, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11419.96875
tensor(11419.9688, grad_fn=<NegBackward0>) tensor(11419.9688, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11419.9697265625
tensor(11419.9688, grad_fn=<NegBackward0>) tensor(11419.9697, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11419.96484375
tensor(11419.9688, grad_fn=<NegBackward0>) tensor(11419.9648, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11419.962890625
tensor(11419.9648, grad_fn=<NegBackward0>) tensor(11419.9629, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11419.9638671875
tensor(11419.9629, grad_fn=<NegBackward0>) tensor(11419.9639, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11419.9619140625
tensor(11419.9629, grad_fn=<NegBackward0>) tensor(11419.9619, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11419.9638671875
tensor(11419.9619, grad_fn=<NegBackward0>) tensor(11419.9639, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11419.9599609375
tensor(11419.9619, grad_fn=<NegBackward0>) tensor(11419.9600, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11419.958984375
tensor(11419.9600, grad_fn=<NegBackward0>) tensor(11419.9590, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11419.958984375
tensor(11419.9590, grad_fn=<NegBackward0>) tensor(11419.9590, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11419.9580078125
tensor(11419.9590, grad_fn=<NegBackward0>) tensor(11419.9580, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11419.9580078125
tensor(11419.9580, grad_fn=<NegBackward0>) tensor(11419.9580, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11419.9580078125
tensor(11419.9580, grad_fn=<NegBackward0>) tensor(11419.9580, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11419.9560546875
tensor(11419.9580, grad_fn=<NegBackward0>) tensor(11419.9561, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11419.955078125
tensor(11419.9561, grad_fn=<NegBackward0>) tensor(11419.9551, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11419.9560546875
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9561, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11419.9541015625
tensor(11419.9551, grad_fn=<NegBackward0>) tensor(11419.9541, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11419.9560546875
tensor(11419.9541, grad_fn=<NegBackward0>) tensor(11419.9561, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11419.9541015625
tensor(11419.9541, grad_fn=<NegBackward0>) tensor(11419.9541, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11419.9541015625
tensor(11419.9541, grad_fn=<NegBackward0>) tensor(11419.9541, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11419.953125
tensor(11419.9541, grad_fn=<NegBackward0>) tensor(11419.9531, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11419.953125
tensor(11419.9531, grad_fn=<NegBackward0>) tensor(11419.9531, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11419.955078125
tensor(11419.9531, grad_fn=<NegBackward0>) tensor(11419.9551, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11419.9521484375
tensor(11419.9531, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11419.9521484375
tensor(11419.9521, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11419.955078125
tensor(11419.9521, grad_fn=<NegBackward0>) tensor(11419.9551, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11419.951171875
tensor(11419.9521, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11419.951171875
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11419.951171875
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11419.951171875
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11419.953125
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9531, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11419.94921875
tensor(11419.9512, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11419.9501953125
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9502, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11419.9501953125
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9502, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11419.94921875
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11419.9521484375
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11419.9638671875
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9639, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11419.9501953125
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9502, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11419.9482421875
tensor(11419.9492, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11419.94921875
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11419.94921875
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11419.953125
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9531, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11419.9501953125
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9502, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11419.9482421875
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11419.951171875
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9512, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11419.9482421875
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11419.94921875
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9492, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11419.9990234375
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9990, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11419.9482421875
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9482, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11419.9462890625
tensor(11419.9482, grad_fn=<NegBackward0>) tensor(11419.9463, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11419.9501953125
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9502, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11419.947265625
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11419.9560546875
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9561, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11419.9462890625
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9463, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11419.947265625
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11419.947265625
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11419.953125
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9531, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11419.9521484375
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9521, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11419.947265625
tensor(11419.9463, grad_fn=<NegBackward0>) tensor(11419.9473, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.7309, 0.2691],
        [0.2729, 0.7271]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5267, 0.4733], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1991, 0.0973],
         [0.5047, 0.4003]],

        [[0.5446, 0.0996],
         [0.5565, 0.6776]],

        [[0.7153, 0.0996],
         [0.5896, 0.5067]],

        [[0.6546, 0.0995],
         [0.7140, 0.5083]],

        [[0.6460, 0.0847],
         [0.5824, 0.7057]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9841602586080228
[0.9840320165194881, 0.9840320165194881] [0.9841602586080228, 0.9841602586080228] [11419.9765625, 11419.947265625]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11659.122007493004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20126.970703125
inf tensor(20126.9707, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11979.7861328125
tensor(20126.9707, grad_fn=<NegBackward0>) tensor(11979.7861, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11976.12109375
tensor(11979.7861, grad_fn=<NegBackward0>) tensor(11976.1211, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11975.8876953125
tensor(11976.1211, grad_fn=<NegBackward0>) tensor(11975.8877, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11975.80078125
tensor(11975.8877, grad_fn=<NegBackward0>) tensor(11975.8008, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11975.7529296875
tensor(11975.8008, grad_fn=<NegBackward0>) tensor(11975.7529, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11975.7255859375
tensor(11975.7529, grad_fn=<NegBackward0>) tensor(11975.7256, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11975.70703125
tensor(11975.7256, grad_fn=<NegBackward0>) tensor(11975.7070, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11975.6943359375
tensor(11975.7070, grad_fn=<NegBackward0>) tensor(11975.6943, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11975.68359375
tensor(11975.6943, grad_fn=<NegBackward0>) tensor(11975.6836, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11975.6787109375
tensor(11975.6836, grad_fn=<NegBackward0>) tensor(11975.6787, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11975.6728515625
tensor(11975.6787, grad_fn=<NegBackward0>) tensor(11975.6729, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11975.6669921875
tensor(11975.6729, grad_fn=<NegBackward0>) tensor(11975.6670, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11975.6640625
tensor(11975.6670, grad_fn=<NegBackward0>) tensor(11975.6641, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11975.6611328125
tensor(11975.6641, grad_fn=<NegBackward0>) tensor(11975.6611, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11975.658203125
tensor(11975.6611, grad_fn=<NegBackward0>) tensor(11975.6582, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11975.6572265625
tensor(11975.6582, grad_fn=<NegBackward0>) tensor(11975.6572, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11975.6533203125
tensor(11975.6572, grad_fn=<NegBackward0>) tensor(11975.6533, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11975.65234375
tensor(11975.6533, grad_fn=<NegBackward0>) tensor(11975.6523, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11975.65234375
tensor(11975.6523, grad_fn=<NegBackward0>) tensor(11975.6523, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11975.650390625
tensor(11975.6523, grad_fn=<NegBackward0>) tensor(11975.6504, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11975.6494140625
tensor(11975.6504, grad_fn=<NegBackward0>) tensor(11975.6494, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11975.6494140625
tensor(11975.6494, grad_fn=<NegBackward0>) tensor(11975.6494, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11975.6484375
tensor(11975.6494, grad_fn=<NegBackward0>) tensor(11975.6484, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11975.646484375
tensor(11975.6484, grad_fn=<NegBackward0>) tensor(11975.6465, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11975.646484375
tensor(11975.6465, grad_fn=<NegBackward0>) tensor(11975.6465, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11975.6484375
tensor(11975.6465, grad_fn=<NegBackward0>) tensor(11975.6484, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11975.6455078125
tensor(11975.6465, grad_fn=<NegBackward0>) tensor(11975.6455, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11975.6494140625
tensor(11975.6455, grad_fn=<NegBackward0>) tensor(11975.6494, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11975.6455078125
tensor(11975.6455, grad_fn=<NegBackward0>) tensor(11975.6455, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11975.642578125
tensor(11975.6455, grad_fn=<NegBackward0>) tensor(11975.6426, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11975.64453125
tensor(11975.6426, grad_fn=<NegBackward0>) tensor(11975.6445, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11975.6435546875
tensor(11975.6426, grad_fn=<NegBackward0>) tensor(11975.6436, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -11975.642578125
tensor(11975.6426, grad_fn=<NegBackward0>) tensor(11975.6426, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11975.642578125
tensor(11975.6426, grad_fn=<NegBackward0>) tensor(11975.6426, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11975.6435546875
tensor(11975.6426, grad_fn=<NegBackward0>) tensor(11975.6436, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11975.642578125
tensor(11975.6426, grad_fn=<NegBackward0>) tensor(11975.6426, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11975.6416015625
tensor(11975.6426, grad_fn=<NegBackward0>) tensor(11975.6416, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11975.6416015625
tensor(11975.6416, grad_fn=<NegBackward0>) tensor(11975.6416, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11975.642578125
tensor(11975.6416, grad_fn=<NegBackward0>) tensor(11975.6426, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11975.640625
tensor(11975.6416, grad_fn=<NegBackward0>) tensor(11975.6406, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11975.6416015625
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6416, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11975.6416015625
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6416, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11975.6416015625
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6416, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -11975.640625
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6406, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11975.640625
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6406, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11975.6474609375
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6475, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11975.640625
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6406, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11975.6435546875
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6436, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11975.640625
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6406, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11975.638671875
tensor(11975.6406, grad_fn=<NegBackward0>) tensor(11975.6387, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11975.64453125
tensor(11975.6387, grad_fn=<NegBackward0>) tensor(11975.6445, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11975.6396484375
tensor(11975.6387, grad_fn=<NegBackward0>) tensor(11975.6396, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11975.6396484375
tensor(11975.6387, grad_fn=<NegBackward0>) tensor(11975.6396, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11975.6396484375
tensor(11975.6387, grad_fn=<NegBackward0>) tensor(11975.6396, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -11975.6416015625
tensor(11975.6387, grad_fn=<NegBackward0>) tensor(11975.6416, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5500 due to no improvement.
pi: tensor([[0.4862, 0.5138],
        [0.4799, 0.5201]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3356, 0.6644], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3653, 0.0883],
         [0.5649, 0.2652]],

        [[0.6606, 0.1131],
         [0.6352, 0.6333]],

        [[0.7038, 0.0936],
         [0.5958, 0.5585]],

        [[0.5064, 0.0998],
         [0.5734, 0.6439]],

        [[0.5061, 0.1024],
         [0.6682, 0.6361]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 89
Adjusted Rand Index: 0.6044444444444445
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 88
Adjusted Rand Index: 0.5735404496948732
Global Adjusted Rand Index: 0.07656263314026039
Average Adjusted Rand Index: 0.8117574139919878
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25304.513671875
inf tensor(25304.5137, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12431.1533203125
tensor(25304.5137, grad_fn=<NegBackward0>) tensor(12431.1533, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11777.34375
tensor(12431.1533, grad_fn=<NegBackward0>) tensor(11777.3438, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11716.8466796875
tensor(11777.3438, grad_fn=<NegBackward0>) tensor(11716.8467, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11708.681640625
tensor(11716.8467, grad_fn=<NegBackward0>) tensor(11708.6816, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11697.400390625
tensor(11708.6816, grad_fn=<NegBackward0>) tensor(11697.4004, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11688.97265625
tensor(11697.4004, grad_fn=<NegBackward0>) tensor(11688.9727, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11681.431640625
tensor(11688.9727, grad_fn=<NegBackward0>) tensor(11681.4316, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11674.828125
tensor(11681.4316, grad_fn=<NegBackward0>) tensor(11674.8281, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11667.6494140625
tensor(11674.8281, grad_fn=<NegBackward0>) tensor(11667.6494, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11667.58203125
tensor(11667.6494, grad_fn=<NegBackward0>) tensor(11667.5820, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11667.546875
tensor(11667.5820, grad_fn=<NegBackward0>) tensor(11667.5469, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11667.490234375
tensor(11667.5469, grad_fn=<NegBackward0>) tensor(11667.4902, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11657.947265625
tensor(11667.4902, grad_fn=<NegBackward0>) tensor(11657.9473, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11657.9267578125
tensor(11657.9473, grad_fn=<NegBackward0>) tensor(11657.9268, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11657.9091796875
tensor(11657.9268, grad_fn=<NegBackward0>) tensor(11657.9092, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11657.89453125
tensor(11657.9092, grad_fn=<NegBackward0>) tensor(11657.8945, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11657.8798828125
tensor(11657.8945, grad_fn=<NegBackward0>) tensor(11657.8799, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11651.1875
tensor(11657.8799, grad_fn=<NegBackward0>) tensor(11651.1875, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11651.1796875
tensor(11651.1875, grad_fn=<NegBackward0>) tensor(11651.1797, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11651.1748046875
tensor(11651.1797, grad_fn=<NegBackward0>) tensor(11651.1748, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11651.1708984375
tensor(11651.1748, grad_fn=<NegBackward0>) tensor(11651.1709, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11651.1650390625
tensor(11651.1709, grad_fn=<NegBackward0>) tensor(11651.1650, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11651.162109375
tensor(11651.1650, grad_fn=<NegBackward0>) tensor(11651.1621, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11651.158203125
tensor(11651.1621, grad_fn=<NegBackward0>) tensor(11651.1582, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11651.1552734375
tensor(11651.1582, grad_fn=<NegBackward0>) tensor(11651.1553, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11651.1533203125
tensor(11651.1553, grad_fn=<NegBackward0>) tensor(11651.1533, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11651.1494140625
tensor(11651.1533, grad_fn=<NegBackward0>) tensor(11651.1494, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11651.1484375
tensor(11651.1494, grad_fn=<NegBackward0>) tensor(11651.1484, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11651.1435546875
tensor(11651.1484, grad_fn=<NegBackward0>) tensor(11651.1436, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11647.6044921875
tensor(11651.1436, grad_fn=<NegBackward0>) tensor(11647.6045, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11647.6025390625
tensor(11647.6045, grad_fn=<NegBackward0>) tensor(11647.6025, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11647.6015625
tensor(11647.6025, grad_fn=<NegBackward0>) tensor(11647.6016, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11647.599609375
tensor(11647.6016, grad_fn=<NegBackward0>) tensor(11647.5996, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11647.599609375
tensor(11647.5996, grad_fn=<NegBackward0>) tensor(11647.5996, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11647.5986328125
tensor(11647.5996, grad_fn=<NegBackward0>) tensor(11647.5986, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11647.5966796875
tensor(11647.5986, grad_fn=<NegBackward0>) tensor(11647.5967, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11647.5966796875
tensor(11647.5967, grad_fn=<NegBackward0>) tensor(11647.5967, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11647.5947265625
tensor(11647.5967, grad_fn=<NegBackward0>) tensor(11647.5947, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11647.5947265625
tensor(11647.5947, grad_fn=<NegBackward0>) tensor(11647.5947, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11647.59375
tensor(11647.5947, grad_fn=<NegBackward0>) tensor(11647.5938, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11647.5927734375
tensor(11647.5938, grad_fn=<NegBackward0>) tensor(11647.5928, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11647.591796875
tensor(11647.5928, grad_fn=<NegBackward0>) tensor(11647.5918, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11647.591796875
tensor(11647.5918, grad_fn=<NegBackward0>) tensor(11647.5918, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11647.5908203125
tensor(11647.5918, grad_fn=<NegBackward0>) tensor(11647.5908, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11647.5927734375
tensor(11647.5908, grad_fn=<NegBackward0>) tensor(11647.5928, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11647.58984375
tensor(11647.5908, grad_fn=<NegBackward0>) tensor(11647.5898, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11647.58984375
tensor(11647.5898, grad_fn=<NegBackward0>) tensor(11647.5898, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11647.58984375
tensor(11647.5898, grad_fn=<NegBackward0>) tensor(11647.5898, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11647.587890625
tensor(11647.5898, grad_fn=<NegBackward0>) tensor(11647.5879, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11647.587890625
tensor(11647.5879, grad_fn=<NegBackward0>) tensor(11647.5879, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11647.587890625
tensor(11647.5879, grad_fn=<NegBackward0>) tensor(11647.5879, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11647.5888671875
tensor(11647.5879, grad_fn=<NegBackward0>) tensor(11647.5889, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11647.587890625
tensor(11647.5879, grad_fn=<NegBackward0>) tensor(11647.5879, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11647.5869140625
tensor(11647.5879, grad_fn=<NegBackward0>) tensor(11647.5869, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11647.587890625
tensor(11647.5869, grad_fn=<NegBackward0>) tensor(11647.5879, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11647.5859375
tensor(11647.5869, grad_fn=<NegBackward0>) tensor(11647.5859, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11647.5859375
tensor(11647.5859, grad_fn=<NegBackward0>) tensor(11647.5859, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11647.5908203125
tensor(11647.5859, grad_fn=<NegBackward0>) tensor(11647.5908, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11647.5869140625
tensor(11647.5859, grad_fn=<NegBackward0>) tensor(11647.5869, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11647.5859375
tensor(11647.5859, grad_fn=<NegBackward0>) tensor(11647.5859, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11647.5849609375
tensor(11647.5859, grad_fn=<NegBackward0>) tensor(11647.5850, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11647.5849609375
tensor(11647.5850, grad_fn=<NegBackward0>) tensor(11647.5850, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11647.5849609375
tensor(11647.5850, grad_fn=<NegBackward0>) tensor(11647.5850, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11647.583984375
tensor(11647.5850, grad_fn=<NegBackward0>) tensor(11647.5840, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11647.5830078125
tensor(11647.5840, grad_fn=<NegBackward0>) tensor(11647.5830, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11647.583984375
tensor(11647.5830, grad_fn=<NegBackward0>) tensor(11647.5840, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11647.58203125
tensor(11647.5830, grad_fn=<NegBackward0>) tensor(11647.5820, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11647.5576171875
tensor(11647.5820, grad_fn=<NegBackward0>) tensor(11647.5576, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11647.568359375
tensor(11647.5576, grad_fn=<NegBackward0>) tensor(11647.5684, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11647.556640625
tensor(11647.5576, grad_fn=<NegBackward0>) tensor(11647.5566, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11647.560546875
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5605, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11647.556640625
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5566, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11647.5791015625
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5791, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11647.556640625
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5566, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11647.5673828125
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5674, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11647.5576171875
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5576, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11647.5576171875
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5576, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11647.5556640625
tensor(11647.5566, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11647.5595703125
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5596, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11647.5556640625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11647.5625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5625, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11647.5556640625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11647.568359375
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5684, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11647.5576171875
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5576, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11647.609375
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.6094, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11647.5556640625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11647.5634765625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5635, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11647.5556640625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11647.5556640625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11647.5556640625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11647.5556640625
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11647.5546875
tensor(11647.5557, grad_fn=<NegBackward0>) tensor(11647.5547, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11647.556640625
tensor(11647.5547, grad_fn=<NegBackward0>) tensor(11647.5566, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11647.5546875
tensor(11647.5547, grad_fn=<NegBackward0>) tensor(11647.5547, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11647.556640625
tensor(11647.5547, grad_fn=<NegBackward0>) tensor(11647.5566, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11647.556640625
tensor(11647.5547, grad_fn=<NegBackward0>) tensor(11647.5566, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11647.5556640625
tensor(11647.5547, grad_fn=<NegBackward0>) tensor(11647.5557, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11647.5546875
tensor(11647.5547, grad_fn=<NegBackward0>) tensor(11647.5547, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11647.5595703125
tensor(11647.5547, grad_fn=<NegBackward0>) tensor(11647.5596, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7686, 0.2314],
        [0.2193, 0.7807]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5500, 0.4500], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4141, 0.0929],
         [0.6792, 0.1945]],

        [[0.6069, 0.1158],
         [0.6886, 0.6178]],

        [[0.6127, 0.0936],
         [0.5720, 0.5490]],

        [[0.6471, 0.1015],
         [0.6815, 0.6610]],

        [[0.7189, 0.1093],
         [0.6740, 0.5145]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999860917265
Average Adjusted Rand Index: 0.9919995611635631
[0.07656263314026039, 0.9919999860917265] [0.8117574139919878, 0.9919995611635631] [11975.6416015625, 11647.5673828125]
-------------------------------------
This iteration is 34
True Objective function: Loss = -11619.81792712653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25013.208984375
inf tensor(25013.2090, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11963.5048828125
tensor(25013.2090, grad_fn=<NegBackward0>) tensor(11963.5049, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11960.5439453125
tensor(11963.5049, grad_fn=<NegBackward0>) tensor(11960.5439, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11960.3095703125
tensor(11960.5439, grad_fn=<NegBackward0>) tensor(11960.3096, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11960.208984375
tensor(11960.3096, grad_fn=<NegBackward0>) tensor(11960.2090, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11960.154296875
tensor(11960.2090, grad_fn=<NegBackward0>) tensor(11960.1543, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11960.1201171875
tensor(11960.1543, grad_fn=<NegBackward0>) tensor(11960.1201, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11960.0986328125
tensor(11960.1201, grad_fn=<NegBackward0>) tensor(11960.0986, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11960.08203125
tensor(11960.0986, grad_fn=<NegBackward0>) tensor(11960.0820, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11960.0673828125
tensor(11960.0820, grad_fn=<NegBackward0>) tensor(11960.0674, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11960.0556640625
tensor(11960.0674, grad_fn=<NegBackward0>) tensor(11960.0557, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11960.041015625
tensor(11960.0557, grad_fn=<NegBackward0>) tensor(11960.0410, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11959.9638671875
tensor(11960.0410, grad_fn=<NegBackward0>) tensor(11959.9639, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11880.8974609375
tensor(11959.9639, grad_fn=<NegBackward0>) tensor(11880.8975, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11879.78125
tensor(11880.8975, grad_fn=<NegBackward0>) tensor(11879.7812, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11873.583984375
tensor(11879.7812, grad_fn=<NegBackward0>) tensor(11873.5840, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11870.4609375
tensor(11873.5840, grad_fn=<NegBackward0>) tensor(11870.4609, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11870.2451171875
tensor(11870.4609, grad_fn=<NegBackward0>) tensor(11870.2451, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11870.22265625
tensor(11870.2451, grad_fn=<NegBackward0>) tensor(11870.2227, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11870.216796875
tensor(11870.2227, grad_fn=<NegBackward0>) tensor(11870.2168, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11870.212890625
tensor(11870.2168, grad_fn=<NegBackward0>) tensor(11870.2129, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11870.208984375
tensor(11870.2129, grad_fn=<NegBackward0>) tensor(11870.2090, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11870.20703125
tensor(11870.2090, grad_fn=<NegBackward0>) tensor(11870.2070, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11870.205078125
tensor(11870.2070, grad_fn=<NegBackward0>) tensor(11870.2051, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11870.2041015625
tensor(11870.2051, grad_fn=<NegBackward0>) tensor(11870.2041, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11870.2041015625
tensor(11870.2041, grad_fn=<NegBackward0>) tensor(11870.2041, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11870.203125
tensor(11870.2041, grad_fn=<NegBackward0>) tensor(11870.2031, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11870.201171875
tensor(11870.2031, grad_fn=<NegBackward0>) tensor(11870.2012, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11870.1982421875
tensor(11870.2012, grad_fn=<NegBackward0>) tensor(11870.1982, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11870.1953125
tensor(11870.1982, grad_fn=<NegBackward0>) tensor(11870.1953, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11870.1943359375
tensor(11870.1953, grad_fn=<NegBackward0>) tensor(11870.1943, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11870.1923828125
tensor(11870.1943, grad_fn=<NegBackward0>) tensor(11870.1924, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11870.19140625
tensor(11870.1924, grad_fn=<NegBackward0>) tensor(11870.1914, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11870.1904296875
tensor(11870.1914, grad_fn=<NegBackward0>) tensor(11870.1904, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11870.1884765625
tensor(11870.1904, grad_fn=<NegBackward0>) tensor(11870.1885, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11870.1767578125
tensor(11870.1885, grad_fn=<NegBackward0>) tensor(11870.1768, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11870.173828125
tensor(11870.1768, grad_fn=<NegBackward0>) tensor(11870.1738, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11870.1728515625
tensor(11870.1738, grad_fn=<NegBackward0>) tensor(11870.1729, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11870.173828125
tensor(11870.1729, grad_fn=<NegBackward0>) tensor(11870.1738, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11870.1748046875
tensor(11870.1729, grad_fn=<NegBackward0>) tensor(11870.1748, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11870.173828125
tensor(11870.1729, grad_fn=<NegBackward0>) tensor(11870.1738, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -11870.1748046875
tensor(11870.1729, grad_fn=<NegBackward0>) tensor(11870.1748, grad_fn=<NegBackward0>)
4
Iteration 4200: Loss = -11870.171875
tensor(11870.1729, grad_fn=<NegBackward0>) tensor(11870.1719, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11870.169921875
tensor(11870.1719, grad_fn=<NegBackward0>) tensor(11870.1699, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11870.1669921875
tensor(11870.1699, grad_fn=<NegBackward0>) tensor(11870.1670, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11870.1689453125
tensor(11870.1670, grad_fn=<NegBackward0>) tensor(11870.1689, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11870.1748046875
tensor(11870.1670, grad_fn=<NegBackward0>) tensor(11870.1748, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11870.1640625
tensor(11870.1670, grad_fn=<NegBackward0>) tensor(11870.1641, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11870.162109375
tensor(11870.1641, grad_fn=<NegBackward0>) tensor(11870.1621, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11870.16796875
tensor(11870.1621, grad_fn=<NegBackward0>) tensor(11870.1680, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11870.1650390625
tensor(11870.1621, grad_fn=<NegBackward0>) tensor(11870.1650, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11870.1611328125
tensor(11870.1621, grad_fn=<NegBackward0>) tensor(11870.1611, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11870.1669921875
tensor(11870.1611, grad_fn=<NegBackward0>) tensor(11870.1670, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11870.158203125
tensor(11870.1611, grad_fn=<NegBackward0>) tensor(11870.1582, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11870.18359375
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1836, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11870.158203125
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1582, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11870.1591796875
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1592, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11870.16015625
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1602, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11870.158203125
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1582, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11870.158203125
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1582, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11870.1611328125
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1611, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11870.173828125
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1738, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11870.1591796875
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1592, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11870.16796875
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1680, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11870.166015625
tensor(11870.1582, grad_fn=<NegBackward0>) tensor(11870.1660, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[0.3589, 0.6411],
        [0.6005, 0.3995]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4184, 0.5816], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3428, 0.0956],
         [0.5098, 0.2704]],

        [[0.7052, 0.0950],
         [0.6790, 0.6733]],

        [[0.6731, 0.0939],
         [0.6314, 0.7132]],

        [[0.5650, 0.0994],
         [0.5310, 0.5297]],

        [[0.7162, 0.0993],
         [0.5582, 0.6268]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 87
Adjusted Rand Index: 0.5433058572066954
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.05571109543284218
Average Adjusted Rand Index: 0.8531443502362099
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24433.341796875
inf tensor(24433.3418, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12401.0283203125
tensor(24433.3418, grad_fn=<NegBackward0>) tensor(12401.0283, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11872.7138671875
tensor(12401.0283, grad_fn=<NegBackward0>) tensor(11872.7139, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11734.58203125
tensor(11872.7139, grad_fn=<NegBackward0>) tensor(11734.5820, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11694.1884765625
tensor(11734.5820, grad_fn=<NegBackward0>) tensor(11694.1885, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11657.9296875
tensor(11694.1885, grad_fn=<NegBackward0>) tensor(11657.9297, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11642.5380859375
tensor(11657.9297, grad_fn=<NegBackward0>) tensor(11642.5381, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11635.4736328125
tensor(11642.5381, grad_fn=<NegBackward0>) tensor(11635.4736, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11635.3671875
tensor(11635.4736, grad_fn=<NegBackward0>) tensor(11635.3672, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11635.2900390625
tensor(11635.3672, grad_fn=<NegBackward0>) tensor(11635.2900, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11625.1298828125
tensor(11635.2900, grad_fn=<NegBackward0>) tensor(11625.1299, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11623.5927734375
tensor(11625.1299, grad_fn=<NegBackward0>) tensor(11623.5928, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11623.3984375
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 35%|███▌      | 35/100 [8:45:53<15:57:00, 883.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 36%|███▌      | 36/100 [9:02:26<16:17:39, 916.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 37%|███▋      | 37/100 [9:15:29<15:20:04, 876.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 38%|███▊      | 38/100 [9:31:01<15:22:57, 893.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 39%|███▉      | 39/100 [9:43:58<14:32:31, 858.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 40%|████      | 40/100 [9:58:51<14:28:35, 868.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 41%|████      | 41/100 [10:09:58<13:14:41, 808.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 42%|████▏     | 42/100 [10:24:49<13:25:19, 833.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 43%|████▎     | 43/100 [10:41:47<14:04:06, 888.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 44%|████▍     | 44/100 [10:55:00<13:22:38, 859.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 45%|████▌     | 45/100 [11:06:06<12:14:47, 801.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 46%|████▌     | 46/100 [11:19:40<12:04:50, 805.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 47%|████▋     | 47/100 [11:37:27<13:00:48, 883.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 48%|████▊     | 48/100 [11:49:01<11:56:35, 826.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 49%|████▉     | 49/100 [12:06:09<12:34:05, 887.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 50%|█████     | 50/100 [12:20:26<12:11:44, 878.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 51%|█████     | 51/100 [12:36:01<12:11:00, 895.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 52%|█████▏    | 52/100 [12:53:12<12:28:53, 936.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 53%|█████▎    | 53/100 [13:07:09<11:49:53, 906.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 54%|█████▍    | 54/100 [13:19:32<10:57:19, 857.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 55%|█████▌    | 55/100 [13:35:47<11:09:21, 892.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 56%|█████▌    | 56/100 [13:51:46<11:09:11, 912.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 57%|█████▋    | 57/100 [14:05:38<10:36:45, 888.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 58%|█████▊    | 58/100 [14:22:17<10:45:02, 921.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 59%|█████▉    | 59/100 [14:39:46<10:55:53, 959.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 60%|██████    | 60/100 [14:55:02<10:31:07, 946.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 61%|██████    | 61/100 [15:11:40<10:25:18, 962.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 62%|██████▏   | 62/100 [15:24:18<9:30:30, 900.80s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 63%|██████▎   | 63/100 [15:40:31<9:28:56, 922.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 64%|██████▍   | 64/100 [15:55:29<9:09:07, 915.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 65%|██████▌   | 65/100 [16:13:23<9:21:39, 962.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 66%|██████▌   | 66/100 [16:30:07<9:12:34, 975.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 67%|██████▋   | 67/100 [16:45:18<8:45:38, 955.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 68%|██████▊   | 68/100 [17:00:54<8:26:37, 949.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 69%|██████▉   | 69/100 [17:14:53<7:53:34, 916.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
tensor(11623.5928, grad_fn=<NegBackward0>) tensor(11623.3984, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11623.3740234375
tensor(11623.3984, grad_fn=<NegBackward0>) tensor(11623.3740, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11623.353515625
tensor(11623.3740, grad_fn=<NegBackward0>) tensor(11623.3535, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11623.3359375
tensor(11623.3535, grad_fn=<NegBackward0>) tensor(11623.3359, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11623.32421875
tensor(11623.3359, grad_fn=<NegBackward0>) tensor(11623.3242, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11623.3154296875
tensor(11623.3242, grad_fn=<NegBackward0>) tensor(11623.3154, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11623.3056640625
tensor(11623.3154, grad_fn=<NegBackward0>) tensor(11623.3057, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11623.296875
tensor(11623.3057, grad_fn=<NegBackward0>) tensor(11623.2969, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11623.287109375
tensor(11623.2969, grad_fn=<NegBackward0>) tensor(11623.2871, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11623.28125
tensor(11623.2871, grad_fn=<NegBackward0>) tensor(11623.2812, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11623.275390625
tensor(11623.2812, grad_fn=<NegBackward0>) tensor(11623.2754, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11623.271484375
tensor(11623.2754, grad_fn=<NegBackward0>) tensor(11623.2715, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11623.267578125
tensor(11623.2715, grad_fn=<NegBackward0>) tensor(11623.2676, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11623.2705078125
tensor(11623.2676, grad_fn=<NegBackward0>) tensor(11623.2705, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11616.2734375
tensor(11623.2676, grad_fn=<NegBackward0>) tensor(11616.2734, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11616.248046875
tensor(11616.2734, grad_fn=<NegBackward0>) tensor(11616.2480, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11616.24609375
tensor(11616.2480, grad_fn=<NegBackward0>) tensor(11616.2461, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11616.244140625
tensor(11616.2461, grad_fn=<NegBackward0>) tensor(11616.2441, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11616.2412109375
tensor(11616.2441, grad_fn=<NegBackward0>) tensor(11616.2412, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11616.2392578125
tensor(11616.2412, grad_fn=<NegBackward0>) tensor(11616.2393, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11616.2353515625
tensor(11616.2393, grad_fn=<NegBackward0>) tensor(11616.2354, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11616.203125
tensor(11616.2354, grad_fn=<NegBackward0>) tensor(11616.2031, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11616.2021484375
tensor(11616.2031, grad_fn=<NegBackward0>) tensor(11616.2021, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11616.2001953125
tensor(11616.2021, grad_fn=<NegBackward0>) tensor(11616.2002, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11616.2060546875
tensor(11616.2002, grad_fn=<NegBackward0>) tensor(11616.2061, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11616.19921875
tensor(11616.2002, grad_fn=<NegBackward0>) tensor(11616.1992, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11616.197265625
tensor(11616.1992, grad_fn=<NegBackward0>) tensor(11616.1973, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11616.1962890625
tensor(11616.1973, grad_fn=<NegBackward0>) tensor(11616.1963, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11616.1953125
tensor(11616.1963, grad_fn=<NegBackward0>) tensor(11616.1953, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11616.2041015625
tensor(11616.1953, grad_fn=<NegBackward0>) tensor(11616.2041, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11616.1943359375
tensor(11616.1953, grad_fn=<NegBackward0>) tensor(11616.1943, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11616.1923828125
tensor(11616.1943, grad_fn=<NegBackward0>) tensor(11616.1924, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11616.1923828125
tensor(11616.1924, grad_fn=<NegBackward0>) tensor(11616.1924, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11616.1923828125
tensor(11616.1924, grad_fn=<NegBackward0>) tensor(11616.1924, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11616.2041015625
tensor(11616.1924, grad_fn=<NegBackward0>) tensor(11616.2041, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11616.19140625
tensor(11616.1924, grad_fn=<NegBackward0>) tensor(11616.1914, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11616.1904296875
tensor(11616.1914, grad_fn=<NegBackward0>) tensor(11616.1904, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11616.19140625
tensor(11616.1904, grad_fn=<NegBackward0>) tensor(11616.1914, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11616.1904296875
tensor(11616.1904, grad_fn=<NegBackward0>) tensor(11616.1904, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11616.1904296875
tensor(11616.1904, grad_fn=<NegBackward0>) tensor(11616.1904, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11616.189453125
tensor(11616.1904, grad_fn=<NegBackward0>) tensor(11616.1895, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11616.19140625
tensor(11616.1895, grad_fn=<NegBackward0>) tensor(11616.1914, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11616.1875
tensor(11616.1895, grad_fn=<NegBackward0>) tensor(11616.1875, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11616.1884765625
tensor(11616.1875, grad_fn=<NegBackward0>) tensor(11616.1885, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11616.1923828125
tensor(11616.1875, grad_fn=<NegBackward0>) tensor(11616.1924, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11616.1865234375
tensor(11616.1875, grad_fn=<NegBackward0>) tensor(11616.1865, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11616.1865234375
tensor(11616.1865, grad_fn=<NegBackward0>) tensor(11616.1865, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11616.185546875
tensor(11616.1865, grad_fn=<NegBackward0>) tensor(11616.1855, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11616.1875
tensor(11616.1855, grad_fn=<NegBackward0>) tensor(11616.1875, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11616.1865234375
tensor(11616.1855, grad_fn=<NegBackward0>) tensor(11616.1865, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11616.1845703125
tensor(11616.1855, grad_fn=<NegBackward0>) tensor(11616.1846, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11616.18359375
tensor(11616.1846, grad_fn=<NegBackward0>) tensor(11616.1836, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11616.1845703125
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1846, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11616.185546875
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1855, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11616.1845703125
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1846, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11616.185546875
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1855, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11616.18359375
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1836, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11616.1865234375
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1865, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11616.18359375
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1836, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11616.1826171875
tensor(11616.1836, grad_fn=<NegBackward0>) tensor(11616.1826, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11616.1904296875
tensor(11616.1826, grad_fn=<NegBackward0>) tensor(11616.1904, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11616.18359375
tensor(11616.1826, grad_fn=<NegBackward0>) tensor(11616.1836, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11616.197265625
tensor(11616.1826, grad_fn=<NegBackward0>) tensor(11616.1973, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11616.1845703125
tensor(11616.1826, grad_fn=<NegBackward0>) tensor(11616.1846, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11616.1826171875
tensor(11616.1826, grad_fn=<NegBackward0>) tensor(11616.1826, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11616.181640625
tensor(11616.1826, grad_fn=<NegBackward0>) tensor(11616.1816, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11616.181640625
tensor(11616.1816, grad_fn=<NegBackward0>) tensor(11616.1816, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11616.1845703125
tensor(11616.1816, grad_fn=<NegBackward0>) tensor(11616.1846, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11616.18359375
tensor(11616.1816, grad_fn=<NegBackward0>) tensor(11616.1836, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11616.1826171875
tensor(11616.1816, grad_fn=<NegBackward0>) tensor(11616.1826, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11616.1904296875
tensor(11616.1816, grad_fn=<NegBackward0>) tensor(11616.1904, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11616.1826171875
tensor(11616.1816, grad_fn=<NegBackward0>) tensor(11616.1826, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.7335, 0.2665],
        [0.2858, 0.7142]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5495, 0.4505], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3921, 0.0962],
         [0.6842, 0.1962]],

        [[0.5883, 0.0955],
         [0.6152, 0.5446]],

        [[0.7163, 0.1007],
         [0.7161, 0.6582]],

        [[0.6659, 0.1024],
         [0.5085, 0.7066]],

        [[0.6742, 0.1016],
         [0.7045, 0.7271]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[0.05571109543284218, 1.0] [0.8531443502362099, 1.0] [11870.166015625, 11616.1826171875]
-------------------------------------
This iteration is 35
True Objective function: Loss = -11411.756547355853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22852.138671875
inf tensor(22852.1387, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12148.904296875
tensor(22852.1387, grad_fn=<NegBackward0>) tensor(12148.9043, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12140.78125
tensor(12148.9043, grad_fn=<NegBackward0>) tensor(12140.7812, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12028.3740234375
tensor(12140.7812, grad_fn=<NegBackward0>) tensor(12028.3740, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11720.2060546875
tensor(12028.3740, grad_fn=<NegBackward0>) tensor(11720.2061, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11683.12890625
tensor(11720.2061, grad_fn=<NegBackward0>) tensor(11683.1289, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11673.3291015625
tensor(11683.1289, grad_fn=<NegBackward0>) tensor(11673.3291, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11660.96875
tensor(11673.3291, grad_fn=<NegBackward0>) tensor(11660.9688, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11652.7978515625
tensor(11660.9688, grad_fn=<NegBackward0>) tensor(11652.7979, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11641.4208984375
tensor(11652.7979, grad_fn=<NegBackward0>) tensor(11641.4209, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11625.8046875
tensor(11641.4209, grad_fn=<NegBackward0>) tensor(11625.8047, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11625.6357421875
tensor(11625.8047, grad_fn=<NegBackward0>) tensor(11625.6357, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11619.37890625
tensor(11625.6357, grad_fn=<NegBackward0>) tensor(11619.3789, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11608.62109375
tensor(11619.3789, grad_fn=<NegBackward0>) tensor(11608.6211, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11608.5478515625
tensor(11608.6211, grad_fn=<NegBackward0>) tensor(11608.5479, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11608.509765625
tensor(11608.5479, grad_fn=<NegBackward0>) tensor(11608.5098, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11601.6708984375
tensor(11608.5098, grad_fn=<NegBackward0>) tensor(11601.6709, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11601.560546875
tensor(11601.6709, grad_fn=<NegBackward0>) tensor(11601.5605, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11601.5419921875
tensor(11601.5605, grad_fn=<NegBackward0>) tensor(11601.5420, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11601.525390625
tensor(11601.5420, grad_fn=<NegBackward0>) tensor(11601.5254, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11601.5126953125
tensor(11601.5254, grad_fn=<NegBackward0>) tensor(11601.5127, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11601.5009765625
tensor(11601.5127, grad_fn=<NegBackward0>) tensor(11601.5010, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11601.4228515625
tensor(11601.5010, grad_fn=<NegBackward0>) tensor(11601.4229, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11599.1728515625
tensor(11601.4229, grad_fn=<NegBackward0>) tensor(11599.1729, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11599.1474609375
tensor(11599.1729, grad_fn=<NegBackward0>) tensor(11599.1475, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11599.0888671875
tensor(11599.1475, grad_fn=<NegBackward0>) tensor(11599.0889, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11599.08203125
tensor(11599.0889, grad_fn=<NegBackward0>) tensor(11599.0820, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11599.076171875
tensor(11599.0820, grad_fn=<NegBackward0>) tensor(11599.0762, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11599.0703125
tensor(11599.0762, grad_fn=<NegBackward0>) tensor(11599.0703, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11599.06640625
tensor(11599.0703, grad_fn=<NegBackward0>) tensor(11599.0664, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11591.806640625
tensor(11599.0664, grad_fn=<NegBackward0>) tensor(11591.8066, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11591.7626953125
tensor(11591.8066, grad_fn=<NegBackward0>) tensor(11591.7627, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11591.7578125
tensor(11591.7627, grad_fn=<NegBackward0>) tensor(11591.7578, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11591.75390625
tensor(11591.7578, grad_fn=<NegBackward0>) tensor(11591.7539, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11591.75
tensor(11591.7539, grad_fn=<NegBackward0>) tensor(11591.7500, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11591.5546875
tensor(11591.7500, grad_fn=<NegBackward0>) tensor(11591.5547, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11591.552734375
tensor(11591.5547, grad_fn=<NegBackward0>) tensor(11591.5527, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11591.548828125
tensor(11591.5527, grad_fn=<NegBackward0>) tensor(11591.5488, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11591.546875
tensor(11591.5488, grad_fn=<NegBackward0>) tensor(11591.5469, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11591.5439453125
tensor(11591.5469, grad_fn=<NegBackward0>) tensor(11591.5439, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11591.54296875
tensor(11591.5439, grad_fn=<NegBackward0>) tensor(11591.5430, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11577.111328125
tensor(11591.5430, grad_fn=<NegBackward0>) tensor(11577.1113, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11577.06640625
tensor(11577.1113, grad_fn=<NegBackward0>) tensor(11577.0664, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11577.0634765625
tensor(11577.0664, grad_fn=<NegBackward0>) tensor(11577.0635, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11577.0625
tensor(11577.0635, grad_fn=<NegBackward0>) tensor(11577.0625, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11577.0625
tensor(11577.0625, grad_fn=<NegBackward0>) tensor(11577.0625, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11567.30078125
tensor(11577.0625, grad_fn=<NegBackward0>) tensor(11567.3008, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11567.294921875
tensor(11567.3008, grad_fn=<NegBackward0>) tensor(11567.2949, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11567.2890625
tensor(11567.2949, grad_fn=<NegBackward0>) tensor(11567.2891, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11567.287109375
tensor(11567.2891, grad_fn=<NegBackward0>) tensor(11567.2871, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11567.2880859375
tensor(11567.2871, grad_fn=<NegBackward0>) tensor(11567.2881, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11567.2919921875
tensor(11567.2871, grad_fn=<NegBackward0>) tensor(11567.2920, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11567.294921875
tensor(11567.2871, grad_fn=<NegBackward0>) tensor(11567.2949, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11567.2841796875
tensor(11567.2871, grad_fn=<NegBackward0>) tensor(11567.2842, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11567.2822265625
tensor(11567.2842, grad_fn=<NegBackward0>) tensor(11567.2822, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11567.2841796875
tensor(11567.2822, grad_fn=<NegBackward0>) tensor(11567.2842, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11567.2822265625
tensor(11567.2822, grad_fn=<NegBackward0>) tensor(11567.2822, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11567.2841796875
tensor(11567.2822, grad_fn=<NegBackward0>) tensor(11567.2842, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11567.28125
tensor(11567.2822, grad_fn=<NegBackward0>) tensor(11567.2812, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11567.283203125
tensor(11567.2812, grad_fn=<NegBackward0>) tensor(11567.2832, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11567.2802734375
tensor(11567.2812, grad_fn=<NegBackward0>) tensor(11567.2803, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11567.2783203125
tensor(11567.2803, grad_fn=<NegBackward0>) tensor(11567.2783, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11567.2783203125
tensor(11567.2783, grad_fn=<NegBackward0>) tensor(11567.2783, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11567.2802734375
tensor(11567.2783, grad_fn=<NegBackward0>) tensor(11567.2803, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11567.2783203125
tensor(11567.2783, grad_fn=<NegBackward0>) tensor(11567.2783, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11567.30078125
tensor(11567.2783, grad_fn=<NegBackward0>) tensor(11567.3008, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11567.2783203125
tensor(11567.2783, grad_fn=<NegBackward0>) tensor(11567.2783, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11567.27734375
tensor(11567.2783, grad_fn=<NegBackward0>) tensor(11567.2773, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11567.2783203125
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2783, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11567.27734375
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2773, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11567.27734375
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2773, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11567.2890625
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2891, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11567.27734375
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2773, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11567.279296875
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2793, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11567.279296875
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2793, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11567.27734375
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2773, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11567.2763671875
tensor(11567.2773, grad_fn=<NegBackward0>) tensor(11567.2764, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11567.2763671875
tensor(11567.2764, grad_fn=<NegBackward0>) tensor(11567.2764, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11567.3046875
tensor(11567.2764, grad_fn=<NegBackward0>) tensor(11567.3047, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11567.27734375
tensor(11567.2764, grad_fn=<NegBackward0>) tensor(11567.2773, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11567.28515625
tensor(11567.2764, grad_fn=<NegBackward0>) tensor(11567.2852, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11567.2734375
tensor(11567.2764, grad_fn=<NegBackward0>) tensor(11567.2734, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11567.2783203125
tensor(11567.2734, grad_fn=<NegBackward0>) tensor(11567.2783, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11567.2646484375
tensor(11567.2734, grad_fn=<NegBackward0>) tensor(11567.2646, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11567.2646484375
tensor(11567.2646, grad_fn=<NegBackward0>) tensor(11567.2646, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11567.271484375
tensor(11567.2646, grad_fn=<NegBackward0>) tensor(11567.2715, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11567.263671875
tensor(11567.2646, grad_fn=<NegBackward0>) tensor(11567.2637, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11567.2646484375
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.2646, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11567.265625
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.2656, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11567.263671875
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.2637, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11567.337890625
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.3379, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11567.263671875
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.2637, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11567.263671875
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.2637, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11567.2646484375
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.2646, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11567.4384765625
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.4385, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11567.259765625
tensor(11567.2637, grad_fn=<NegBackward0>) tensor(11567.2598, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11567.267578125
tensor(11567.2598, grad_fn=<NegBackward0>) tensor(11567.2676, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11567.259765625
tensor(11567.2598, grad_fn=<NegBackward0>) tensor(11567.2598, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11567.2626953125
tensor(11567.2598, grad_fn=<NegBackward0>) tensor(11567.2627, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11567.279296875
tensor(11567.2598, grad_fn=<NegBackward0>) tensor(11567.2793, grad_fn=<NegBackward0>)
2
pi: tensor([[0.5704, 0.4296],
        [0.4926, 0.5074]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5037, 0.4963], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2218, 0.1057],
         [0.6180, 0.3723]],

        [[0.5619, 0.0883],
         [0.5633, 0.5257]],

        [[0.5246, 0.0988],
         [0.7139, 0.5917]],

        [[0.5477, 0.1012],
         [0.5188, 0.6799]],

        [[0.5551, 0.0946],
         [0.5488, 0.6145]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 17
Adjusted Rand Index: 0.42980069291406464
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.445114585008359
Average Adjusted Rand Index: 0.8859601385828129
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24116.646484375
inf tensor(24116.6465, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12142.6337890625
tensor(24116.6465, grad_fn=<NegBackward0>) tensor(12142.6338, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11995.36328125
tensor(12142.6338, grad_fn=<NegBackward0>) tensor(11995.3633, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11692.3232421875
tensor(11995.3633, grad_fn=<NegBackward0>) tensor(11692.3232, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11675.03515625
tensor(11692.3232, grad_fn=<NegBackward0>) tensor(11675.0352, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11667.4375
tensor(11675.0352, grad_fn=<NegBackward0>) tensor(11667.4375, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11667.2958984375
tensor(11667.4375, grad_fn=<NegBackward0>) tensor(11667.2959, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11667.234375
tensor(11667.2959, grad_fn=<NegBackward0>) tensor(11667.2344, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11667.1943359375
tensor(11667.2344, grad_fn=<NegBackward0>) tensor(11667.1943, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11667.1669921875
tensor(11667.1943, grad_fn=<NegBackward0>) tensor(11667.1670, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11667.142578125
tensor(11667.1670, grad_fn=<NegBackward0>) tensor(11667.1426, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11666.1474609375
tensor(11667.1426, grad_fn=<NegBackward0>) tensor(11666.1475, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11650.07421875
tensor(11666.1475, grad_fn=<NegBackward0>) tensor(11650.0742, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11649.9755859375
tensor(11650.0742, grad_fn=<NegBackward0>) tensor(11649.9756, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11649.96484375
tensor(11649.9756, grad_fn=<NegBackward0>) tensor(11649.9648, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11644.1162109375
tensor(11649.9648, grad_fn=<NegBackward0>) tensor(11644.1162, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11644.107421875
tensor(11644.1162, grad_fn=<NegBackward0>) tensor(11644.1074, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11629.8642578125
tensor(11644.1074, grad_fn=<NegBackward0>) tensor(11629.8643, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11629.798828125
tensor(11629.8643, grad_fn=<NegBackward0>) tensor(11629.7988, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11619.466796875
tensor(11629.7988, grad_fn=<NegBackward0>) tensor(11619.4668, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11619.45703125
tensor(11619.4668, grad_fn=<NegBackward0>) tensor(11619.4570, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11613.0078125
tensor(11619.4570, grad_fn=<NegBackward0>) tensor(11613.0078, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11613.0
tensor(11613.0078, grad_fn=<NegBackward0>) tensor(11613., grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11613.0
tensor(11613., grad_fn=<NegBackward0>) tensor(11613., grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11612.9951171875
tensor(11613., grad_fn=<NegBackward0>) tensor(11612.9951, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11603.744140625
tensor(11612.9951, grad_fn=<NegBackward0>) tensor(11603.7441, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11603.73828125
tensor(11603.7441, grad_fn=<NegBackward0>) tensor(11603.7383, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11603.736328125
tensor(11603.7383, grad_fn=<NegBackward0>) tensor(11603.7363, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11603.732421875
tensor(11603.7363, grad_fn=<NegBackward0>) tensor(11603.7324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11603.7333984375
tensor(11603.7324, grad_fn=<NegBackward0>) tensor(11603.7334, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11603.7333984375
tensor(11603.7324, grad_fn=<NegBackward0>) tensor(11603.7334, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -11594.4931640625
tensor(11603.7324, grad_fn=<NegBackward0>) tensor(11594.4932, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11594.4794921875
tensor(11594.4932, grad_fn=<NegBackward0>) tensor(11594.4795, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11594.4755859375
tensor(11594.4795, grad_fn=<NegBackward0>) tensor(11594.4756, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11594.4755859375
tensor(11594.4756, grad_fn=<NegBackward0>) tensor(11594.4756, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11594.474609375
tensor(11594.4756, grad_fn=<NegBackward0>) tensor(11594.4746, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11594.47265625
tensor(11594.4746, grad_fn=<NegBackward0>) tensor(11594.4727, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11594.4736328125
tensor(11594.4727, grad_fn=<NegBackward0>) tensor(11594.4736, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11594.474609375
tensor(11594.4727, grad_fn=<NegBackward0>) tensor(11594.4746, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11594.474609375
tensor(11594.4727, grad_fn=<NegBackward0>) tensor(11594.4746, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -11594.47265625
tensor(11594.4727, grad_fn=<NegBackward0>) tensor(11594.4727, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11594.4736328125
tensor(11594.4727, grad_fn=<NegBackward0>) tensor(11594.4736, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11594.47265625
tensor(11594.4727, grad_fn=<NegBackward0>) tensor(11594.4727, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11594.4716796875
tensor(11594.4727, grad_fn=<NegBackward0>) tensor(11594.4717, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11594.4716796875
tensor(11594.4717, grad_fn=<NegBackward0>) tensor(11594.4717, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11594.470703125
tensor(11594.4717, grad_fn=<NegBackward0>) tensor(11594.4707, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11594.470703125
tensor(11594.4707, grad_fn=<NegBackward0>) tensor(11594.4707, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11594.4697265625
tensor(11594.4707, grad_fn=<NegBackward0>) tensor(11594.4697, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11594.46875
tensor(11594.4697, grad_fn=<NegBackward0>) tensor(11594.4688, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11592.8115234375
tensor(11594.4688, grad_fn=<NegBackward0>) tensor(11592.8115, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11592.1142578125
tensor(11592.8115, grad_fn=<NegBackward0>) tensor(11592.1143, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11592.11328125
tensor(11592.1143, grad_fn=<NegBackward0>) tensor(11592.1133, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11592.1123046875
tensor(11592.1133, grad_fn=<NegBackward0>) tensor(11592.1123, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11592.11328125
tensor(11592.1123, grad_fn=<NegBackward0>) tensor(11592.1133, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11592.1123046875
tensor(11592.1123, grad_fn=<NegBackward0>) tensor(11592.1123, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11592.1123046875
tensor(11592.1123, grad_fn=<NegBackward0>) tensor(11592.1123, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11592.1123046875
tensor(11592.1123, grad_fn=<NegBackward0>) tensor(11592.1123, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11592.11328125
tensor(11592.1123, grad_fn=<NegBackward0>) tensor(11592.1133, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11592.111328125
tensor(11592.1123, grad_fn=<NegBackward0>) tensor(11592.1113, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11592.111328125
tensor(11592.1113, grad_fn=<NegBackward0>) tensor(11592.1113, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11592.1103515625
tensor(11592.1113, grad_fn=<NegBackward0>) tensor(11592.1104, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11592.1123046875
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1123, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11592.111328125
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1113, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11592.1171875
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1172, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11592.1123046875
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1123, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11592.1103515625
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1104, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11592.1123046875
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1123, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11592.1103515625
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1104, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11592.109375
tensor(11592.1104, grad_fn=<NegBackward0>) tensor(11592.1094, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11592.07421875
tensor(11592.1094, grad_fn=<NegBackward0>) tensor(11592.0742, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11592.0751953125
tensor(11592.0742, grad_fn=<NegBackward0>) tensor(11592.0752, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11592.0830078125
tensor(11592.0742, grad_fn=<NegBackward0>) tensor(11592.0830, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11592.0751953125
tensor(11592.0742, grad_fn=<NegBackward0>) tensor(11592.0752, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11592.07421875
tensor(11592.0742, grad_fn=<NegBackward0>) tensor(11592.0742, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11585.41796875
tensor(11592.0742, grad_fn=<NegBackward0>) tensor(11585.4180, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11585.419921875
tensor(11585.4180, grad_fn=<NegBackward0>) tensor(11585.4199, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11585.4140625
tensor(11585.4180, grad_fn=<NegBackward0>) tensor(11585.4141, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11585.4140625
tensor(11585.4141, grad_fn=<NegBackward0>) tensor(11585.4141, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11585.435546875
tensor(11585.4141, grad_fn=<NegBackward0>) tensor(11585.4355, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11585.4130859375
tensor(11585.4141, grad_fn=<NegBackward0>) tensor(11585.4131, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11585.4140625
tensor(11585.4131, grad_fn=<NegBackward0>) tensor(11585.4141, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11585.4248046875
tensor(11585.4131, grad_fn=<NegBackward0>) tensor(11585.4248, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11585.4140625
tensor(11585.4131, grad_fn=<NegBackward0>) tensor(11585.4141, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11585.4169921875
tensor(11585.4131, grad_fn=<NegBackward0>) tensor(11585.4170, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -11585.4140625
tensor(11585.4131, grad_fn=<NegBackward0>) tensor(11585.4141, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.6083, 0.3917],
        [0.4984, 0.5016]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5072, 0.4928], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2142, 0.1062],
         [0.6326, 0.3820]],

        [[0.6987, 0.0968],
         [0.5981, 0.5859]],

        [[0.7139, 0.0988],
         [0.6600, 0.7198]],

        [[0.5510, 0.1012],
         [0.7150, 0.7020]],

        [[0.7081, 0.0947],
         [0.6326, 0.6652]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 27
Adjusted Rand Index: 0.2030410147018918
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.50026613462232
Average Adjusted Rand Index: 0.8406082029403784
[0.445114585008359, 0.50026613462232] [0.8859601385828129, 0.8406082029403784] [11567.2587890625, 11585.4140625]
-------------------------------------
This iteration is 36
True Objective function: Loss = -11351.399207576182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21830.537109375
inf tensor(21830.5371, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11772.1904296875
tensor(21830.5371, grad_fn=<NegBackward0>) tensor(11772.1904, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11629.21875
tensor(11772.1904, grad_fn=<NegBackward0>) tensor(11629.2188, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11574.9306640625
tensor(11629.2188, grad_fn=<NegBackward0>) tensor(11574.9307, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11459.931640625
tensor(11574.9307, grad_fn=<NegBackward0>) tensor(11459.9316, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11393.9375
tensor(11459.9316, grad_fn=<NegBackward0>) tensor(11393.9375, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11367.92578125
tensor(11393.9375, grad_fn=<NegBackward0>) tensor(11367.9258, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11361.5
tensor(11367.9258, grad_fn=<NegBackward0>) tensor(11361.5000, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11361.404296875
tensor(11361.5000, grad_fn=<NegBackward0>) tensor(11361.4043, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11361.3408203125
tensor(11361.4043, grad_fn=<NegBackward0>) tensor(11361.3408, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11348.5185546875
tensor(11361.3408, grad_fn=<NegBackward0>) tensor(11348.5186, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11348.3720703125
tensor(11348.5186, grad_fn=<NegBackward0>) tensor(11348.3721, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11347.4248046875
tensor(11348.3721, grad_fn=<NegBackward0>) tensor(11347.4248, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11347.3857421875
tensor(11347.4248, grad_fn=<NegBackward0>) tensor(11347.3857, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11347.375
tensor(11347.3857, grad_fn=<NegBackward0>) tensor(11347.3750, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11347.365234375
tensor(11347.3750, grad_fn=<NegBackward0>) tensor(11347.3652, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11347.357421875
tensor(11347.3652, grad_fn=<NegBackward0>) tensor(11347.3574, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11347.3505859375
tensor(11347.3574, grad_fn=<NegBackward0>) tensor(11347.3506, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11347.3447265625
tensor(11347.3506, grad_fn=<NegBackward0>) tensor(11347.3447, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11347.3388671875
tensor(11347.3447, grad_fn=<NegBackward0>) tensor(11347.3389, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11347.3349609375
tensor(11347.3389, grad_fn=<NegBackward0>) tensor(11347.3350, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11347.33203125
tensor(11347.3350, grad_fn=<NegBackward0>) tensor(11347.3320, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11347.328125
tensor(11347.3320, grad_fn=<NegBackward0>) tensor(11347.3281, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11347.3251953125
tensor(11347.3281, grad_fn=<NegBackward0>) tensor(11347.3252, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11347.322265625
tensor(11347.3252, grad_fn=<NegBackward0>) tensor(11347.3223, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11347.3203125
tensor(11347.3223, grad_fn=<NegBackward0>) tensor(11347.3203, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11347.3193359375
tensor(11347.3203, grad_fn=<NegBackward0>) tensor(11347.3193, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11347.31640625
tensor(11347.3193, grad_fn=<NegBackward0>) tensor(11347.3164, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11347.31640625
tensor(11347.3164, grad_fn=<NegBackward0>) tensor(11347.3164, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11347.3134765625
tensor(11347.3164, grad_fn=<NegBackward0>) tensor(11347.3135, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11347.3125
tensor(11347.3135, grad_fn=<NegBackward0>) tensor(11347.3125, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11347.310546875
tensor(11347.3125, grad_fn=<NegBackward0>) tensor(11347.3105, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11347.310546875
tensor(11347.3105, grad_fn=<NegBackward0>) tensor(11347.3105, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11347.3095703125
tensor(11347.3105, grad_fn=<NegBackward0>) tensor(11347.3096, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11347.3076171875
tensor(11347.3096, grad_fn=<NegBackward0>) tensor(11347.3076, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11347.306640625
tensor(11347.3076, grad_fn=<NegBackward0>) tensor(11347.3066, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11347.306640625
tensor(11347.3066, grad_fn=<NegBackward0>) tensor(11347.3066, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11347.3056640625
tensor(11347.3066, grad_fn=<NegBackward0>) tensor(11347.3057, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11347.3056640625
tensor(11347.3057, grad_fn=<NegBackward0>) tensor(11347.3057, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11347.3037109375
tensor(11347.3057, grad_fn=<NegBackward0>) tensor(11347.3037, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11347.3046875
tensor(11347.3037, grad_fn=<NegBackward0>) tensor(11347.3047, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11347.302734375
tensor(11347.3037, grad_fn=<NegBackward0>) tensor(11347.3027, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11347.306640625
tensor(11347.3027, grad_fn=<NegBackward0>) tensor(11347.3066, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11347.302734375
tensor(11347.3027, grad_fn=<NegBackward0>) tensor(11347.3027, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11347.30078125
tensor(11347.3027, grad_fn=<NegBackward0>) tensor(11347.3008, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11347.30078125
tensor(11347.3008, grad_fn=<NegBackward0>) tensor(11347.3008, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11347.30078125
tensor(11347.3008, grad_fn=<NegBackward0>) tensor(11347.3008, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11347.306640625
tensor(11347.3008, grad_fn=<NegBackward0>) tensor(11347.3066, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11347.30078125
tensor(11347.3008, grad_fn=<NegBackward0>) tensor(11347.3008, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11347.30078125
tensor(11347.3008, grad_fn=<NegBackward0>) tensor(11347.3008, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11347.2998046875
tensor(11347.3008, grad_fn=<NegBackward0>) tensor(11347.2998, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11347.30078125
tensor(11347.2998, grad_fn=<NegBackward0>) tensor(11347.3008, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11347.3017578125
tensor(11347.2998, grad_fn=<NegBackward0>) tensor(11347.3018, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11347.2998046875
tensor(11347.2998, grad_fn=<NegBackward0>) tensor(11347.2998, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11347.3017578125
tensor(11347.2998, grad_fn=<NegBackward0>) tensor(11347.3018, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11347.2978515625
tensor(11347.2998, grad_fn=<NegBackward0>) tensor(11347.2979, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11347.2978515625
tensor(11347.2979, grad_fn=<NegBackward0>) tensor(11347.2979, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11347.298828125
tensor(11347.2979, grad_fn=<NegBackward0>) tensor(11347.2988, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11347.298828125
tensor(11347.2979, grad_fn=<NegBackward0>) tensor(11347.2988, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11347.2978515625
tensor(11347.2979, grad_fn=<NegBackward0>) tensor(11347.2979, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11347.298828125
tensor(11347.2979, grad_fn=<NegBackward0>) tensor(11347.2988, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11347.2978515625
tensor(11347.2979, grad_fn=<NegBackward0>) tensor(11347.2979, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11347.2958984375
tensor(11347.2979, grad_fn=<NegBackward0>) tensor(11347.2959, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11347.2958984375
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2959, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11347.296875
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2969, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11347.296875
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2969, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11347.2958984375
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2959, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11347.2978515625
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2979, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11347.2958984375
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2959, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11347.2958984375
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2959, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11347.294921875
tensor(11347.2959, grad_fn=<NegBackward0>) tensor(11347.2949, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11347.2958984375
tensor(11347.2949, grad_fn=<NegBackward0>) tensor(11347.2959, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11347.2958984375
tensor(11347.2949, grad_fn=<NegBackward0>) tensor(11347.2959, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11347.296875
tensor(11347.2949, grad_fn=<NegBackward0>) tensor(11347.2969, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11347.3095703125
tensor(11347.2949, grad_fn=<NegBackward0>) tensor(11347.3096, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11347.2978515625
tensor(11347.2949, grad_fn=<NegBackward0>) tensor(11347.2979, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7367, 0.2633],
        [0.2337, 0.7663]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4684, 0.5316], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4005, 0.0982],
         [0.5362, 0.1974]],

        [[0.6189, 0.0964],
         [0.7208, 0.5226]],

        [[0.5511, 0.0913],
         [0.7123, 0.7296]],

        [[0.7032, 0.0989],
         [0.6313, 0.6652]],

        [[0.5580, 0.0925],
         [0.6796, 0.6473]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21459.115234375
inf tensor(21459.1152, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12136.5908203125
tensor(21459.1152, grad_fn=<NegBackward0>) tensor(12136.5908, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11861.583984375
tensor(12136.5908, grad_fn=<NegBackward0>) tensor(11861.5840, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11679.7568359375
tensor(11861.5840, grad_fn=<NegBackward0>) tensor(11679.7568, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11662.984375
tensor(11679.7568, grad_fn=<NegBackward0>) tensor(11662.9844, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11660.4140625
tensor(11662.9844, grad_fn=<NegBackward0>) tensor(11660.4141, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11660.0205078125
tensor(11660.4141, grad_fn=<NegBackward0>) tensor(11660.0205, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11659.779296875
tensor(11660.0205, grad_fn=<NegBackward0>) tensor(11659.7793, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11653.380859375
tensor(11659.7793, grad_fn=<NegBackward0>) tensor(11653.3809, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11653.2783203125
tensor(11653.3809, grad_fn=<NegBackward0>) tensor(11653.2783, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11652.69921875
tensor(11653.2783, grad_fn=<NegBackward0>) tensor(11652.6992, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11652.6474609375
tensor(11652.6992, grad_fn=<NegBackward0>) tensor(11652.6475, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11652.6171875
tensor(11652.6475, grad_fn=<NegBackward0>) tensor(11652.6172, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11652.4404296875
tensor(11652.6172, grad_fn=<NegBackward0>) tensor(11652.4404, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11649.31640625
tensor(11652.4404, grad_fn=<NegBackward0>) tensor(11649.3164, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11646.791015625
tensor(11649.3164, grad_fn=<NegBackward0>) tensor(11646.7910, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11646.6162109375
tensor(11646.7910, grad_fn=<NegBackward0>) tensor(11646.6162, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11646.5869140625
tensor(11646.6162, grad_fn=<NegBackward0>) tensor(11646.5869, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11646.5673828125
tensor(11646.5869, grad_fn=<NegBackward0>) tensor(11646.5674, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11646.5400390625
tensor(11646.5674, grad_fn=<NegBackward0>) tensor(11646.5400, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11644.072265625
tensor(11646.5400, grad_fn=<NegBackward0>) tensor(11644.0723, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11644.0625
tensor(11644.0723, grad_fn=<NegBackward0>) tensor(11644.0625, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11644.0322265625
tensor(11644.0625, grad_fn=<NegBackward0>) tensor(11644.0322, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11643.9990234375
tensor(11644.0322, grad_fn=<NegBackward0>) tensor(11643.9990, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11642.865234375
tensor(11643.9990, grad_fn=<NegBackward0>) tensor(11642.8652, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11642.8564453125
tensor(11642.8652, grad_fn=<NegBackward0>) tensor(11642.8564, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11642.8515625
tensor(11642.8564, grad_fn=<NegBackward0>) tensor(11642.8516, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11642.8388671875
tensor(11642.8516, grad_fn=<NegBackward0>) tensor(11642.8389, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11641.078125
tensor(11642.8389, grad_fn=<NegBackward0>) tensor(11641.0781, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11636.970703125
tensor(11641.0781, grad_fn=<NegBackward0>) tensor(11636.9707, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11630.9443359375
tensor(11636.9707, grad_fn=<NegBackward0>) tensor(11630.9443, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11630.869140625
tensor(11630.9443, grad_fn=<NegBackward0>) tensor(11630.8691, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11630.86328125
tensor(11630.8691, grad_fn=<NegBackward0>) tensor(11630.8633, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11630.7666015625
tensor(11630.8633, grad_fn=<NegBackward0>) tensor(11630.7666, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11630.763671875
tensor(11630.7666, grad_fn=<NegBackward0>) tensor(11630.7637, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11630.763671875
tensor(11630.7637, grad_fn=<NegBackward0>) tensor(11630.7637, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11630.7607421875
tensor(11630.7637, grad_fn=<NegBackward0>) tensor(11630.7607, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11630.7587890625
tensor(11630.7607, grad_fn=<NegBackward0>) tensor(11630.7588, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11630.7568359375
tensor(11630.7588, grad_fn=<NegBackward0>) tensor(11630.7568, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11630.755859375
tensor(11630.7568, grad_fn=<NegBackward0>) tensor(11630.7559, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11630.7548828125
tensor(11630.7559, grad_fn=<NegBackward0>) tensor(11630.7549, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11630.7509765625
tensor(11630.7549, grad_fn=<NegBackward0>) tensor(11630.7510, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11630.76171875
tensor(11630.7510, grad_fn=<NegBackward0>) tensor(11630.7617, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11630.75
tensor(11630.7510, grad_fn=<NegBackward0>) tensor(11630.7500, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11630.7529296875
tensor(11630.7500, grad_fn=<NegBackward0>) tensor(11630.7529, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11630.705078125
tensor(11630.7500, grad_fn=<NegBackward0>) tensor(11630.7051, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11630.7021484375
tensor(11630.7051, grad_fn=<NegBackward0>) tensor(11630.7021, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11630.705078125
tensor(11630.7021, grad_fn=<NegBackward0>) tensor(11630.7051, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11630.701171875
tensor(11630.7021, grad_fn=<NegBackward0>) tensor(11630.7012, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11630.701171875
tensor(11630.7012, grad_fn=<NegBackward0>) tensor(11630.7012, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11630.69921875
tensor(11630.7012, grad_fn=<NegBackward0>) tensor(11630.6992, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11630.69921875
tensor(11630.6992, grad_fn=<NegBackward0>) tensor(11630.6992, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11630.701171875
tensor(11630.6992, grad_fn=<NegBackward0>) tensor(11630.7012, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11630.6982421875
tensor(11630.6992, grad_fn=<NegBackward0>) tensor(11630.6982, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11630.697265625
tensor(11630.6982, grad_fn=<NegBackward0>) tensor(11630.6973, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11630.697265625
tensor(11630.6973, grad_fn=<NegBackward0>) tensor(11630.6973, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11630.697265625
tensor(11630.6973, grad_fn=<NegBackward0>) tensor(11630.6973, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11630.6962890625
tensor(11630.6973, grad_fn=<NegBackward0>) tensor(11630.6963, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11630.6962890625
tensor(11630.6963, grad_fn=<NegBackward0>) tensor(11630.6963, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11630.6943359375
tensor(11630.6963, grad_fn=<NegBackward0>) tensor(11630.6943, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11630.6953125
tensor(11630.6943, grad_fn=<NegBackward0>) tensor(11630.6953, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11630.697265625
tensor(11630.6943, grad_fn=<NegBackward0>) tensor(11630.6973, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11630.6962890625
tensor(11630.6943, grad_fn=<NegBackward0>) tensor(11630.6963, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11630.7060546875
tensor(11630.6943, grad_fn=<NegBackward0>) tensor(11630.7061, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -11630.6923828125
tensor(11630.6943, grad_fn=<NegBackward0>) tensor(11630.6924, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11630.693359375
tensor(11630.6924, grad_fn=<NegBackward0>) tensor(11630.6934, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11630.6943359375
tensor(11630.6924, grad_fn=<NegBackward0>) tensor(11630.6943, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11630.6943359375
tensor(11630.6924, grad_fn=<NegBackward0>) tensor(11630.6943, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11630.697265625
tensor(11630.6924, grad_fn=<NegBackward0>) tensor(11630.6973, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11630.693359375
tensor(11630.6924, grad_fn=<NegBackward0>) tensor(11630.6934, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.1856, 0.8144],
        [0.6929, 0.3071]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3709, 0.6291], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3208, 0.0983],
         [0.7269, 0.2681]],

        [[0.6861, 0.0956],
         [0.5371, 0.5439]],

        [[0.5814, 0.0864],
         [0.5746, 0.7039]],

        [[0.6779, 0.0989],
         [0.7207, 0.6058]],

        [[0.6408, 0.0946],
         [0.6217, 0.5233]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 84
Adjusted Rand Index: 0.45717386738651833
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6365003576249006
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 86
Adjusted Rand Index: 0.5135353535353535
Global Adjusted Rand Index: -5.2901747887151434e-05
Average Adjusted Rand Index: 0.7134412574366444
[1.0, -5.2901747887151434e-05] [1.0, 0.7134412574366444] [11347.2978515625, 11630.693359375]
-------------------------------------
This iteration is 37
True Objective function: Loss = -11721.685299341018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22218.515625
inf tensor(22218.5156, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12751.2451171875
tensor(22218.5156, grad_fn=<NegBackward0>) tensor(12751.2451, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12494.53125
tensor(12751.2451, grad_fn=<NegBackward0>) tensor(12494.5312, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12167.408203125
tensor(12494.5312, grad_fn=<NegBackward0>) tensor(12167.4082, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12121.4833984375
tensor(12167.4082, grad_fn=<NegBackward0>) tensor(12121.4834, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12090.587890625
tensor(12121.4834, grad_fn=<NegBackward0>) tensor(12090.5879, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12061.9482421875
tensor(12090.5879, grad_fn=<NegBackward0>) tensor(12061.9482, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12061.37109375
tensor(12061.9482, grad_fn=<NegBackward0>) tensor(12061.3711, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12061.224609375
tensor(12061.3711, grad_fn=<NegBackward0>) tensor(12061.2246, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12037.59375
tensor(12061.2246, grad_fn=<NegBackward0>) tensor(12037.5938, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12031.5068359375
tensor(12037.5938, grad_fn=<NegBackward0>) tensor(12031.5068, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12031.439453125
tensor(12031.5068, grad_fn=<NegBackward0>) tensor(12031.4395, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12031.3955078125
tensor(12031.4395, grad_fn=<NegBackward0>) tensor(12031.3955, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12020.0361328125
tensor(12031.3955, grad_fn=<NegBackward0>) tensor(12020.0361, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12019.9814453125
tensor(12020.0361, grad_fn=<NegBackward0>) tensor(12019.9814, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12019.9453125
tensor(12019.9814, grad_fn=<NegBackward0>) tensor(12019.9453, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12019.8837890625
tensor(12019.9453, grad_fn=<NegBackward0>) tensor(12019.8838, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12004.484375
tensor(12019.8838, grad_fn=<NegBackward0>) tensor(12004.4844, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12004.478515625
tensor(12004.4844, grad_fn=<NegBackward0>) tensor(12004.4785, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12004.4609375
tensor(12004.4785, grad_fn=<NegBackward0>) tensor(12004.4609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12004.451171875
tensor(12004.4609, grad_fn=<NegBackward0>) tensor(12004.4512, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12004.44921875
tensor(12004.4512, grad_fn=<NegBackward0>) tensor(12004.4492, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12004.4375
tensor(12004.4492, grad_fn=<NegBackward0>) tensor(12004.4375, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12004.4287109375
tensor(12004.4375, grad_fn=<NegBackward0>) tensor(12004.4287, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12004.4248046875
tensor(12004.4287, grad_fn=<NegBackward0>) tensor(12004.4248, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12004.4130859375
tensor(12004.4248, grad_fn=<NegBackward0>) tensor(12004.4131, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12001.087890625
tensor(12004.4131, grad_fn=<NegBackward0>) tensor(12001.0879, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11995.9375
tensor(12001.0879, grad_fn=<NegBackward0>) tensor(11995.9375, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11995.892578125
tensor(11995.9375, grad_fn=<NegBackward0>) tensor(11995.8926, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11995.8759765625
tensor(11995.8926, grad_fn=<NegBackward0>) tensor(11995.8760, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11995.87109375
tensor(11995.8760, grad_fn=<NegBackward0>) tensor(11995.8711, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11995.8662109375
tensor(11995.8711, grad_fn=<NegBackward0>) tensor(11995.8662, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11995.8623046875
tensor(11995.8662, grad_fn=<NegBackward0>) tensor(11995.8623, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11995.8671875
tensor(11995.8623, grad_fn=<NegBackward0>) tensor(11995.8672, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11995.85546875
tensor(11995.8623, grad_fn=<NegBackward0>) tensor(11995.8555, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11995.853515625
tensor(11995.8555, grad_fn=<NegBackward0>) tensor(11995.8535, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11995.859375
tensor(11995.8535, grad_fn=<NegBackward0>) tensor(11995.8594, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11995.8505859375
tensor(11995.8535, grad_fn=<NegBackward0>) tensor(11995.8506, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11995.84765625
tensor(11995.8506, grad_fn=<NegBackward0>) tensor(11995.8477, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11995.8466796875
tensor(11995.8477, grad_fn=<NegBackward0>) tensor(11995.8467, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11995.8447265625
tensor(11995.8467, grad_fn=<NegBackward0>) tensor(11995.8447, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11995.8447265625
tensor(11995.8447, grad_fn=<NegBackward0>) tensor(11995.8447, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11995.84375
tensor(11995.8447, grad_fn=<NegBackward0>) tensor(11995.8438, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11995.841796875
tensor(11995.8438, grad_fn=<NegBackward0>) tensor(11995.8418, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11995.8408203125
tensor(11995.8418, grad_fn=<NegBackward0>) tensor(11995.8408, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11995.8388671875
tensor(11995.8408, grad_fn=<NegBackward0>) tensor(11995.8389, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11995.837890625
tensor(11995.8389, grad_fn=<NegBackward0>) tensor(11995.8379, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11995.837890625
tensor(11995.8379, grad_fn=<NegBackward0>) tensor(11995.8379, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11995.8369140625
tensor(11995.8379, grad_fn=<NegBackward0>) tensor(11995.8369, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11995.8359375
tensor(11995.8369, grad_fn=<NegBackward0>) tensor(11995.8359, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11995.8359375
tensor(11995.8359, grad_fn=<NegBackward0>) tensor(11995.8359, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11995.8349609375
tensor(11995.8359, grad_fn=<NegBackward0>) tensor(11995.8350, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11995.8349609375
tensor(11995.8350, grad_fn=<NegBackward0>) tensor(11995.8350, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11995.8349609375
tensor(11995.8350, grad_fn=<NegBackward0>) tensor(11995.8350, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11995.8349609375
tensor(11995.8350, grad_fn=<NegBackward0>) tensor(11995.8350, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11995.8427734375
tensor(11995.8350, grad_fn=<NegBackward0>) tensor(11995.8428, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11995.8330078125
tensor(11995.8350, grad_fn=<NegBackward0>) tensor(11995.8330, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11995.8330078125
tensor(11995.8330, grad_fn=<NegBackward0>) tensor(11995.8330, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11995.83203125
tensor(11995.8330, grad_fn=<NegBackward0>) tensor(11995.8320, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11995.8330078125
tensor(11995.8320, grad_fn=<NegBackward0>) tensor(11995.8330, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11995.8310546875
tensor(11995.8320, grad_fn=<NegBackward0>) tensor(11995.8311, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11995.83203125
tensor(11995.8311, grad_fn=<NegBackward0>) tensor(11995.8320, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11995.83203125
tensor(11995.8311, grad_fn=<NegBackward0>) tensor(11995.8320, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11995.833984375
tensor(11995.8311, grad_fn=<NegBackward0>) tensor(11995.8340, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11995.833984375
tensor(11995.8311, grad_fn=<NegBackward0>) tensor(11995.8340, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11995.8310546875
tensor(11995.8311, grad_fn=<NegBackward0>) tensor(11995.8311, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11995.833984375
tensor(11995.8311, grad_fn=<NegBackward0>) tensor(11995.8340, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11995.830078125
tensor(11995.8311, grad_fn=<NegBackward0>) tensor(11995.8301, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11995.8369140625
tensor(11995.8301, grad_fn=<NegBackward0>) tensor(11995.8369, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11995.830078125
tensor(11995.8301, grad_fn=<NegBackward0>) tensor(11995.8301, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11995.828125
tensor(11995.8301, grad_fn=<NegBackward0>) tensor(11995.8281, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11995.638671875
tensor(11995.8281, grad_fn=<NegBackward0>) tensor(11995.6387, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11995.6318359375
tensor(11995.6387, grad_fn=<NegBackward0>) tensor(11995.6318, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11995.6318359375
tensor(11995.6318, grad_fn=<NegBackward0>) tensor(11995.6318, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11995.62890625
tensor(11995.6318, grad_fn=<NegBackward0>) tensor(11995.6289, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11995.615234375
tensor(11995.6289, grad_fn=<NegBackward0>) tensor(11995.6152, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11995.6318359375
tensor(11995.6152, grad_fn=<NegBackward0>) tensor(11995.6318, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11995.6142578125
tensor(11995.6152, grad_fn=<NegBackward0>) tensor(11995.6143, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11995.6142578125
tensor(11995.6143, grad_fn=<NegBackward0>) tensor(11995.6143, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11995.6171875
tensor(11995.6143, grad_fn=<NegBackward0>) tensor(11995.6172, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11995.61328125
tensor(11995.6143, grad_fn=<NegBackward0>) tensor(11995.6133, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11995.61328125
tensor(11995.6133, grad_fn=<NegBackward0>) tensor(11995.6133, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11995.6123046875
tensor(11995.6133, grad_fn=<NegBackward0>) tensor(11995.6123, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11995.61328125
tensor(11995.6123, grad_fn=<NegBackward0>) tensor(11995.6133, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11995.619140625
tensor(11995.6123, grad_fn=<NegBackward0>) tensor(11995.6191, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11995.62109375
tensor(11995.6123, grad_fn=<NegBackward0>) tensor(11995.6211, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11995.6875
tensor(11995.6123, grad_fn=<NegBackward0>) tensor(11995.6875, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11995.6123046875
tensor(11995.6123, grad_fn=<NegBackward0>) tensor(11995.6123, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11995.611328125
tensor(11995.6123, grad_fn=<NegBackward0>) tensor(11995.6113, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11995.6123046875
tensor(11995.6113, grad_fn=<NegBackward0>) tensor(11995.6123, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11995.6123046875
tensor(11995.6113, grad_fn=<NegBackward0>) tensor(11995.6123, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11995.630859375
tensor(11995.6113, grad_fn=<NegBackward0>) tensor(11995.6309, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11995.6142578125
tensor(11995.6113, grad_fn=<NegBackward0>) tensor(11995.6143, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11995.658203125
tensor(11995.6113, grad_fn=<NegBackward0>) tensor(11995.6582, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.5262, 0.4738],
        [0.5380, 0.4620]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3554, 0.0804],
         [0.6916, 0.2944]],

        [[0.6956, 0.0924],
         [0.5423, 0.5966]],

        [[0.5721, 0.1022],
         [0.5695, 0.5588]],

        [[0.5751, 0.0904],
         [0.5065, 0.7303]],

        [[0.6952, 0.0931],
         [0.6424, 0.5067]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207385189720222
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8081003563518231
Global Adjusted Rand Index: 0.04651854290969658
Average Adjusted Rand Index: 0.9299293912263853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21013.541015625
inf tensor(21013.5410, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12713.41796875
tensor(21013.5410, grad_fn=<NegBackward0>) tensor(12713.4180, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11901.9345703125
tensor(12713.4180, grad_fn=<NegBackward0>) tensor(11901.9346, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11754.404296875
tensor(11901.9346, grad_fn=<NegBackward0>) tensor(11754.4043, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11720.24609375
tensor(11754.4043, grad_fn=<NegBackward0>) tensor(11720.2461, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11710.8427734375
tensor(11720.2461, grad_fn=<NegBackward0>) tensor(11710.8428, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11710.556640625
tensor(11710.8428, grad_fn=<NegBackward0>) tensor(11710.5566, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11710.3896484375
tensor(11710.5566, grad_fn=<NegBackward0>) tensor(11710.3896, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11710.27734375
tensor(11710.3896, grad_fn=<NegBackward0>) tensor(11710.2773, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11710.2001953125
tensor(11710.2773, grad_fn=<NegBackward0>) tensor(11710.2002, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11710.1435546875
tensor(11710.2002, grad_fn=<NegBackward0>) tensor(11710.1436, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11710.099609375
tensor(11710.1436, grad_fn=<NegBackward0>) tensor(11710.0996, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11710.06640625
tensor(11710.0996, grad_fn=<NegBackward0>) tensor(11710.0664, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11710.0400390625
tensor(11710.0664, grad_fn=<NegBackward0>) tensor(11710.0400, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11710.017578125
tensor(11710.0400, grad_fn=<NegBackward0>) tensor(11710.0176, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11710.001953125
tensor(11710.0176, grad_fn=<NegBackward0>) tensor(11710.0020, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11709.984375
tensor(11710.0020, grad_fn=<NegBackward0>) tensor(11709.9844, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11709.97265625
tensor(11709.9844, grad_fn=<NegBackward0>) tensor(11709.9727, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11709.9716796875
tensor(11709.9727, grad_fn=<NegBackward0>) tensor(11709.9717, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11709.9521484375
tensor(11709.9717, grad_fn=<NegBackward0>) tensor(11709.9521, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11709.9443359375
tensor(11709.9521, grad_fn=<NegBackward0>) tensor(11709.9443, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11709.935546875
tensor(11709.9443, grad_fn=<NegBackward0>) tensor(11709.9355, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11709.923828125
tensor(11709.9355, grad_fn=<NegBackward0>) tensor(11709.9238, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11709.7861328125
tensor(11709.9238, grad_fn=<NegBackward0>) tensor(11709.7861, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11709.7802734375
tensor(11709.7861, grad_fn=<NegBackward0>) tensor(11709.7803, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11709.7763671875
tensor(11709.7803, grad_fn=<NegBackward0>) tensor(11709.7764, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11709.7724609375
tensor(11709.7764, grad_fn=<NegBackward0>) tensor(11709.7725, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11709.7685546875
tensor(11709.7725, grad_fn=<NegBackward0>) tensor(11709.7686, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11709.765625
tensor(11709.7686, grad_fn=<NegBackward0>) tensor(11709.7656, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11709.7626953125
tensor(11709.7656, grad_fn=<NegBackward0>) tensor(11709.7627, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11709.763671875
tensor(11709.7627, grad_fn=<NegBackward0>) tensor(11709.7637, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11709.7568359375
tensor(11709.7627, grad_fn=<NegBackward0>) tensor(11709.7568, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11709.7548828125
tensor(11709.7568, grad_fn=<NegBackward0>) tensor(11709.7549, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11709.75390625
tensor(11709.7549, grad_fn=<NegBackward0>) tensor(11709.7539, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11709.7509765625
tensor(11709.7539, grad_fn=<NegBackward0>) tensor(11709.7510, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11709.7509765625
tensor(11709.7510, grad_fn=<NegBackward0>) tensor(11709.7510, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11709.7490234375
tensor(11709.7510, grad_fn=<NegBackward0>) tensor(11709.7490, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11709.75
tensor(11709.7490, grad_fn=<NegBackward0>) tensor(11709.7500, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11709.7470703125
tensor(11709.7490, grad_fn=<NegBackward0>) tensor(11709.7471, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11709.7451171875
tensor(11709.7471, grad_fn=<NegBackward0>) tensor(11709.7451, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11709.744140625
tensor(11709.7451, grad_fn=<NegBackward0>) tensor(11709.7441, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11709.7431640625
tensor(11709.7441, grad_fn=<NegBackward0>) tensor(11709.7432, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11709.7412109375
tensor(11709.7432, grad_fn=<NegBackward0>) tensor(11709.7412, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11709.740234375
tensor(11709.7412, grad_fn=<NegBackward0>) tensor(11709.7402, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11709.7412109375
tensor(11709.7402, grad_fn=<NegBackward0>) tensor(11709.7412, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11709.7392578125
tensor(11709.7402, grad_fn=<NegBackward0>) tensor(11709.7393, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11709.7373046875
tensor(11709.7393, grad_fn=<NegBackward0>) tensor(11709.7373, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11709.7392578125
tensor(11709.7373, grad_fn=<NegBackward0>) tensor(11709.7393, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11709.736328125
tensor(11709.7373, grad_fn=<NegBackward0>) tensor(11709.7363, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11709.73828125
tensor(11709.7363, grad_fn=<NegBackward0>) tensor(11709.7383, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11709.740234375
tensor(11709.7363, grad_fn=<NegBackward0>) tensor(11709.7402, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11709.734375
tensor(11709.7363, grad_fn=<NegBackward0>) tensor(11709.7344, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11709.736328125
tensor(11709.7344, grad_fn=<NegBackward0>) tensor(11709.7363, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11709.734375
tensor(11709.7344, grad_fn=<NegBackward0>) tensor(11709.7344, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11709.7333984375
tensor(11709.7344, grad_fn=<NegBackward0>) tensor(11709.7334, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11709.7333984375
tensor(11709.7334, grad_fn=<NegBackward0>) tensor(11709.7334, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11709.7333984375
tensor(11709.7334, grad_fn=<NegBackward0>) tensor(11709.7334, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11709.7392578125
tensor(11709.7334, grad_fn=<NegBackward0>) tensor(11709.7393, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11709.732421875
tensor(11709.7334, grad_fn=<NegBackward0>) tensor(11709.7324, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11709.732421875
tensor(11709.7324, grad_fn=<NegBackward0>) tensor(11709.7324, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11709.7314453125
tensor(11709.7324, grad_fn=<NegBackward0>) tensor(11709.7314, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11709.7333984375
tensor(11709.7314, grad_fn=<NegBackward0>) tensor(11709.7334, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11709.7314453125
tensor(11709.7314, grad_fn=<NegBackward0>) tensor(11709.7314, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11709.7353515625
tensor(11709.7314, grad_fn=<NegBackward0>) tensor(11709.7354, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11709.7294921875
tensor(11709.7314, grad_fn=<NegBackward0>) tensor(11709.7295, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11709.732421875
tensor(11709.7295, grad_fn=<NegBackward0>) tensor(11709.7324, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11709.7294921875
tensor(11709.7295, grad_fn=<NegBackward0>) tensor(11709.7295, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11709.73046875
tensor(11709.7295, grad_fn=<NegBackward0>) tensor(11709.7305, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11709.7314453125
tensor(11709.7295, grad_fn=<NegBackward0>) tensor(11709.7314, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11709.7294921875
tensor(11709.7295, grad_fn=<NegBackward0>) tensor(11709.7295, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11709.7314453125
tensor(11709.7295, grad_fn=<NegBackward0>) tensor(11709.7314, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11709.728515625
tensor(11709.7295, grad_fn=<NegBackward0>) tensor(11709.7285, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11709.7294921875
tensor(11709.7285, grad_fn=<NegBackward0>) tensor(11709.7295, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11709.7333984375
tensor(11709.7285, grad_fn=<NegBackward0>) tensor(11709.7334, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11709.7294921875
tensor(11709.7285, grad_fn=<NegBackward0>) tensor(11709.7295, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11709.728515625
tensor(11709.7285, grad_fn=<NegBackward0>) tensor(11709.7285, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11709.7275390625
tensor(11709.7285, grad_fn=<NegBackward0>) tensor(11709.7275, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11709.728515625
tensor(11709.7275, grad_fn=<NegBackward0>) tensor(11709.7285, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11709.728515625
tensor(11709.7275, grad_fn=<NegBackward0>) tensor(11709.7285, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11709.728515625
tensor(11709.7275, grad_fn=<NegBackward0>) tensor(11709.7285, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11709.7353515625
tensor(11709.7275, grad_fn=<NegBackward0>) tensor(11709.7354, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11709.751953125
tensor(11709.7275, grad_fn=<NegBackward0>) tensor(11709.7520, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.7239, 0.2761],
        [0.2303, 0.7697]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4074, 0.5926], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2025, 0.0833],
         [0.6541, 0.4009]],

        [[0.6095, 0.0925],
         [0.5486, 0.6987]],

        [[0.6685, 0.1026],
         [0.5991, 0.6054]],

        [[0.5382, 0.0909],
         [0.5996, 0.5253]],

        [[0.6838, 0.0951],
         [0.6584, 0.5137]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919982176116094
Average Adjusted Rand Index: 0.9919998119331364
[0.04651854290969658, 0.9919982176116094] [0.9299293912263853, 0.9919998119331364] [11995.658203125, 11709.751953125]
-------------------------------------
This iteration is 38
True Objective function: Loss = -11620.921894275833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21505.09765625
inf tensor(21505.0977, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12468.4091796875
tensor(21505.0977, grad_fn=<NegBackward0>) tensor(12468.4092, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12398.0234375
tensor(12468.4092, grad_fn=<NegBackward0>) tensor(12398.0234, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11854.41796875
tensor(12398.0234, grad_fn=<NegBackward0>) tensor(11854.4180, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11767.349609375
tensor(11854.4180, grad_fn=<NegBackward0>) tensor(11767.3496, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11700.57421875
tensor(11767.3496, grad_fn=<NegBackward0>) tensor(11700.5742, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11688.6396484375
tensor(11700.5742, grad_fn=<NegBackward0>) tensor(11688.6396, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11677.072265625
tensor(11688.6396, grad_fn=<NegBackward0>) tensor(11677.0723, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11668.3564453125
tensor(11677.0723, grad_fn=<NegBackward0>) tensor(11668.3564, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11666.2822265625
tensor(11668.3564, grad_fn=<NegBackward0>) tensor(11666.2822, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11660.17578125
tensor(11666.2822, grad_fn=<NegBackward0>) tensor(11660.1758, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11649.720703125
tensor(11660.1758, grad_fn=<NegBackward0>) tensor(11649.7207, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11649.6318359375
tensor(11649.7207, grad_fn=<NegBackward0>) tensor(11649.6318, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11638.373046875
tensor(11649.6318, grad_fn=<NegBackward0>) tensor(11638.3730, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11632.66015625
tensor(11638.3730, grad_fn=<NegBackward0>) tensor(11632.6602, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11632.623046875
tensor(11632.6602, grad_fn=<NegBackward0>) tensor(11632.6230, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11632.5947265625
tensor(11632.6230, grad_fn=<NegBackward0>) tensor(11632.5947, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11632.5732421875
tensor(11632.5947, grad_fn=<NegBackward0>) tensor(11632.5732, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11632.5537109375
tensor(11632.5732, grad_fn=<NegBackward0>) tensor(11632.5537, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11632.5380859375
tensor(11632.5537, grad_fn=<NegBackward0>) tensor(11632.5381, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11632.5244140625
tensor(11632.5381, grad_fn=<NegBackward0>) tensor(11632.5244, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11632.513671875
tensor(11632.5244, grad_fn=<NegBackward0>) tensor(11632.5137, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11632.5029296875
tensor(11632.5137, grad_fn=<NegBackward0>) tensor(11632.5029, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11632.494140625
tensor(11632.5029, grad_fn=<NegBackward0>) tensor(11632.4941, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11632.4873046875
tensor(11632.4941, grad_fn=<NegBackward0>) tensor(11632.4873, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11632.4794921875
tensor(11632.4873, grad_fn=<NegBackward0>) tensor(11632.4795, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11632.4736328125
tensor(11632.4795, grad_fn=<NegBackward0>) tensor(11632.4736, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11632.466796875
tensor(11632.4736, grad_fn=<NegBackward0>) tensor(11632.4668, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11632.462890625
tensor(11632.4668, grad_fn=<NegBackward0>) tensor(11632.4629, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11632.458984375
tensor(11632.4629, grad_fn=<NegBackward0>) tensor(11632.4590, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11632.455078125
tensor(11632.4590, grad_fn=<NegBackward0>) tensor(11632.4551, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11632.4501953125
tensor(11632.4551, grad_fn=<NegBackward0>) tensor(11632.4502, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11632.443359375
tensor(11632.4502, grad_fn=<NegBackward0>) tensor(11632.4434, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11608.2265625
tensor(11632.4434, grad_fn=<NegBackward0>) tensor(11608.2266, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11608.21484375
tensor(11608.2266, grad_fn=<NegBackward0>) tensor(11608.2148, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11608.2021484375
tensor(11608.2148, grad_fn=<NegBackward0>) tensor(11608.2021, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11608.19140625
tensor(11608.2021, grad_fn=<NegBackward0>) tensor(11608.1914, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11608.1884765625
tensor(11608.1914, grad_fn=<NegBackward0>) tensor(11608.1885, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11608.18359375
tensor(11608.1885, grad_fn=<NegBackward0>) tensor(11608.1836, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11608.18359375
tensor(11608.1836, grad_fn=<NegBackward0>) tensor(11608.1836, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11608.1796875
tensor(11608.1836, grad_fn=<NegBackward0>) tensor(11608.1797, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11608.177734375
tensor(11608.1797, grad_fn=<NegBackward0>) tensor(11608.1777, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11608.17578125
tensor(11608.1777, grad_fn=<NegBackward0>) tensor(11608.1758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11608.171875
tensor(11608.1758, grad_fn=<NegBackward0>) tensor(11608.1719, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11608.1689453125
tensor(11608.1719, grad_fn=<NegBackward0>) tensor(11608.1689, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11608.16796875
tensor(11608.1689, grad_fn=<NegBackward0>) tensor(11608.1680, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11608.166015625
tensor(11608.1680, grad_fn=<NegBackward0>) tensor(11608.1660, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11608.166015625
tensor(11608.1660, grad_fn=<NegBackward0>) tensor(11608.1660, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11608.166015625
tensor(11608.1660, grad_fn=<NegBackward0>) tensor(11608.1660, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11608.1640625
tensor(11608.1660, grad_fn=<NegBackward0>) tensor(11608.1641, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11608.162109375
tensor(11608.1641, grad_fn=<NegBackward0>) tensor(11608.1621, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11608.1630859375
tensor(11608.1621, grad_fn=<NegBackward0>) tensor(11608.1631, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11608.1611328125
tensor(11608.1621, grad_fn=<NegBackward0>) tensor(11608.1611, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11608.162109375
tensor(11608.1611, grad_fn=<NegBackward0>) tensor(11608.1621, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11608.1591796875
tensor(11608.1611, grad_fn=<NegBackward0>) tensor(11608.1592, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11608.158203125
tensor(11608.1592, grad_fn=<NegBackward0>) tensor(11608.1582, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11608.1611328125
tensor(11608.1582, grad_fn=<NegBackward0>) tensor(11608.1611, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11608.158203125
tensor(11608.1582, grad_fn=<NegBackward0>) tensor(11608.1582, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11608.1572265625
tensor(11608.1582, grad_fn=<NegBackward0>) tensor(11608.1572, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11608.1572265625
tensor(11608.1572, grad_fn=<NegBackward0>) tensor(11608.1572, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11608.16015625
tensor(11608.1572, grad_fn=<NegBackward0>) tensor(11608.1602, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11608.1513671875
tensor(11608.1572, grad_fn=<NegBackward0>) tensor(11608.1514, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11608.142578125
tensor(11608.1514, grad_fn=<NegBackward0>) tensor(11608.1426, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11608.142578125
tensor(11608.1426, grad_fn=<NegBackward0>) tensor(11608.1426, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11608.1416015625
tensor(11608.1426, grad_fn=<NegBackward0>) tensor(11608.1416, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11608.1435546875
tensor(11608.1416, grad_fn=<NegBackward0>) tensor(11608.1436, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11608.1435546875
tensor(11608.1416, grad_fn=<NegBackward0>) tensor(11608.1436, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11608.140625
tensor(11608.1416, grad_fn=<NegBackward0>) tensor(11608.1406, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11608.1416015625
tensor(11608.1406, grad_fn=<NegBackward0>) tensor(11608.1416, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11608.15234375
tensor(11608.1406, grad_fn=<NegBackward0>) tensor(11608.1523, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11608.140625
tensor(11608.1406, grad_fn=<NegBackward0>) tensor(11608.1406, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11608.14453125
tensor(11608.1406, grad_fn=<NegBackward0>) tensor(11608.1445, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11608.1435546875
tensor(11608.1406, grad_fn=<NegBackward0>) tensor(11608.1436, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11608.1376953125
tensor(11608.1406, grad_fn=<NegBackward0>) tensor(11608.1377, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11608.13671875
tensor(11608.1377, grad_fn=<NegBackward0>) tensor(11608.1367, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11608.1416015625
tensor(11608.1367, grad_fn=<NegBackward0>) tensor(11608.1416, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11608.1337890625
tensor(11608.1367, grad_fn=<NegBackward0>) tensor(11608.1338, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11608.1337890625
tensor(11608.1338, grad_fn=<NegBackward0>) tensor(11608.1338, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11608.1357421875
tensor(11608.1338, grad_fn=<NegBackward0>) tensor(11608.1357, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11608.134765625
tensor(11608.1338, grad_fn=<NegBackward0>) tensor(11608.1348, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11608.1328125
tensor(11608.1338, grad_fn=<NegBackward0>) tensor(11608.1328, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11608.1328125
tensor(11608.1328, grad_fn=<NegBackward0>) tensor(11608.1328, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11608.1318359375
tensor(11608.1328, grad_fn=<NegBackward0>) tensor(11608.1318, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11608.1318359375
tensor(11608.1318, grad_fn=<NegBackward0>) tensor(11608.1318, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11608.1318359375
tensor(11608.1318, grad_fn=<NegBackward0>) tensor(11608.1318, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11608.15625
tensor(11608.1318, grad_fn=<NegBackward0>) tensor(11608.1562, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11608.130859375
tensor(11608.1318, grad_fn=<NegBackward0>) tensor(11608.1309, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11608.130859375
tensor(11608.1309, grad_fn=<NegBackward0>) tensor(11608.1309, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11608.1484375
tensor(11608.1309, grad_fn=<NegBackward0>) tensor(11608.1484, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11608.1298828125
tensor(11608.1309, grad_fn=<NegBackward0>) tensor(11608.1299, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11608.1962890625
tensor(11608.1299, grad_fn=<NegBackward0>) tensor(11608.1963, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11608.1337890625
tensor(11608.1299, grad_fn=<NegBackward0>) tensor(11608.1338, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11608.1630859375
tensor(11608.1299, grad_fn=<NegBackward0>) tensor(11608.1631, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11608.130859375
tensor(11608.1299, grad_fn=<NegBackward0>) tensor(11608.1309, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11608.130859375
tensor(11608.1299, grad_fn=<NegBackward0>) tensor(11608.1309, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.7745, 0.2255],
        [0.2094, 0.7906]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4875, 0.5125], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1925, 0.1177],
         [0.5401, 0.3988]],

        [[0.6702, 0.1084],
         [0.6932, 0.5886]],

        [[0.7060, 0.0951],
         [0.6838, 0.6284]],

        [[0.6229, 0.0987],
         [0.6428, 0.7216]],

        [[0.5760, 0.0890],
         [0.7093, 0.5245]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320640625895
Average Adjusted Rand Index: 0.9841601267189862
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21048.001953125
inf tensor(21048.0020, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12034.1552734375
tensor(21048.0020, grad_fn=<NegBackward0>) tensor(12034.1553, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11932.4287109375
tensor(12034.1553, grad_fn=<NegBackward0>) tensor(11932.4287, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11901.8427734375
tensor(11932.4287, grad_fn=<NegBackward0>) tensor(11901.8428, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11814.73046875
tensor(11901.8428, grad_fn=<NegBackward0>) tensor(11814.7305, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11793.63671875
tensor(11814.7305, grad_fn=<NegBackward0>) tensor(11793.6367, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11793.3896484375
tensor(11793.6367, grad_fn=<NegBackward0>) tensor(11793.3896, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11793.2958984375
tensor(11793.3896, grad_fn=<NegBackward0>) tensor(11793.2959, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11793.23828125
tensor(11793.2959, grad_fn=<NegBackward0>) tensor(11793.2383, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11793.2001953125
tensor(11793.2383, grad_fn=<NegBackward0>) tensor(11793.2002, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11793.171875
tensor(11793.2002, grad_fn=<NegBackward0>) tensor(11793.1719, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11793.142578125
tensor(11793.1719, grad_fn=<NegBackward0>) tensor(11793.1426, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11793.0966796875
tensor(11793.1426, grad_fn=<NegBackward0>) tensor(11793.0967, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11793.078125
tensor(11793.0967, grad_fn=<NegBackward0>) tensor(11793.0781, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11793.068359375
tensor(11793.0781, grad_fn=<NegBackward0>) tensor(11793.0684, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11793.060546875
tensor(11793.0684, grad_fn=<NegBackward0>) tensor(11793.0605, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11793.0537109375
tensor(11793.0605, grad_fn=<NegBackward0>) tensor(11793.0537, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11793.0498046875
tensor(11793.0537, grad_fn=<NegBackward0>) tensor(11793.0498, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11793.044921875
tensor(11793.0498, grad_fn=<NegBackward0>) tensor(11793.0449, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11793.0400390625
tensor(11793.0449, grad_fn=<NegBackward0>) tensor(11793.0400, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11793.0361328125
tensor(11793.0400, grad_fn=<NegBackward0>) tensor(11793.0361, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11793.0341796875
tensor(11793.0361, grad_fn=<NegBackward0>) tensor(11793.0342, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11793.0322265625
tensor(11793.0342, grad_fn=<NegBackward0>) tensor(11793.0322, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11793.029296875
tensor(11793.0322, grad_fn=<NegBackward0>) tensor(11793.0293, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11793.02734375
tensor(11793.0293, grad_fn=<NegBackward0>) tensor(11793.0273, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11793.025390625
tensor(11793.0273, grad_fn=<NegBackward0>) tensor(11793.0254, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11793.0244140625
tensor(11793.0254, grad_fn=<NegBackward0>) tensor(11793.0244, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11793.0224609375
tensor(11793.0244, grad_fn=<NegBackward0>) tensor(11793.0225, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11793.0224609375
tensor(11793.0225, grad_fn=<NegBackward0>) tensor(11793.0225, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11793.021484375
tensor(11793.0225, grad_fn=<NegBackward0>) tensor(11793.0215, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11793.0205078125
tensor(11793.0215, grad_fn=<NegBackward0>) tensor(11793.0205, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11793.0185546875
tensor(11793.0205, grad_fn=<NegBackward0>) tensor(11793.0186, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11793.0185546875
tensor(11793.0186, grad_fn=<NegBackward0>) tensor(11793.0186, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11793.0185546875
tensor(11793.0186, grad_fn=<NegBackward0>) tensor(11793.0186, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11793.0166015625
tensor(11793.0186, grad_fn=<NegBackward0>) tensor(11793.0166, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11793.0166015625
tensor(11793.0166, grad_fn=<NegBackward0>) tensor(11793.0166, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11793.0146484375
tensor(11793.0166, grad_fn=<NegBackward0>) tensor(11793.0146, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11793.013671875
tensor(11793.0146, grad_fn=<NegBackward0>) tensor(11793.0137, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11793.013671875
tensor(11793.0137, grad_fn=<NegBackward0>) tensor(11793.0137, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11793.0126953125
tensor(11793.0137, grad_fn=<NegBackward0>) tensor(11793.0127, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11793.013671875
tensor(11793.0127, grad_fn=<NegBackward0>) tensor(11793.0137, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11793.0126953125
tensor(11793.0127, grad_fn=<NegBackward0>) tensor(11793.0127, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11793.01171875
tensor(11793.0127, grad_fn=<NegBackward0>) tensor(11793.0117, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11793.01171875
tensor(11793.0117, grad_fn=<NegBackward0>) tensor(11793.0117, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11793.0107421875
tensor(11793.0117, grad_fn=<NegBackward0>) tensor(11793.0107, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11793.0234375
tensor(11793.0107, grad_fn=<NegBackward0>) tensor(11793.0234, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11793.009765625
tensor(11793.0107, grad_fn=<NegBackward0>) tensor(11793.0098, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11793.013671875
tensor(11793.0098, grad_fn=<NegBackward0>) tensor(11793.0137, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11793.0107421875
tensor(11793.0098, grad_fn=<NegBackward0>) tensor(11793.0107, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11793.0107421875
tensor(11793.0098, grad_fn=<NegBackward0>) tensor(11793.0107, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11793.0107421875
tensor(11793.0098, grad_fn=<NegBackward0>) tensor(11793.0107, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -11793.013671875
tensor(11793.0098, grad_fn=<NegBackward0>) tensor(11793.0137, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.7106, 0.2894],
        [0.3805, 0.6195]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4837, 0.5163], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2103, 0.1177],
         [0.6939, 0.3959]],

        [[0.6015, 0.1082],
         [0.5023, 0.6484]],

        [[0.6384, 0.0951],
         [0.6399, 0.5132]],

        [[0.5953, 0.0985],
         [0.6385, 0.6263]],

        [[0.5444, 0.0952],
         [0.5090, 0.6442]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 26
Adjusted Rand Index: 0.22468617203145608
Global Adjusted Rand Index: 0.4833828186449044
Average Adjusted Rand Index: 0.8290973611252774
[0.9840320640625895, 0.4833828186449044] [0.9841601267189862, 0.8290973611252774] [11608.130859375, 11793.013671875]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11741.900245710178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20846.6953125
inf tensor(20846.6953, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12533.435546875
tensor(20846.6953, grad_fn=<NegBackward0>) tensor(12533.4355, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12491.7421875
tensor(12533.4355, grad_fn=<NegBackward0>) tensor(12491.7422, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11816.919921875
tensor(12491.7422, grad_fn=<NegBackward0>) tensor(11816.9199, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11766.830078125
tensor(11816.9199, grad_fn=<NegBackward0>) tensor(11766.8301, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11742.685546875
tensor(11766.8301, grad_fn=<NegBackward0>) tensor(11742.6855, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11736.5595703125
tensor(11742.6855, grad_fn=<NegBackward0>) tensor(11736.5596, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11736.357421875
tensor(11736.5596, grad_fn=<NegBackward0>) tensor(11736.3574, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11735.681640625
tensor(11736.3574, grad_fn=<NegBackward0>) tensor(11735.6816, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11732.6767578125
tensor(11735.6816, grad_fn=<NegBackward0>) tensor(11732.6768, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11727.111328125
tensor(11732.6768, grad_fn=<NegBackward0>) tensor(11727.1113, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11727.0625
tensor(11727.1113, grad_fn=<NegBackward0>) tensor(11727.0625, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11727.021484375
tensor(11727.0625, grad_fn=<NegBackward0>) tensor(11727.0215, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11726.982421875
tensor(11727.0215, grad_fn=<NegBackward0>) tensor(11726.9824, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11726.9599609375
tensor(11726.9824, grad_fn=<NegBackward0>) tensor(11726.9600, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11726.9423828125
tensor(11726.9600, grad_fn=<NegBackward0>) tensor(11726.9424, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11726.9287109375
tensor(11726.9424, grad_fn=<NegBackward0>) tensor(11726.9287, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11726.912109375
tensor(11726.9287, grad_fn=<NegBackward0>) tensor(11726.9121, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11726.6875
tensor(11726.9121, grad_fn=<NegBackward0>) tensor(11726.6875, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11726.6748046875
tensor(11726.6875, grad_fn=<NegBackward0>) tensor(11726.6748, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11726.6669921875
tensor(11726.6748, grad_fn=<NegBackward0>) tensor(11726.6670, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11726.6591796875
tensor(11726.6670, grad_fn=<NegBackward0>) tensor(11726.6592, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11726.654296875
tensor(11726.6592, grad_fn=<NegBackward0>) tensor(11726.6543, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11726.6484375
tensor(11726.6543, grad_fn=<NegBackward0>) tensor(11726.6484, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11726.64453125
tensor(11726.6484, grad_fn=<NegBackward0>) tensor(11726.6445, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11726.6416015625
tensor(11726.6445, grad_fn=<NegBackward0>) tensor(11726.6416, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11726.634765625
tensor(11726.6416, grad_fn=<NegBackward0>) tensor(11726.6348, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11726.630859375
tensor(11726.6348, grad_fn=<NegBackward0>) tensor(11726.6309, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11726.62890625
tensor(11726.6309, grad_fn=<NegBackward0>) tensor(11726.6289, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11726.625
tensor(11726.6289, grad_fn=<NegBackward0>) tensor(11726.6250, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11726.6220703125
tensor(11726.6250, grad_fn=<NegBackward0>) tensor(11726.6221, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11726.619140625
tensor(11726.6221, grad_fn=<NegBackward0>) tensor(11726.6191, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11726.623046875
tensor(11726.6191, grad_fn=<NegBackward0>) tensor(11726.6230, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11726.6103515625
tensor(11726.6191, grad_fn=<NegBackward0>) tensor(11726.6104, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11726.609375
tensor(11726.6104, grad_fn=<NegBackward0>) tensor(11726.6094, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11726.607421875
tensor(11726.6094, grad_fn=<NegBackward0>) tensor(11726.6074, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11726.60546875
tensor(11726.6074, grad_fn=<NegBackward0>) tensor(11726.6055, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11726.6044921875
tensor(11726.6055, grad_fn=<NegBackward0>) tensor(11726.6045, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11726.60546875
tensor(11726.6045, grad_fn=<NegBackward0>) tensor(11726.6055, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11726.6015625
tensor(11726.6045, grad_fn=<NegBackward0>) tensor(11726.6016, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11726.6005859375
tensor(11726.6016, grad_fn=<NegBackward0>) tensor(11726.6006, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11726.6005859375
tensor(11726.6006, grad_fn=<NegBackward0>) tensor(11726.6006, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11726.599609375
tensor(11726.6006, grad_fn=<NegBackward0>) tensor(11726.5996, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11726.599609375
tensor(11726.5996, grad_fn=<NegBackward0>) tensor(11726.5996, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11726.5947265625
tensor(11726.5996, grad_fn=<NegBackward0>) tensor(11726.5947, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11726.595703125
tensor(11726.5947, grad_fn=<NegBackward0>) tensor(11726.5957, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11726.5908203125
tensor(11726.5947, grad_fn=<NegBackward0>) tensor(11726.5908, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11726.416015625
tensor(11726.5908, grad_fn=<NegBackward0>) tensor(11726.4160, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11726.416015625
tensor(11726.4160, grad_fn=<NegBackward0>) tensor(11726.4160, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11726.4140625
tensor(11726.4160, grad_fn=<NegBackward0>) tensor(11726.4141, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11726.4111328125
tensor(11726.4141, grad_fn=<NegBackward0>) tensor(11726.4111, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11726.412109375
tensor(11726.4111, grad_fn=<NegBackward0>) tensor(11726.4121, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11726.4208984375
tensor(11726.4111, grad_fn=<NegBackward0>) tensor(11726.4209, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11726.41015625
tensor(11726.4111, grad_fn=<NegBackward0>) tensor(11726.4102, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11726.4228515625
tensor(11726.4102, grad_fn=<NegBackward0>) tensor(11726.4229, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11726.4091796875
tensor(11726.4102, grad_fn=<NegBackward0>) tensor(11726.4092, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11726.4091796875
tensor(11726.4092, grad_fn=<NegBackward0>) tensor(11726.4092, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11726.4091796875
tensor(11726.4092, grad_fn=<NegBackward0>) tensor(11726.4092, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11726.408203125
tensor(11726.4092, grad_fn=<NegBackward0>) tensor(11726.4082, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11726.4130859375
tensor(11726.4082, grad_fn=<NegBackward0>) tensor(11726.4131, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11726.4091796875
tensor(11726.4082, grad_fn=<NegBackward0>) tensor(11726.4092, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11726.41015625
tensor(11726.4082, grad_fn=<NegBackward0>) tensor(11726.4102, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11726.4072265625
tensor(11726.4082, grad_fn=<NegBackward0>) tensor(11726.4072, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11726.41015625
tensor(11726.4072, grad_fn=<NegBackward0>) tensor(11726.4102, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11726.4072265625
tensor(11726.4072, grad_fn=<NegBackward0>) tensor(11726.4072, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11726.4072265625
tensor(11726.4072, grad_fn=<NegBackward0>) tensor(11726.4072, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11726.4052734375
tensor(11726.4072, grad_fn=<NegBackward0>) tensor(11726.4053, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11726.4091796875
tensor(11726.4053, grad_fn=<NegBackward0>) tensor(11726.4092, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11726.40625
tensor(11726.4053, grad_fn=<NegBackward0>) tensor(11726.4062, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11726.40625
tensor(11726.4053, grad_fn=<NegBackward0>) tensor(11726.4062, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11726.404296875
tensor(11726.4053, grad_fn=<NegBackward0>) tensor(11726.4043, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11726.408203125
tensor(11726.4043, grad_fn=<NegBackward0>) tensor(11726.4082, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11726.4140625
tensor(11726.4043, grad_fn=<NegBackward0>) tensor(11726.4141, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11726.40625
tensor(11726.4043, grad_fn=<NegBackward0>) tensor(11726.4062, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11726.404296875
tensor(11726.4043, grad_fn=<NegBackward0>) tensor(11726.4043, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11726.40625
tensor(11726.4043, grad_fn=<NegBackward0>) tensor(11726.4062, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11726.40234375
tensor(11726.4043, grad_fn=<NegBackward0>) tensor(11726.4023, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11726.4033203125
tensor(11726.4023, grad_fn=<NegBackward0>) tensor(11726.4033, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11726.400390625
tensor(11726.4023, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11726.4013671875
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4014, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11726.40234375
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4023, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11726.4013671875
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4014, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11726.4111328125
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4111, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11726.4150390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4150, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11726.4013671875
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4014, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11726.400390625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4004, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11726.41796875
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4180, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11726.40234375
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4023, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11726.416015625
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4160, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11726.41796875
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4180, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11726.4130859375
tensor(11726.4004, grad_fn=<NegBackward0>) tensor(11726.4131, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7520, 0.2480],
        [0.2289, 0.7711]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5394, 0.4606], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4051, 0.0990],
         [0.7160, 0.2008]],

        [[0.5855, 0.1006],
         [0.5418, 0.6606]],

        [[0.5330, 0.1061],
         [0.5684, 0.6435]],

        [[0.6010, 0.1148],
         [0.7089, 0.5402]],

        [[0.7219, 0.1048],
         [0.5887, 0.5483]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681922650727868
Average Adjusted Rand Index: 0.9684841076153855
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21513.158203125
inf tensor(21513.1582, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12534.7080078125
tensor(21513.1582, grad_fn=<NegBackward0>) tensor(12534.7080, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12505.66796875
tensor(12534.7080, grad_fn=<NegBackward0>) tensor(12505.6680, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11955.546875
tensor(12505.6680, grad_fn=<NegBackward0>) tensor(11955.5469, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11902.123046875
tensor(11955.5469, grad_fn=<NegBackward0>) tensor(11902.1230, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11896.609375
tensor(11902.1230, grad_fn=<NegBackward0>) tensor(11896.6094, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11896.25
tensor(11896.6094, grad_fn=<NegBackward0>) tensor(11896.2500, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11896.0595703125
tensor(11896.2500, grad_fn=<NegBackward0>) tensor(11896.0596, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11895.939453125
tensor(11896.0596, grad_fn=<NegBackward0>) tensor(11895.9395, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11895.859375
tensor(11895.9395, grad_fn=<NegBackward0>) tensor(11895.8594, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11895.80078125
tensor(11895.8594, grad_fn=<NegBackward0>) tensor(11895.8008, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11895.755859375
tensor(11895.8008, grad_fn=<NegBackward0>) tensor(11895.7559, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11895.7216796875
tensor(11895.7559, grad_fn=<NegBackward0>) tensor(11895.7217, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11895.6953125
tensor(11895.7217, grad_fn=<NegBackward0>) tensor(11895.6953, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11895.671875
tensor(11895.6953, grad_fn=<NegBackward0>) tensor(11895.6719, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11895.65234375
tensor(11895.6719, grad_fn=<NegBackward0>) tensor(11895.6523, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11895.6357421875
tensor(11895.6523, grad_fn=<NegBackward0>) tensor(11895.6357, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11895.6201171875
tensor(11895.6357, grad_fn=<NegBackward0>) tensor(11895.6201, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11895.6025390625
tensor(11895.6201, grad_fn=<NegBackward0>) tensor(11895.6025, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11895.5888671875
tensor(11895.6025, grad_fn=<NegBackward0>) tensor(11895.5889, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11895.5869140625
tensor(11895.5889, grad_fn=<NegBackward0>) tensor(11895.5869, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11895.580078125
tensor(11895.5869, grad_fn=<NegBackward0>) tensor(11895.5801, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11895.564453125
tensor(11895.5801, grad_fn=<NegBackward0>) tensor(11895.5645, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11895.5595703125
tensor(11895.5645, grad_fn=<NegBackward0>) tensor(11895.5596, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11895.5546875
tensor(11895.5596, grad_fn=<NegBackward0>) tensor(11895.5547, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11895.552734375
tensor(11895.5547, grad_fn=<NegBackward0>) tensor(11895.5527, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11895.544921875
tensor(11895.5527, grad_fn=<NegBackward0>) tensor(11895.5449, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11895.541015625
tensor(11895.5449, grad_fn=<NegBackward0>) tensor(11895.5410, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11895.53515625
tensor(11895.5410, grad_fn=<NegBackward0>) tensor(11895.5352, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11895.533203125
tensor(11895.5352, grad_fn=<NegBackward0>) tensor(11895.5332, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11895.53515625
tensor(11895.5332, grad_fn=<NegBackward0>) tensor(11895.5352, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11895.529296875
tensor(11895.5332, grad_fn=<NegBackward0>) tensor(11895.5293, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11895.53125
tensor(11895.5293, grad_fn=<NegBackward0>) tensor(11895.5312, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11895.525390625
tensor(11895.5293, grad_fn=<NegBackward0>) tensor(11895.5254, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11895.521484375
tensor(11895.5254, grad_fn=<NegBackward0>) tensor(11895.5215, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11895.521484375
tensor(11895.5215, grad_fn=<NegBackward0>) tensor(11895.5215, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11895.5205078125
tensor(11895.5215, grad_fn=<NegBackward0>) tensor(11895.5205, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11895.521484375
tensor(11895.5205, grad_fn=<NegBackward0>) tensor(11895.5215, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11895.51953125
tensor(11895.5205, grad_fn=<NegBackward0>) tensor(11895.5195, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11895.5185546875
tensor(11895.5195, grad_fn=<NegBackward0>) tensor(11895.5186, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11895.5234375
tensor(11895.5186, grad_fn=<NegBackward0>) tensor(11895.5234, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11895.521484375
tensor(11895.5186, grad_fn=<NegBackward0>) tensor(11895.5215, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11895.5126953125
tensor(11895.5186, grad_fn=<NegBackward0>) tensor(11895.5127, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11895.5107421875
tensor(11895.5127, grad_fn=<NegBackward0>) tensor(11895.5107, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11895.509765625
tensor(11895.5107, grad_fn=<NegBackward0>) tensor(11895.5098, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11895.509765625
tensor(11895.5098, grad_fn=<NegBackward0>) tensor(11895.5098, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11895.5087890625
tensor(11895.5098, grad_fn=<NegBackward0>) tensor(11895.5088, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11895.5068359375
tensor(11895.5088, grad_fn=<NegBackward0>) tensor(11895.5068, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11895.5068359375
tensor(11895.5068, grad_fn=<NegBackward0>) tensor(11895.5068, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11895.5048828125
tensor(11895.5068, grad_fn=<NegBackward0>) tensor(11895.5049, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11895.5107421875
tensor(11895.5049, grad_fn=<NegBackward0>) tensor(11895.5107, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11895.5048828125
tensor(11895.5049, grad_fn=<NegBackward0>) tensor(11895.5049, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11895.50390625
tensor(11895.5049, grad_fn=<NegBackward0>) tensor(11895.5039, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11895.50390625
tensor(11895.5039, grad_fn=<NegBackward0>) tensor(11895.5039, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11895.501953125
tensor(11895.5039, grad_fn=<NegBackward0>) tensor(11895.5020, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11895.501953125
tensor(11895.5020, grad_fn=<NegBackward0>) tensor(11895.5020, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11895.5009765625
tensor(11895.5020, grad_fn=<NegBackward0>) tensor(11895.5010, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11895.501953125
tensor(11895.5010, grad_fn=<NegBackward0>) tensor(11895.5020, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11895.50390625
tensor(11895.5010, grad_fn=<NegBackward0>) tensor(11895.5039, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11895.5
tensor(11895.5010, grad_fn=<NegBackward0>) tensor(11895.5000, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11895.5009765625
tensor(11895.5000, grad_fn=<NegBackward0>) tensor(11895.5010, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11895.4990234375
tensor(11895.5000, grad_fn=<NegBackward0>) tensor(11895.4990, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11895.5
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.5000, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11895.501953125
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.5020, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11895.4990234375
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.4990, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11895.5
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.5000, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11895.501953125
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.5020, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11895.5048828125
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.5049, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11895.5
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.5000, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11895.505859375
tensor(11895.4990, grad_fn=<NegBackward0>) tensor(11895.5059, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.6334, 0.3666],
        [0.4824, 0.5176]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4641, 0.5359], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2167, 0.0991],
         [0.5713, 0.4001]],

        [[0.5712, 0.1002],
         [0.5620, 0.6480]],

        [[0.6100, 0.1062],
         [0.5102, 0.6860]],

        [[0.5357, 0.1118],
         [0.7079, 0.5262]],

        [[0.6433, 0.1050],
         [0.5604, 0.6833]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.08080808080808081
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.523228435291725
Average Adjusted Rand Index: 0.7846457237770018
[0.9681922650727868, 0.523228435291725] [0.9684841076153855, 0.7846457237770018] [11726.4130859375, 11895.505859375]
-------------------------------------
This iteration is 40
True Objective function: Loss = -11678.145096087863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24311.396484375
inf tensor(24311.3965, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11716.1328125
tensor(24311.3965, grad_fn=<NegBackward0>) tensor(11716.1328, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11672.5654296875
tensor(11716.1328, grad_fn=<NegBackward0>) tensor(11672.5654, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11672.189453125
tensor(11672.5654, grad_fn=<NegBackward0>) tensor(11672.1895, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11672.044921875
tensor(11672.1895, grad_fn=<NegBackward0>) tensor(11672.0449, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11671.966796875
tensor(11672.0449, grad_fn=<NegBackward0>) tensor(11671.9668, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11671.9169921875
tensor(11671.9668, grad_fn=<NegBackward0>) tensor(11671.9170, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11671.884765625
tensor(11671.9170, grad_fn=<NegBackward0>) tensor(11671.8848, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11671.8603515625
tensor(11671.8848, grad_fn=<NegBackward0>) tensor(11671.8604, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11671.8447265625
tensor(11671.8604, grad_fn=<NegBackward0>) tensor(11671.8447, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11671.83203125
tensor(11671.8447, grad_fn=<NegBackward0>) tensor(11671.8320, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11671.8212890625
tensor(11671.8320, grad_fn=<NegBackward0>) tensor(11671.8213, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11671.814453125
tensor(11671.8213, grad_fn=<NegBackward0>) tensor(11671.8145, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11671.8076171875
tensor(11671.8145, grad_fn=<NegBackward0>) tensor(11671.8076, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11671.8076171875
tensor(11671.8076, grad_fn=<NegBackward0>) tensor(11671.8076, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11671.7978515625
tensor(11671.8076, grad_fn=<NegBackward0>) tensor(11671.7979, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11671.7939453125
tensor(11671.7979, grad_fn=<NegBackward0>) tensor(11671.7939, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11671.791015625
tensor(11671.7939, grad_fn=<NegBackward0>) tensor(11671.7910, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11671.7880859375
tensor(11671.7910, grad_fn=<NegBackward0>) tensor(11671.7881, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11671.78515625
tensor(11671.7881, grad_fn=<NegBackward0>) tensor(11671.7852, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11671.7841796875
tensor(11671.7852, grad_fn=<NegBackward0>) tensor(11671.7842, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11671.7822265625
tensor(11671.7842, grad_fn=<NegBackward0>) tensor(11671.7822, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11671.7822265625
tensor(11671.7822, grad_fn=<NegBackward0>) tensor(11671.7822, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11671.779296875
tensor(11671.7822, grad_fn=<NegBackward0>) tensor(11671.7793, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11671.7861328125
tensor(11671.7793, grad_fn=<NegBackward0>) tensor(11671.7861, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11671.77734375
tensor(11671.7793, grad_fn=<NegBackward0>) tensor(11671.7773, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11671.775390625
tensor(11671.7773, grad_fn=<NegBackward0>) tensor(11671.7754, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11671.7744140625
tensor(11671.7754, grad_fn=<NegBackward0>) tensor(11671.7744, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11671.7734375
tensor(11671.7744, grad_fn=<NegBackward0>) tensor(11671.7734, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11671.783203125
tensor(11671.7734, grad_fn=<NegBackward0>) tensor(11671.7832, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11671.7724609375
tensor(11671.7734, grad_fn=<NegBackward0>) tensor(11671.7725, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11671.771484375
tensor(11671.7725, grad_fn=<NegBackward0>) tensor(11671.7715, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11671.7705078125
tensor(11671.7715, grad_fn=<NegBackward0>) tensor(11671.7705, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11671.767578125
tensor(11671.7705, grad_fn=<NegBackward0>) tensor(11671.7676, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11671.771484375
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7715, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11671.767578125
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7676, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11671.7685546875
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7686, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11671.7685546875
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7686, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11671.7685546875
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7686, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -11671.7666015625
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11671.7666015625
tensor(11671.7666, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11671.7666015625
tensor(11671.7666, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11671.7666015625
tensor(11671.7666, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11671.765625
tensor(11671.7666, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11671.767578125
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7676, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11671.7646484375
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11671.7666015625
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11671.765625
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11671.767578125
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7676, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11671.763671875
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7637, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11671.7646484375
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11671.7646484375
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11671.765625
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11671.7724609375
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7725, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -11671.7646484375
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.7666, 0.2334],
        [0.3082, 0.6918]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4706, 0.5294], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4079, 0.0999],
         [0.6143, 0.1988]],

        [[0.7104, 0.1031],
         [0.5643, 0.5726]],

        [[0.5465, 0.0942],
         [0.5713, 0.5302]],

        [[0.5115, 0.1060],
         [0.6669, 0.6330]],

        [[0.6479, 0.0879],
         [0.5674, 0.6577]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22172.375
inf tensor(22172.3750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11766.23046875
tensor(22172.3750, grad_fn=<NegBackward0>) tensor(11766.2305, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11672.7822265625
tensor(11766.2305, grad_fn=<NegBackward0>) tensor(11672.7822, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11672.2919921875
tensor(11672.7822, grad_fn=<NegBackward0>) tensor(11672.2920, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11672.103515625
tensor(11672.2920, grad_fn=<NegBackward0>) tensor(11672.1035, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11672.0029296875
tensor(11672.1035, grad_fn=<NegBackward0>) tensor(11672.0029, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11671.9423828125
tensor(11672.0029, grad_fn=<NegBackward0>) tensor(11671.9424, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11671.9033203125
tensor(11671.9424, grad_fn=<NegBackward0>) tensor(11671.9033, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11671.876953125
tensor(11671.9033, grad_fn=<NegBackward0>) tensor(11671.8770, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11671.85546875
tensor(11671.8770, grad_fn=<NegBackward0>) tensor(11671.8555, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11671.83984375
tensor(11671.8555, grad_fn=<NegBackward0>) tensor(11671.8398, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11671.8291015625
tensor(11671.8398, grad_fn=<NegBackward0>) tensor(11671.8291, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11671.8193359375
tensor(11671.8291, grad_fn=<NegBackward0>) tensor(11671.8193, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11671.8125
tensor(11671.8193, grad_fn=<NegBackward0>) tensor(11671.8125, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11671.8056640625
tensor(11671.8125, grad_fn=<NegBackward0>) tensor(11671.8057, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11671.7998046875
tensor(11671.8057, grad_fn=<NegBackward0>) tensor(11671.7998, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11671.796875
tensor(11671.7998, grad_fn=<NegBackward0>) tensor(11671.7969, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11671.7939453125
tensor(11671.7969, grad_fn=<NegBackward0>) tensor(11671.7939, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11671.7900390625
tensor(11671.7939, grad_fn=<NegBackward0>) tensor(11671.7900, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11671.787109375
tensor(11671.7900, grad_fn=<NegBackward0>) tensor(11671.7871, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11671.78515625
tensor(11671.7871, grad_fn=<NegBackward0>) tensor(11671.7852, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11671.783203125
tensor(11671.7852, grad_fn=<NegBackward0>) tensor(11671.7832, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11671.78125
tensor(11671.7832, grad_fn=<NegBackward0>) tensor(11671.7812, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11671.7802734375
tensor(11671.7812, grad_fn=<NegBackward0>) tensor(11671.7803, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11671.779296875
tensor(11671.7803, grad_fn=<NegBackward0>) tensor(11671.7793, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11671.77734375
tensor(11671.7793, grad_fn=<NegBackward0>) tensor(11671.7773, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11671.775390625
tensor(11671.7773, grad_fn=<NegBackward0>) tensor(11671.7754, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11671.7744140625
tensor(11671.7754, grad_fn=<NegBackward0>) tensor(11671.7744, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11671.775390625
tensor(11671.7744, grad_fn=<NegBackward0>) tensor(11671.7754, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11671.7734375
tensor(11671.7744, grad_fn=<NegBackward0>) tensor(11671.7734, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11671.7724609375
tensor(11671.7734, grad_fn=<NegBackward0>) tensor(11671.7725, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11671.7724609375
tensor(11671.7725, grad_fn=<NegBackward0>) tensor(11671.7725, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11671.7734375
tensor(11671.7725, grad_fn=<NegBackward0>) tensor(11671.7734, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11671.7705078125
tensor(11671.7725, grad_fn=<NegBackward0>) tensor(11671.7705, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11671.76953125
tensor(11671.7705, grad_fn=<NegBackward0>) tensor(11671.7695, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11671.76953125
tensor(11671.7695, grad_fn=<NegBackward0>) tensor(11671.7695, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11671.7685546875
tensor(11671.7695, grad_fn=<NegBackward0>) tensor(11671.7686, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11671.78125
tensor(11671.7686, grad_fn=<NegBackward0>) tensor(11671.7812, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11671.767578125
tensor(11671.7686, grad_fn=<NegBackward0>) tensor(11671.7676, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11671.767578125
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7676, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11671.7685546875
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7686, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11671.7666015625
tensor(11671.7676, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11671.765625
tensor(11671.7666, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11671.765625
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11671.765625
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11671.765625
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11671.765625
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11671.767578125
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7676, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11671.7646484375
tensor(11671.7656, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11671.7646484375
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11671.7646484375
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11671.765625
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11671.7734375
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7734, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11671.7646484375
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11671.7646484375
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11671.7646484375
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11671.7705078125
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7705, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11671.763671875
tensor(11671.7646, grad_fn=<NegBackward0>) tensor(11671.7637, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11671.7666015625
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11671.765625
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11671.7626953125
tensor(11671.7637, grad_fn=<NegBackward0>) tensor(11671.7627, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11671.7646484375
tensor(11671.7627, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11671.763671875
tensor(11671.7627, grad_fn=<NegBackward0>) tensor(11671.7637, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11671.7666015625
tensor(11671.7627, grad_fn=<NegBackward0>) tensor(11671.7666, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11671.7626953125
tensor(11671.7627, grad_fn=<NegBackward0>) tensor(11671.7627, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11671.7626953125
tensor(11671.7627, grad_fn=<NegBackward0>) tensor(11671.7627, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11671.76171875
tensor(11671.7627, grad_fn=<NegBackward0>) tensor(11671.7617, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11671.7626953125
tensor(11671.7617, grad_fn=<NegBackward0>) tensor(11671.7627, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11671.7646484375
tensor(11671.7617, grad_fn=<NegBackward0>) tensor(11671.7646, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11671.765625
tensor(11671.7617, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11671.763671875
tensor(11671.7617, grad_fn=<NegBackward0>) tensor(11671.7637, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11671.765625
tensor(11671.7617, grad_fn=<NegBackward0>) tensor(11671.7656, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6889, 0.3111],
        [0.2348, 0.7652]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5295, 0.4705], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.0999],
         [0.5630, 0.4079]],

        [[0.7166, 0.1031],
         [0.5866, 0.6739]],

        [[0.5879, 0.0942],
         [0.5930, 0.6544]],

        [[0.6212, 0.1060],
         [0.7205, 0.6988]],

        [[0.6575, 0.0879],
         [0.6787, 0.6706]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11671.7646484375, 11671.765625]
-------------------------------------
This iteration is 41
True Objective function: Loss = -11982.483232998846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20885.19140625
inf tensor(20885.1914, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12563.642578125
tensor(20885.1914, grad_fn=<NegBackward0>) tensor(12563.6426, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12035.7685546875
tensor(12563.6426, grad_fn=<NegBackward0>) tensor(12035.7686, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12026.23046875
tensor(12035.7686, grad_fn=<NegBackward0>) tensor(12026.2305, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12023.0029296875
tensor(12026.2305, grad_fn=<NegBackward0>) tensor(12023.0029, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12013.705078125
tensor(12023.0029, grad_fn=<NegBackward0>) tensor(12013.7051, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11998.17578125
tensor(12013.7051, grad_fn=<NegBackward0>) tensor(11998.1758, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11998.0625
tensor(11998.1758, grad_fn=<NegBackward0>) tensor(11998.0625, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11997.9892578125
tensor(11998.0625, grad_fn=<NegBackward0>) tensor(11997.9893, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11995.9990234375
tensor(11997.9893, grad_fn=<NegBackward0>) tensor(11995.9990, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11990.6142578125
tensor(11995.9990, grad_fn=<NegBackward0>) tensor(11990.6143, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11990.5791015625
tensor(11990.6143, grad_fn=<NegBackward0>) tensor(11990.5791, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11986.6171875
tensor(11990.5791, grad_fn=<NegBackward0>) tensor(11986.6172, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11986.5966796875
tensor(11986.6172, grad_fn=<NegBackward0>) tensor(11986.5967, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11986.580078125
tensor(11986.5967, grad_fn=<NegBackward0>) tensor(11986.5801, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11986.5673828125
tensor(11986.5801, grad_fn=<NegBackward0>) tensor(11986.5674, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11986.556640625
tensor(11986.5674, grad_fn=<NegBackward0>) tensor(11986.5566, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11976.7802734375
tensor(11986.5566, grad_fn=<NegBackward0>) tensor(11976.7803, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11976.7685546875
tensor(11976.7803, grad_fn=<NegBackward0>) tensor(11976.7686, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11976.7607421875
tensor(11976.7686, grad_fn=<NegBackward0>) tensor(11976.7607, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11976.755859375
tensor(11976.7607, grad_fn=<NegBackward0>) tensor(11976.7559, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11976.75
tensor(11976.7559, grad_fn=<NegBackward0>) tensor(11976.7500, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11976.7451171875
tensor(11976.7500, grad_fn=<NegBackward0>) tensor(11976.7451, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11976.7412109375
tensor(11976.7451, grad_fn=<NegBackward0>) tensor(11976.7412, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11976.73828125
tensor(11976.7412, grad_fn=<NegBackward0>) tensor(11976.7383, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11976.7353515625
tensor(11976.7383, grad_fn=<NegBackward0>) tensor(11976.7354, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11976.732421875
tensor(11976.7354, grad_fn=<NegBackward0>) tensor(11976.7324, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11976.7294921875
tensor(11976.7324, grad_fn=<NegBackward0>) tensor(11976.7295, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11976.7275390625
tensor(11976.7295, grad_fn=<NegBackward0>) tensor(11976.7275, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11976.7255859375
tensor(11976.7275, grad_fn=<NegBackward0>) tensor(11976.7256, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11976.7236328125
tensor(11976.7256, grad_fn=<NegBackward0>) tensor(11976.7236, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11976.720703125
tensor(11976.7236, grad_fn=<NegBackward0>) tensor(11976.7207, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11976.7197265625
tensor(11976.7207, grad_fn=<NegBackward0>) tensor(11976.7197, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11976.7177734375
tensor(11976.7197, grad_fn=<NegBackward0>) tensor(11976.7178, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11976.71484375
tensor(11976.7178, grad_fn=<NegBackward0>) tensor(11976.7148, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11976.705078125
tensor(11976.7148, grad_fn=<NegBackward0>) tensor(11976.7051, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11976.6982421875
tensor(11976.7051, grad_fn=<NegBackward0>) tensor(11976.6982, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11976.6982421875
tensor(11976.6982, grad_fn=<NegBackward0>) tensor(11976.6982, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11976.6962890625
tensor(11976.6982, grad_fn=<NegBackward0>) tensor(11976.6963, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11976.697265625
tensor(11976.6963, grad_fn=<NegBackward0>) tensor(11976.6973, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11976.6962890625
tensor(11976.6963, grad_fn=<NegBackward0>) tensor(11976.6963, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11976.6943359375
tensor(11976.6963, grad_fn=<NegBackward0>) tensor(11976.6943, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11976.6943359375
tensor(11976.6943, grad_fn=<NegBackward0>) tensor(11976.6943, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11976.693359375
tensor(11976.6943, grad_fn=<NegBackward0>) tensor(11976.6934, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11976.6923828125
tensor(11976.6934, grad_fn=<NegBackward0>) tensor(11976.6924, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11976.69140625
tensor(11976.6924, grad_fn=<NegBackward0>) tensor(11976.6914, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11976.6923828125
tensor(11976.6914, grad_fn=<NegBackward0>) tensor(11976.6924, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11976.69140625
tensor(11976.6914, grad_fn=<NegBackward0>) tensor(11976.6914, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11976.6904296875
tensor(11976.6914, grad_fn=<NegBackward0>) tensor(11976.6904, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11976.6904296875
tensor(11976.6904, grad_fn=<NegBackward0>) tensor(11976.6904, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11976.6904296875
tensor(11976.6904, grad_fn=<NegBackward0>) tensor(11976.6904, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11976.689453125
tensor(11976.6904, grad_fn=<NegBackward0>) tensor(11976.6895, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11976.6884765625
tensor(11976.6895, grad_fn=<NegBackward0>) tensor(11976.6885, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11976.6884765625
tensor(11976.6885, grad_fn=<NegBackward0>) tensor(11976.6885, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11976.689453125
tensor(11976.6885, grad_fn=<NegBackward0>) tensor(11976.6895, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11976.6875
tensor(11976.6885, grad_fn=<NegBackward0>) tensor(11976.6875, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11976.6884765625
tensor(11976.6875, grad_fn=<NegBackward0>) tensor(11976.6885, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11976.6865234375
tensor(11976.6875, grad_fn=<NegBackward0>) tensor(11976.6865, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11976.6865234375
tensor(11976.6865, grad_fn=<NegBackward0>) tensor(11976.6865, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11976.697265625
tensor(11976.6865, grad_fn=<NegBackward0>) tensor(11976.6973, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11976.6865234375
tensor(11976.6865, grad_fn=<NegBackward0>) tensor(11976.6865, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11976.69140625
tensor(11976.6865, grad_fn=<NegBackward0>) tensor(11976.6914, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11976.6884765625
tensor(11976.6865, grad_fn=<NegBackward0>) tensor(11976.6885, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11976.6865234375
tensor(11976.6865, grad_fn=<NegBackward0>) tensor(11976.6865, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11976.685546875
tensor(11976.6865, grad_fn=<NegBackward0>) tensor(11976.6855, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11976.6865234375
tensor(11976.6855, grad_fn=<NegBackward0>) tensor(11976.6865, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11976.6845703125
tensor(11976.6855, grad_fn=<NegBackward0>) tensor(11976.6846, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11976.6923828125
tensor(11976.6846, grad_fn=<NegBackward0>) tensor(11976.6924, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11976.6884765625
tensor(11976.6846, grad_fn=<NegBackward0>) tensor(11976.6885, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11976.6865234375
tensor(11976.6846, grad_fn=<NegBackward0>) tensor(11976.6865, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11976.68359375
tensor(11976.6846, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11976.6962890625
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6963, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11976.68359375
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11976.6875
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6875, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11976.68359375
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11976.68359375
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11976.6845703125
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6846, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11976.68359375
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11976.763671875
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.7637, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11976.6826171875
tensor(11976.6836, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11976.68359375
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11976.6904296875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6904, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11976.6826171875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11976.6884765625
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6885, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11976.68359375
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11976.68359375
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11976.6826171875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11976.8330078125
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.8330, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11976.6826171875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11976.8876953125
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.8877, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11976.6826171875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11976.6826171875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11976.6826171875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11976.68359375
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6836, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11976.6826171875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6826, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11976.6923828125
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6924, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11976.7919921875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.7920, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11976.6904296875
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6904, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11976.6845703125
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6846, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11976.6845703125
tensor(11976.6826, grad_fn=<NegBackward0>) tensor(11976.6846, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7761, 0.2239],
        [0.2613, 0.7387]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6406, 0.3594], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4012, 0.0994],
         [0.6638, 0.1987]],

        [[0.5799, 0.1050],
         [0.6081, 0.5239]],

        [[0.6397, 0.1012],
         [0.5656, 0.6040]],

        [[0.7306, 0.0954],
         [0.6736, 0.6476]],

        [[0.5852, 0.1051],
         [0.6014, 0.6915]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21201.58203125
inf tensor(21201.5820, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12916.5439453125
tensor(21201.5820, grad_fn=<NegBackward0>) tensor(12916.5439, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12288.9814453125
tensor(12916.5439, grad_fn=<NegBackward0>) tensor(12288.9814, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12268.28515625
tensor(12288.9814, grad_fn=<NegBackward0>) tensor(12268.2852, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12267.4560546875
tensor(12268.2852, grad_fn=<NegBackward0>) tensor(12267.4561, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12267.322265625
tensor(12267.4561, grad_fn=<NegBackward0>) tensor(12267.3223, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12267.25390625
tensor(12267.3223, grad_fn=<NegBackward0>) tensor(12267.2539, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12267.203125
tensor(12267.2539, grad_fn=<NegBackward0>) tensor(12267.2031, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12264.4560546875
tensor(12267.2031, grad_fn=<NegBackward0>) tensor(12264.4561, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12264.4150390625
tensor(12264.4561, grad_fn=<NegBackward0>) tensor(12264.4150, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12264.376953125
tensor(12264.4150, grad_fn=<NegBackward0>) tensor(12264.3770, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12261.638671875
tensor(12264.3770, grad_fn=<NegBackward0>) tensor(12261.6387, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12261.6171875
tensor(12261.6387, grad_fn=<NegBackward0>) tensor(12261.6172, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12261.6064453125
tensor(12261.6172, grad_fn=<NegBackward0>) tensor(12261.6064, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12261.599609375
tensor(12261.6064, grad_fn=<NegBackward0>) tensor(12261.5996, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12261.5927734375
tensor(12261.5996, grad_fn=<NegBackward0>) tensor(12261.5928, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12261.578125
tensor(12261.5928, grad_fn=<NegBackward0>) tensor(12261.5781, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12261.572265625
tensor(12261.5781, grad_fn=<NegBackward0>) tensor(12261.5723, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12261.568359375
tensor(12261.5723, grad_fn=<NegBackward0>) tensor(12261.5684, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12261.56640625
tensor(12261.5684, grad_fn=<NegBackward0>) tensor(12261.5664, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12261.5751953125
tensor(12261.5664, grad_fn=<NegBackward0>) tensor(12261.5752, grad_fn=<NegBackward0>)
1
Iteration 2100: Loss = -12261.560546875
tensor(12261.5664, grad_fn=<NegBackward0>) tensor(12261.5605, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12261.5595703125
tensor(12261.5605, grad_fn=<NegBackward0>) tensor(12261.5596, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12261.5576171875
tensor(12261.5596, grad_fn=<NegBackward0>) tensor(12261.5576, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12261.556640625
tensor(12261.5576, grad_fn=<NegBackward0>) tensor(12261.5566, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12261.5537109375
tensor(12261.5566, grad_fn=<NegBackward0>) tensor(12261.5537, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12258.8310546875
tensor(12261.5537, grad_fn=<NegBackward0>) tensor(12258.8311, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12258.728515625
tensor(12258.8311, grad_fn=<NegBackward0>) tensor(12258.7285, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12258.7265625
tensor(12258.7285, grad_fn=<NegBackward0>) tensor(12258.7266, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12258.7236328125
tensor(12258.7266, grad_fn=<NegBackward0>) tensor(12258.7236, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12258.728515625
tensor(12258.7236, grad_fn=<NegBackward0>) tensor(12258.7285, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -12258.72265625
tensor(12258.7236, grad_fn=<NegBackward0>) tensor(12258.7227, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12258.7236328125
tensor(12258.7227, grad_fn=<NegBackward0>) tensor(12258.7236, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12258.7216796875
tensor(12258.7227, grad_fn=<NegBackward0>) tensor(12258.7217, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12258.7197265625
tensor(12258.7217, grad_fn=<NegBackward0>) tensor(12258.7197, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12258.7197265625
tensor(12258.7197, grad_fn=<NegBackward0>) tensor(12258.7197, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12258.71875
tensor(12258.7197, grad_fn=<NegBackward0>) tensor(12258.7188, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12258.71875
tensor(12258.7188, grad_fn=<NegBackward0>) tensor(12258.7188, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12258.716796875
tensor(12258.7188, grad_fn=<NegBackward0>) tensor(12258.7168, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12258.7099609375
tensor(12258.7168, grad_fn=<NegBackward0>) tensor(12258.7100, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12258.7001953125
tensor(12258.7100, grad_fn=<NegBackward0>) tensor(12258.7002, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12258.703125
tensor(12258.7002, grad_fn=<NegBackward0>) tensor(12258.7031, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12258.701171875
tensor(12258.7002, grad_fn=<NegBackward0>) tensor(12258.7012, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -12258.7021484375
tensor(12258.7002, grad_fn=<NegBackward0>) tensor(12258.7021, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -12258.7001953125
tensor(12258.7002, grad_fn=<NegBackward0>) tensor(12258.7002, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12258.7001953125
tensor(12258.7002, grad_fn=<NegBackward0>) tensor(12258.7002, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12258.69921875
tensor(12258.7002, grad_fn=<NegBackward0>) tensor(12258.6992, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12258.7001953125
tensor(12258.6992, grad_fn=<NegBackward0>) tensor(12258.7002, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12258.69921875
tensor(12258.6992, grad_fn=<NegBackward0>) tensor(12258.6992, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12258.7021484375
tensor(12258.6992, grad_fn=<NegBackward0>) tensor(12258.7021, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12258.6982421875
tensor(12258.6992, grad_fn=<NegBackward0>) tensor(12258.6982, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12258.7060546875
tensor(12258.6982, grad_fn=<NegBackward0>) tensor(12258.7061, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12258.69921875
tensor(12258.6982, grad_fn=<NegBackward0>) tensor(12258.6992, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -12258.69921875
tensor(12258.6982, grad_fn=<NegBackward0>) tensor(12258.6992, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -12258.7060546875
tensor(12258.6982, grad_fn=<NegBackward0>) tensor(12258.7061, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -12258.6953125
tensor(12258.6982, grad_fn=<NegBackward0>) tensor(12258.6953, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12258.697265625
tensor(12258.6953, grad_fn=<NegBackward0>) tensor(12258.6973, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12258.6953125
tensor(12258.6953, grad_fn=<NegBackward0>) tensor(12258.6953, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12258.6962890625
tensor(12258.6953, grad_fn=<NegBackward0>) tensor(12258.6963, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12258.6962890625
tensor(12258.6953, grad_fn=<NegBackward0>) tensor(12258.6963, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12258.6943359375
tensor(12258.6953, grad_fn=<NegBackward0>) tensor(12258.6943, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12258.693359375
tensor(12258.6943, grad_fn=<NegBackward0>) tensor(12258.6934, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12258.693359375
tensor(12258.6934, grad_fn=<NegBackward0>) tensor(12258.6934, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12258.693359375
tensor(12258.6934, grad_fn=<NegBackward0>) tensor(12258.6934, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12258.6953125
tensor(12258.6934, grad_fn=<NegBackward0>) tensor(12258.6953, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12258.6943359375
tensor(12258.6934, grad_fn=<NegBackward0>) tensor(12258.6943, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12258.69921875
tensor(12258.6934, grad_fn=<NegBackward0>) tensor(12258.6992, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12258.6953125
tensor(12258.6934, grad_fn=<NegBackward0>) tensor(12258.6953, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -12258.703125
tensor(12258.6934, grad_fn=<NegBackward0>) tensor(12258.7031, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.3143, 0.6857],
        [0.5945, 0.4055]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6639, 0.3361], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3138, 0.0960],
         [0.5567, 0.3425]],

        [[0.6344, 0.1024],
         [0.6603, 0.6052]],

        [[0.6280, 0.0971],
         [0.5802, 0.6709]],

        [[0.6458, 0.0934],
         [0.6996, 0.5911]],

        [[0.5666, 0.1014],
         [0.6273, 0.6047]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9202940985099877
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448573745636614
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8823523358261945
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448275862068966
Global Adjusted Rand Index: 0.033443589714872886
Average Adjusted Rand Index: 0.8826267858851284
[1.0, 0.033443589714872886] [1.0, 0.8826267858851284] [11976.6845703125, 12258.703125]
-------------------------------------
This iteration is 42
True Objective function: Loss = -11644.127517962863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22381.560546875
inf tensor(22381.5605, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12424.6806640625
tensor(22381.5605, grad_fn=<NegBackward0>) tensor(12424.6807, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11888.7607421875
tensor(12424.6807, grad_fn=<NegBackward0>) tensor(11888.7607, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11839.2958984375
tensor(11888.7607, grad_fn=<NegBackward0>) tensor(11839.2959, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11837.888671875
tensor(11839.2959, grad_fn=<NegBackward0>) tensor(11837.8887, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11830.6884765625
tensor(11837.8887, grad_fn=<NegBackward0>) tensor(11830.6885, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11819.96484375
tensor(11830.6885, grad_fn=<NegBackward0>) tensor(11819.9648, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11819.8427734375
tensor(11819.9648, grad_fn=<NegBackward0>) tensor(11819.8428, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11819.744140625
tensor(11819.8428, grad_fn=<NegBackward0>) tensor(11819.7441, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11819.7001953125
tensor(11819.7441, grad_fn=<NegBackward0>) tensor(11819.7002, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11819.6689453125
tensor(11819.7002, grad_fn=<NegBackward0>) tensor(11819.6689, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11819.6474609375
tensor(11819.6689, grad_fn=<NegBackward0>) tensor(11819.6475, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11819.62890625
tensor(11819.6475, grad_fn=<NegBackward0>) tensor(11819.6289, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11819.615234375
tensor(11819.6289, grad_fn=<NegBackward0>) tensor(11819.6152, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11819.6025390625
tensor(11819.6152, grad_fn=<NegBackward0>) tensor(11819.6025, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11819.591796875
tensor(11819.6025, grad_fn=<NegBackward0>) tensor(11819.5918, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11819.583984375
tensor(11819.5918, grad_fn=<NegBackward0>) tensor(11819.5840, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11819.576171875
tensor(11819.5840, grad_fn=<NegBackward0>) tensor(11819.5762, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11819.5703125
tensor(11819.5762, grad_fn=<NegBackward0>) tensor(11819.5703, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11819.5654296875
tensor(11819.5703, grad_fn=<NegBackward0>) tensor(11819.5654, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11819.5595703125
tensor(11819.5654, grad_fn=<NegBackward0>) tensor(11819.5596, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11819.5556640625
tensor(11819.5596, grad_fn=<NegBackward0>) tensor(11819.5557, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11819.552734375
tensor(11819.5557, grad_fn=<NegBackward0>) tensor(11819.5527, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11819.5498046875
tensor(11819.5527, grad_fn=<NegBackward0>) tensor(11819.5498, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11819.5478515625
tensor(11819.5498, grad_fn=<NegBackward0>) tensor(11819.5479, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11819.5458984375
tensor(11819.5479, grad_fn=<NegBackward0>) tensor(11819.5459, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11819.5419921875
tensor(11819.5459, grad_fn=<NegBackward0>) tensor(11819.5420, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11819.5400390625
tensor(11819.5420, grad_fn=<NegBackward0>) tensor(11819.5400, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11819.484375
tensor(11819.5400, grad_fn=<NegBackward0>) tensor(11819.4844, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11819.451171875
tensor(11819.4844, grad_fn=<NegBackward0>) tensor(11819.4512, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11819.4501953125
tensor(11819.4512, grad_fn=<NegBackward0>) tensor(11819.4502, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11819.44921875
tensor(11819.4502, grad_fn=<NegBackward0>) tensor(11819.4492, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11819.447265625
tensor(11819.4492, grad_fn=<NegBackward0>) tensor(11819.4473, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11819.447265625
tensor(11819.4473, grad_fn=<NegBackward0>) tensor(11819.4473, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11819.4443359375
tensor(11819.4473, grad_fn=<NegBackward0>) tensor(11819.4443, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11819.4443359375
tensor(11819.4443, grad_fn=<NegBackward0>) tensor(11819.4443, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11819.4443359375
tensor(11819.4443, grad_fn=<NegBackward0>) tensor(11819.4443, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11819.4443359375
tensor(11819.4443, grad_fn=<NegBackward0>) tensor(11819.4443, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11819.443359375
tensor(11819.4443, grad_fn=<NegBackward0>) tensor(11819.4434, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11819.44140625
tensor(11819.4434, grad_fn=<NegBackward0>) tensor(11819.4414, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11819.4404296875
tensor(11819.4414, grad_fn=<NegBackward0>) tensor(11819.4404, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11819.4404296875
tensor(11819.4404, grad_fn=<NegBackward0>) tensor(11819.4404, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11819.439453125
tensor(11819.4404, grad_fn=<NegBackward0>) tensor(11819.4395, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11819.4404296875
tensor(11819.4395, grad_fn=<NegBackward0>) tensor(11819.4404, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11819.4423828125
tensor(11819.4395, grad_fn=<NegBackward0>) tensor(11819.4424, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11819.439453125
tensor(11819.4395, grad_fn=<NegBackward0>) tensor(11819.4395, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11819.4384765625
tensor(11819.4395, grad_fn=<NegBackward0>) tensor(11819.4385, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11819.4404296875
tensor(11819.4385, grad_fn=<NegBackward0>) tensor(11819.4404, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11819.4375
tensor(11819.4385, grad_fn=<NegBackward0>) tensor(11819.4375, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11819.4365234375
tensor(11819.4375, grad_fn=<NegBackward0>) tensor(11819.4365, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11819.4384765625
tensor(11819.4365, grad_fn=<NegBackward0>) tensor(11819.4385, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11819.4384765625
tensor(11819.4365, grad_fn=<NegBackward0>) tensor(11819.4385, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11819.4423828125
tensor(11819.4365, grad_fn=<NegBackward0>) tensor(11819.4424, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11819.396484375
tensor(11819.4365, grad_fn=<NegBackward0>) tensor(11819.3965, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11819.3955078125
tensor(11819.3965, grad_fn=<NegBackward0>) tensor(11819.3955, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11819.3955078125
tensor(11819.3955, grad_fn=<NegBackward0>) tensor(11819.3955, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11819.3984375
tensor(11819.3955, grad_fn=<NegBackward0>) tensor(11819.3984, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11819.396484375
tensor(11819.3955, grad_fn=<NegBackward0>) tensor(11819.3965, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11819.3955078125
tensor(11819.3955, grad_fn=<NegBackward0>) tensor(11819.3955, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11819.3955078125
tensor(11819.3955, grad_fn=<NegBackward0>) tensor(11819.3955, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11819.39453125
tensor(11819.3955, grad_fn=<NegBackward0>) tensor(11819.3945, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11819.3955078125
tensor(11819.3945, grad_fn=<NegBackward0>) tensor(11819.3955, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11819.404296875
tensor(11819.3945, grad_fn=<NegBackward0>) tensor(11819.4043, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11819.3935546875
tensor(11819.3945, grad_fn=<NegBackward0>) tensor(11819.3936, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11819.3935546875
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.3936, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11819.4091796875
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.4092, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11819.3935546875
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.3936, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11819.3935546875
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.3936, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11819.4013671875
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.4014, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11819.39453125
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.3945, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11819.3994140625
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.3994, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11819.392578125
tensor(11819.3936, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11819.3916015625
tensor(11819.3926, grad_fn=<NegBackward0>) tensor(11819.3916, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11819.3935546875
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3936, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11819.3974609375
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3975, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11819.3935546875
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3936, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11819.3916015625
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3916, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11819.3955078125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3955, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11819.3916015625
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3916, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11819.39453125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3945, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11819.4013671875
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.4014, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11819.3916015625
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3916, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11819.3974609375
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3975, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11819.3974609375
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3975, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11819.392578125
tensor(11819.3916, grad_fn=<NegBackward0>) tensor(11819.3926, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.5005, 0.4995],
        [0.4203, 0.5797]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3924, 0.1048],
         [0.5694, 0.2190]],

        [[0.7289, 0.1012],
         [0.5738, 0.5776]],

        [[0.5675, 0.0999],
         [0.6296, 0.6156]],

        [[0.6444, 0.0985],
         [0.5661, 0.5223]],

        [[0.7011, 0.1046],
         [0.5471, 0.5360]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 78
Adjusted Rand Index: 0.3077515494541765
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.46679077916119216
Average Adjusted Rand Index: 0.8535501218239716
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22668.07421875
inf tensor(22668.0742, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12232.708984375
tensor(22668.0742, grad_fn=<NegBackward0>) tensor(12232.7090, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11933.884765625
tensor(12232.7090, grad_fn=<NegBackward0>) tensor(11933.8848, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11924.392578125
tensor(11933.8848, grad_fn=<NegBackward0>) tensor(11924.3926, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11919.1103515625
tensor(11924.3926, grad_fn=<NegBackward0>) tensor(11919.1104, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11916.525390625
tensor(11919.1104, grad_fn=<NegBackward0>) tensor(11916.5254, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11916.447265625
tensor(11916.5254, grad_fn=<NegBackward0>) tensor(11916.4473, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11916.390625
tensor(11916.4473, grad_fn=<NegBackward0>) tensor(11916.3906, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11914.3818359375
tensor(11916.3906, grad_fn=<NegBackward0>) tensor(11914.3818, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11910.7451171875
tensor(11914.3818, grad_fn=<NegBackward0>) tensor(11910.7451, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11910.7041015625
tensor(11910.7451, grad_fn=<NegBackward0>) tensor(11910.7041, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11910.603515625
tensor(11910.7041, grad_fn=<NegBackward0>) tensor(11910.6035, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11910.576171875
tensor(11910.6035, grad_fn=<NegBackward0>) tensor(11910.5762, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11910.5322265625
tensor(11910.5762, grad_fn=<NegBackward0>) tensor(11910.5322, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11910.5234375
tensor(11910.5322, grad_fn=<NegBackward0>) tensor(11910.5234, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11910.5146484375
tensor(11910.5234, grad_fn=<NegBackward0>) tensor(11910.5146, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11910.5029296875
tensor(11910.5146, grad_fn=<NegBackward0>) tensor(11910.5029, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11910.466796875
tensor(11910.5029, grad_fn=<NegBackward0>) tensor(11910.4668, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11910.4638671875
tensor(11910.4668, grad_fn=<NegBackward0>) tensor(11910.4639, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11910.4619140625
tensor(11910.4639, grad_fn=<NegBackward0>) tensor(11910.4619, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11910.4580078125
tensor(11910.4619, grad_fn=<NegBackward0>) tensor(11910.4580, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11910.45703125
tensor(11910.4580, grad_fn=<NegBackward0>) tensor(11910.4570, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11910.455078125
tensor(11910.4570, grad_fn=<NegBackward0>) tensor(11910.4551, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11910.453125
tensor(11910.4551, grad_fn=<NegBackward0>) tensor(11910.4531, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11910.453125
tensor(11910.4531, grad_fn=<NegBackward0>) tensor(11910.4531, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11910.4462890625
tensor(11910.4531, grad_fn=<NegBackward0>) tensor(11910.4463, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11910.43359375
tensor(11910.4463, grad_fn=<NegBackward0>) tensor(11910.4336, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11910.4326171875
tensor(11910.4336, grad_fn=<NegBackward0>) tensor(11910.4326, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11910.431640625
tensor(11910.4326, grad_fn=<NegBackward0>) tensor(11910.4316, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11910.435546875
tensor(11910.4316, grad_fn=<NegBackward0>) tensor(11910.4355, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11910.431640625
tensor(11910.4316, grad_fn=<NegBackward0>) tensor(11910.4316, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11910.4296875
tensor(11910.4316, grad_fn=<NegBackward0>) tensor(11910.4297, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11910.4287109375
tensor(11910.4297, grad_fn=<NegBackward0>) tensor(11910.4287, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11910.4287109375
tensor(11910.4287, grad_fn=<NegBackward0>) tensor(11910.4287, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11910.4306640625
tensor(11910.4287, grad_fn=<NegBackward0>) tensor(11910.4307, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11910.427734375
tensor(11910.4287, grad_fn=<NegBackward0>) tensor(11910.4277, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11910.427734375
tensor(11910.4277, grad_fn=<NegBackward0>) tensor(11910.4277, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11910.42578125
tensor(11910.4277, grad_fn=<NegBackward0>) tensor(11910.4258, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11910.421875
tensor(11910.4258, grad_fn=<NegBackward0>) tensor(11910.4219, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11910.421875
tensor(11910.4219, grad_fn=<NegBackward0>) tensor(11910.4219, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11910.421875
tensor(11910.4219, grad_fn=<NegBackward0>) tensor(11910.4219, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11910.419921875
tensor(11910.4219, grad_fn=<NegBackward0>) tensor(11910.4199, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11910.419921875
tensor(11910.4199, grad_fn=<NegBackward0>) tensor(11910.4199, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11910.4208984375
tensor(11910.4199, grad_fn=<NegBackward0>) tensor(11910.4209, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11910.421875
tensor(11910.4199, grad_fn=<NegBackward0>) tensor(11910.4219, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11910.4208984375
tensor(11910.4199, grad_fn=<NegBackward0>) tensor(11910.4209, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11910.4189453125
tensor(11910.4199, grad_fn=<NegBackward0>) tensor(11910.4189, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11910.4189453125
tensor(11910.4189, grad_fn=<NegBackward0>) tensor(11910.4189, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11910.4189453125
tensor(11910.4189, grad_fn=<NegBackward0>) tensor(11910.4189, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11910.4150390625
tensor(11910.4189, grad_fn=<NegBackward0>) tensor(11910.4150, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11910.4140625
tensor(11910.4150, grad_fn=<NegBackward0>) tensor(11910.4141, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11910.4150390625
tensor(11910.4141, grad_fn=<NegBackward0>) tensor(11910.4150, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11910.416015625
tensor(11910.4141, grad_fn=<NegBackward0>) tensor(11910.4160, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11910.4150390625
tensor(11910.4141, grad_fn=<NegBackward0>) tensor(11910.4150, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11910.4130859375
tensor(11910.4141, grad_fn=<NegBackward0>) tensor(11910.4131, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11910.4140625
tensor(11910.4131, grad_fn=<NegBackward0>) tensor(11910.4141, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11910.4140625
tensor(11910.4131, grad_fn=<NegBackward0>) tensor(11910.4141, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11910.4150390625
tensor(11910.4131, grad_fn=<NegBackward0>) tensor(11910.4150, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11910.4208984375
tensor(11910.4131, grad_fn=<NegBackward0>) tensor(11910.4209, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11910.2109375
tensor(11910.4131, grad_fn=<NegBackward0>) tensor(11910.2109, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11909.828125
tensor(11910.2109, grad_fn=<NegBackward0>) tensor(11909.8281, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11909.822265625
tensor(11909.8281, grad_fn=<NegBackward0>) tensor(11909.8223, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11909.8212890625
tensor(11909.8223, grad_fn=<NegBackward0>) tensor(11909.8213, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11909.8232421875
tensor(11909.8213, grad_fn=<NegBackward0>) tensor(11909.8232, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11909.8203125
tensor(11909.8213, grad_fn=<NegBackward0>) tensor(11909.8203, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11909.8212890625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8213, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11909.822265625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8223, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11909.822265625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8223, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11909.8203125
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8203, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11909.822265625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8223, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11909.822265625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8223, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11909.8212890625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8213, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11909.8212890625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8213, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11909.8203125
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8203, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11909.8251953125
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8252, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11909.8203125
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8203, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11909.8203125
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8203, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11909.8212890625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8213, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11909.8212890625
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8213, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11909.8203125
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8203, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11909.8203125
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11909.8203, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11907.8232421875
tensor(11909.8203, grad_fn=<NegBackward0>) tensor(11907.8232, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11907.78125
tensor(11907.8232, grad_fn=<NegBackward0>) tensor(11907.7812, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11907.7802734375
tensor(11907.7812, grad_fn=<NegBackward0>) tensor(11907.7803, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11907.7802734375
tensor(11907.7803, grad_fn=<NegBackward0>) tensor(11907.7803, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11907.779296875
tensor(11907.7803, grad_fn=<NegBackward0>) tensor(11907.7793, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11907.78125
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7812, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11907.7802734375
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7803, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11907.9111328125
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.9111, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11907.779296875
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7793, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11907.7880859375
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7881, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11907.779296875
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7793, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11907.78125
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7812, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11907.78125
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7812, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11907.779296875
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7793, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11907.8173828125
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.8174, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11907.7802734375
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7803, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11907.78515625
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7852, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11907.779296875
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7793, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11907.7802734375
tensor(11907.7793, grad_fn=<NegBackward0>) tensor(11907.7803, grad_fn=<NegBackward0>)
1
pi: tensor([[0.2983, 0.7017],
        [0.7472, 0.2528]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4926, 0.5074], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2775, 0.1038],
         [0.5187, 0.3294]],

        [[0.5386, 0.1009],
         [0.6536, 0.5562]],

        [[0.6418, 0.1008],
         [0.6302, 0.6439]],

        [[0.5207, 0.0957],
         [0.6554, 0.6566]],

        [[0.7154, 0.1029],
         [0.5054, 0.6779]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824283882000855
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.04649181058060934
Average Adjusted Rand Index: 0.9141006794521808
[0.46679077916119216, 0.04649181058060934] [0.8535501218239716, 0.9141006794521808] [11819.392578125, 11907.7802734375]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11358.73154668366
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23175.794921875
inf tensor(23175.7949, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11973.4482421875
tensor(23175.7949, grad_fn=<NegBackward0>) tensor(11973.4482, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11737.173828125
tensor(11973.4482, grad_fn=<NegBackward0>) tensor(11737.1738, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11345.8466796875
tensor(11737.1738, grad_fn=<NegBackward0>) tensor(11345.8467, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11343.759765625
tensor(11345.8467, grad_fn=<NegBackward0>) tensor(11343.7598, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11343.3330078125
tensor(11343.7598, grad_fn=<NegBackward0>) tensor(11343.3330, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11343.130859375
tensor(11343.3330, grad_fn=<NegBackward0>) tensor(11343.1309, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11343.0166015625
tensor(11343.1309, grad_fn=<NegBackward0>) tensor(11343.0166, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11342.9375
tensor(11343.0166, grad_fn=<NegBackward0>) tensor(11342.9375, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11342.8642578125
tensor(11342.9375, grad_fn=<NegBackward0>) tensor(11342.8643, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11340.337890625
tensor(11342.8643, grad_fn=<NegBackward0>) tensor(11340.3379, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11340.29296875
tensor(11340.3379, grad_fn=<NegBackward0>) tensor(11340.2930, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11340.2109375
tensor(11340.2930, grad_fn=<NegBackward0>) tensor(11340.2109, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11340.0859375
tensor(11340.2109, grad_fn=<NegBackward0>) tensor(11340.0859, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11340.072265625
tensor(11340.0859, grad_fn=<NegBackward0>) tensor(11340.0723, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11340.0478515625
tensor(11340.0723, grad_fn=<NegBackward0>) tensor(11340.0479, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11340.017578125
tensor(11340.0479, grad_fn=<NegBackward0>) tensor(11340.0176, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11340.009765625
tensor(11340.0176, grad_fn=<NegBackward0>) tensor(11340.0098, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11340.0009765625
tensor(11340.0098, grad_fn=<NegBackward0>) tensor(11340.0010, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11339.994140625
tensor(11340.0010, grad_fn=<NegBackward0>) tensor(11339.9941, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11339.990234375
tensor(11339.9941, grad_fn=<NegBackward0>) tensor(11339.9902, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11339.986328125
tensor(11339.9902, grad_fn=<NegBackward0>) tensor(11339.9863, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11339.982421875
tensor(11339.9863, grad_fn=<NegBackward0>) tensor(11339.9824, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11339.9755859375
tensor(11339.9824, grad_fn=<NegBackward0>) tensor(11339.9756, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11339.9306640625
tensor(11339.9756, grad_fn=<NegBackward0>) tensor(11339.9307, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11339.92578125
tensor(11339.9307, grad_fn=<NegBackward0>) tensor(11339.9258, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11339.92578125
tensor(11339.9258, grad_fn=<NegBackward0>) tensor(11339.9258, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11339.921875
tensor(11339.9258, grad_fn=<NegBackward0>) tensor(11339.9219, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11339.9208984375
tensor(11339.9219, grad_fn=<NegBackward0>) tensor(11339.9209, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11339.9189453125
tensor(11339.9209, grad_fn=<NegBackward0>) tensor(11339.9189, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11339.916015625
tensor(11339.9189, grad_fn=<NegBackward0>) tensor(11339.9160, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11339.9150390625
tensor(11339.9160, grad_fn=<NegBackward0>) tensor(11339.9150, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11339.9130859375
tensor(11339.9150, grad_fn=<NegBackward0>) tensor(11339.9131, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11339.912109375
tensor(11339.9131, grad_fn=<NegBackward0>) tensor(11339.9121, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11339.9150390625
tensor(11339.9121, grad_fn=<NegBackward0>) tensor(11339.9150, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11339.9111328125
tensor(11339.9121, grad_fn=<NegBackward0>) tensor(11339.9111, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11339.91015625
tensor(11339.9111, grad_fn=<NegBackward0>) tensor(11339.9102, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11339.908203125
tensor(11339.9102, grad_fn=<NegBackward0>) tensor(11339.9082, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11339.9072265625
tensor(11339.9082, grad_fn=<NegBackward0>) tensor(11339.9072, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11339.908203125
tensor(11339.9072, grad_fn=<NegBackward0>) tensor(11339.9082, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11339.9052734375
tensor(11339.9072, grad_fn=<NegBackward0>) tensor(11339.9053, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11339.9052734375
tensor(11339.9053, grad_fn=<NegBackward0>) tensor(11339.9053, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11339.9033203125
tensor(11339.9053, grad_fn=<NegBackward0>) tensor(11339.9033, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11339.9013671875
tensor(11339.9033, grad_fn=<NegBackward0>) tensor(11339.9014, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11339.900390625
tensor(11339.9014, grad_fn=<NegBackward0>) tensor(11339.9004, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11339.8994140625
tensor(11339.9004, grad_fn=<NegBackward0>) tensor(11339.8994, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11339.8994140625
tensor(11339.8994, grad_fn=<NegBackward0>) tensor(11339.8994, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11339.8984375
tensor(11339.8994, grad_fn=<NegBackward0>) tensor(11339.8984, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11339.8974609375
tensor(11339.8984, grad_fn=<NegBackward0>) tensor(11339.8975, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11339.896484375
tensor(11339.8975, grad_fn=<NegBackward0>) tensor(11339.8965, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11339.8955078125
tensor(11339.8965, grad_fn=<NegBackward0>) tensor(11339.8955, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11339.8955078125
tensor(11339.8955, grad_fn=<NegBackward0>) tensor(11339.8955, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11339.89453125
tensor(11339.8955, grad_fn=<NegBackward0>) tensor(11339.8945, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11339.896484375
tensor(11339.8945, grad_fn=<NegBackward0>) tensor(11339.8965, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11339.8955078125
tensor(11339.8945, grad_fn=<NegBackward0>) tensor(11339.8955, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11339.89453125
tensor(11339.8945, grad_fn=<NegBackward0>) tensor(11339.8945, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11339.8974609375
tensor(11339.8945, grad_fn=<NegBackward0>) tensor(11339.8975, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11339.892578125
tensor(11339.8945, grad_fn=<NegBackward0>) tensor(11339.8926, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11339.8935546875
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8936, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11339.89453125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8945, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11339.8935546875
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8936, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -11339.89453125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8945, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -11339.892578125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8926, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11339.892578125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8926, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11339.892578125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8926, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11339.892578125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8926, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11339.8935546875
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8936, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11339.8935546875
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8936, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11339.89453125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8945, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11339.89453125
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8945, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11339.8935546875
tensor(11339.8926, grad_fn=<NegBackward0>) tensor(11339.8936, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.8299, 0.1701],
        [0.2335, 0.7665]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5202, 0.4798], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2031, 0.1071],
         [0.6187, 0.3885]],

        [[0.5337, 0.1031],
         [0.5366, 0.7203]],

        [[0.6964, 0.1030],
         [0.5547, 0.7021]],

        [[0.7070, 0.0907],
         [0.6611, 0.6184]],

        [[0.6864, 0.0961],
         [0.6594, 0.6135]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9603166173361501
Average Adjusted Rand Index: 0.9599952658369197
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21460.896484375
inf tensor(21460.8965, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11970.380859375
tensor(21460.8965, grad_fn=<NegBackward0>) tensor(11970.3809, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11686.7119140625
tensor(11970.3809, grad_fn=<NegBackward0>) tensor(11686.7119, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11353.0322265625
tensor(11686.7119, grad_fn=<NegBackward0>) tensor(11353.0322, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11349.3408203125
tensor(11353.0322, grad_fn=<NegBackward0>) tensor(11349.3408, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11343.1396484375
tensor(11349.3408, grad_fn=<NegBackward0>) tensor(11343.1396, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11342.9873046875
tensor(11343.1396, grad_fn=<NegBackward0>) tensor(11342.9873, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11342.8935546875
tensor(11342.9873, grad_fn=<NegBackward0>) tensor(11342.8936, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11342.826171875
tensor(11342.8936, grad_fn=<NegBackward0>) tensor(11342.8262, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11342.779296875
tensor(11342.8262, grad_fn=<NegBackward0>) tensor(11342.7793, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11342.74609375
tensor(11342.7793, grad_fn=<NegBackward0>) tensor(11342.7461, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11342.7236328125
tensor(11342.7461, grad_fn=<NegBackward0>) tensor(11342.7236, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11342.705078125
tensor(11342.7236, grad_fn=<NegBackward0>) tensor(11342.7051, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11342.69140625
tensor(11342.7051, grad_fn=<NegBackward0>) tensor(11342.6914, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11342.6796875
tensor(11342.6914, grad_fn=<NegBackward0>) tensor(11342.6797, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11342.6591796875
tensor(11342.6797, grad_fn=<NegBackward0>) tensor(11342.6592, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11342.62109375
tensor(11342.6592, grad_fn=<NegBackward0>) tensor(11342.6211, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11342.6142578125
tensor(11342.6211, grad_fn=<NegBackward0>) tensor(11342.6143, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11342.6064453125
tensor(11342.6143, grad_fn=<NegBackward0>) tensor(11342.6064, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11342.16796875
tensor(11342.6064, grad_fn=<NegBackward0>) tensor(11342.1680, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11340.0908203125
tensor(11342.1680, grad_fn=<NegBackward0>) tensor(11340.0908, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11340.0859375
tensor(11340.0908, grad_fn=<NegBackward0>) tensor(11340.0859, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11340.0751953125
tensor(11340.0859, grad_fn=<NegBackward0>) tensor(11340.0752, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11339.966796875
tensor(11340.0752, grad_fn=<NegBackward0>) tensor(11339.9668, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11339.9658203125
tensor(11339.9668, grad_fn=<NegBackward0>) tensor(11339.9658, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11339.962890625
tensor(11339.9658, grad_fn=<NegBackward0>) tensor(11339.9629, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11339.9609375
tensor(11339.9629, grad_fn=<NegBackward0>) tensor(11339.9609, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11339.9580078125
tensor(11339.9609, grad_fn=<NegBackward0>) tensor(11339.9580, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11339.95703125
tensor(11339.9580, grad_fn=<NegBackward0>) tensor(11339.9570, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11339.9541015625
tensor(11339.9570, grad_fn=<NegBackward0>) tensor(11339.9541, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11339.8857421875
tensor(11339.9541, grad_fn=<NegBackward0>) tensor(11339.8857, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11339.884765625
tensor(11339.8857, grad_fn=<NegBackward0>) tensor(11339.8848, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11339.8818359375
tensor(11339.8848, grad_fn=<NegBackward0>) tensor(11339.8818, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11339.8818359375
tensor(11339.8818, grad_fn=<NegBackward0>) tensor(11339.8818, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11339.8798828125
tensor(11339.8818, grad_fn=<NegBackward0>) tensor(11339.8799, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11339.8798828125
tensor(11339.8799, grad_fn=<NegBackward0>) tensor(11339.8799, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11339.87890625
tensor(11339.8799, grad_fn=<NegBackward0>) tensor(11339.8789, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11339.87890625
tensor(11339.8789, grad_fn=<NegBackward0>) tensor(11339.8789, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11339.8779296875
tensor(11339.8789, grad_fn=<NegBackward0>) tensor(11339.8779, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11339.8779296875
tensor(11339.8779, grad_fn=<NegBackward0>) tensor(11339.8779, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11339.8759765625
tensor(11339.8779, grad_fn=<NegBackward0>) tensor(11339.8760, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11339.875
tensor(11339.8760, grad_fn=<NegBackward0>) tensor(11339.8750, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11339.875
tensor(11339.8750, grad_fn=<NegBackward0>) tensor(11339.8750, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11339.875
tensor(11339.8750, grad_fn=<NegBackward0>) tensor(11339.8750, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11339.875
tensor(11339.8750, grad_fn=<NegBackward0>) tensor(11339.8750, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11339.8740234375
tensor(11339.8750, grad_fn=<NegBackward0>) tensor(11339.8740, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11339.875
tensor(11339.8740, grad_fn=<NegBackward0>) tensor(11339.8750, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11339.873046875
tensor(11339.8740, grad_fn=<NegBackward0>) tensor(11339.8730, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11339.875
tensor(11339.8730, grad_fn=<NegBackward0>) tensor(11339.8750, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11339.8740234375
tensor(11339.8730, grad_fn=<NegBackward0>) tensor(11339.8740, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11339.873046875
tensor(11339.8730, grad_fn=<NegBackward0>) tensor(11339.8730, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11339.8720703125
tensor(11339.8730, grad_fn=<NegBackward0>) tensor(11339.8721, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11339.8720703125
tensor(11339.8721, grad_fn=<NegBackward0>) tensor(11339.8721, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11339.8720703125
tensor(11339.8721, grad_fn=<NegBackward0>) tensor(11339.8721, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11339.87109375
tensor(11339.8721, grad_fn=<NegBackward0>) tensor(11339.8711, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11339.87109375
tensor(11339.8711, grad_fn=<NegBackward0>) tensor(11339.8711, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11339.875
tensor(11339.8711, grad_fn=<NegBackward0>) tensor(11339.8750, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11339.8701171875
tensor(11339.8711, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11339.87109375
tensor(11339.8701, grad_fn=<NegBackward0>) tensor(11339.8711, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11339.8701171875
tensor(11339.8701, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11339.8701171875
tensor(11339.8701, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11339.8701171875
tensor(11339.8701, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11339.869140625
tensor(11339.8701, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11339.8701171875
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11339.87109375
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8711, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11339.869140625
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11339.8701171875
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11339.8701171875
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11339.869140625
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11339.869140625
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11339.8681640625
tensor(11339.8691, grad_fn=<NegBackward0>) tensor(11339.8682, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11339.8701171875
tensor(11339.8682, grad_fn=<NegBackward0>) tensor(11339.8701, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11339.869140625
tensor(11339.8682, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11339.869140625
tensor(11339.8682, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11339.869140625
tensor(11339.8682, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11339.869140625
tensor(11339.8682, grad_fn=<NegBackward0>) tensor(11339.8691, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.8299, 0.1701],
        [0.2353, 0.7647]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5179, 0.4821], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2031, 0.1071],
         [0.5627, 0.3886]],

        [[0.6257, 0.1032],
         [0.7226, 0.5455]],

        [[0.5317, 0.1030],
         [0.7177, 0.5690]],

        [[0.5679, 0.0907],
         [0.6805, 0.6547]],

        [[0.5139, 0.0961],
         [0.7240, 0.6217]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9603166173361501
Average Adjusted Rand Index: 0.9599952658369197
[0.9603166173361501, 0.9603166173361501] [0.9599952658369197, 0.9599952658369197] [11339.8935546875, 11339.869140625]
-------------------------------------
This iteration is 44
True Objective function: Loss = -11653.731403016836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22297.583984375
inf tensor(22297.5840, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12233.564453125
tensor(22297.5840, grad_fn=<NegBackward0>) tensor(12233.5645, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11825.8603515625
tensor(12233.5645, grad_fn=<NegBackward0>) tensor(11825.8604, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11823.615234375
tensor(11825.8604, grad_fn=<NegBackward0>) tensor(11823.6152, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11823.240234375
tensor(11823.6152, grad_fn=<NegBackward0>) tensor(11823.2402, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11823.076171875
tensor(11823.2402, grad_fn=<NegBackward0>) tensor(11823.0762, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11822.9775390625
tensor(11823.0762, grad_fn=<NegBackward0>) tensor(11822.9775, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11821.02734375
tensor(11822.9775, grad_fn=<NegBackward0>) tensor(11821.0273, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11819.97265625
tensor(11821.0273, grad_fn=<NegBackward0>) tensor(11819.9727, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11819.8125
tensor(11819.9727, grad_fn=<NegBackward0>) tensor(11819.8125, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11818.171875
tensor(11819.8125, grad_fn=<NegBackward0>) tensor(11818.1719, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11817.345703125
tensor(11818.1719, grad_fn=<NegBackward0>) tensor(11817.3457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11817.3134765625
tensor(11817.3457, grad_fn=<NegBackward0>) tensor(11817.3135, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11817.296875
tensor(11817.3135, grad_fn=<NegBackward0>) tensor(11817.2969, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11817.2861328125
tensor(11817.2969, grad_fn=<NegBackward0>) tensor(11817.2861, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11817.279296875
tensor(11817.2861, grad_fn=<NegBackward0>) tensor(11817.2793, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11817.271484375
tensor(11817.2793, grad_fn=<NegBackward0>) tensor(11817.2715, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11817.265625
tensor(11817.2715, grad_fn=<NegBackward0>) tensor(11817.2656, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11817.1884765625
tensor(11817.2656, grad_fn=<NegBackward0>) tensor(11817.1885, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11816.91796875
tensor(11817.1885, grad_fn=<NegBackward0>) tensor(11816.9180, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11816.9140625
tensor(11816.9180, grad_fn=<NegBackward0>) tensor(11816.9141, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11816.9111328125
tensor(11816.9141, grad_fn=<NegBackward0>) tensor(11816.9111, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11816.9169921875
tensor(11816.9111, grad_fn=<NegBackward0>) tensor(11816.9170, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -11816.9052734375
tensor(11816.9111, grad_fn=<NegBackward0>) tensor(11816.9053, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11816.9033203125
tensor(11816.9053, grad_fn=<NegBackward0>) tensor(11816.9033, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11816.896484375
tensor(11816.9033, grad_fn=<NegBackward0>) tensor(11816.8965, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11807.3916015625
tensor(11816.8965, grad_fn=<NegBackward0>) tensor(11807.3916, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11807.3876953125
tensor(11807.3916, grad_fn=<NegBackward0>) tensor(11807.3877, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11807.373046875
tensor(11807.3877, grad_fn=<NegBackward0>) tensor(11807.3730, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11807.30859375
tensor(11807.3730, grad_fn=<NegBackward0>) tensor(11807.3086, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11807.3076171875
tensor(11807.3086, grad_fn=<NegBackward0>) tensor(11807.3076, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11807.306640625
tensor(11807.3076, grad_fn=<NegBackward0>) tensor(11807.3066, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11807.306640625
tensor(11807.3066, grad_fn=<NegBackward0>) tensor(11807.3066, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11807.3056640625
tensor(11807.3066, grad_fn=<NegBackward0>) tensor(11807.3057, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11807.3046875
tensor(11807.3057, grad_fn=<NegBackward0>) tensor(11807.3047, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11807.3046875
tensor(11807.3047, grad_fn=<NegBackward0>) tensor(11807.3047, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11807.3037109375
tensor(11807.3047, grad_fn=<NegBackward0>) tensor(11807.3037, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11807.3037109375
tensor(11807.3037, grad_fn=<NegBackward0>) tensor(11807.3037, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11807.314453125
tensor(11807.3037, grad_fn=<NegBackward0>) tensor(11807.3145, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11807.30078125
tensor(11807.3037, grad_fn=<NegBackward0>) tensor(11807.3008, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11807.3095703125
tensor(11807.3008, grad_fn=<NegBackward0>) tensor(11807.3096, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11807.30078125
tensor(11807.3008, grad_fn=<NegBackward0>) tensor(11807.3008, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11807.302734375
tensor(11807.3008, grad_fn=<NegBackward0>) tensor(11807.3027, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11807.3037109375
tensor(11807.3008, grad_fn=<NegBackward0>) tensor(11807.3037, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11807.2998046875
tensor(11807.3008, grad_fn=<NegBackward0>) tensor(11807.2998, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11807.30078125
tensor(11807.2998, grad_fn=<NegBackward0>) tensor(11807.3008, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11807.302734375
tensor(11807.2998, grad_fn=<NegBackward0>) tensor(11807.3027, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11807.2978515625
tensor(11807.2998, grad_fn=<NegBackward0>) tensor(11807.2979, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11807.298828125
tensor(11807.2979, grad_fn=<NegBackward0>) tensor(11807.2988, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11807.298828125
tensor(11807.2979, grad_fn=<NegBackward0>) tensor(11807.2988, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11807.2978515625
tensor(11807.2979, grad_fn=<NegBackward0>) tensor(11807.2979, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11807.2978515625
tensor(11807.2979, grad_fn=<NegBackward0>) tensor(11807.2979, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11807.2998046875
tensor(11807.2979, grad_fn=<NegBackward0>) tensor(11807.2998, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11807.296875
tensor(11807.2979, grad_fn=<NegBackward0>) tensor(11807.2969, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11807.296875
tensor(11807.2969, grad_fn=<NegBackward0>) tensor(11807.2969, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11807.2958984375
tensor(11807.2969, grad_fn=<NegBackward0>) tensor(11807.2959, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11807.296875
tensor(11807.2959, grad_fn=<NegBackward0>) tensor(11807.2969, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11807.2998046875
tensor(11807.2959, grad_fn=<NegBackward0>) tensor(11807.2998, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11807.294921875
tensor(11807.2959, grad_fn=<NegBackward0>) tensor(11807.2949, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11807.294921875
tensor(11807.2949, grad_fn=<NegBackward0>) tensor(11807.2949, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11807.30078125
tensor(11807.2949, grad_fn=<NegBackward0>) tensor(11807.3008, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11807.2841796875
tensor(11807.2949, grad_fn=<NegBackward0>) tensor(11807.2842, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11807.2841796875
tensor(11807.2842, grad_fn=<NegBackward0>) tensor(11807.2842, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11807.2841796875
tensor(11807.2842, grad_fn=<NegBackward0>) tensor(11807.2842, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11807.2841796875
tensor(11807.2842, grad_fn=<NegBackward0>) tensor(11807.2842, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11807.283203125
tensor(11807.2842, grad_fn=<NegBackward0>) tensor(11807.2832, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11807.283203125
tensor(11807.2832, grad_fn=<NegBackward0>) tensor(11807.2832, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11807.283203125
tensor(11807.2832, grad_fn=<NegBackward0>) tensor(11807.2832, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11807.28515625
tensor(11807.2832, grad_fn=<NegBackward0>) tensor(11807.2852, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11807.2841796875
tensor(11807.2832, grad_fn=<NegBackward0>) tensor(11807.2842, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11807.2841796875
tensor(11807.2832, grad_fn=<NegBackward0>) tensor(11807.2842, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11807.2890625
tensor(11807.2832, grad_fn=<NegBackward0>) tensor(11807.2891, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11807.28515625
tensor(11807.2832, grad_fn=<NegBackward0>) tensor(11807.2852, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.5947, 0.4053],
        [0.5344, 0.4656]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4968, 0.5032], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2184, 0.0992],
         [0.5108, 0.3899]],

        [[0.6611, 0.0985],
         [0.6287, 0.6600]],

        [[0.7107, 0.1109],
         [0.6684, 0.6458]],

        [[0.7243, 0.1096],
         [0.5622, 0.6718]],

        [[0.5275, 0.0960],
         [0.5437, 0.6096]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 30
Adjusted Rand Index: 0.15421916488112936
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5059660370904003
Average Adjusted Rand Index: 0.8150049604453564
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23379.912109375
inf tensor(23379.9121, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11897.8408203125
tensor(23379.9121, grad_fn=<NegBackward0>) tensor(11897.8408, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11896.6982421875
tensor(11897.8408, grad_fn=<NegBackward0>) tensor(11896.6982, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11896.5380859375
tensor(11896.6982, grad_fn=<NegBackward0>) tensor(11896.5381, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11896.4677734375
tensor(11896.5381, grad_fn=<NegBackward0>) tensor(11896.4678, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11896.4326171875
tensor(11896.4678, grad_fn=<NegBackward0>) tensor(11896.4326, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11896.4072265625
tensor(11896.4326, grad_fn=<NegBackward0>) tensor(11896.4072, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11896.392578125
tensor(11896.4072, grad_fn=<NegBackward0>) tensor(11896.3926, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11896.3857421875
tensor(11896.3926, grad_fn=<NegBackward0>) tensor(11896.3857, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11896.375
tensor(11896.3857, grad_fn=<NegBackward0>) tensor(11896.3750, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11896.3701171875
tensor(11896.3750, grad_fn=<NegBackward0>) tensor(11896.3701, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11896.3662109375
tensor(11896.3701, grad_fn=<NegBackward0>) tensor(11896.3662, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11896.361328125
tensor(11896.3662, grad_fn=<NegBackward0>) tensor(11896.3613, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11896.3662109375
tensor(11896.3613, grad_fn=<NegBackward0>) tensor(11896.3662, grad_fn=<NegBackward0>)
1
Iteration 1400: Loss = -11896.357421875
tensor(11896.3613, grad_fn=<NegBackward0>) tensor(11896.3574, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11896.3544921875
tensor(11896.3574, grad_fn=<NegBackward0>) tensor(11896.3545, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11896.353515625
tensor(11896.3545, grad_fn=<NegBackward0>) tensor(11896.3535, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11896.3515625
tensor(11896.3535, grad_fn=<NegBackward0>) tensor(11896.3516, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11896.3515625
tensor(11896.3516, grad_fn=<NegBackward0>) tensor(11896.3516, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11896.361328125
tensor(11896.3516, grad_fn=<NegBackward0>) tensor(11896.3613, grad_fn=<NegBackward0>)
1
Iteration 2000: Loss = -11896.3486328125
tensor(11896.3516, grad_fn=<NegBackward0>) tensor(11896.3486, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11896.3486328125
tensor(11896.3486, grad_fn=<NegBackward0>) tensor(11896.3486, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11896.34765625
tensor(11896.3486, grad_fn=<NegBackward0>) tensor(11896.3477, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11896.3486328125
tensor(11896.3477, grad_fn=<NegBackward0>) tensor(11896.3486, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -11896.3466796875
tensor(11896.3477, grad_fn=<NegBackward0>) tensor(11896.3467, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11896.3466796875
tensor(11896.3467, grad_fn=<NegBackward0>) tensor(11896.3467, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11896.345703125
tensor(11896.3467, grad_fn=<NegBackward0>) tensor(11896.3457, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11896.3447265625
tensor(11896.3457, grad_fn=<NegBackward0>) tensor(11896.3447, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11896.345703125
tensor(11896.3447, grad_fn=<NegBackward0>) tensor(11896.3457, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11896.3447265625
tensor(11896.3447, grad_fn=<NegBackward0>) tensor(11896.3447, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11896.3447265625
tensor(11896.3447, grad_fn=<NegBackward0>) tensor(11896.3447, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11896.34375
tensor(11896.3447, grad_fn=<NegBackward0>) tensor(11896.3438, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11896.3466796875
tensor(11896.3438, grad_fn=<NegBackward0>) tensor(11896.3467, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11896.3447265625
tensor(11896.3438, grad_fn=<NegBackward0>) tensor(11896.3447, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -11896.34765625
tensor(11896.3438, grad_fn=<NegBackward0>) tensor(11896.3477, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -11896.353515625
tensor(11896.3438, grad_fn=<NegBackward0>) tensor(11896.3535, grad_fn=<NegBackward0>)
4
Iteration 3600: Loss = -11896.34375
tensor(11896.3438, grad_fn=<NegBackward0>) tensor(11896.3438, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11896.3427734375
tensor(11896.3438, grad_fn=<NegBackward0>) tensor(11896.3428, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11896.3505859375
tensor(11896.3428, grad_fn=<NegBackward0>) tensor(11896.3506, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11896.3427734375
tensor(11896.3428, grad_fn=<NegBackward0>) tensor(11896.3428, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11896.3427734375
tensor(11896.3428, grad_fn=<NegBackward0>) tensor(11896.3428, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11896.3427734375
tensor(11896.3428, grad_fn=<NegBackward0>) tensor(11896.3428, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11896.341796875
tensor(11896.3428, grad_fn=<NegBackward0>) tensor(11896.3418, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11896.3427734375
tensor(11896.3418, grad_fn=<NegBackward0>) tensor(11896.3428, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11896.341796875
tensor(11896.3418, grad_fn=<NegBackward0>) tensor(11896.3418, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11896.3408203125
tensor(11896.3418, grad_fn=<NegBackward0>) tensor(11896.3408, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11896.3408203125
tensor(11896.3408, grad_fn=<NegBackward0>) tensor(11896.3408, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11896.341796875
tensor(11896.3408, grad_fn=<NegBackward0>) tensor(11896.3418, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11896.3427734375
tensor(11896.3408, grad_fn=<NegBackward0>) tensor(11896.3428, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11896.341796875
tensor(11896.3408, grad_fn=<NegBackward0>) tensor(11896.3418, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11896.341796875
tensor(11896.3408, grad_fn=<NegBackward0>) tensor(11896.3418, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -11896.341796875
tensor(11896.3408, grad_fn=<NegBackward0>) tensor(11896.3418, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.3324, 0.6676],
        [0.6944, 0.3056]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4607, 0.5393], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3181, 0.0976],
         [0.7023, 0.2773]],

        [[0.6880, 0.1031],
         [0.6434, 0.6016]],

        [[0.6881, 0.1105],
         [0.7131, 0.6998]],

        [[0.5927, 0.1048],
         [0.5184, 0.6251]],

        [[0.5693, 0.0949],
         [0.5002, 0.6969]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.03969192634333277
Average Adjusted Rand Index: 0.8832269610439385
[0.5059660370904003, 0.03969192634333277] [0.8150049604453564, 0.8832269610439385] [11807.28515625, 11896.341796875]
-------------------------------------
This iteration is 45
True Objective function: Loss = -11401.591528817185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20786.044921875
inf tensor(20786.0449, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12203.6533203125
tensor(20786.0449, grad_fn=<NegBackward0>) tensor(12203.6533, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12078.42578125
tensor(12203.6533, grad_fn=<NegBackward0>) tensor(12078.4258, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11567.8466796875
tensor(12078.4258, grad_fn=<NegBackward0>) tensor(11567.8467, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11493.935546875
tensor(11567.8467, grad_fn=<NegBackward0>) tensor(11493.9355, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11492.990234375
tensor(11493.9355, grad_fn=<NegBackward0>) tensor(11492.9902, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11471.4658203125
tensor(11492.9902, grad_fn=<NegBackward0>) tensor(11471.4658, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11451.0576171875
tensor(11471.4658, grad_fn=<NegBackward0>) tensor(11451.0576, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11442.7490234375
tensor(11451.0576, grad_fn=<NegBackward0>) tensor(11442.7490, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11442.60546875
tensor(11442.7490, grad_fn=<NegBackward0>) tensor(11442.6055, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11442.5361328125
tensor(11442.6055, grad_fn=<NegBackward0>) tensor(11442.5361, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11430.7255859375
tensor(11442.5361, grad_fn=<NegBackward0>) tensor(11430.7256, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11428.6865234375
tensor(11430.7256, grad_fn=<NegBackward0>) tensor(11428.6865, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11416.2138671875
tensor(11428.6865, grad_fn=<NegBackward0>) tensor(11416.2139, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11416.16796875
tensor(11416.2139, grad_fn=<NegBackward0>) tensor(11416.1680, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11411.26171875
tensor(11416.1680, grad_fn=<NegBackward0>) tensor(11411.2617, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11411.18359375
tensor(11411.2617, grad_fn=<NegBackward0>) tensor(11411.1836, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11397.7099609375
tensor(11411.1836, grad_fn=<NegBackward0>) tensor(11397.7100, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11397.689453125
tensor(11397.7100, grad_fn=<NegBackward0>) tensor(11397.6895, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11397.6669921875
tensor(11397.6895, grad_fn=<NegBackward0>) tensor(11397.6670, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11397.65625
tensor(11397.6670, grad_fn=<NegBackward0>) tensor(11397.6562, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11397.6494140625
tensor(11397.6562, grad_fn=<NegBackward0>) tensor(11397.6494, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11397.642578125
tensor(11397.6494, grad_fn=<NegBackward0>) tensor(11397.6426, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11397.63671875
tensor(11397.6426, grad_fn=<NegBackward0>) tensor(11397.6367, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11397.6376953125
tensor(11397.6367, grad_fn=<NegBackward0>) tensor(11397.6377, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11397.6298828125
tensor(11397.6367, grad_fn=<NegBackward0>) tensor(11397.6299, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11397.626953125
tensor(11397.6299, grad_fn=<NegBackward0>) tensor(11397.6270, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11397.623046875
tensor(11397.6270, grad_fn=<NegBackward0>) tensor(11397.6230, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11397.62109375
tensor(11397.6230, grad_fn=<NegBackward0>) tensor(11397.6211, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11397.6181640625
tensor(11397.6211, grad_fn=<NegBackward0>) tensor(11397.6182, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11397.615234375
tensor(11397.6182, grad_fn=<NegBackward0>) tensor(11397.6152, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11397.6123046875
tensor(11397.6152, grad_fn=<NegBackward0>) tensor(11397.6123, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11397.609375
tensor(11397.6123, grad_fn=<NegBackward0>) tensor(11397.6094, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11397.6044921875
tensor(11397.6094, grad_fn=<NegBackward0>) tensor(11397.6045, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11397.607421875
tensor(11397.6045, grad_fn=<NegBackward0>) tensor(11397.6074, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11397.6044921875
tensor(11397.6045, grad_fn=<NegBackward0>) tensor(11397.6045, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11397.6025390625
tensor(11397.6045, grad_fn=<NegBackward0>) tensor(11397.6025, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11397.6015625
tensor(11397.6025, grad_fn=<NegBackward0>) tensor(11397.6016, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11397.599609375
tensor(11397.6016, grad_fn=<NegBackward0>) tensor(11397.5996, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11397.599609375
tensor(11397.5996, grad_fn=<NegBackward0>) tensor(11397.5996, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11397.5986328125
tensor(11397.5996, grad_fn=<NegBackward0>) tensor(11397.5986, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11397.59765625
tensor(11397.5986, grad_fn=<NegBackward0>) tensor(11397.5977, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11397.5966796875
tensor(11397.5977, grad_fn=<NegBackward0>) tensor(11397.5967, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11397.5966796875
tensor(11397.5967, grad_fn=<NegBackward0>) tensor(11397.5967, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11397.6103515625
tensor(11397.5967, grad_fn=<NegBackward0>) tensor(11397.6104, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11397.59375
tensor(11397.5967, grad_fn=<NegBackward0>) tensor(11397.5938, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11397.59375
tensor(11397.5938, grad_fn=<NegBackward0>) tensor(11397.5938, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11397.59375
tensor(11397.5938, grad_fn=<NegBackward0>) tensor(11397.5938, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11397.59375
tensor(11397.5938, grad_fn=<NegBackward0>) tensor(11397.5938, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11397.5927734375
tensor(11397.5938, grad_fn=<NegBackward0>) tensor(11397.5928, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11397.591796875
tensor(11397.5928, grad_fn=<NegBackward0>) tensor(11397.5918, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11397.591796875
tensor(11397.5918, grad_fn=<NegBackward0>) tensor(11397.5918, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11397.58984375
tensor(11397.5918, grad_fn=<NegBackward0>) tensor(11397.5898, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11397.58984375
tensor(11397.5898, grad_fn=<NegBackward0>) tensor(11397.5898, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11397.58984375
tensor(11397.5898, grad_fn=<NegBackward0>) tensor(11397.5898, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11397.6123046875
tensor(11397.5898, grad_fn=<NegBackward0>) tensor(11397.6123, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11397.587890625
tensor(11397.5898, grad_fn=<NegBackward0>) tensor(11397.5879, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11397.58984375
tensor(11397.5879, grad_fn=<NegBackward0>) tensor(11397.5898, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11397.5888671875
tensor(11397.5879, grad_fn=<NegBackward0>) tensor(11397.5889, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11397.587890625
tensor(11397.5879, grad_fn=<NegBackward0>) tensor(11397.5879, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11397.587890625
tensor(11397.5879, grad_fn=<NegBackward0>) tensor(11397.5879, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11397.6064453125
tensor(11397.5879, grad_fn=<NegBackward0>) tensor(11397.6064, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11397.587890625
tensor(11397.5879, grad_fn=<NegBackward0>) tensor(11397.5879, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11397.5869140625
tensor(11397.5879, grad_fn=<NegBackward0>) tensor(11397.5869, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11397.587890625
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5879, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11397.587890625
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5879, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11397.591796875
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5918, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11397.5869140625
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5869, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11397.5986328125
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5986, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11397.5869140625
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5869, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11397.587890625
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5879, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11397.5859375
tensor(11397.5869, grad_fn=<NegBackward0>) tensor(11397.5859, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11397.5849609375
tensor(11397.5859, grad_fn=<NegBackward0>) tensor(11397.5850, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11397.6162109375
tensor(11397.5850, grad_fn=<NegBackward0>) tensor(11397.6162, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11397.583984375
tensor(11397.5850, grad_fn=<NegBackward0>) tensor(11397.5840, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11397.591796875
tensor(11397.5840, grad_fn=<NegBackward0>) tensor(11397.5918, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11397.5849609375
tensor(11397.5840, grad_fn=<NegBackward0>) tensor(11397.5850, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11397.5849609375
tensor(11397.5840, grad_fn=<NegBackward0>) tensor(11397.5850, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11397.5859375
tensor(11397.5840, grad_fn=<NegBackward0>) tensor(11397.5859, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11397.5849609375
tensor(11397.5840, grad_fn=<NegBackward0>) tensor(11397.5850, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7406, 0.2594],
        [0.2465, 0.7535]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4603, 0.5397], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4093, 0.1003],
         [0.5241, 0.2000]],

        [[0.5961, 0.0885],
         [0.7122, 0.6595]],

        [[0.5046, 0.1008],
         [0.6694, 0.6998]],

        [[0.6232, 0.0951],
         [0.6739, 0.5082]],

        [[0.6720, 0.1036],
         [0.6400, 0.6502]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22629.705078125
inf tensor(22629.7051, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12203.6171875
tensor(22629.7051, grad_fn=<NegBackward0>) tensor(12203.6172, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12192.4052734375
tensor(12203.6172, grad_fn=<NegBackward0>) tensor(12192.4053, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12160.537109375
tensor(12192.4053, grad_fn=<NegBackward0>) tensor(12160.5371, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11931.951171875
tensor(12160.5371, grad_fn=<NegBackward0>) tensor(11931.9512, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11900.302734375
tensor(11931.9512, grad_fn=<NegBackward0>) tensor(11900.3027, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11899.45703125
tensor(11900.3027, grad_fn=<NegBackward0>) tensor(11899.4570, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11899.048828125
tensor(11899.4570, grad_fn=<NegBackward0>) tensor(11899.0488, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11898.9560546875
tensor(11899.0488, grad_fn=<NegBackward0>) tensor(11898.9561, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11898.8984375
tensor(11898.9561, grad_fn=<NegBackward0>) tensor(11898.8984, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11898.8515625
tensor(11898.8984, grad_fn=<NegBackward0>) tensor(11898.8516, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11898.8232421875
tensor(11898.8516, grad_fn=<NegBackward0>) tensor(11898.8232, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11898.7998046875
tensor(11898.8232, grad_fn=<NegBackward0>) tensor(11898.7998, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11898.783203125
tensor(11898.7998, grad_fn=<NegBackward0>) tensor(11898.7832, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11898.7666015625
tensor(11898.7832, grad_fn=<NegBackward0>) tensor(11898.7666, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11898.0185546875
tensor(11898.7666, grad_fn=<NegBackward0>) tensor(11898.0186, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11897.984375
tensor(11898.0186, grad_fn=<NegBackward0>) tensor(11897.9844, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11897.974609375
tensor(11897.9844, grad_fn=<NegBackward0>) tensor(11897.9746, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11897.96484375
tensor(11897.9746, grad_fn=<NegBackward0>) tensor(11897.9648, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11897.95703125
tensor(11897.9648, grad_fn=<NegBackward0>) tensor(11897.9570, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11897.947265625
tensor(11897.9570, grad_fn=<NegBackward0>) tensor(11897.9473, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11897.931640625
tensor(11897.9473, grad_fn=<NegBackward0>) tensor(11897.9316, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11897.9033203125
tensor(11897.9316, grad_fn=<NegBackward0>) tensor(11897.9033, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11897.751953125
tensor(11897.9033, grad_fn=<NegBackward0>) tensor(11897.7520, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11846.9501953125
tensor(11897.7520, grad_fn=<NegBackward0>) tensor(11846.9502, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11771.9736328125
tensor(11846.9502, grad_fn=<NegBackward0>) tensor(11771.9736, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11753.34765625
tensor(11771.9736, grad_fn=<NegBackward0>) tensor(11753.3477, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11742.740234375
tensor(11753.3477, grad_fn=<NegBackward0>) tensor(11742.7402, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11741.109375
tensor(11742.7402, grad_fn=<NegBackward0>) tensor(11741.1094, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11739.59375
tensor(11741.1094, grad_fn=<NegBackward0>) tensor(11739.5938, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11737.10546875
tensor(11739.5938, grad_fn=<NegBackward0>) tensor(11737.1055, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11736.7119140625
tensor(11737.1055, grad_fn=<NegBackward0>) tensor(11736.7119, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11736.4990234375
tensor(11736.7119, grad_fn=<NegBackward0>) tensor(11736.4990, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11736.4736328125
tensor(11736.4990, grad_fn=<NegBackward0>) tensor(11736.4736, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11736.443359375
tensor(11736.4736, grad_fn=<NegBackward0>) tensor(11736.4434, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11736.4384765625
tensor(11736.4434, grad_fn=<NegBackward0>) tensor(11736.4385, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11736.4345703125
tensor(11736.4385, grad_fn=<NegBackward0>) tensor(11736.4346, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11736.158203125
tensor(11736.4346, grad_fn=<NegBackward0>) tensor(11736.1582, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11727.66015625
tensor(11736.1582, grad_fn=<NegBackward0>) tensor(11727.6602, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11727.5439453125
tensor(11727.6602, grad_fn=<NegBackward0>) tensor(11727.5439, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11727.533203125
tensor(11727.5439, grad_fn=<NegBackward0>) tensor(11727.5332, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11727.5283203125
tensor(11727.5332, grad_fn=<NegBackward0>) tensor(11727.5283, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11727.5400390625
tensor(11727.5283, grad_fn=<NegBackward0>) tensor(11727.5400, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11718.044921875
tensor(11727.5283, grad_fn=<NegBackward0>) tensor(11718.0449, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11718.044921875
tensor(11718.0449, grad_fn=<NegBackward0>) tensor(11718.0449, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11718.041015625
tensor(11718.0449, grad_fn=<NegBackward0>) tensor(11718.0410, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11718.0380859375
tensor(11718.0410, grad_fn=<NegBackward0>) tensor(11718.0381, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11718.0107421875
tensor(11718.0381, grad_fn=<NegBackward0>) tensor(11718.0107, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11718.00390625
tensor(11718.0107, grad_fn=<NegBackward0>) tensor(11718.0039, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11718.00390625
tensor(11718.0039, grad_fn=<NegBackward0>) tensor(11718.0039, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11718.0068359375
tensor(11718.0039, grad_fn=<NegBackward0>) tensor(11718.0068, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11718.00390625
tensor(11718.0039, grad_fn=<NegBackward0>) tensor(11718.0039, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11718.00390625
tensor(11718.0039, grad_fn=<NegBackward0>) tensor(11718.0039, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11718.001953125
tensor(11718.0039, grad_fn=<NegBackward0>) tensor(11718.0020, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11718.0009765625
tensor(11718.0020, grad_fn=<NegBackward0>) tensor(11718.0010, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11718.0009765625
tensor(11718.0010, grad_fn=<NegBackward0>) tensor(11718.0010, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11718.0029296875
tensor(11718.0010, grad_fn=<NegBackward0>) tensor(11718.0029, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11718.0029296875
tensor(11718.0010, grad_fn=<NegBackward0>) tensor(11718.0029, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11718.0009765625
tensor(11718.0010, grad_fn=<NegBackward0>) tensor(11718.0010, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11709.3251953125
tensor(11718.0010, grad_fn=<NegBackward0>) tensor(11709.3252, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11709.3251953125
tensor(11709.3252, grad_fn=<NegBackward0>) tensor(11709.3252, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11709.1611328125
tensor(11709.3252, grad_fn=<NegBackward0>) tensor(11709.1611, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11709.1591796875
tensor(11709.1611, grad_fn=<NegBackward0>) tensor(11709.1592, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11709.158203125
tensor(11709.1592, grad_fn=<NegBackward0>) tensor(11709.1582, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11709.158203125
tensor(11709.1582, grad_fn=<NegBackward0>) tensor(11709.1582, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11709.158203125
tensor(11709.1582, grad_fn=<NegBackward0>) tensor(11709.1582, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11709.1669921875
tensor(11709.1582, grad_fn=<NegBackward0>) tensor(11709.1670, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11709.15625
tensor(11709.1582, grad_fn=<NegBackward0>) tensor(11709.1562, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11709.1572265625
tensor(11709.1562, grad_fn=<NegBackward0>) tensor(11709.1572, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11709.1572265625
tensor(11709.1562, grad_fn=<NegBackward0>) tensor(11709.1572, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11709.158203125
tensor(11709.1562, grad_fn=<NegBackward0>) tensor(11709.1582, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11709.166015625
tensor(11709.1562, grad_fn=<NegBackward0>) tensor(11709.1660, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11709.19140625
tensor(11709.1562, grad_fn=<NegBackward0>) tensor(11709.1914, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.5184, 0.4816],
        [0.3491, 0.6509]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4629, 0.5371], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3563, 0.1003],
         [0.5644, 0.2480]],

        [[0.6620, 0.0885],
         [0.6374, 0.5824]],

        [[0.5733, 0.1095],
         [0.5221, 0.5689]],

        [[0.5947, 0.0972],
         [0.6413, 0.7289]],

        [[0.6171, 0.0971],
         [0.5377, 0.7162]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5734430082256169
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.4853143013999451
Global Adjusted Rand Index: 0.08341246231481654
Average Adjusted Rand Index: 0.7807231039844041
[1.0, 0.08341246231481654] [1.0, 0.7807231039844041] [11397.5849609375, 11709.19140625]
-------------------------------------
This iteration is 46
True Objective function: Loss = -12020.255550269521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21440.75
inf tensor(21440.7500, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12892.8671875
tensor(21440.7500, grad_fn=<NegBackward0>) tensor(12892.8672, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12371.04296875
tensor(12892.8672, grad_fn=<NegBackward0>) tensor(12371.0430, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12162.576171875
tensor(12371.0430, grad_fn=<NegBackward0>) tensor(12162.5762, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12115.7236328125
tensor(12162.5762, grad_fn=<NegBackward0>) tensor(12115.7236, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12075.15234375
tensor(12115.7236, grad_fn=<NegBackward0>) tensor(12075.1523, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12068.2451171875
tensor(12075.1523, grad_fn=<NegBackward0>) tensor(12068.2451, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12067.662109375
tensor(12068.2451, grad_fn=<NegBackward0>) tensor(12067.6621, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12041.7216796875
tensor(12067.6621, grad_fn=<NegBackward0>) tensor(12041.7217, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12034.8349609375
tensor(12041.7217, grad_fn=<NegBackward0>) tensor(12034.8350, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12029.0400390625
tensor(12034.8350, grad_fn=<NegBackward0>) tensor(12029.0400, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12022.2529296875
tensor(12029.0400, grad_fn=<NegBackward0>) tensor(12022.2529, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12022.2138671875
tensor(12022.2529, grad_fn=<NegBackward0>) tensor(12022.2139, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12022.18359375
tensor(12022.2139, grad_fn=<NegBackward0>) tensor(12022.1836, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12022.16015625
tensor(12022.1836, grad_fn=<NegBackward0>) tensor(12022.1602, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12022.1396484375
tensor(12022.1602, grad_fn=<NegBackward0>) tensor(12022.1396, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12022.12109375
tensor(12022.1396, grad_fn=<NegBackward0>) tensor(12022.1211, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12021.5517578125
tensor(12022.1211, grad_fn=<NegBackward0>) tensor(12021.5518, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12012.6181640625
tensor(12021.5518, grad_fn=<NegBackward0>) tensor(12012.6182, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12012.6064453125
tensor(12012.6182, grad_fn=<NegBackward0>) tensor(12012.6064, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12012.59375
tensor(12012.6064, grad_fn=<NegBackward0>) tensor(12012.5938, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12012.583984375
tensor(12012.5938, grad_fn=<NegBackward0>) tensor(12012.5840, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12012.5771484375
tensor(12012.5840, grad_fn=<NegBackward0>) tensor(12012.5771, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12012.5703125
tensor(12012.5771, grad_fn=<NegBackward0>) tensor(12012.5703, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12012.5654296875
tensor(12012.5703, grad_fn=<NegBackward0>) tensor(12012.5654, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12012.5625
tensor(12012.5654, grad_fn=<NegBackward0>) tensor(12012.5625, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12012.55859375
tensor(12012.5625, grad_fn=<NegBackward0>) tensor(12012.5586, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12012.5693359375
tensor(12012.5586, grad_fn=<NegBackward0>) tensor(12012.5693, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -12012.55078125
tensor(12012.5586, grad_fn=<NegBackward0>) tensor(12012.5508, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12012.548828125
tensor(12012.5508, grad_fn=<NegBackward0>) tensor(12012.5488, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12012.544921875
tensor(12012.5488, grad_fn=<NegBackward0>) tensor(12012.5449, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12012.541015625
tensor(12012.5449, grad_fn=<NegBackward0>) tensor(12012.5410, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12012.5166015625
tensor(12012.5410, grad_fn=<NegBackward0>) tensor(12012.5166, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12012.515625
tensor(12012.5166, grad_fn=<NegBackward0>) tensor(12012.5156, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12012.5126953125
tensor(12012.5156, grad_fn=<NegBackward0>) tensor(12012.5127, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12012.5126953125
tensor(12012.5127, grad_fn=<NegBackward0>) tensor(12012.5127, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12012.509765625
tensor(12012.5127, grad_fn=<NegBackward0>) tensor(12012.5098, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12012.5087890625
tensor(12012.5098, grad_fn=<NegBackward0>) tensor(12012.5088, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12012.5087890625
tensor(12012.5088, grad_fn=<NegBackward0>) tensor(12012.5088, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12012.5068359375
tensor(12012.5088, grad_fn=<NegBackward0>) tensor(12012.5068, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12012.5048828125
tensor(12012.5068, grad_fn=<NegBackward0>) tensor(12012.5049, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12012.5029296875
tensor(12012.5049, grad_fn=<NegBackward0>) tensor(12012.5029, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12012.5048828125
tensor(12012.5029, grad_fn=<NegBackward0>) tensor(12012.5049, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12012.4990234375
tensor(12012.5029, grad_fn=<NegBackward0>) tensor(12012.4990, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12004.69921875
tensor(12012.4990, grad_fn=<NegBackward0>) tensor(12004.6992, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12004.68359375
tensor(12004.6992, grad_fn=<NegBackward0>) tensor(12004.6836, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12004.6826171875
tensor(12004.6836, grad_fn=<NegBackward0>) tensor(12004.6826, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12004.6826171875
tensor(12004.6826, grad_fn=<NegBackward0>) tensor(12004.6826, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12004.681640625
tensor(12004.6826, grad_fn=<NegBackward0>) tensor(12004.6816, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12004.6806640625
tensor(12004.6816, grad_fn=<NegBackward0>) tensor(12004.6807, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12004.6806640625
tensor(12004.6807, grad_fn=<NegBackward0>) tensor(12004.6807, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12004.6796875
tensor(12004.6807, grad_fn=<NegBackward0>) tensor(12004.6797, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12004.6796875
tensor(12004.6797, grad_fn=<NegBackward0>) tensor(12004.6797, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12004.6689453125
tensor(12004.6797, grad_fn=<NegBackward0>) tensor(12004.6689, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12004.669921875
tensor(12004.6689, grad_fn=<NegBackward0>) tensor(12004.6699, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12004.671875
tensor(12004.6689, grad_fn=<NegBackward0>) tensor(12004.6719, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12004.6669921875
tensor(12004.6689, grad_fn=<NegBackward0>) tensor(12004.6670, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12004.6689453125
tensor(12004.6670, grad_fn=<NegBackward0>) tensor(12004.6689, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12004.66796875
tensor(12004.6670, grad_fn=<NegBackward0>) tensor(12004.6680, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12004.666015625
tensor(12004.6670, grad_fn=<NegBackward0>) tensor(12004.6660, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12004.666015625
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6660, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12004.6669921875
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6670, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12004.66796875
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6680, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12004.67578125
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6758, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -12004.6669921875
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6670, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -12004.666015625
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6660, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12004.666015625
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6660, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12004.6767578125
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6768, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12004.6650390625
tensor(12004.6660, grad_fn=<NegBackward0>) tensor(12004.6650, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12004.66796875
tensor(12004.6650, grad_fn=<NegBackward0>) tensor(12004.6680, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12004.6640625
tensor(12004.6650, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12004.6640625
tensor(12004.6641, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12004.6689453125
tensor(12004.6641, grad_fn=<NegBackward0>) tensor(12004.6689, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12004.669921875
tensor(12004.6641, grad_fn=<NegBackward0>) tensor(12004.6699, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12004.6630859375
tensor(12004.6641, grad_fn=<NegBackward0>) tensor(12004.6631, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12004.6650390625
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6650, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12004.6630859375
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6631, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12004.6640625
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12004.6640625
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12004.6630859375
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6631, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12004.6630859375
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6631, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12004.6640625
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12004.6640625
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12004.6669921875
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6670, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12004.6630859375
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6631, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12004.6640625
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12004.6630859375
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6631, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12004.6640625
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12004.6630859375
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6631, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12004.662109375
tensor(12004.6631, grad_fn=<NegBackward0>) tensor(12004.6621, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12004.6640625
tensor(12004.6621, grad_fn=<NegBackward0>) tensor(12004.6641, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12004.6787109375
tensor(12004.6621, grad_fn=<NegBackward0>) tensor(12004.6787, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12004.6611328125
tensor(12004.6621, grad_fn=<NegBackward0>) tensor(12004.6611, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12004.662109375
tensor(12004.6611, grad_fn=<NegBackward0>) tensor(12004.6621, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12004.6611328125
tensor(12004.6611, grad_fn=<NegBackward0>) tensor(12004.6611, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12004.66015625
tensor(12004.6611, grad_fn=<NegBackward0>) tensor(12004.6602, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12004.6708984375
tensor(12004.6602, grad_fn=<NegBackward0>) tensor(12004.6709, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12004.662109375
tensor(12004.6602, grad_fn=<NegBackward0>) tensor(12004.6621, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -12004.6787109375
tensor(12004.6602, grad_fn=<NegBackward0>) tensor(12004.6787, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -12004.6611328125
tensor(12004.6602, grad_fn=<NegBackward0>) tensor(12004.6611, grad_fn=<NegBackward0>)
4
pi: tensor([[0.7667, 0.2333],
        [0.2843, 0.7157]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5855, 0.4145], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3967, 0.0966],
         [0.5259, 0.2149]],

        [[0.7220, 0.1080],
         [0.7248, 0.5716]],

        [[0.6623, 0.0948],
         [0.5756, 0.5588]],

        [[0.6804, 0.0995],
         [0.6103, 0.6270]],

        [[0.5929, 0.1064],
         [0.5353, 0.6990]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840287558582878
Average Adjusted Rand Index: 0.9841616161616162
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21757.265625
inf tensor(21757.2656, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12912.697265625
tensor(21757.2656, grad_fn=<NegBackward0>) tensor(12912.6973, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12588.1083984375
tensor(12912.6973, grad_fn=<NegBackward0>) tensor(12588.1084, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12276.25390625
tensor(12588.1084, grad_fn=<NegBackward0>) tensor(12276.2539, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12182.3896484375
tensor(12276.2539, grad_fn=<NegBackward0>) tensor(12182.3896, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12121.3466796875
tensor(12182.3896, grad_fn=<NegBackward0>) tensor(12121.3467, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12103.248046875
tensor(12121.3467, grad_fn=<NegBackward0>) tensor(12103.2480, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12102.62109375
tensor(12103.2480, grad_fn=<NegBackward0>) tensor(12102.6211, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12061.65234375
tensor(12102.6211, grad_fn=<NegBackward0>) tensor(12061.6523, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12050.5537109375
tensor(12061.6523, grad_fn=<NegBackward0>) tensor(12050.5537, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12050.0244140625
tensor(12050.5537, grad_fn=<NegBackward0>) tensor(12050.0244, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12049.8828125
tensor(12050.0244, grad_fn=<NegBackward0>) tensor(12049.8828, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12026.298828125
tensor(12049.8828, grad_fn=<NegBackward0>) tensor(12026.2988, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12022.93359375
tensor(12026.2988, grad_fn=<NegBackward0>) tensor(12022.9336, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12022.892578125
tensor(12022.9336, grad_fn=<NegBackward0>) tensor(12022.8926, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12022.85546875
tensor(12022.8926, grad_fn=<NegBackward0>) tensor(12022.8555, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12022.8203125
tensor(12022.8555, grad_fn=<NegBackward0>) tensor(12022.8203, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12022.80078125
tensor(12022.8203, grad_fn=<NegBackward0>) tensor(12022.8008, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12022.78125
tensor(12022.8008, grad_fn=<NegBackward0>) tensor(12022.7812, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12012.23828125
tensor(12022.7812, grad_fn=<NegBackward0>) tensor(12012.2383, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12012.201171875
tensor(12012.2383, grad_fn=<NegBackward0>) tensor(12012.2012, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12012.1904296875
tensor(12012.2012, grad_fn=<NegBackward0>) tensor(12012.1904, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12012.1826171875
tensor(12012.1904, grad_fn=<NegBackward0>) tensor(12012.1826, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12012.1728515625
tensor(12012.1826, grad_fn=<NegBackward0>) tensor(12012.1729, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12012.1669921875
tensor(12012.1729, grad_fn=<NegBackward0>) tensor(12012.1670, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12012.1611328125
tensor(12012.1670, grad_fn=<NegBackward0>) tensor(12012.1611, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12012.154296875
tensor(12012.1611, grad_fn=<NegBackward0>) tensor(12012.1543, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12012.150390625
tensor(12012.1543, grad_fn=<NegBackward0>) tensor(12012.1504, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12012.1455078125
tensor(12012.1504, grad_fn=<NegBackward0>) tensor(12012.1455, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12012.142578125
tensor(12012.1455, grad_fn=<NegBackward0>) tensor(12012.1426, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12012.138671875
tensor(12012.1426, grad_fn=<NegBackward0>) tensor(12012.1387, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12012.1357421875
tensor(12012.1387, grad_fn=<NegBackward0>) tensor(12012.1357, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12012.130859375
tensor(12012.1357, grad_fn=<NegBackward0>) tensor(12012.1309, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12012.12890625
tensor(12012.1309, grad_fn=<NegBackward0>) tensor(12012.1289, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12012.1279296875
tensor(12012.1289, grad_fn=<NegBackward0>) tensor(12012.1279, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12012.125
tensor(12012.1279, grad_fn=<NegBackward0>) tensor(12012.1250, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12012.1220703125
tensor(12012.1250, grad_fn=<NegBackward0>) tensor(12012.1221, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12012.12109375
tensor(12012.1221, grad_fn=<NegBackward0>) tensor(12012.1211, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12012.119140625
tensor(12012.1211, grad_fn=<NegBackward0>) tensor(12012.1191, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12012.1181640625
tensor(12012.1191, grad_fn=<NegBackward0>) tensor(12012.1182, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12012.1171875
tensor(12012.1182, grad_fn=<NegBackward0>) tensor(12012.1172, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12012.1142578125
tensor(12012.1172, grad_fn=<NegBackward0>) tensor(12012.1143, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12012.1142578125
tensor(12012.1143, grad_fn=<NegBackward0>) tensor(12012.1143, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12012.1123046875
tensor(12012.1143, grad_fn=<NegBackward0>) tensor(12012.1123, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12012.111328125
tensor(12012.1123, grad_fn=<NegBackward0>) tensor(12012.1113, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12012.109375
tensor(12012.1113, grad_fn=<NegBackward0>) tensor(12012.1094, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12012.109375
tensor(12012.1094, grad_fn=<NegBackward0>) tensor(12012.1094, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12012.1083984375
tensor(12012.1094, grad_fn=<NegBackward0>) tensor(12012.1084, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12012.109375
tensor(12012.1084, grad_fn=<NegBackward0>) tensor(12012.1094, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12012.1064453125
tensor(12012.1084, grad_fn=<NegBackward0>) tensor(12012.1064, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12012.1083984375
tensor(12012.1064, grad_fn=<NegBackward0>) tensor(12012.1084, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12012.1044921875
tensor(12012.1064, grad_fn=<NegBackward0>) tensor(12012.1045, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12012.103515625
tensor(12012.1045, grad_fn=<NegBackward0>) tensor(12012.1035, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12012.099609375
tensor(12012.1035, grad_fn=<NegBackward0>) tensor(12012.0996, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12012.099609375
tensor(12012.0996, grad_fn=<NegBackward0>) tensor(12012.0996, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12012.0986328125
tensor(12012.0996, grad_fn=<NegBackward0>) tensor(12012.0986, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12012.099609375
tensor(12012.0986, grad_fn=<NegBackward0>) tensor(12012.0996, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12012.103515625
tensor(12012.0986, grad_fn=<NegBackward0>) tensor(12012.1035, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12012.09765625
tensor(12012.0986, grad_fn=<NegBackward0>) tensor(12012.0977, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12012.103515625
tensor(12012.0977, grad_fn=<NegBackward0>) tensor(12012.1035, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12012.09765625
tensor(12012.0977, grad_fn=<NegBackward0>) tensor(12012.0977, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12012.095703125
tensor(12012.0977, grad_fn=<NegBackward0>) tensor(12012.0957, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12012.0986328125
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.0986, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12012.0966796875
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.0967, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12012.0966796875
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.0967, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -12012.095703125
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.0957, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12012.095703125
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.0957, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12012.103515625
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.1035, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12012.1435546875
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.1436, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12012.1025390625
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.1025, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12012.0947265625
tensor(12012.0957, grad_fn=<NegBackward0>) tensor(12012.0947, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12012.0947265625
tensor(12012.0947, grad_fn=<NegBackward0>) tensor(12012.0947, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12012.0947265625
tensor(12012.0947, grad_fn=<NegBackward0>) tensor(12012.0947, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12012.0947265625
tensor(12012.0947, grad_fn=<NegBackward0>) tensor(12012.0947, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12012.0927734375
tensor(12012.0947, grad_fn=<NegBackward0>) tensor(12012.0928, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12012.091796875
tensor(12012.0928, grad_fn=<NegBackward0>) tensor(12012.0918, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12012.09765625
tensor(12012.0918, grad_fn=<NegBackward0>) tensor(12012.0977, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12004.6904296875
tensor(12012.0918, grad_fn=<NegBackward0>) tensor(12004.6904, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12004.6982421875
tensor(12004.6904, grad_fn=<NegBackward0>) tensor(12004.6982, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12004.6884765625
tensor(12004.6904, grad_fn=<NegBackward0>) tensor(12004.6885, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12004.703125
tensor(12004.6885, grad_fn=<NegBackward0>) tensor(12004.7031, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12004.6875
tensor(12004.6885, grad_fn=<NegBackward0>) tensor(12004.6875, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12004.689453125
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6895, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12004.6904296875
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6904, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12004.787109375
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.7871, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12004.6875
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6875, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12004.6875
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6875, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12004.6904296875
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6904, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12004.6884765625
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6885, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -12004.70703125
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.7070, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -12004.689453125
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6895, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -12004.6875
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6875, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12004.6865234375
tensor(12004.6875, grad_fn=<NegBackward0>) tensor(12004.6865, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12004.6865234375
tensor(12004.6865, grad_fn=<NegBackward0>) tensor(12004.6865, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12004.7041015625
tensor(12004.6865, grad_fn=<NegBackward0>) tensor(12004.7041, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12004.6884765625
tensor(12004.6865, grad_fn=<NegBackward0>) tensor(12004.6885, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -12004.6884765625
tensor(12004.6865, grad_fn=<NegBackward0>) tensor(12004.6885, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -12004.6875
tensor(12004.6865, grad_fn=<NegBackward0>) tensor(12004.6875, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -12004.69140625
tensor(12004.6865, grad_fn=<NegBackward0>) tensor(12004.6914, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.7150, 0.2850],
        [0.2334, 0.7666]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4150, 0.5850], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2143, 0.0968],
         [0.6500, 0.3968]],

        [[0.5924, 0.1084],
         [0.5497, 0.7259]],

        [[0.7252, 0.0955],
         [0.7004, 0.7038]],

        [[0.6274, 0.0994],
         [0.5095, 0.6259]],

        [[0.5506, 0.1064],
         [0.7107, 0.6349]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840287558582878
Average Adjusted Rand Index: 0.9841616161616162
[0.9840287558582878, 0.9840287558582878] [0.9841616161616162, 0.9841616161616162] [12004.6689453125, 12004.69140625]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11542.027949635527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22504.412109375
inf tensor(22504.4121, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11922.90234375
tensor(22504.4121, grad_fn=<NegBackward0>) tensor(11922.9023, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11865.76171875
tensor(11922.9023, grad_fn=<NegBackward0>) tensor(11865.7617, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11865.2353515625
tensor(11865.7617, grad_fn=<NegBackward0>) tensor(11865.2354, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11865.0439453125
tensor(11865.2354, grad_fn=<NegBackward0>) tensor(11865.0439, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11864.9365234375
tensor(11865.0439, grad_fn=<NegBackward0>) tensor(11864.9365, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11864.84765625
tensor(11864.9365, grad_fn=<NegBackward0>) tensor(11864.8477, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11864.6669921875
tensor(11864.8477, grad_fn=<NegBackward0>) tensor(11864.6670, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11854.2900390625
tensor(11864.6670, grad_fn=<NegBackward0>) tensor(11854.2900, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11853.990234375
tensor(11854.2900, grad_fn=<NegBackward0>) tensor(11853.9902, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11853.9677734375
tensor(11853.9902, grad_fn=<NegBackward0>) tensor(11853.9678, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11853.955078125
tensor(11853.9678, grad_fn=<NegBackward0>) tensor(11853.9551, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11853.9443359375
tensor(11853.9551, grad_fn=<NegBackward0>) tensor(11853.9443, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11853.9365234375
tensor(11853.9443, grad_fn=<NegBackward0>) tensor(11853.9365, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11853.931640625
tensor(11853.9365, grad_fn=<NegBackward0>) tensor(11853.9316, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11853.92578125
tensor(11853.9316, grad_fn=<NegBackward0>) tensor(11853.9258, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11853.923828125
tensor(11853.9258, grad_fn=<NegBackward0>) tensor(11853.9238, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11853.919921875
tensor(11853.9238, grad_fn=<NegBackward0>) tensor(11853.9199, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11853.91796875
tensor(11853.9199, grad_fn=<NegBackward0>) tensor(11853.9180, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11853.9140625
tensor(11853.9180, grad_fn=<NegBackward0>) tensor(11853.9141, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11853.912109375
tensor(11853.9141, grad_fn=<NegBackward0>) tensor(11853.9121, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11853.9111328125
tensor(11853.9121, grad_fn=<NegBackward0>) tensor(11853.9111, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11853.9091796875
tensor(11853.9111, grad_fn=<NegBackward0>) tensor(11853.9092, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11853.908203125
tensor(11853.9092, grad_fn=<NegBackward0>) tensor(11853.9082, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11853.9072265625
tensor(11853.9082, grad_fn=<NegBackward0>) tensor(11853.9072, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11853.90625
tensor(11853.9072, grad_fn=<NegBackward0>) tensor(11853.9062, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11853.9052734375
tensor(11853.9062, grad_fn=<NegBackward0>) tensor(11853.9053, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11853.904296875
tensor(11853.9053, grad_fn=<NegBackward0>) tensor(11853.9043, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11853.9033203125
tensor(11853.9043, grad_fn=<NegBackward0>) tensor(11853.9033, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11853.90234375
tensor(11853.9033, grad_fn=<NegBackward0>) tensor(11853.9023, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11853.9013671875
tensor(11853.9023, grad_fn=<NegBackward0>) tensor(11853.9014, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11853.900390625
tensor(11853.9014, grad_fn=<NegBackward0>) tensor(11853.9004, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11853.900390625
tensor(11853.9004, grad_fn=<NegBackward0>) tensor(11853.9004, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11853.900390625
tensor(11853.9004, grad_fn=<NegBackward0>) tensor(11853.9004, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11853.8994140625
tensor(11853.9004, grad_fn=<NegBackward0>) tensor(11853.8994, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11853.8984375
tensor(11853.8994, grad_fn=<NegBackward0>) tensor(11853.8984, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11853.8994140625
tensor(11853.8984, grad_fn=<NegBackward0>) tensor(11853.8994, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11853.8984375
tensor(11853.8984, grad_fn=<NegBackward0>) tensor(11853.8984, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11853.8984375
tensor(11853.8984, grad_fn=<NegBackward0>) tensor(11853.8984, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11853.8994140625
tensor(11853.8984, grad_fn=<NegBackward0>) tensor(11853.8994, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11853.896484375
tensor(11853.8984, grad_fn=<NegBackward0>) tensor(11853.8965, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11853.8955078125
tensor(11853.8965, grad_fn=<NegBackward0>) tensor(11853.8955, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11853.8984375
tensor(11853.8955, grad_fn=<NegBackward0>) tensor(11853.8984, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11853.896484375
tensor(11853.8955, grad_fn=<NegBackward0>) tensor(11853.8965, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11853.896484375
tensor(11853.8955, grad_fn=<NegBackward0>) tensor(11853.8965, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11853.8955078125
tensor(11853.8955, grad_fn=<NegBackward0>) tensor(11853.8955, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11853.89453125
tensor(11853.8955, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11853.8974609375
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8975, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11853.8955078125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8955, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11853.89453125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11853.896484375
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8965, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11853.89453125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11853.89453125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11853.8955078125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8955, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11853.89453125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11853.89453125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11853.896484375
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8965, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11853.89453125
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11853.8935546875
tensor(11853.8945, grad_fn=<NegBackward0>) tensor(11853.8936, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11853.8955078125
tensor(11853.8936, grad_fn=<NegBackward0>) tensor(11853.8955, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11853.8935546875
tensor(11853.8936, grad_fn=<NegBackward0>) tensor(11853.8936, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11853.89453125
tensor(11853.8936, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11853.892578125
tensor(11853.8936, grad_fn=<NegBackward0>) tensor(11853.8926, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11853.89453125
tensor(11853.8926, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11853.89453125
tensor(11853.8926, grad_fn=<NegBackward0>) tensor(11853.8945, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11853.8935546875
tensor(11853.8926, grad_fn=<NegBackward0>) tensor(11853.8936, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11853.8984375
tensor(11853.8926, grad_fn=<NegBackward0>) tensor(11853.8984, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11853.8935546875
tensor(11853.8926, grad_fn=<NegBackward0>) tensor(11853.8936, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.6138, 0.3862],
        [0.4241, 0.5759]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8864, 0.1136], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2191, 0.1094],
         [0.6159, 0.3969]],

        [[0.5920, 0.0967],
         [0.6857, 0.6443]],

        [[0.6211, 0.0935],
         [0.5473, 0.5542]],

        [[0.6849, 0.0997],
         [0.6590, 0.7121]],

        [[0.5660, 0.1054],
         [0.5531, 0.6683]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.03531373143376781
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 15
Adjusted Rand Index: 0.48510267107807936
Global Adjusted Rand Index: 0.22509053546640523
Average Adjusted Rand Index: 0.6899577879288623
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25008.056640625
inf tensor(25008.0566, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12157.755859375
tensor(25008.0566, grad_fn=<NegBackward0>) tensor(12157.7559, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11834.67578125
tensor(12157.7559, grad_fn=<NegBackward0>) tensor(11834.6758, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11723.2314453125
tensor(11834.6758, grad_fn=<NegBackward0>) tensor(11723.2314, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11659.802734375
tensor(11723.2314, grad_fn=<NegBackward0>) tensor(11659.8027, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11655.3134765625
tensor(11659.8027, grad_fn=<NegBackward0>) tensor(11655.3135, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11655.1982421875
tensor(11655.3135, grad_fn=<NegBackward0>) tensor(11655.1982, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11655.1376953125
tensor(11655.1982, grad_fn=<NegBackward0>) tensor(11655.1377, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11655.0986328125
tensor(11655.1377, grad_fn=<NegBackward0>) tensor(11655.0986, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11655.064453125
tensor(11655.0986, grad_fn=<NegBackward0>) tensor(11655.0645, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11653.8720703125
tensor(11655.0645, grad_fn=<NegBackward0>) tensor(11653.8721, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11653.845703125
tensor(11653.8721, grad_fn=<NegBackward0>) tensor(11653.8457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11653.8271484375
tensor(11653.8457, grad_fn=<NegBackward0>) tensor(11653.8271, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11653.8193359375
tensor(11653.8271, grad_fn=<NegBackward0>) tensor(11653.8193, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11653.80859375
tensor(11653.8193, grad_fn=<NegBackward0>) tensor(11653.8086, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11653.8017578125
tensor(11653.8086, grad_fn=<NegBackward0>) tensor(11653.8018, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11653.796875
tensor(11653.8018, grad_fn=<NegBackward0>) tensor(11653.7969, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11653.79296875
tensor(11653.7969, grad_fn=<NegBackward0>) tensor(11653.7930, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11653.7900390625
tensor(11653.7930, grad_fn=<NegBackward0>) tensor(11653.7900, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11653.78515625
tensor(11653.7900, grad_fn=<NegBackward0>) tensor(11653.7852, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11653.7841796875
tensor(11653.7852, grad_fn=<NegBackward0>) tensor(11653.7842, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11653.779296875
tensor(11653.7842, grad_fn=<NegBackward0>) tensor(11653.7793, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11653.77734375
tensor(11653.7793, grad_fn=<NegBackward0>) tensor(11653.7773, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11653.7724609375
tensor(11653.7773, grad_fn=<NegBackward0>) tensor(11653.7725, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11653.76953125
tensor(11653.7725, grad_fn=<NegBackward0>) tensor(11653.7695, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11653.7685546875
tensor(11653.7695, grad_fn=<NegBackward0>) tensor(11653.7686, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11653.7646484375
tensor(11653.7686, grad_fn=<NegBackward0>) tensor(11653.7646, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11653.765625
tensor(11653.7646, grad_fn=<NegBackward0>) tensor(11653.7656, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11653.763671875
tensor(11653.7646, grad_fn=<NegBackward0>) tensor(11653.7637, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11653.763671875
tensor(11653.7637, grad_fn=<NegBackward0>) tensor(11653.7637, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11653.7607421875
tensor(11653.7637, grad_fn=<NegBackward0>) tensor(11653.7607, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11653.76171875
tensor(11653.7607, grad_fn=<NegBackward0>) tensor(11653.7617, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11653.7607421875
tensor(11653.7607, grad_fn=<NegBackward0>) tensor(11653.7607, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11653.7578125
tensor(11653.7607, grad_fn=<NegBackward0>) tensor(11653.7578, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11653.7587890625
tensor(11653.7578, grad_fn=<NegBackward0>) tensor(11653.7588, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11653.7587890625
tensor(11653.7578, grad_fn=<NegBackward0>) tensor(11653.7588, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -11653.7587890625
tensor(11653.7578, grad_fn=<NegBackward0>) tensor(11653.7588, grad_fn=<NegBackward0>)
3
Iteration 3700: Loss = -11653.7568359375
tensor(11653.7578, grad_fn=<NegBackward0>) tensor(11653.7568, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11653.75390625
tensor(11653.7568, grad_fn=<NegBackward0>) tensor(11653.7539, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11653.728515625
tensor(11653.7539, grad_fn=<NegBackward0>) tensor(11653.7285, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11653.7275390625
tensor(11653.7285, grad_fn=<NegBackward0>) tensor(11653.7275, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11653.728515625
tensor(11653.7275, grad_fn=<NegBackward0>) tensor(11653.7285, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11653.728515625
tensor(11653.7275, grad_fn=<NegBackward0>) tensor(11653.7285, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11653.7265625
tensor(11653.7275, grad_fn=<NegBackward0>) tensor(11653.7266, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11653.7275390625
tensor(11653.7266, grad_fn=<NegBackward0>) tensor(11653.7275, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11653.7275390625
tensor(11653.7266, grad_fn=<NegBackward0>) tensor(11653.7275, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11653.7275390625
tensor(11653.7266, grad_fn=<NegBackward0>) tensor(11653.7275, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11653.7265625
tensor(11653.7266, grad_fn=<NegBackward0>) tensor(11653.7266, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11653.7255859375
tensor(11653.7266, grad_fn=<NegBackward0>) tensor(11653.7256, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11653.7265625
tensor(11653.7256, grad_fn=<NegBackward0>) tensor(11653.7266, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11653.7255859375
tensor(11653.7256, grad_fn=<NegBackward0>) tensor(11653.7256, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11653.724609375
tensor(11653.7256, grad_fn=<NegBackward0>) tensor(11653.7246, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11653.724609375
tensor(11653.7246, grad_fn=<NegBackward0>) tensor(11653.7246, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11653.7255859375
tensor(11653.7246, grad_fn=<NegBackward0>) tensor(11653.7256, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11653.7255859375
tensor(11653.7246, grad_fn=<NegBackward0>) tensor(11653.7256, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11653.7333984375
tensor(11653.7246, grad_fn=<NegBackward0>) tensor(11653.7334, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11653.7236328125
tensor(11653.7246, grad_fn=<NegBackward0>) tensor(11653.7236, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11653.724609375
tensor(11653.7236, grad_fn=<NegBackward0>) tensor(11653.7246, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11653.724609375
tensor(11653.7236, grad_fn=<NegBackward0>) tensor(11653.7246, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11653.724609375
tensor(11653.7236, grad_fn=<NegBackward0>) tensor(11653.7246, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11653.724609375
tensor(11653.7236, grad_fn=<NegBackward0>) tensor(11653.7246, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -11653.724609375
tensor(11653.7236, grad_fn=<NegBackward0>) tensor(11653.7246, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.7801, 0.2199],
        [0.3590, 0.6410]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.5126e-06, 9.9999e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4123, 0.1459],
         [0.6174, 0.1872]],

        [[0.6563, 0.0975],
         [0.6419, 0.6577]],

        [[0.6101, 0.0938],
         [0.6358, 0.6933]],

        [[0.5739, 0.1000],
         [0.5197, 0.5946]],

        [[0.6553, 0.1019],
         [0.5668, 0.6374]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.7185541391019914
Average Adjusted Rand Index: 0.8
[0.22509053546640523, 0.7185541391019914] [0.6899577879288623, 0.8] [11853.8935546875, 11653.724609375]
-------------------------------------
This iteration is 48
True Objective function: Loss = -11533.280210618004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23441.365234375
inf tensor(23441.3652, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12283.3994140625
tensor(23441.3652, grad_fn=<NegBackward0>) tensor(12283.3994, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12263.017578125
tensor(12283.3994, grad_fn=<NegBackward0>) tensor(12263.0176, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11883.2802734375
tensor(12263.0176, grad_fn=<NegBackward0>) tensor(11883.2803, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11800.63671875
tensor(11883.2803, grad_fn=<NegBackward0>) tensor(11800.6367, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11782.7158203125
tensor(11800.6367, grad_fn=<NegBackward0>) tensor(11782.7158, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11775.17578125
tensor(11782.7158, grad_fn=<NegBackward0>) tensor(11775.1758, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11768.67578125
tensor(11775.1758, grad_fn=<NegBackward0>) tensor(11768.6758, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11748.46484375
tensor(11768.6758, grad_fn=<NegBackward0>) tensor(11748.4648, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11748.2978515625
tensor(11748.4648, grad_fn=<NegBackward0>) tensor(11748.2979, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11748.2119140625
tensor(11748.2979, grad_fn=<NegBackward0>) tensor(11748.2119, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11738.279296875
tensor(11748.2119, grad_fn=<NegBackward0>) tensor(11738.2793, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11727.013671875
tensor(11738.2793, grad_fn=<NegBackward0>) tensor(11727.0137, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11719.9140625
tensor(11727.0137, grad_fn=<NegBackward0>) tensor(11719.9141, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11713.0009765625
tensor(11719.9141, grad_fn=<NegBackward0>) tensor(11713.0010, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11708.3251953125
tensor(11713.0010, grad_fn=<NegBackward0>) tensor(11708.3252, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11708.201171875
tensor(11708.3252, grad_fn=<NegBackward0>) tensor(11708.2012, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11706.548828125
tensor(11708.2012, grad_fn=<NegBackward0>) tensor(11706.5488, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11697.9130859375
tensor(11706.5488, grad_fn=<NegBackward0>) tensor(11697.9131, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11697.896484375
tensor(11697.9131, grad_fn=<NegBackward0>) tensor(11697.8965, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11691.4326171875
tensor(11697.8965, grad_fn=<NegBackward0>) tensor(11691.4326, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11691.41015625
tensor(11691.4326, grad_fn=<NegBackward0>) tensor(11691.4102, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11691.3935546875
tensor(11691.4102, grad_fn=<NegBackward0>) tensor(11691.3936, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11686.31640625
tensor(11691.3936, grad_fn=<NegBackward0>) tensor(11686.3164, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11686.3095703125
tensor(11686.3164, grad_fn=<NegBackward0>) tensor(11686.3096, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11686.302734375
tensor(11686.3096, grad_fn=<NegBackward0>) tensor(11686.3027, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11686.298828125
tensor(11686.3027, grad_fn=<NegBackward0>) tensor(11686.2988, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11686.29296875
tensor(11686.2988, grad_fn=<NegBackward0>) tensor(11686.2930, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11686.2900390625
tensor(11686.2930, grad_fn=<NegBackward0>) tensor(11686.2900, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11686.287109375
tensor(11686.2900, grad_fn=<NegBackward0>) tensor(11686.2871, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11686.283203125
tensor(11686.2871, grad_fn=<NegBackward0>) tensor(11686.2832, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11686.2822265625
tensor(11686.2832, grad_fn=<NegBackward0>) tensor(11686.2822, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11686.279296875
tensor(11686.2822, grad_fn=<NegBackward0>) tensor(11686.2793, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11686.2763671875
tensor(11686.2793, grad_fn=<NegBackward0>) tensor(11686.2764, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11686.2744140625
tensor(11686.2764, grad_fn=<NegBackward0>) tensor(11686.2744, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11686.2734375
tensor(11686.2744, grad_fn=<NegBackward0>) tensor(11686.2734, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11686.2705078125
tensor(11686.2734, grad_fn=<NegBackward0>) tensor(11686.2705, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11686.26953125
tensor(11686.2705, grad_fn=<NegBackward0>) tensor(11686.2695, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11686.26953125
tensor(11686.2695, grad_fn=<NegBackward0>) tensor(11686.2695, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11686.2685546875
tensor(11686.2695, grad_fn=<NegBackward0>) tensor(11686.2686, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11686.265625
tensor(11686.2686, grad_fn=<NegBackward0>) tensor(11686.2656, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11686.2646484375
tensor(11686.2656, grad_fn=<NegBackward0>) tensor(11686.2646, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11686.263671875
tensor(11686.2646, grad_fn=<NegBackward0>) tensor(11686.2637, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11686.26171875
tensor(11686.2637, grad_fn=<NegBackward0>) tensor(11686.2617, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11686.2607421875
tensor(11686.2617, grad_fn=<NegBackward0>) tensor(11686.2607, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11686.2607421875
tensor(11686.2607, grad_fn=<NegBackward0>) tensor(11686.2607, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11686.2587890625
tensor(11686.2607, grad_fn=<NegBackward0>) tensor(11686.2588, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11686.2587890625
tensor(11686.2588, grad_fn=<NegBackward0>) tensor(11686.2588, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11686.2568359375
tensor(11686.2588, grad_fn=<NegBackward0>) tensor(11686.2568, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11686.255859375
tensor(11686.2568, grad_fn=<NegBackward0>) tensor(11686.2559, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11685.6240234375
tensor(11686.2559, grad_fn=<NegBackward0>) tensor(11685.6240, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11685.62109375
tensor(11685.6240, grad_fn=<NegBackward0>) tensor(11685.6211, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11685.62109375
tensor(11685.6211, grad_fn=<NegBackward0>) tensor(11685.6211, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11685.6201171875
tensor(11685.6211, grad_fn=<NegBackward0>) tensor(11685.6201, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11685.619140625
tensor(11685.6201, grad_fn=<NegBackward0>) tensor(11685.6191, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11685.619140625
tensor(11685.6191, grad_fn=<NegBackward0>) tensor(11685.6191, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11685.6181640625
tensor(11685.6191, grad_fn=<NegBackward0>) tensor(11685.6182, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11685.623046875
tensor(11685.6182, grad_fn=<NegBackward0>) tensor(11685.6230, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11685.6123046875
tensor(11685.6182, grad_fn=<NegBackward0>) tensor(11685.6123, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11685.5986328125
tensor(11685.6123, grad_fn=<NegBackward0>) tensor(11685.5986, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11680.48046875
tensor(11685.5986, grad_fn=<NegBackward0>) tensor(11680.4805, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11679.9853515625
tensor(11680.4805, grad_fn=<NegBackward0>) tensor(11679.9854, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11679.9296875
tensor(11679.9854, grad_fn=<NegBackward0>) tensor(11679.9297, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11679.873046875
tensor(11679.9297, grad_fn=<NegBackward0>) tensor(11679.8730, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11679.087890625
tensor(11679.8730, grad_fn=<NegBackward0>) tensor(11679.0879, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11679.0234375
tensor(11679.0879, grad_fn=<NegBackward0>) tensor(11679.0234, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11679.0185546875
tensor(11679.0234, grad_fn=<NegBackward0>) tensor(11679.0186, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11679.0146484375
tensor(11679.0186, grad_fn=<NegBackward0>) tensor(11679.0146, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11679.0166015625
tensor(11679.0146, grad_fn=<NegBackward0>) tensor(11679.0166, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11679.0146484375
tensor(11679.0146, grad_fn=<NegBackward0>) tensor(11679.0146, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11679.015625
tensor(11679.0146, grad_fn=<NegBackward0>) tensor(11679.0156, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11679.013671875
tensor(11679.0146, grad_fn=<NegBackward0>) tensor(11679.0137, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11679.0126953125
tensor(11679.0137, grad_fn=<NegBackward0>) tensor(11679.0127, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11679.0146484375
tensor(11679.0127, grad_fn=<NegBackward0>) tensor(11679.0146, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11679.0107421875
tensor(11679.0127, grad_fn=<NegBackward0>) tensor(11679.0107, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11679.005859375
tensor(11679.0107, grad_fn=<NegBackward0>) tensor(11679.0059, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11679.001953125
tensor(11679.0059, grad_fn=<NegBackward0>) tensor(11679.0020, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11678.9873046875
tensor(11679.0020, grad_fn=<NegBackward0>) tensor(11678.9873, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11678.9892578125
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11678.9893, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11678.9873046875
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11678.9873, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11678.9873046875
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11678.9873, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11679.048828125
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11679.0488, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11678.9873046875
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11678.9873, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11678.9873046875
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11678.9873, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11678.9873046875
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11678.9873, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11678.9853515625
tensor(11678.9873, grad_fn=<NegBackward0>) tensor(11678.9854, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11678.9853515625
tensor(11678.9854, grad_fn=<NegBackward0>) tensor(11678.9854, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11678.9873046875
tensor(11678.9854, grad_fn=<NegBackward0>) tensor(11678.9873, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11678.986328125
tensor(11678.9854, grad_fn=<NegBackward0>) tensor(11678.9863, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11678.9853515625
tensor(11678.9854, grad_fn=<NegBackward0>) tensor(11678.9854, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11678.984375
tensor(11678.9854, grad_fn=<NegBackward0>) tensor(11678.9844, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11678.9912109375
tensor(11678.9844, grad_fn=<NegBackward0>) tensor(11678.9912, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11679.01953125
tensor(11678.9844, grad_fn=<NegBackward0>) tensor(11679.0195, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11678.921875
tensor(11678.9844, grad_fn=<NegBackward0>) tensor(11678.9219, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11678.8779296875
tensor(11678.9219, grad_fn=<NegBackward0>) tensor(11678.8779, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11678.87890625
tensor(11678.8779, grad_fn=<NegBackward0>) tensor(11678.8789, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11675.724609375
tensor(11678.8779, grad_fn=<NegBackward0>) tensor(11675.7246, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11675.6767578125
tensor(11675.7246, grad_fn=<NegBackward0>) tensor(11675.6768, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11675.6748046875
tensor(11675.6768, grad_fn=<NegBackward0>) tensor(11675.6748, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11675.6767578125
tensor(11675.6748, grad_fn=<NegBackward0>) tensor(11675.6768, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7444, 0.2556],
        [0.3359, 0.6641]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0979, 0.9021], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3981, 0.0887],
         [0.7005, 0.2044]],

        [[0.6809, 0.1056],
         [0.5209, 0.6485]],

        [[0.6763, 0.1018],
         [0.5140, 0.6718]],

        [[0.5409, 0.1059],
         [0.5098, 0.6471]],

        [[0.5680, 0.0974],
         [0.5353, 0.6447]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.006482982171799027
time is 1
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6138955031790879
Average Adjusted Rand Index: 0.7987034035656402
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20912.021484375
inf tensor(20912.0215, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12280.3974609375
tensor(20912.0215, grad_fn=<NegBackward0>) tensor(12280.3975, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12208.9306640625
tensor(12280.3975, grad_fn=<NegBackward0>) tensor(12208.9307, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11833.59765625
tensor(12208.9307, grad_fn=<NegBackward0>) tensor(11833.5977, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11752.6953125
tensor(11833.5977, grad_fn=<NegBackward0>) tensor(11752.6953, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11732.62109375
tensor(11752.6953, grad_fn=<NegBackward0>) tensor(11732.6211, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11723.201171875
tensor(11732.6211, grad_fn=<NegBackward0>) tensor(11723.2012, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11713.189453125
tensor(11723.2012, grad_fn=<NegBackward0>) tensor(11713.1895, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11712.8056640625
tensor(11713.1895, grad_fn=<NegBackward0>) tensor(11712.8057, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11712.7138671875
tensor(11712.8057, grad_fn=<NegBackward0>) tensor(11712.7139, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11700.259765625
tensor(11712.7139, grad_fn=<NegBackward0>) tensor(11700.2598, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11700.189453125
tensor(11700.2598, grad_fn=<NegBackward0>) tensor(11700.1895, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11700.125
tensor(11700.1895, grad_fn=<NegBackward0>) tensor(11700.1250, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11693.78515625
tensor(11700.1250, grad_fn=<NegBackward0>) tensor(11693.7852, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11693.7587890625
tensor(11693.7852, grad_fn=<NegBackward0>) tensor(11693.7588, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11693.73828125
tensor(11693.7588, grad_fn=<NegBackward0>) tensor(11693.7383, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11693.705078125
tensor(11693.7383, grad_fn=<NegBackward0>) tensor(11693.7051, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11690.8681640625
tensor(11693.7051, grad_fn=<NegBackward0>) tensor(11690.8682, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11690.8505859375
tensor(11690.8682, grad_fn=<NegBackward0>) tensor(11690.8506, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11690.833984375
tensor(11690.8506, grad_fn=<NegBackward0>) tensor(11690.8340, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11690.0654296875
tensor(11690.8340, grad_fn=<NegBackward0>) tensor(11690.0654, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11690.056640625
tensor(11690.0654, grad_fn=<NegBackward0>) tensor(11690.0566, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11690.0478515625
tensor(11690.0566, grad_fn=<NegBackward0>) tensor(11690.0479, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11690.0654296875
tensor(11690.0479, grad_fn=<NegBackward0>) tensor(11690.0654, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -11690.037109375
tensor(11690.0479, grad_fn=<NegBackward0>) tensor(11690.0371, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11690.0322265625
tensor(11690.0371, grad_fn=<NegBackward0>) tensor(11690.0322, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11690.0263671875
tensor(11690.0322, grad_fn=<NegBackward0>) tensor(11690.0264, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11690.0205078125
tensor(11690.0264, grad_fn=<NegBackward0>) tensor(11690.0205, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11690.0205078125
tensor(11690.0205, grad_fn=<NegBackward0>) tensor(11690.0205, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11690.01171875
tensor(11690.0205, grad_fn=<NegBackward0>) tensor(11690.0117, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11689.98046875
tensor(11690.0117, grad_fn=<NegBackward0>) tensor(11689.9805, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11689.9755859375
tensor(11689.9805, grad_fn=<NegBackward0>) tensor(11689.9756, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11689.9736328125
tensor(11689.9756, grad_fn=<NegBackward0>) tensor(11689.9736, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11689.97265625
tensor(11689.9736, grad_fn=<NegBackward0>) tensor(11689.9727, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11689.970703125
tensor(11689.9727, grad_fn=<NegBackward0>) tensor(11689.9707, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11689.96875
tensor(11689.9707, grad_fn=<NegBackward0>) tensor(11689.9688, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11689.9677734375
tensor(11689.9688, grad_fn=<NegBackward0>) tensor(11689.9678, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11689.966796875
tensor(11689.9678, grad_fn=<NegBackward0>) tensor(11689.9668, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11689.96484375
tensor(11689.9668, grad_fn=<NegBackward0>) tensor(11689.9648, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11683.103515625
tensor(11689.9648, grad_fn=<NegBackward0>) tensor(11683.1035, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11683.099609375
tensor(11683.1035, grad_fn=<NegBackward0>) tensor(11683.0996, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11683.0986328125
tensor(11683.0996, grad_fn=<NegBackward0>) tensor(11683.0986, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11683.0986328125
tensor(11683.0986, grad_fn=<NegBackward0>) tensor(11683.0986, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11683.095703125
tensor(11683.0986, grad_fn=<NegBackward0>) tensor(11683.0957, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11683.095703125
tensor(11683.0957, grad_fn=<NegBackward0>) tensor(11683.0957, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11683.0927734375
tensor(11683.0957, grad_fn=<NegBackward0>) tensor(11683.0928, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11682.2666015625
tensor(11683.0928, grad_fn=<NegBackward0>) tensor(11682.2666, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11682.265625
tensor(11682.2666, grad_fn=<NegBackward0>) tensor(11682.2656, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11682.265625
tensor(11682.2656, grad_fn=<NegBackward0>) tensor(11682.2656, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11682.2646484375
tensor(11682.2656, grad_fn=<NegBackward0>) tensor(11682.2646, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11682.2646484375
tensor(11682.2646, grad_fn=<NegBackward0>) tensor(11682.2646, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11682.2705078125
tensor(11682.2646, grad_fn=<NegBackward0>) tensor(11682.2705, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11682.26171875
tensor(11682.2646, grad_fn=<NegBackward0>) tensor(11682.2617, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11682.2626953125
tensor(11682.2617, grad_fn=<NegBackward0>) tensor(11682.2627, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11682.265625
tensor(11682.2617, grad_fn=<NegBackward0>) tensor(11682.2656, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11682.2607421875
tensor(11682.2617, grad_fn=<NegBackward0>) tensor(11682.2607, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11682.2607421875
tensor(11682.2607, grad_fn=<NegBackward0>) tensor(11682.2607, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11682.2578125
tensor(11682.2607, grad_fn=<NegBackward0>) tensor(11682.2578, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11682.2548828125
tensor(11682.2578, grad_fn=<NegBackward0>) tensor(11682.2549, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11682.255859375
tensor(11682.2549, grad_fn=<NegBackward0>) tensor(11682.2559, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11682.2548828125
tensor(11682.2549, grad_fn=<NegBackward0>) tensor(11682.2549, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11682.255859375
tensor(11682.2549, grad_fn=<NegBackward0>) tensor(11682.2559, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11682.2578125
tensor(11682.2549, grad_fn=<NegBackward0>) tensor(11682.2578, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11682.25390625
tensor(11682.2549, grad_fn=<NegBackward0>) tensor(11682.2539, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11682.25390625
tensor(11682.2539, grad_fn=<NegBackward0>) tensor(11682.2539, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11682.25390625
tensor(11682.2539, grad_fn=<NegBackward0>) tensor(11682.2539, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11682.25390625
tensor(11682.2539, grad_fn=<NegBackward0>) tensor(11682.2539, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11682.2548828125
tensor(11682.2539, grad_fn=<NegBackward0>) tensor(11682.2549, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11682.25390625
tensor(11682.2539, grad_fn=<NegBackward0>) tensor(11682.2539, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11682.2529296875
tensor(11682.2539, grad_fn=<NegBackward0>) tensor(11682.2529, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11682.2529296875
tensor(11682.2529, grad_fn=<NegBackward0>) tensor(11682.2529, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11682.251953125
tensor(11682.2529, grad_fn=<NegBackward0>) tensor(11682.2520, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11682.2529296875
tensor(11682.2520, grad_fn=<NegBackward0>) tensor(11682.2529, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11682.2509765625
tensor(11682.2520, grad_fn=<NegBackward0>) tensor(11682.2510, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11682.25
tensor(11682.2510, grad_fn=<NegBackward0>) tensor(11682.2500, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11682.2451171875
tensor(11682.2500, grad_fn=<NegBackward0>) tensor(11682.2451, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11682.2451171875
tensor(11682.2451, grad_fn=<NegBackward0>) tensor(11682.2451, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11682.244140625
tensor(11682.2451, grad_fn=<NegBackward0>) tensor(11682.2441, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11682.2490234375
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2490, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11682.244140625
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2441, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11682.2451171875
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2451, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11682.244140625
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2441, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11682.244140625
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2441, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11682.2490234375
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2490, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11682.2734375
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2734, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11682.251953125
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2520, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11682.2431640625
tensor(11682.2441, grad_fn=<NegBackward0>) tensor(11682.2432, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11682.2529296875
tensor(11682.2432, grad_fn=<NegBackward0>) tensor(11682.2529, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11682.248046875
tensor(11682.2432, grad_fn=<NegBackward0>) tensor(11682.2480, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11682.2451171875
tensor(11682.2432, grad_fn=<NegBackward0>) tensor(11682.2451, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11682.291015625
tensor(11682.2432, grad_fn=<NegBackward0>) tensor(11682.2910, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11682.248046875
tensor(11682.2432, grad_fn=<NegBackward0>) tensor(11682.2480, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.6728, 0.3272],
        [0.2146, 0.7854]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 7.5259e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1948, 0.2018],
         [0.6892, 0.3993]],

        [[0.5678, 0.1056],
         [0.6186, 0.6426]],

        [[0.6344, 0.1018],
         [0.7245, 0.5449]],

        [[0.6985, 0.1055],
         [0.7107, 0.5146]],

        [[0.5028, 0.0974],
         [0.6177, 0.6887]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6783482224459868
Average Adjusted Rand Index: 0.8
[0.6138955031790879, 0.6783482224459868] [0.7987034035656402, 0.8] [11675.6767578125, 11682.248046875]
-------------------------------------
This iteration is 49
True Objective function: Loss = -11390.850482070342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23009.912109375
inf tensor(23009.9121, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11971.7861328125
tensor(23009.9121, grad_fn=<NegBackward0>) tensor(11971.7861, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11686.4326171875
tensor(11971.7861, grad_fn=<NegBackward0>) tensor(11686.4326, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11681.6708984375
tensor(11686.4326, grad_fn=<NegBackward0>) tensor(11681.6709, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11681.23828125
tensor(11681.6709, grad_fn=<NegBackward0>) tensor(11681.2383, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11681.109375
tensor(11681.2383, grad_fn=<NegBackward0>) tensor(11681.1094, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11681.015625
tensor(11681.1094, grad_fn=<NegBackward0>) tensor(11681.0156, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11679.3798828125
tensor(11681.0156, grad_fn=<NegBackward0>) tensor(11679.3799, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11679.263671875
tensor(11679.3799, grad_fn=<NegBackward0>) tensor(11679.2637, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11679.2373046875
tensor(11679.2637, grad_fn=<NegBackward0>) tensor(11679.2373, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11679.21484375
tensor(11679.2373, grad_fn=<NegBackward0>) tensor(11679.2148, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11677.8056640625
tensor(11679.2148, grad_fn=<NegBackward0>) tensor(11677.8057, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11677.634765625
tensor(11677.8057, grad_fn=<NegBackward0>) tensor(11677.6348, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11677.6279296875
tensor(11677.6348, grad_fn=<NegBackward0>) tensor(11677.6279, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11677.62109375
tensor(11677.6279, grad_fn=<NegBackward0>) tensor(11677.6211, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11677.6171875
tensor(11677.6211, grad_fn=<NegBackward0>) tensor(11677.6172, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11677.61328125
tensor(11677.6172, grad_fn=<NegBackward0>) tensor(11677.6133, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11677.609375
tensor(11677.6133, grad_fn=<NegBackward0>) tensor(11677.6094, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11677.6044921875
tensor(11677.6094, grad_fn=<NegBackward0>) tensor(11677.6045, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11677.595703125
tensor(11677.6045, grad_fn=<NegBackward0>) tensor(11677.5957, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11677.591796875
tensor(11677.5957, grad_fn=<NegBackward0>) tensor(11677.5918, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11677.462890625
tensor(11677.5918, grad_fn=<NegBackward0>) tensor(11677.4629, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11677.4609375
tensor(11677.4629, grad_fn=<NegBackward0>) tensor(11677.4609, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11677.4580078125
tensor(11677.4609, grad_fn=<NegBackward0>) tensor(11677.4580, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11677.4580078125
tensor(11677.4580, grad_fn=<NegBackward0>) tensor(11677.4580, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11677.4599609375
tensor(11677.4580, grad_fn=<NegBackward0>) tensor(11677.4600, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11673.4453125
tensor(11677.4580, grad_fn=<NegBackward0>) tensor(11673.4453, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11672.52734375
tensor(11673.4453, grad_fn=<NegBackward0>) tensor(11672.5273, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11672.5205078125
tensor(11672.5273, grad_fn=<NegBackward0>) tensor(11672.5205, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11672.515625
tensor(11672.5205, grad_fn=<NegBackward0>) tensor(11672.5156, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11672.515625
tensor(11672.5156, grad_fn=<NegBackward0>) tensor(11672.5156, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11672.513671875
tensor(11672.5156, grad_fn=<NegBackward0>) tensor(11672.5137, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11672.513671875
tensor(11672.5137, grad_fn=<NegBackward0>) tensor(11672.5137, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11672.513671875
tensor(11672.5137, grad_fn=<NegBackward0>) tensor(11672.5137, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11672.2607421875
tensor(11672.5137, grad_fn=<NegBackward0>) tensor(11672.2607, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11670.2880859375
tensor(11672.2607, grad_fn=<NegBackward0>) tensor(11670.2881, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11670.2822265625
tensor(11670.2881, grad_fn=<NegBackward0>) tensor(11670.2822, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11670.271484375
tensor(11670.2822, grad_fn=<NegBackward0>) tensor(11670.2715, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11670.2724609375
tensor(11670.2715, grad_fn=<NegBackward0>) tensor(11670.2725, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11670.271484375
tensor(11670.2715, grad_fn=<NegBackward0>) tensor(11670.2715, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11670.2705078125
tensor(11670.2715, grad_fn=<NegBackward0>) tensor(11670.2705, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11670.271484375
tensor(11670.2705, grad_fn=<NegBackward0>) tensor(11670.2715, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11670.26953125
tensor(11670.2705, grad_fn=<NegBackward0>) tensor(11670.2695, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11670.26953125
tensor(11670.2695, grad_fn=<NegBackward0>) tensor(11670.2695, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11670.2705078125
tensor(11670.2695, grad_fn=<NegBackward0>) tensor(11670.2705, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11670.26953125
tensor(11670.2695, grad_fn=<NegBackward0>) tensor(11670.2695, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11670.267578125
tensor(11670.2695, grad_fn=<NegBackward0>) tensor(11670.2676, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11670.2685546875
tensor(11670.2676, grad_fn=<NegBackward0>) tensor(11670.2686, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11670.267578125
tensor(11670.2676, grad_fn=<NegBackward0>) tensor(11670.2676, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11670.2744140625
tensor(11670.2676, grad_fn=<NegBackward0>) tensor(11670.2744, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11670.2666015625
tensor(11670.2676, grad_fn=<NegBackward0>) tensor(11670.2666, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11670.267578125
tensor(11670.2666, grad_fn=<NegBackward0>) tensor(11670.2676, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11670.2734375
tensor(11670.2666, grad_fn=<NegBackward0>) tensor(11670.2734, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11670.2666015625
tensor(11670.2666, grad_fn=<NegBackward0>) tensor(11670.2666, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11670.265625
tensor(11670.2666, grad_fn=<NegBackward0>) tensor(11670.2656, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11670.2666015625
tensor(11670.2656, grad_fn=<NegBackward0>) tensor(11670.2666, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11670.265625
tensor(11670.2656, grad_fn=<NegBackward0>) tensor(11670.2656, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11670.265625
tensor(11670.2656, grad_fn=<NegBackward0>) tensor(11670.2656, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11670.26171875
tensor(11670.2656, grad_fn=<NegBackward0>) tensor(11670.2617, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11670.259765625
tensor(11670.2617, grad_fn=<NegBackward0>) tensor(11670.2598, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11670.259765625
tensor(11670.2598, grad_fn=<NegBackward0>) tensor(11670.2598, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11670.259765625
tensor(11670.2598, grad_fn=<NegBackward0>) tensor(11670.2598, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11670.2578125
tensor(11670.2598, grad_fn=<NegBackward0>) tensor(11670.2578, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11670.25
tensor(11670.2578, grad_fn=<NegBackward0>) tensor(11670.2500, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11670.2470703125
tensor(11670.2500, grad_fn=<NegBackward0>) tensor(11670.2471, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11670.248046875
tensor(11670.2471, grad_fn=<NegBackward0>) tensor(11670.2480, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11670.2470703125
tensor(11670.2471, grad_fn=<NegBackward0>) tensor(11670.2471, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11670.2470703125
tensor(11670.2471, grad_fn=<NegBackward0>) tensor(11670.2471, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11670.2626953125
tensor(11670.2471, grad_fn=<NegBackward0>) tensor(11670.2627, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11670.24609375
tensor(11670.2471, grad_fn=<NegBackward0>) tensor(11670.2461, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11670.2470703125
tensor(11670.2461, grad_fn=<NegBackward0>) tensor(11670.2471, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11670.25
tensor(11670.2461, grad_fn=<NegBackward0>) tensor(11670.2500, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11670.2470703125
tensor(11670.2461, grad_fn=<NegBackward0>) tensor(11670.2471, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11670.25390625
tensor(11670.2461, grad_fn=<NegBackward0>) tensor(11670.2539, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11670.248046875
tensor(11670.2461, grad_fn=<NegBackward0>) tensor(11670.2480, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.1035, 0.8965],
        [0.5364, 0.4636]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4534, 0.5466], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3695, 0.0988],
         [0.5925, 0.2272]],

        [[0.7067, 0.1028],
         [0.6833, 0.6133]],

        [[0.5710, 0.0950],
         [0.5863, 0.7199]],

        [[0.5210, 0.1055],
         [0.5442, 0.5568]],

        [[0.5164, 0.1051],
         [0.6570, 0.5323]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 71
Adjusted Rand Index: 0.16808080808080808
time is 2
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 77
Adjusted Rand Index: 0.2859132729738646
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.16478220324845758
Average Adjusted Rand Index: 0.6907988162109346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24860.369140625
inf tensor(24860.3691, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12109.3134765625
tensor(24860.3691, grad_fn=<NegBackward0>) tensor(12109.3135, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11817.2119140625
tensor(12109.3135, grad_fn=<NegBackward0>) tensor(11817.2119, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11576.923828125
tensor(11817.2119, grad_fn=<NegBackward0>) tensor(11576.9238, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11465.1396484375
tensor(11576.9238, grad_fn=<NegBackward0>) tensor(11465.1396, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11435.470703125
tensor(11465.1396, grad_fn=<NegBackward0>) tensor(11435.4707, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11435.177734375
tensor(11435.4707, grad_fn=<NegBackward0>) tensor(11435.1777, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11435.0185546875
tensor(11435.1777, grad_fn=<NegBackward0>) tensor(11435.0186, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11416.591796875
tensor(11435.0186, grad_fn=<NegBackward0>) tensor(11416.5918, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11416.3642578125
tensor(11416.5918, grad_fn=<NegBackward0>) tensor(11416.3643, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11409.208984375
tensor(11416.3643, grad_fn=<NegBackward0>) tensor(11409.2090, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11409.171875
tensor(11409.2090, grad_fn=<NegBackward0>) tensor(11409.1719, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11405.5517578125
tensor(11409.1719, grad_fn=<NegBackward0>) tensor(11405.5518, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11402.8466796875
tensor(11405.5518, grad_fn=<NegBackward0>) tensor(11402.8467, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11402.826171875
tensor(11402.8467, grad_fn=<NegBackward0>) tensor(11402.8262, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11402.810546875
tensor(11402.8262, grad_fn=<NegBackward0>) tensor(11402.8105, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11402.7978515625
tensor(11402.8105, grad_fn=<NegBackward0>) tensor(11402.7979, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11402.771484375
tensor(11402.7979, grad_fn=<NegBackward0>) tensor(11402.7715, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11396.623046875
tensor(11402.7715, grad_fn=<NegBackward0>) tensor(11396.6230, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11396.611328125
tensor(11396.6230, grad_fn=<NegBackward0>) tensor(11396.6113, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11387.9189453125
tensor(11396.6113, grad_fn=<NegBackward0>) tensor(11387.9189, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11387.912109375
tensor(11387.9189, grad_fn=<NegBackward0>) tensor(11387.9121, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11387.90625
tensor(11387.9121, grad_fn=<NegBackward0>) tensor(11387.9062, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11387.900390625
tensor(11387.9062, grad_fn=<NegBackward0>) tensor(11387.9004, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11387.8974609375
tensor(11387.9004, grad_fn=<NegBackward0>) tensor(11387.8975, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11387.8935546875
tensor(11387.8975, grad_fn=<NegBackward0>) tensor(11387.8936, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11387.8896484375
tensor(11387.8936, grad_fn=<NegBackward0>) tensor(11387.8896, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11387.88671875
tensor(11387.8896, grad_fn=<NegBackward0>) tensor(11387.8867, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11387.8857421875
tensor(11387.8867, grad_fn=<NegBackward0>) tensor(11387.8857, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11387.8828125
tensor(11387.8857, grad_fn=<NegBackward0>) tensor(11387.8828, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11387.8818359375
tensor(11387.8828, grad_fn=<NegBackward0>) tensor(11387.8818, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11387.8798828125
tensor(11387.8818, grad_fn=<NegBackward0>) tensor(11387.8799, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11387.876953125
tensor(11387.8799, grad_fn=<NegBackward0>) tensor(11387.8770, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11387.876953125
tensor(11387.8770, grad_fn=<NegBackward0>) tensor(11387.8770, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11387.875
tensor(11387.8770, grad_fn=<NegBackward0>) tensor(11387.8750, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11387.8740234375
tensor(11387.8750, grad_fn=<NegBackward0>) tensor(11387.8740, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11387.873046875
tensor(11387.8740, grad_fn=<NegBackward0>) tensor(11387.8730, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11387.8720703125
tensor(11387.8730, grad_fn=<NegBackward0>) tensor(11387.8721, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11387.8701171875
tensor(11387.8721, grad_fn=<NegBackward0>) tensor(11387.8701, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11387.8701171875
tensor(11387.8701, grad_fn=<NegBackward0>) tensor(11387.8701, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11387.8701171875
tensor(11387.8701, grad_fn=<NegBackward0>) tensor(11387.8701, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11387.869140625
tensor(11387.8701, grad_fn=<NegBackward0>) tensor(11387.8691, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11387.8671875
tensor(11387.8691, grad_fn=<NegBackward0>) tensor(11387.8672, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11387.8671875
tensor(11387.8672, grad_fn=<NegBackward0>) tensor(11387.8672, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11387.8681640625
tensor(11387.8672, grad_fn=<NegBackward0>) tensor(11387.8682, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11387.8662109375
tensor(11387.8672, grad_fn=<NegBackward0>) tensor(11387.8662, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11387.8662109375
tensor(11387.8662, grad_fn=<NegBackward0>) tensor(11387.8662, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11387.8642578125
tensor(11387.8662, grad_fn=<NegBackward0>) tensor(11387.8643, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11387.8642578125
tensor(11387.8643, grad_fn=<NegBackward0>) tensor(11387.8643, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11387.86328125
tensor(11387.8643, grad_fn=<NegBackward0>) tensor(11387.8633, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11387.869140625
tensor(11387.8633, grad_fn=<NegBackward0>) tensor(11387.8691, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11387.8623046875
tensor(11387.8633, grad_fn=<NegBackward0>) tensor(11387.8623, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11387.8623046875
tensor(11387.8623, grad_fn=<NegBackward0>) tensor(11387.8623, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11387.86328125
tensor(11387.8623, grad_fn=<NegBackward0>) tensor(11387.8633, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11387.8603515625
tensor(11387.8623, grad_fn=<NegBackward0>) tensor(11387.8604, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11387.861328125
tensor(11387.8604, grad_fn=<NegBackward0>) tensor(11387.8613, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11387.8603515625
tensor(11387.8604, grad_fn=<NegBackward0>) tensor(11387.8604, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11387.8583984375
tensor(11387.8604, grad_fn=<NegBackward0>) tensor(11387.8584, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11387.857421875
tensor(11387.8584, grad_fn=<NegBackward0>) tensor(11387.8574, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11387.8359375
tensor(11387.8574, grad_fn=<NegBackward0>) tensor(11387.8359, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11387.81640625
tensor(11387.8359, grad_fn=<NegBackward0>) tensor(11387.8164, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11387.818359375
tensor(11387.8164, grad_fn=<NegBackward0>) tensor(11387.8184, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11387.822265625
tensor(11387.8164, grad_fn=<NegBackward0>) tensor(11387.8223, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11387.814453125
tensor(11387.8164, grad_fn=<NegBackward0>) tensor(11387.8145, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11387.8134765625
tensor(11387.8145, grad_fn=<NegBackward0>) tensor(11387.8135, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11387.822265625
tensor(11387.8135, grad_fn=<NegBackward0>) tensor(11387.8223, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11387.8134765625
tensor(11387.8135, grad_fn=<NegBackward0>) tensor(11387.8135, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11387.8154296875
tensor(11387.8135, grad_fn=<NegBackward0>) tensor(11387.8154, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11387.814453125
tensor(11387.8135, grad_fn=<NegBackward0>) tensor(11387.8145, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11387.814453125
tensor(11387.8135, grad_fn=<NegBackward0>) tensor(11387.8145, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11387.814453125
tensor(11387.8135, grad_fn=<NegBackward0>) tensor(11387.8145, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11387.8125
tensor(11387.8135, grad_fn=<NegBackward0>) tensor(11387.8125, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11387.8125
tensor(11387.8125, grad_fn=<NegBackward0>) tensor(11387.8125, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11387.8173828125
tensor(11387.8125, grad_fn=<NegBackward0>) tensor(11387.8174, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11387.8125
tensor(11387.8125, grad_fn=<NegBackward0>) tensor(11387.8125, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11387.8125
tensor(11387.8125, grad_fn=<NegBackward0>) tensor(11387.8125, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11387.8134765625
tensor(11387.8125, grad_fn=<NegBackward0>) tensor(11387.8135, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11387.810546875
tensor(11387.8125, grad_fn=<NegBackward0>) tensor(11387.8105, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11387.810546875
tensor(11387.8105, grad_fn=<NegBackward0>) tensor(11387.8105, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11387.810546875
tensor(11387.8105, grad_fn=<NegBackward0>) tensor(11387.8105, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11387.8095703125
tensor(11387.8105, grad_fn=<NegBackward0>) tensor(11387.8096, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11387.810546875
tensor(11387.8096, grad_fn=<NegBackward0>) tensor(11387.8105, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11387.814453125
tensor(11387.8096, grad_fn=<NegBackward0>) tensor(11387.8145, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11387.814453125
tensor(11387.8096, grad_fn=<NegBackward0>) tensor(11387.8145, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11387.810546875
tensor(11387.8096, grad_fn=<NegBackward0>) tensor(11387.8105, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11387.810546875
tensor(11387.8096, grad_fn=<NegBackward0>) tensor(11387.8105, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7688, 0.2312],
        [0.2344, 0.7656]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5477, 0.4522], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1942, 0.0989],
         [0.6404, 0.3979]],

        [[0.6246, 0.0970],
         [0.5984, 0.6373]],

        [[0.7027, 0.0950],
         [0.6201, 0.6527]],

        [[0.6036, 0.1040],
         [0.6385, 0.5317]],

        [[0.6348, 0.1053],
         [0.7236, 0.6359]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[0.16478220324845758, 1.0] [0.6907988162109346, 1.0] [11670.248046875, 11387.810546875]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11551.344253266367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22731.29296875
inf tensor(22731.2930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12252.5380859375
tensor(22731.2930, grad_fn=<NegBackward0>) tensor(12252.5381, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12224.3251953125
tensor(12252.5381, grad_fn=<NegBackward0>) tensor(12224.3252, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11725.0283203125
tensor(12224.3252, grad_fn=<NegBackward0>) tensor(11725.0283, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11603.373046875
tensor(11725.0283, grad_fn=<NegBackward0>) tensor(11603.3730, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11575.0615234375
tensor(11603.3730, grad_fn=<NegBackward0>) tensor(11575.0615, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11574.49609375
tensor(11575.0615, grad_fn=<NegBackward0>) tensor(11574.4961, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11574.2431640625
tensor(11574.4961, grad_fn=<NegBackward0>) tensor(11574.2432, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11574.095703125
tensor(11574.2432, grad_fn=<NegBackward0>) tensor(11574.0957, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11573.9951171875
tensor(11574.0957, grad_fn=<NegBackward0>) tensor(11573.9951, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11568.80078125
tensor(11573.9951, grad_fn=<NegBackward0>) tensor(11568.8008, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11556.4716796875
tensor(11568.8008, grad_fn=<NegBackward0>) tensor(11556.4717, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11556.40625
tensor(11556.4717, grad_fn=<NegBackward0>) tensor(11556.4062, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11556.369140625
tensor(11556.4062, grad_fn=<NegBackward0>) tensor(11556.3691, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11556.3447265625
tensor(11556.3691, grad_fn=<NegBackward0>) tensor(11556.3447, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11556.3193359375
tensor(11556.3447, grad_fn=<NegBackward0>) tensor(11556.3193, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11556.2216796875
tensor(11556.3193, grad_fn=<NegBackward0>) tensor(11556.2217, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11543.7490234375
tensor(11556.2217, grad_fn=<NegBackward0>) tensor(11543.7490, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11543.724609375
tensor(11543.7490, grad_fn=<NegBackward0>) tensor(11543.7246, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11543.712890625
tensor(11543.7246, grad_fn=<NegBackward0>) tensor(11543.7129, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11543.701171875
tensor(11543.7129, grad_fn=<NegBackward0>) tensor(11543.7012, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11543.6923828125
tensor(11543.7012, grad_fn=<NegBackward0>) tensor(11543.6924, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11543.6845703125
tensor(11543.6924, grad_fn=<NegBackward0>) tensor(11543.6846, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11543.6767578125
tensor(11543.6846, grad_fn=<NegBackward0>) tensor(11543.6768, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11543.669921875
tensor(11543.6768, grad_fn=<NegBackward0>) tensor(11543.6699, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11543.666015625
tensor(11543.6699, grad_fn=<NegBackward0>) tensor(11543.6660, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11543.662109375
tensor(11543.6660, grad_fn=<NegBackward0>) tensor(11543.6621, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11543.6591796875
tensor(11543.6621, grad_fn=<NegBackward0>) tensor(11543.6592, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11543.654296875
tensor(11543.6592, grad_fn=<NegBackward0>) tensor(11543.6543, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11543.65234375
tensor(11543.6543, grad_fn=<NegBackward0>) tensor(11543.6523, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11543.6484375
tensor(11543.6523, grad_fn=<NegBackward0>) tensor(11543.6484, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11543.646484375
tensor(11543.6484, grad_fn=<NegBackward0>) tensor(11543.6465, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11543.6455078125
tensor(11543.6465, grad_fn=<NegBackward0>) tensor(11543.6455, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11543.642578125
tensor(11543.6455, grad_fn=<NegBackward0>) tensor(11543.6426, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11543.6396484375
tensor(11543.6426, grad_fn=<NegBackward0>) tensor(11543.6396, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11543.6376953125
tensor(11543.6396, grad_fn=<NegBackward0>) tensor(11543.6377, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11543.6376953125
tensor(11543.6377, grad_fn=<NegBackward0>) tensor(11543.6377, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11543.6435546875
tensor(11543.6377, grad_fn=<NegBackward0>) tensor(11543.6436, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11543.6337890625
tensor(11543.6377, grad_fn=<NegBackward0>) tensor(11543.6338, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11543.6328125
tensor(11543.6338, grad_fn=<NegBackward0>) tensor(11543.6328, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11543.630859375
tensor(11543.6328, grad_fn=<NegBackward0>) tensor(11543.6309, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11543.630859375
tensor(11543.6309, grad_fn=<NegBackward0>) tensor(11543.6309, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11543.62890625
tensor(11543.6309, grad_fn=<NegBackward0>) tensor(11543.6289, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11543.6279296875
tensor(11543.6289, grad_fn=<NegBackward0>) tensor(11543.6279, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11543.626953125
tensor(11543.6279, grad_fn=<NegBackward0>) tensor(11543.6270, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11543.6259765625
tensor(11543.6270, grad_fn=<NegBackward0>) tensor(11543.6260, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11543.6240234375
tensor(11543.6260, grad_fn=<NegBackward0>) tensor(11543.6240, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11543.5595703125
tensor(11543.6240, grad_fn=<NegBackward0>) tensor(11543.5596, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11543.556640625
tensor(11543.5596, grad_fn=<NegBackward0>) tensor(11543.5566, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11543.5546875
tensor(11543.5566, grad_fn=<NegBackward0>) tensor(11543.5547, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11543.5537109375
tensor(11543.5547, grad_fn=<NegBackward0>) tensor(11543.5537, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11543.5517578125
tensor(11543.5537, grad_fn=<NegBackward0>) tensor(11543.5518, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11543.5361328125
tensor(11543.5518, grad_fn=<NegBackward0>) tensor(11543.5361, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11543.5361328125
tensor(11543.5361, grad_fn=<NegBackward0>) tensor(11543.5361, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11543.5322265625
tensor(11543.5361, grad_fn=<NegBackward0>) tensor(11543.5322, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11543.5302734375
tensor(11543.5322, grad_fn=<NegBackward0>) tensor(11543.5303, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11543.53125
tensor(11543.5303, grad_fn=<NegBackward0>) tensor(11543.5312, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11543.5283203125
tensor(11543.5303, grad_fn=<NegBackward0>) tensor(11543.5283, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11543.529296875
tensor(11543.5283, grad_fn=<NegBackward0>) tensor(11543.5293, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11543.5400390625
tensor(11543.5283, grad_fn=<NegBackward0>) tensor(11543.5400, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11543.52734375
tensor(11543.5283, grad_fn=<NegBackward0>) tensor(11543.5273, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11543.5283203125
tensor(11543.5273, grad_fn=<NegBackward0>) tensor(11543.5283, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11543.5263671875
tensor(11543.5273, grad_fn=<NegBackward0>) tensor(11543.5264, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11543.5263671875
tensor(11543.5264, grad_fn=<NegBackward0>) tensor(11543.5264, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11543.525390625
tensor(11543.5264, grad_fn=<NegBackward0>) tensor(11543.5254, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11543.525390625
tensor(11543.5254, grad_fn=<NegBackward0>) tensor(11543.5254, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11543.5263671875
tensor(11543.5254, grad_fn=<NegBackward0>) tensor(11543.5264, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11543.525390625
tensor(11543.5254, grad_fn=<NegBackward0>) tensor(11543.5254, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11543.5244140625
tensor(11543.5254, grad_fn=<NegBackward0>) tensor(11543.5244, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11543.5244140625
tensor(11543.5244, grad_fn=<NegBackward0>) tensor(11543.5244, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11543.5234375
tensor(11543.5244, grad_fn=<NegBackward0>) tensor(11543.5234, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11543.5302734375
tensor(11543.5234, grad_fn=<NegBackward0>) tensor(11543.5303, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11543.5302734375
tensor(11543.5234, grad_fn=<NegBackward0>) tensor(11543.5303, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11543.5244140625
tensor(11543.5234, grad_fn=<NegBackward0>) tensor(11543.5244, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11543.5224609375
tensor(11543.5234, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11543.5234375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5234, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11543.5234375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5234, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11543.5224609375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11543.5234375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5234, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11543.5224609375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11543.5224609375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11543.5498046875
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5498, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11543.5244140625
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5244, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11543.5224609375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11543.5224609375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11543.52734375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5273, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11543.5224609375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11543.521484375
tensor(11543.5225, grad_fn=<NegBackward0>) tensor(11543.5215, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11543.5244140625
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5244, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11543.5263671875
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5264, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11543.521484375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5215, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11543.521484375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5215, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11543.5244140625
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5244, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11543.521484375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5215, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11543.521484375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5215, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11543.525390625
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5254, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11543.5458984375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5459, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11543.5224609375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11543.5224609375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5225, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11543.5380859375
tensor(11543.5215, grad_fn=<NegBackward0>) tensor(11543.5381, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.6975, 0.3025],
        [0.2603, 0.7397]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4786, 0.5214], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4040, 0.1103],
         [0.6897, 0.2014]],

        [[0.6572, 0.1042],
         [0.5497, 0.6921]],

        [[0.6396, 0.1078],
         [0.5519, 0.5262]],

        [[0.6397, 0.1029],
         [0.7076, 0.6687]],

        [[0.6261, 0.0919],
         [0.5703, 0.6668]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21594.951171875
inf tensor(21594.9512, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12237.98828125
tensor(21594.9512, grad_fn=<NegBackward0>) tensor(12237.9883, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11706.27734375
tensor(12237.9883, grad_fn=<NegBackward0>) tensor(11706.2773, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11610.8310546875
tensor(11706.2773, grad_fn=<NegBackward0>) tensor(11610.8311, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11596.3115234375
tensor(11610.8311, grad_fn=<NegBackward0>) tensor(11596.3115, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11579.7041015625
tensor(11596.3115, grad_fn=<NegBackward0>) tensor(11579.7041, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11579.5478515625
tensor(11579.7041, grad_fn=<NegBackward0>) tensor(11579.5479, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11571.326171875
tensor(11579.5479, grad_fn=<NegBackward0>) tensor(11571.3262, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11571.2314453125
tensor(11571.3262, grad_fn=<NegBackward0>) tensor(11571.2314, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11571.1884765625
tensor(11571.2314, grad_fn=<NegBackward0>) tensor(11571.1885, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11571.154296875
tensor(11571.1885, grad_fn=<NegBackward0>) tensor(11571.1543, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11571.0146484375
tensor(11571.1543, grad_fn=<NegBackward0>) tensor(11571.0146, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11563.564453125
tensor(11571.0146, grad_fn=<NegBackward0>) tensor(11563.5645, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11544.0849609375
tensor(11563.5645, grad_fn=<NegBackward0>) tensor(11544.0850, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11543.6865234375
tensor(11544.0850, grad_fn=<NegBackward0>) tensor(11543.6865, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11543.6748046875
tensor(11543.6865, grad_fn=<NegBackward0>) tensor(11543.6748, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11543.666015625
tensor(11543.6748, grad_fn=<NegBackward0>) tensor(11543.6660, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11543.658203125
tensor(11543.6660, grad_fn=<NegBackward0>) tensor(11543.6582, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11543.65234375
tensor(11543.6582, grad_fn=<NegBackward0>) tensor(11543.6523, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11543.6474609375
tensor(11543.6523, grad_fn=<NegBackward0>) tensor(11543.6475, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11543.642578125
tensor(11543.6475, grad_fn=<NegBackward0>) tensor(11543.6426, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11543.638671875
tensor(11543.6426, grad_fn=<NegBackward0>) tensor(11543.6387, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11543.634765625
tensor(11543.6387, grad_fn=<NegBackward0>) tensor(11543.6348, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11543.6318359375
tensor(11543.6348, grad_fn=<NegBackward0>) tensor(11543.6318, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11543.6298828125
tensor(11543.6318, grad_fn=<NegBackward0>) tensor(11543.6299, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11543.6279296875
tensor(11543.6299, grad_fn=<NegBackward0>) tensor(11543.6279, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11543.625
tensor(11543.6279, grad_fn=<NegBackward0>) tensor(11543.6250, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11543.6240234375
tensor(11543.6250, grad_fn=<NegBackward0>) tensor(11543.6240, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11543.6201171875
tensor(11543.6240, grad_fn=<NegBackward0>) tensor(11543.6201, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11543.6142578125
tensor(11543.6201, grad_fn=<NegBackward0>) tensor(11543.6143, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11543.6123046875
tensor(11543.6143, grad_fn=<NegBackward0>) tensor(11543.6123, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11543.609375
tensor(11543.6123, grad_fn=<NegBackward0>) tensor(11543.6094, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11543.6025390625
tensor(11543.6094, grad_fn=<NegBackward0>) tensor(11543.6025, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11543.5615234375
tensor(11543.6025, grad_fn=<NegBackward0>) tensor(11543.5615, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11543.560546875
tensor(11543.5615, grad_fn=<NegBackward0>) tensor(11543.5605, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11543.5595703125
tensor(11543.5605, grad_fn=<NegBackward0>) tensor(11543.5596, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11543.5576171875
tensor(11543.5596, grad_fn=<NegBackward0>) tensor(11543.5576, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11543.556640625
tensor(11543.5576, grad_fn=<NegBackward0>) tensor(11543.5566, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11543.556640625
tensor(11543.5566, grad_fn=<NegBackward0>) tensor(11543.5566, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11543.5546875
tensor(11543.5566, grad_fn=<NegBackward0>) tensor(11543.5547, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11543.5556640625
tensor(11543.5547, grad_fn=<NegBackward0>) tensor(11543.5557, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11543.5546875
tensor(11543.5547, grad_fn=<NegBackward0>) tensor(11543.5547, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11543.5546875
tensor(11543.5547, grad_fn=<NegBackward0>) tensor(11543.5547, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11543.5537109375
tensor(11543.5547, grad_fn=<NegBackward0>) tensor(11543.5537, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11543.5537109375
tensor(11543.5537, grad_fn=<NegBackward0>) tensor(11543.5537, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11543.552734375
tensor(11543.5537, grad_fn=<NegBackward0>) tensor(11543.5527, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11543.552734375
tensor(11543.5527, grad_fn=<NegBackward0>) tensor(11543.5527, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11543.552734375
tensor(11543.5527, grad_fn=<NegBackward0>) tensor(11543.5527, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11543.55078125
tensor(11543.5527, grad_fn=<NegBackward0>) tensor(11543.5508, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11543.5517578125
tensor(11543.5508, grad_fn=<NegBackward0>) tensor(11543.5518, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11543.5517578125
tensor(11543.5508, grad_fn=<NegBackward0>) tensor(11543.5518, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11543.552734375
tensor(11543.5508, grad_fn=<NegBackward0>) tensor(11543.5527, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11543.55078125
tensor(11543.5508, grad_fn=<NegBackward0>) tensor(11543.5508, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11543.5498046875
tensor(11543.5508, grad_fn=<NegBackward0>) tensor(11543.5498, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11543.5498046875
tensor(11543.5498, grad_fn=<NegBackward0>) tensor(11543.5498, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11543.5498046875
tensor(11543.5498, grad_fn=<NegBackward0>) tensor(11543.5498, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11543.5517578125
tensor(11543.5498, grad_fn=<NegBackward0>) tensor(11543.5518, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11543.552734375
tensor(11543.5498, grad_fn=<NegBackward0>) tensor(11543.5527, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11543.55078125
tensor(11543.5498, grad_fn=<NegBackward0>) tensor(11543.5508, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11543.552734375
tensor(11543.5498, grad_fn=<NegBackward0>) tensor(11543.5527, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11543.548828125
tensor(11543.5498, grad_fn=<NegBackward0>) tensor(11543.5488, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11543.548828125
tensor(11543.5488, grad_fn=<NegBackward0>) tensor(11543.5488, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11543.5498046875
tensor(11543.5488, grad_fn=<NegBackward0>) tensor(11543.5498, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11543.55078125
tensor(11543.5488, grad_fn=<NegBackward0>) tensor(11543.5508, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11543.5478515625
tensor(11543.5488, grad_fn=<NegBackward0>) tensor(11543.5479, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11543.5498046875
tensor(11543.5479, grad_fn=<NegBackward0>) tensor(11543.5498, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11543.5478515625
tensor(11543.5479, grad_fn=<NegBackward0>) tensor(11543.5479, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11543.5478515625
tensor(11543.5479, grad_fn=<NegBackward0>) tensor(11543.5479, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11543.548828125
tensor(11543.5479, grad_fn=<NegBackward0>) tensor(11543.5488, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11543.548828125
tensor(11543.5479, grad_fn=<NegBackward0>) tensor(11543.5488, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11543.546875
tensor(11543.5479, grad_fn=<NegBackward0>) tensor(11543.5469, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11543.5478515625
tensor(11543.5469, grad_fn=<NegBackward0>) tensor(11543.5479, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11543.5478515625
tensor(11543.5469, grad_fn=<NegBackward0>) tensor(11543.5479, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11543.548828125
tensor(11543.5469, grad_fn=<NegBackward0>) tensor(11543.5488, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11543.5478515625
tensor(11543.5469, grad_fn=<NegBackward0>) tensor(11543.5479, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11543.5478515625
tensor(11543.5469, grad_fn=<NegBackward0>) tensor(11543.5479, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.6990, 0.3010],
        [0.2606, 0.7394]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4782, 0.5218], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4045, 0.1106],
         [0.7151, 0.2005]],

        [[0.6883, 0.1042],
         [0.6991, 0.5939]],

        [[0.6087, 0.1077],
         [0.5212, 0.6844]],

        [[0.6796, 0.1029],
         [0.6746, 0.5843]],

        [[0.6240, 0.0919],
         [0.6114, 0.5490]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919995611635631
[0.9919999730634713, 0.9919999730634713] [0.9919995611635631, 0.9919995611635631] [11543.5380859375, 11543.5478515625]
-------------------------------------
This iteration is 51
True Objective function: Loss = -11561.8330065645
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22580.791015625
inf tensor(22580.7910, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12435.3798828125
tensor(22580.7910, grad_fn=<NegBackward0>) tensor(12435.3799, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12051.662109375
tensor(12435.3799, grad_fn=<NegBackward0>) tensor(12051.6621, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12040.4599609375
tensor(12051.6621, grad_fn=<NegBackward0>) tensor(12040.4600, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12035.0810546875
tensor(12040.4600, grad_fn=<NegBackward0>) tensor(12035.0811, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12034.35546875
tensor(12035.0811, grad_fn=<NegBackward0>) tensor(12034.3555, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12029.5703125
tensor(12034.3555, grad_fn=<NegBackward0>) tensor(12029.5703, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12020.5537109375
tensor(12029.5703, grad_fn=<NegBackward0>) tensor(12020.5537, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11996.4892578125
tensor(12020.5537, grad_fn=<NegBackward0>) tensor(11996.4893, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11991.912109375
tensor(11996.4893, grad_fn=<NegBackward0>) tensor(11991.9121, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11980.0283203125
tensor(11991.9121, grad_fn=<NegBackward0>) tensor(11980.0283, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11953.1044921875
tensor(11980.0283, grad_fn=<NegBackward0>) tensor(11953.1045, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11923.8369140625
tensor(11953.1045, grad_fn=<NegBackward0>) tensor(11923.8369, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11916.7392578125
tensor(11923.8369, grad_fn=<NegBackward0>) tensor(11916.7393, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11909.44140625
tensor(11916.7393, grad_fn=<NegBackward0>) tensor(11909.4414, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11906.2412109375
tensor(11909.4414, grad_fn=<NegBackward0>) tensor(11906.2412, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11905.5380859375
tensor(11906.2412, grad_fn=<NegBackward0>) tensor(11905.5381, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11902.1806640625
tensor(11905.5381, grad_fn=<NegBackward0>) tensor(11902.1807, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11898.5224609375
tensor(11902.1807, grad_fn=<NegBackward0>) tensor(11898.5225, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11898.4716796875
tensor(11898.5225, grad_fn=<NegBackward0>) tensor(11898.4717, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11898.4453125
tensor(11898.4717, grad_fn=<NegBackward0>) tensor(11898.4453, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11898.427734375
tensor(11898.4453, grad_fn=<NegBackward0>) tensor(11898.4277, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11898.3974609375
tensor(11898.4277, grad_fn=<NegBackward0>) tensor(11898.3975, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11898.3759765625
tensor(11898.3975, grad_fn=<NegBackward0>) tensor(11898.3760, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11898.3701171875
tensor(11898.3760, grad_fn=<NegBackward0>) tensor(11898.3701, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11897.8291015625
tensor(11898.3701, grad_fn=<NegBackward0>) tensor(11897.8291, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11894.8671875
tensor(11897.8291, grad_fn=<NegBackward0>) tensor(11894.8672, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11894.861328125
tensor(11894.8672, grad_fn=<NegBackward0>) tensor(11894.8613, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11894.857421875
tensor(11894.8613, grad_fn=<NegBackward0>) tensor(11894.8574, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11894.8544921875
tensor(11894.8574, grad_fn=<NegBackward0>) tensor(11894.8545, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11894.849609375
tensor(11894.8545, grad_fn=<NegBackward0>) tensor(11894.8496, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11894.8447265625
tensor(11894.8496, grad_fn=<NegBackward0>) tensor(11894.8447, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11894.841796875
tensor(11894.8447, grad_fn=<NegBackward0>) tensor(11894.8418, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11894.833984375
tensor(11894.8418, grad_fn=<NegBackward0>) tensor(11894.8340, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11894.826171875
tensor(11894.8340, grad_fn=<NegBackward0>) tensor(11894.8262, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11894.8232421875
tensor(11894.8262, grad_fn=<NegBackward0>) tensor(11894.8232, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11894.822265625
tensor(11894.8232, grad_fn=<NegBackward0>) tensor(11894.8223, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11894.8212890625
tensor(11894.8223, grad_fn=<NegBackward0>) tensor(11894.8213, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11894.8134765625
tensor(11894.8213, grad_fn=<NegBackward0>) tensor(11894.8135, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11894.7744140625
tensor(11894.8135, grad_fn=<NegBackward0>) tensor(11894.7744, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11894.7724609375
tensor(11894.7744, grad_fn=<NegBackward0>) tensor(11894.7725, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11894.77734375
tensor(11894.7725, grad_fn=<NegBackward0>) tensor(11894.7773, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11894.7724609375
tensor(11894.7725, grad_fn=<NegBackward0>) tensor(11894.7725, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11894.771484375
tensor(11894.7725, grad_fn=<NegBackward0>) tensor(11894.7715, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11894.767578125
tensor(11894.7715, grad_fn=<NegBackward0>) tensor(11894.7676, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11894.7666015625
tensor(11894.7676, grad_fn=<NegBackward0>) tensor(11894.7666, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11894.7646484375
tensor(11894.7666, grad_fn=<NegBackward0>) tensor(11894.7646, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11894.763671875
tensor(11894.7646, grad_fn=<NegBackward0>) tensor(11894.7637, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11894.763671875
tensor(11894.7637, grad_fn=<NegBackward0>) tensor(11894.7637, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11894.7646484375
tensor(11894.7637, grad_fn=<NegBackward0>) tensor(11894.7646, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11894.76171875
tensor(11894.7637, grad_fn=<NegBackward0>) tensor(11894.7617, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11894.7685546875
tensor(11894.7617, grad_fn=<NegBackward0>) tensor(11894.7686, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11894.7587890625
tensor(11894.7617, grad_fn=<NegBackward0>) tensor(11894.7588, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11894.76171875
tensor(11894.7588, grad_fn=<NegBackward0>) tensor(11894.7617, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11894.763671875
tensor(11894.7588, grad_fn=<NegBackward0>) tensor(11894.7637, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11894.759765625
tensor(11894.7588, grad_fn=<NegBackward0>) tensor(11894.7598, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11894.7578125
tensor(11894.7588, grad_fn=<NegBackward0>) tensor(11894.7578, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11894.7587890625
tensor(11894.7578, grad_fn=<NegBackward0>) tensor(11894.7588, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11894.7578125
tensor(11894.7578, grad_fn=<NegBackward0>) tensor(11894.7578, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11894.7548828125
tensor(11894.7578, grad_fn=<NegBackward0>) tensor(11894.7549, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11894.74609375
tensor(11894.7549, grad_fn=<NegBackward0>) tensor(11894.7461, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11894.7451171875
tensor(11894.7461, grad_fn=<NegBackward0>) tensor(11894.7451, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11894.74609375
tensor(11894.7451, grad_fn=<NegBackward0>) tensor(11894.7461, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11894.7470703125
tensor(11894.7451, grad_fn=<NegBackward0>) tensor(11894.7471, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11894.7470703125
tensor(11894.7451, grad_fn=<NegBackward0>) tensor(11894.7471, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11894.7451171875
tensor(11894.7451, grad_fn=<NegBackward0>) tensor(11894.7451, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11894.74609375
tensor(11894.7451, grad_fn=<NegBackward0>) tensor(11894.7461, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11894.7392578125
tensor(11894.7451, grad_fn=<NegBackward0>) tensor(11894.7393, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11894.740234375
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7402, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11894.740234375
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7402, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11894.740234375
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7402, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11894.7392578125
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7393, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11894.7421875
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7422, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11894.7412109375
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7412, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11894.7392578125
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7393, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11894.73828125
tensor(11894.7393, grad_fn=<NegBackward0>) tensor(11894.7383, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11894.85546875
tensor(11894.7383, grad_fn=<NegBackward0>) tensor(11894.8555, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11894.73828125
tensor(11894.7383, grad_fn=<NegBackward0>) tensor(11894.7383, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11894.744140625
tensor(11894.7383, grad_fn=<NegBackward0>) tensor(11894.7441, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11894.73828125
tensor(11894.7383, grad_fn=<NegBackward0>) tensor(11894.7383, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11894.7373046875
tensor(11894.7383, grad_fn=<NegBackward0>) tensor(11894.7373, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11894.7421875
tensor(11894.7373, grad_fn=<NegBackward0>) tensor(11894.7422, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11894.736328125
tensor(11894.7373, grad_fn=<NegBackward0>) tensor(11894.7363, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11894.7373046875
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7373, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11894.7373046875
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7373, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11894.7373046875
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7373, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11894.736328125
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7363, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11894.759765625
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7598, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11894.736328125
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7363, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11894.744140625
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7441, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11894.736328125
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7363, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11894.736328125
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7363, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11894.7587890625
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7588, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11894.77734375
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7773, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11894.7568359375
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7568, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11894.7373046875
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7373, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11894.7373046875
tensor(11894.7363, grad_fn=<NegBackward0>) tensor(11894.7373, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.3601, 0.6399],
        [0.6409, 0.3591]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4957, 0.5043], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3173, 0.0949],
         [0.6551, 0.2944]],

        [[0.6106, 0.0994],
         [0.6777, 0.5870]],

        [[0.6938, 0.1035],
         [0.6079, 0.7118]],

        [[0.6110, 0.0935],
         [0.5024, 0.6785]],

        [[0.6153, 0.0981],
         [0.6666, 0.6010]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080890789891884
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.033407334532561075
Average Adjusted Rand Index: 0.8829100081298821
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21717.8515625
inf tensor(21717.8516, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12325.31640625
tensor(21717.8516, grad_fn=<NegBackward0>) tensor(12325.3164, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11867.767578125
tensor(12325.3164, grad_fn=<NegBackward0>) tensor(11867.7676, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11757.1865234375
tensor(11867.7676, grad_fn=<NegBackward0>) tensor(11757.1865, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11733.8427734375
tensor(11757.1865, grad_fn=<NegBackward0>) tensor(11733.8428, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11700.5615234375
tensor(11733.8428, grad_fn=<NegBackward0>) tensor(11700.5615, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11700.1650390625
tensor(11700.5615, grad_fn=<NegBackward0>) tensor(11700.1650, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11699.96484375
tensor(11700.1650, grad_fn=<NegBackward0>) tensor(11699.9648, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11699.822265625
tensor(11699.9648, grad_fn=<NegBackward0>) tensor(11699.8223, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11699.6875
tensor(11699.8223, grad_fn=<NegBackward0>) tensor(11699.6875, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11699.578125
tensor(11699.6875, grad_fn=<NegBackward0>) tensor(11699.5781, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11699.529296875
tensor(11699.5781, grad_fn=<NegBackward0>) tensor(11699.5293, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11699.4951171875
tensor(11699.5293, grad_fn=<NegBackward0>) tensor(11699.4951, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11699.4677734375
tensor(11699.4951, grad_fn=<NegBackward0>) tensor(11699.4678, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11699.4443359375
tensor(11699.4678, grad_fn=<NegBackward0>) tensor(11699.4443, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11699.4208984375
tensor(11699.4443, grad_fn=<NegBackward0>) tensor(11699.4209, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11699.3935546875
tensor(11699.4209, grad_fn=<NegBackward0>) tensor(11699.3936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11699.3603515625
tensor(11699.3936, grad_fn=<NegBackward0>) tensor(11699.3604, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11699.060546875
tensor(11699.3604, grad_fn=<NegBackward0>) tensor(11699.0605, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11699.0244140625
tensor(11699.0605, grad_fn=<NegBackward0>) tensor(11699.0244, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11699.0048828125
tensor(11699.0244, grad_fn=<NegBackward0>) tensor(11699.0049, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11698.94921875
tensor(11699.0049, grad_fn=<NegBackward0>) tensor(11698.9492, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11698.8974609375
tensor(11698.9492, grad_fn=<NegBackward0>) tensor(11698.8975, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11696.548828125
tensor(11698.8975, grad_fn=<NegBackward0>) tensor(11696.5488, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11694.8798828125
tensor(11696.5488, grad_fn=<NegBackward0>) tensor(11694.8799, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11694.751953125
tensor(11694.8799, grad_fn=<NegBackward0>) tensor(11694.7520, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11694.7392578125
tensor(11694.7520, grad_fn=<NegBackward0>) tensor(11694.7393, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11694.7158203125
tensor(11694.7393, grad_fn=<NegBackward0>) tensor(11694.7158, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11693.98046875
tensor(11694.7158, grad_fn=<NegBackward0>) tensor(11693.9805, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11691.75390625
tensor(11693.9805, grad_fn=<NegBackward0>) tensor(11691.7539, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11688.1982421875
tensor(11691.7539, grad_fn=<NegBackward0>) tensor(11688.1982, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11675.349609375
tensor(11688.1982, grad_fn=<NegBackward0>) tensor(11675.3496, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11675.259765625
tensor(11675.3496, grad_fn=<NegBackward0>) tensor(11675.2598, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11675.0830078125
tensor(11675.2598, grad_fn=<NegBackward0>) tensor(11675.0830, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11663.23046875
tensor(11675.0830, grad_fn=<NegBackward0>) tensor(11663.2305, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11657.990234375
tensor(11663.2305, grad_fn=<NegBackward0>) tensor(11657.9902, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11657.9658203125
tensor(11657.9902, grad_fn=<NegBackward0>) tensor(11657.9658, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11651.4453125
tensor(11657.9658, grad_fn=<NegBackward0>) tensor(11651.4453, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11651.421875
tensor(11651.4453, grad_fn=<NegBackward0>) tensor(11651.4219, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11650.0048828125
tensor(11651.4219, grad_fn=<NegBackward0>) tensor(11650.0049, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11629.6640625
tensor(11650.0049, grad_fn=<NegBackward0>) tensor(11629.6641, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11629.6396484375
tensor(11629.6641, grad_fn=<NegBackward0>) tensor(11629.6396, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11623.8095703125
tensor(11629.6396, grad_fn=<NegBackward0>) tensor(11623.8096, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11623.796875
tensor(11623.8096, grad_fn=<NegBackward0>) tensor(11623.7969, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11620.564453125
tensor(11623.7969, grad_fn=<NegBackward0>) tensor(11620.5645, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11620.55859375
tensor(11620.5645, grad_fn=<NegBackward0>) tensor(11620.5586, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11620.55859375
tensor(11620.5586, grad_fn=<NegBackward0>) tensor(11620.5586, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11620.5556640625
tensor(11620.5586, grad_fn=<NegBackward0>) tensor(11620.5557, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11620.5458984375
tensor(11620.5557, grad_fn=<NegBackward0>) tensor(11620.5459, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11610.77734375
tensor(11620.5459, grad_fn=<NegBackward0>) tensor(11610.7773, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11606.9228515625
tensor(11610.7773, grad_fn=<NegBackward0>) tensor(11606.9229, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11590.5830078125
tensor(11606.9229, grad_fn=<NegBackward0>) tensor(11590.5830, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11579.900390625
tensor(11590.5830, grad_fn=<NegBackward0>) tensor(11579.9004, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11579.8388671875
tensor(11579.9004, grad_fn=<NegBackward0>) tensor(11579.8389, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11570.98046875
tensor(11579.8389, grad_fn=<NegBackward0>) tensor(11570.9805, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11570.9765625
tensor(11570.9805, grad_fn=<NegBackward0>) tensor(11570.9766, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11570.9755859375
tensor(11570.9766, grad_fn=<NegBackward0>) tensor(11570.9756, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11570.9736328125
tensor(11570.9756, grad_fn=<NegBackward0>) tensor(11570.9736, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11570.97265625
tensor(11570.9736, grad_fn=<NegBackward0>) tensor(11570.9727, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11570.9736328125
tensor(11570.9727, grad_fn=<NegBackward0>) tensor(11570.9736, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11570.97265625
tensor(11570.9727, grad_fn=<NegBackward0>) tensor(11570.9727, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11570.9697265625
tensor(11570.9727, grad_fn=<NegBackward0>) tensor(11570.9697, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11555.2958984375
tensor(11570.9697, grad_fn=<NegBackward0>) tensor(11555.2959, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11555.2958984375
tensor(11555.2959, grad_fn=<NegBackward0>) tensor(11555.2959, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11555.2919921875
tensor(11555.2959, grad_fn=<NegBackward0>) tensor(11555.2920, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11555.291015625
tensor(11555.2920, grad_fn=<NegBackward0>) tensor(11555.2910, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11555.291015625
tensor(11555.2910, grad_fn=<NegBackward0>) tensor(11555.2910, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11555.291015625
tensor(11555.2910, grad_fn=<NegBackward0>) tensor(11555.2910, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11555.2890625
tensor(11555.2910, grad_fn=<NegBackward0>) tensor(11555.2891, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11555.3525390625
tensor(11555.2891, grad_fn=<NegBackward0>) tensor(11555.3525, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11555.2890625
tensor(11555.2891, grad_fn=<NegBackward0>) tensor(11555.2891, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11555.2919921875
tensor(11555.2891, grad_fn=<NegBackward0>) tensor(11555.2920, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11555.2900390625
tensor(11555.2891, grad_fn=<NegBackward0>) tensor(11555.2900, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11555.29296875
tensor(11555.2891, grad_fn=<NegBackward0>) tensor(11555.2930, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11555.2880859375
tensor(11555.2891, grad_fn=<NegBackward0>) tensor(11555.2881, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11555.2890625
tensor(11555.2881, grad_fn=<NegBackward0>) tensor(11555.2891, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11555.29296875
tensor(11555.2881, grad_fn=<NegBackward0>) tensor(11555.2930, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11555.2919921875
tensor(11555.2881, grad_fn=<NegBackward0>) tensor(11555.2920, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11555.2890625
tensor(11555.2881, grad_fn=<NegBackward0>) tensor(11555.2891, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11555.2880859375
tensor(11555.2881, grad_fn=<NegBackward0>) tensor(11555.2881, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11555.2939453125
tensor(11555.2881, grad_fn=<NegBackward0>) tensor(11555.2939, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11555.287109375
tensor(11555.2881, grad_fn=<NegBackward0>) tensor(11555.2871, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11555.2880859375
tensor(11555.2871, grad_fn=<NegBackward0>) tensor(11555.2881, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11555.287109375
tensor(11555.2871, grad_fn=<NegBackward0>) tensor(11555.2871, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11555.287109375
tensor(11555.2871, grad_fn=<NegBackward0>) tensor(11555.2871, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11555.2861328125
tensor(11555.2871, grad_fn=<NegBackward0>) tensor(11555.2861, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11555.2880859375
tensor(11555.2861, grad_fn=<NegBackward0>) tensor(11555.2881, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11555.28515625
tensor(11555.2861, grad_fn=<NegBackward0>) tensor(11555.2852, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11555.287109375
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2871, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11555.287109375
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2871, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11555.2861328125
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2861, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11555.34765625
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.3477, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11555.28515625
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2852, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11555.2880859375
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2881, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11555.2861328125
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2861, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11555.287109375
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2871, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11555.2890625
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2891, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11555.287109375
tensor(11555.2852, grad_fn=<NegBackward0>) tensor(11555.2871, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7990, 0.2010],
        [0.2455, 0.7545]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4325, 0.5675], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4115, 0.1002],
         [0.6830, 0.1929]],

        [[0.5201, 0.1013],
         [0.7191, 0.6394]],

        [[0.5733, 0.1064],
         [0.7012, 0.5803]],

        [[0.6271, 0.0959],
         [0.6846, 0.7290]],

        [[0.6428, 0.1006],
         [0.5547, 0.7294]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919996552039955
[0.033407334532561075, 0.9919999711388391] [0.8829100081298821, 0.9919996552039955] [11894.7373046875, 11555.287109375]
-------------------------------------
This iteration is 52
True Objective function: Loss = -11564.070249680504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21890.927734375
inf tensor(21890.9277, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12283.45703125
tensor(21890.9277, grad_fn=<NegBackward0>) tensor(12283.4570, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11898.908203125
tensor(12283.4570, grad_fn=<NegBackward0>) tensor(11898.9082, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11736.0927734375
tensor(11898.9082, grad_fn=<NegBackward0>) tensor(11736.0928, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11735.2119140625
tensor(11736.0928, grad_fn=<NegBackward0>) tensor(11735.2119, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11734.912109375
tensor(11735.2119, grad_fn=<NegBackward0>) tensor(11734.9121, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11734.7568359375
tensor(11734.9121, grad_fn=<NegBackward0>) tensor(11734.7568, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11734.6630859375
tensor(11734.7568, grad_fn=<NegBackward0>) tensor(11734.6631, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11734.5966796875
tensor(11734.6631, grad_fn=<NegBackward0>) tensor(11734.5967, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11734.2978515625
tensor(11734.5967, grad_fn=<NegBackward0>) tensor(11734.2979, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11734.1787109375
tensor(11734.2979, grad_fn=<NegBackward0>) tensor(11734.1787, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11734.150390625
tensor(11734.1787, grad_fn=<NegBackward0>) tensor(11734.1504, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11734.1318359375
tensor(11734.1504, grad_fn=<NegBackward0>) tensor(11734.1318, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11734.1162109375
tensor(11734.1318, grad_fn=<NegBackward0>) tensor(11734.1162, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11734.103515625
tensor(11734.1162, grad_fn=<NegBackward0>) tensor(11734.1035, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11734.0947265625
tensor(11734.1035, grad_fn=<NegBackward0>) tensor(11734.0947, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11734.0859375
tensor(11734.0947, grad_fn=<NegBackward0>) tensor(11734.0859, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11734.0791015625
tensor(11734.0859, grad_fn=<NegBackward0>) tensor(11734.0791, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11734.0732421875
tensor(11734.0791, grad_fn=<NegBackward0>) tensor(11734.0732, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11734.0673828125
tensor(11734.0732, grad_fn=<NegBackward0>) tensor(11734.0674, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11734.0634765625
tensor(11734.0674, grad_fn=<NegBackward0>) tensor(11734.0635, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11734.05859375
tensor(11734.0635, grad_fn=<NegBackward0>) tensor(11734.0586, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11734.0556640625
tensor(11734.0586, grad_fn=<NegBackward0>) tensor(11734.0557, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11734.052734375
tensor(11734.0557, grad_fn=<NegBackward0>) tensor(11734.0527, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11734.0498046875
tensor(11734.0527, grad_fn=<NegBackward0>) tensor(11734.0498, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11734.046875
tensor(11734.0498, grad_fn=<NegBackward0>) tensor(11734.0469, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11734.044921875
tensor(11734.0469, grad_fn=<NegBackward0>) tensor(11734.0449, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11734.04296875
tensor(11734.0449, grad_fn=<NegBackward0>) tensor(11734.0430, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11734.041015625
tensor(11734.0430, grad_fn=<NegBackward0>) tensor(11734.0410, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11734.0380859375
tensor(11734.0410, grad_fn=<NegBackward0>) tensor(11734.0381, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11734.037109375
tensor(11734.0381, grad_fn=<NegBackward0>) tensor(11734.0371, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11734.0361328125
tensor(11734.0371, grad_fn=<NegBackward0>) tensor(11734.0361, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11734.03515625
tensor(11734.0361, grad_fn=<NegBackward0>) tensor(11734.0352, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11734.033203125
tensor(11734.0352, grad_fn=<NegBackward0>) tensor(11734.0332, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11734.0322265625
tensor(11734.0332, grad_fn=<NegBackward0>) tensor(11734.0322, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11734.0224609375
tensor(11734.0322, grad_fn=<NegBackward0>) tensor(11734.0225, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11733.9931640625
tensor(11734.0225, grad_fn=<NegBackward0>) tensor(11733.9932, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11733.98828125
tensor(11733.9932, grad_fn=<NegBackward0>) tensor(11733.9883, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11733.9892578125
tensor(11733.9883, grad_fn=<NegBackward0>) tensor(11733.9893, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11733.9912109375
tensor(11733.9883, grad_fn=<NegBackward0>) tensor(11733.9912, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11733.9873046875
tensor(11733.9883, grad_fn=<NegBackward0>) tensor(11733.9873, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11733.9853515625
tensor(11733.9873, grad_fn=<NegBackward0>) tensor(11733.9854, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11733.9853515625
tensor(11733.9854, grad_fn=<NegBackward0>) tensor(11733.9854, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11733.9853515625
tensor(11733.9854, grad_fn=<NegBackward0>) tensor(11733.9854, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11733.984375
tensor(11733.9854, grad_fn=<NegBackward0>) tensor(11733.9844, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11733.99609375
tensor(11733.9844, grad_fn=<NegBackward0>) tensor(11733.9961, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11733.9833984375
tensor(11733.9844, grad_fn=<NegBackward0>) tensor(11733.9834, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11733.9853515625
tensor(11733.9834, grad_fn=<NegBackward0>) tensor(11733.9854, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11733.982421875
tensor(11733.9834, grad_fn=<NegBackward0>) tensor(11733.9824, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11734.0009765625
tensor(11733.9824, grad_fn=<NegBackward0>) tensor(11734.0010, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11733.9921875
tensor(11733.9824, grad_fn=<NegBackward0>) tensor(11733.9922, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11733.982421875
tensor(11733.9824, grad_fn=<NegBackward0>) tensor(11733.9824, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11734.0029296875
tensor(11733.9824, grad_fn=<NegBackward0>) tensor(11734.0029, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11733.9814453125
tensor(11733.9824, grad_fn=<NegBackward0>) tensor(11733.9814, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11733.98046875
tensor(11733.9814, grad_fn=<NegBackward0>) tensor(11733.9805, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11733.9814453125
tensor(11733.9805, grad_fn=<NegBackward0>) tensor(11733.9814, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11733.98046875
tensor(11733.9805, grad_fn=<NegBackward0>) tensor(11733.9805, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11733.982421875
tensor(11733.9805, grad_fn=<NegBackward0>) tensor(11733.9824, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11733.9794921875
tensor(11733.9805, grad_fn=<NegBackward0>) tensor(11733.9795, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11733.986328125
tensor(11733.9795, grad_fn=<NegBackward0>) tensor(11733.9863, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11733.9833984375
tensor(11733.9795, grad_fn=<NegBackward0>) tensor(11733.9834, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11733.990234375
tensor(11733.9795, grad_fn=<NegBackward0>) tensor(11733.9902, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11733.978515625
tensor(11733.9795, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11733.9794921875
tensor(11733.9785, grad_fn=<NegBackward0>) tensor(11733.9795, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11733.978515625
tensor(11733.9785, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11733.978515625
tensor(11733.9785, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11733.978515625
tensor(11733.9785, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11733.9775390625
tensor(11733.9785, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11733.9775390625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11733.9775390625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11733.9775390625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11733.986328125
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9863, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11733.978515625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11733.9775390625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11733.9775390625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11733.978515625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11733.978515625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11733.9775390625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11733.978515625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9785, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11733.9814453125
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9814, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11733.9775390625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11733.9765625
tensor(11733.9775, grad_fn=<NegBackward0>) tensor(11733.9766, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11733.9794921875
tensor(11733.9766, grad_fn=<NegBackward0>) tensor(11733.9795, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11733.9765625
tensor(11733.9766, grad_fn=<NegBackward0>) tensor(11733.9766, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11733.986328125
tensor(11733.9766, grad_fn=<NegBackward0>) tensor(11733.9863, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11733.9794921875
tensor(11733.9766, grad_fn=<NegBackward0>) tensor(11733.9795, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11733.9970703125
tensor(11733.9766, grad_fn=<NegBackward0>) tensor(11733.9971, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11733.9775390625
tensor(11733.9766, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11733.9775390625
tensor(11733.9766, grad_fn=<NegBackward0>) tensor(11733.9775, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.6212, 0.3788],
        [0.5024, 0.4976]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4596, 0.5404], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2178, 0.0970],
         [0.6781, 0.4043]],

        [[0.6069, 0.0946],
         [0.5531, 0.7089]],

        [[0.6463, 0.1021],
         [0.6813, 0.6334]],

        [[0.7160, 0.1010],
         [0.5991, 0.7298]],

        [[0.5117, 0.1001],
         [0.6761, 0.6177]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 29
Adjusted Rand Index: 0.16949300045420165
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5116856969724113
Average Adjusted Rand Index: 0.8338986000908403
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19517.642578125
inf tensor(19517.6426, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12352.451171875
tensor(19517.6426, grad_fn=<NegBackward0>) tensor(12352.4512, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11622.1533203125
tensor(12352.4512, grad_fn=<NegBackward0>) tensor(11622.1533, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11582.98046875
tensor(11622.1533, grad_fn=<NegBackward0>) tensor(11582.9805, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11582.4111328125
tensor(11582.9805, grad_fn=<NegBackward0>) tensor(11582.4111, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11582.1513671875
tensor(11582.4111, grad_fn=<NegBackward0>) tensor(11582.1514, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11581.4326171875
tensor(11582.1514, grad_fn=<NegBackward0>) tensor(11581.4326, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11572.9140625
tensor(11581.4326, grad_fn=<NegBackward0>) tensor(11572.9141, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11567.54296875
tensor(11572.9141, grad_fn=<NegBackward0>) tensor(11567.5430, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11561.349609375
tensor(11567.5430, grad_fn=<NegBackward0>) tensor(11561.3496, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11561.3125
tensor(11561.3496, grad_fn=<NegBackward0>) tensor(11561.3125, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11561.28515625
tensor(11561.3125, grad_fn=<NegBackward0>) tensor(11561.2852, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11561.263671875
tensor(11561.2852, grad_fn=<NegBackward0>) tensor(11561.2637, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11561.2470703125
tensor(11561.2637, grad_fn=<NegBackward0>) tensor(11561.2471, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11561.232421875
tensor(11561.2471, grad_fn=<NegBackward0>) tensor(11561.2324, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11561.2236328125
tensor(11561.2324, grad_fn=<NegBackward0>) tensor(11561.2236, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11561.212890625
tensor(11561.2236, grad_fn=<NegBackward0>) tensor(11561.2129, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11561.205078125
tensor(11561.2129, grad_fn=<NegBackward0>) tensor(11561.2051, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11561.1982421875
tensor(11561.2051, grad_fn=<NegBackward0>) tensor(11561.1982, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11561.1923828125
tensor(11561.1982, grad_fn=<NegBackward0>) tensor(11561.1924, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11561.1884765625
tensor(11561.1924, grad_fn=<NegBackward0>) tensor(11561.1885, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11561.1826171875
tensor(11561.1885, grad_fn=<NegBackward0>) tensor(11561.1826, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11561.1787109375
tensor(11561.1826, grad_fn=<NegBackward0>) tensor(11561.1787, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11561.17578125
tensor(11561.1787, grad_fn=<NegBackward0>) tensor(11561.1758, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11561.1728515625
tensor(11561.1758, grad_fn=<NegBackward0>) tensor(11561.1729, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11561.1689453125
tensor(11561.1729, grad_fn=<NegBackward0>) tensor(11561.1689, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11561.16796875
tensor(11561.1689, grad_fn=<NegBackward0>) tensor(11561.1680, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11561.1650390625
tensor(11561.1680, grad_fn=<NegBackward0>) tensor(11561.1650, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11561.1640625
tensor(11561.1650, grad_fn=<NegBackward0>) tensor(11561.1641, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11561.16015625
tensor(11561.1641, grad_fn=<NegBackward0>) tensor(11561.1602, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11561.1572265625
tensor(11561.1602, grad_fn=<NegBackward0>) tensor(11561.1572, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11561.1552734375
tensor(11561.1572, grad_fn=<NegBackward0>) tensor(11561.1553, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11561.15234375
tensor(11561.1553, grad_fn=<NegBackward0>) tensor(11561.1523, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11561.150390625
tensor(11561.1523, grad_fn=<NegBackward0>) tensor(11561.1504, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11561.150390625
tensor(11561.1504, grad_fn=<NegBackward0>) tensor(11561.1504, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11561.150390625
tensor(11561.1504, grad_fn=<NegBackward0>) tensor(11561.1504, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11561.1484375
tensor(11561.1504, grad_fn=<NegBackward0>) tensor(11561.1484, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11561.1484375
tensor(11561.1484, grad_fn=<NegBackward0>) tensor(11561.1484, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11561.1455078125
tensor(11561.1484, grad_fn=<NegBackward0>) tensor(11561.1455, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11561.146484375
tensor(11561.1455, grad_fn=<NegBackward0>) tensor(11561.1465, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11561.14453125
tensor(11561.1455, grad_fn=<NegBackward0>) tensor(11561.1445, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11561.146484375
tensor(11561.1445, grad_fn=<NegBackward0>) tensor(11561.1465, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11561.14453125
tensor(11561.1445, grad_fn=<NegBackward0>) tensor(11561.1445, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11561.142578125
tensor(11561.1445, grad_fn=<NegBackward0>) tensor(11561.1426, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11561.1416015625
tensor(11561.1426, grad_fn=<NegBackward0>) tensor(11561.1416, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11561.1416015625
tensor(11561.1416, grad_fn=<NegBackward0>) tensor(11561.1416, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11561.140625
tensor(11561.1416, grad_fn=<NegBackward0>) tensor(11561.1406, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11561.1484375
tensor(11561.1406, grad_fn=<NegBackward0>) tensor(11561.1484, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11561.140625
tensor(11561.1406, grad_fn=<NegBackward0>) tensor(11561.1406, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11561.1376953125
tensor(11561.1406, grad_fn=<NegBackward0>) tensor(11561.1377, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11561.1376953125
tensor(11561.1377, grad_fn=<NegBackward0>) tensor(11561.1377, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11561.1357421875
tensor(11561.1377, grad_fn=<NegBackward0>) tensor(11561.1357, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11561.13671875
tensor(11561.1357, grad_fn=<NegBackward0>) tensor(11561.1367, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11561.13671875
tensor(11561.1357, grad_fn=<NegBackward0>) tensor(11561.1367, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11561.1357421875
tensor(11561.1357, grad_fn=<NegBackward0>) tensor(11561.1357, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11561.134765625
tensor(11561.1357, grad_fn=<NegBackward0>) tensor(11561.1348, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11561.134765625
tensor(11561.1348, grad_fn=<NegBackward0>) tensor(11561.1348, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11561.134765625
tensor(11561.1348, grad_fn=<NegBackward0>) tensor(11561.1348, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11561.1337890625
tensor(11561.1348, grad_fn=<NegBackward0>) tensor(11561.1338, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11561.13671875
tensor(11561.1338, grad_fn=<NegBackward0>) tensor(11561.1367, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11561.1337890625
tensor(11561.1338, grad_fn=<NegBackward0>) tensor(11561.1338, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11561.1337890625
tensor(11561.1338, grad_fn=<NegBackward0>) tensor(11561.1338, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11561.1318359375
tensor(11561.1338, grad_fn=<NegBackward0>) tensor(11561.1318, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11561.134765625
tensor(11561.1318, grad_fn=<NegBackward0>) tensor(11561.1348, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11561.1337890625
tensor(11561.1318, grad_fn=<NegBackward0>) tensor(11561.1338, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11561.1328125
tensor(11561.1318, grad_fn=<NegBackward0>) tensor(11561.1328, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11561.1328125
tensor(11561.1318, grad_fn=<NegBackward0>) tensor(11561.1328, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11561.1328125
tensor(11561.1318, grad_fn=<NegBackward0>) tensor(11561.1328, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.7623, 0.2377],
        [0.2208, 0.7792]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5400, 0.4600], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4065, 0.0970],
         [0.6281, 0.2038]],

        [[0.5020, 0.0946],
         [0.6206, 0.6564]],

        [[0.7250, 0.1020],
         [0.5569, 0.5392]],

        [[0.5097, 0.1009],
         [0.6015, 0.6083]],

        [[0.5561, 0.1002],
         [0.6380, 0.5299]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[0.5116856969724113, 1.0] [0.8338986000908403, 1.0] [11733.9775390625, 11561.1328125]
-------------------------------------
This iteration is 53
True Objective function: Loss = -11143.997778817185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20914.12109375
inf tensor(20914.1211, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11693.19140625
tensor(20914.1211, grad_fn=<NegBackward0>) tensor(11693.1914, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11284.0966796875
tensor(11693.1914, grad_fn=<NegBackward0>) tensor(11284.0967, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11279.25
tensor(11284.0967, grad_fn=<NegBackward0>) tensor(11279.2500, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11278.9658203125
tensor(11279.2500, grad_fn=<NegBackward0>) tensor(11278.9658, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11278.875
tensor(11278.9658, grad_fn=<NegBackward0>) tensor(11278.8750, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11278.8359375
tensor(11278.8750, grad_fn=<NegBackward0>) tensor(11278.8359, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11278.791015625
tensor(11278.8359, grad_fn=<NegBackward0>) tensor(11278.7910, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11278.7685546875
tensor(11278.7910, grad_fn=<NegBackward0>) tensor(11278.7686, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11278.751953125
tensor(11278.7686, grad_fn=<NegBackward0>) tensor(11278.7520, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11278.740234375
tensor(11278.7520, grad_fn=<NegBackward0>) tensor(11278.7402, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11278.732421875
tensor(11278.7402, grad_fn=<NegBackward0>) tensor(11278.7324, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11278.7236328125
tensor(11278.7324, grad_fn=<NegBackward0>) tensor(11278.7236, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11278.7177734375
tensor(11278.7236, grad_fn=<NegBackward0>) tensor(11278.7178, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11278.712890625
tensor(11278.7178, grad_fn=<NegBackward0>) tensor(11278.7129, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11278.708984375
tensor(11278.7129, grad_fn=<NegBackward0>) tensor(11278.7090, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11278.7060546875
tensor(11278.7090, grad_fn=<NegBackward0>) tensor(11278.7061, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11278.7021484375
tensor(11278.7061, grad_fn=<NegBackward0>) tensor(11278.7021, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11278.7001953125
tensor(11278.7021, grad_fn=<NegBackward0>) tensor(11278.7002, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11278.6982421875
tensor(11278.7002, grad_fn=<NegBackward0>) tensor(11278.6982, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11278.6962890625
tensor(11278.6982, grad_fn=<NegBackward0>) tensor(11278.6963, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11278.693359375
tensor(11278.6963, grad_fn=<NegBackward0>) tensor(11278.6934, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11278.6923828125
tensor(11278.6934, grad_fn=<NegBackward0>) tensor(11278.6924, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11278.69140625
tensor(11278.6924, grad_fn=<NegBackward0>) tensor(11278.6914, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11278.6904296875
tensor(11278.6914, grad_fn=<NegBackward0>) tensor(11278.6904, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11278.689453125
tensor(11278.6904, grad_fn=<NegBackward0>) tensor(11278.6895, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11278.689453125
tensor(11278.6895, grad_fn=<NegBackward0>) tensor(11278.6895, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11278.6884765625
tensor(11278.6895, grad_fn=<NegBackward0>) tensor(11278.6885, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11278.6875
tensor(11278.6885, grad_fn=<NegBackward0>) tensor(11278.6875, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11278.6865234375
tensor(11278.6875, grad_fn=<NegBackward0>) tensor(11278.6865, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11278.6875
tensor(11278.6865, grad_fn=<NegBackward0>) tensor(11278.6875, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11278.6845703125
tensor(11278.6865, grad_fn=<NegBackward0>) tensor(11278.6846, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11278.6875
tensor(11278.6846, grad_fn=<NegBackward0>) tensor(11278.6875, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11278.68359375
tensor(11278.6846, grad_fn=<NegBackward0>) tensor(11278.6836, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11278.6826171875
tensor(11278.6836, grad_fn=<NegBackward0>) tensor(11278.6826, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11278.6806640625
tensor(11278.6826, grad_fn=<NegBackward0>) tensor(11278.6807, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11278.6806640625
tensor(11278.6807, grad_fn=<NegBackward0>) tensor(11278.6807, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11278.6845703125
tensor(11278.6807, grad_fn=<NegBackward0>) tensor(11278.6846, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11278.6796875
tensor(11278.6807, grad_fn=<NegBackward0>) tensor(11278.6797, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11278.6787109375
tensor(11278.6797, grad_fn=<NegBackward0>) tensor(11278.6787, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11278.6787109375
tensor(11278.6787, grad_fn=<NegBackward0>) tensor(11278.6787, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11278.6787109375
tensor(11278.6787, grad_fn=<NegBackward0>) tensor(11278.6787, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11278.68359375
tensor(11278.6787, grad_fn=<NegBackward0>) tensor(11278.6836, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11278.677734375
tensor(11278.6787, grad_fn=<NegBackward0>) tensor(11278.6777, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11278.6767578125
tensor(11278.6777, grad_fn=<NegBackward0>) tensor(11278.6768, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11278.677734375
tensor(11278.6768, grad_fn=<NegBackward0>) tensor(11278.6777, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11278.6767578125
tensor(11278.6768, grad_fn=<NegBackward0>) tensor(11278.6768, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11278.6796875
tensor(11278.6768, grad_fn=<NegBackward0>) tensor(11278.6797, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11278.67578125
tensor(11278.6768, grad_fn=<NegBackward0>) tensor(11278.6758, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11278.67578125
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6758, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11278.6767578125
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6768, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11278.67578125
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6758, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11278.677734375
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6777, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11278.67578125
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6758, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11278.6865234375
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6865, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11278.67578125
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6758, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11278.681640625
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6816, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11278.677734375
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6777, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11278.6904296875
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6904, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11278.6767578125
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6768, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11278.6796875
tensor(11278.6758, grad_fn=<NegBackward0>) tensor(11278.6797, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.7961, 0.2039],
        [0.4049, 0.5951]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5038, 0.4962], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.1008],
         [0.5802, 0.3848]],

        [[0.6485, 0.0965],
         [0.6096, 0.6492]],

        [[0.6904, 0.0899],
         [0.5356, 0.6521]],

        [[0.6667, 0.0856],
         [0.5786, 0.7091]],

        [[0.5778, 0.1054],
         [0.5421, 0.5492]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.05494194975121322
Global Adjusted Rand Index: 0.5525380105066097
Average Adjusted Rand Index: 0.8029883899502426
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20284.61328125
inf tensor(20284.6133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11831.462890625
tensor(20284.6133, grad_fn=<NegBackward0>) tensor(11831.4629, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11824.96484375
tensor(11831.4629, grad_fn=<NegBackward0>) tensor(11824.9648, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11549.9873046875
tensor(11824.9648, grad_fn=<NegBackward0>) tensor(11549.9873, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11236.974609375
tensor(11549.9873, grad_fn=<NegBackward0>) tensor(11236.9746, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11217.2822265625
tensor(11236.9746, grad_fn=<NegBackward0>) tensor(11217.2822, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11213.640625
tensor(11217.2822, grad_fn=<NegBackward0>) tensor(11213.6406, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11207.38671875
tensor(11213.6406, grad_fn=<NegBackward0>) tensor(11207.3867, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11198.689453125
tensor(11207.3867, grad_fn=<NegBackward0>) tensor(11198.6895, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11183.466796875
tensor(11198.6895, grad_fn=<NegBackward0>) tensor(11183.4668, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11181.01953125
tensor(11183.4668, grad_fn=<NegBackward0>) tensor(11181.0195, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11180.927734375
tensor(11181.0195, grad_fn=<NegBackward0>) tensor(11180.9277, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11180.8857421875
tensor(11180.9277, grad_fn=<NegBackward0>) tensor(11180.8857, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11172.76953125
tensor(11180.8857, grad_fn=<NegBackward0>) tensor(11172.7695, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11172.70703125
tensor(11172.7695, grad_fn=<NegBackward0>) tensor(11172.7070, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11163.328125
tensor(11172.7070, grad_fn=<NegBackward0>) tensor(11163.3281, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11163.306640625
tensor(11163.3281, grad_fn=<NegBackward0>) tensor(11163.3066, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11163.2919921875
tensor(11163.3066, grad_fn=<NegBackward0>) tensor(11163.2920, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11163.275390625
tensor(11163.2920, grad_fn=<NegBackward0>) tensor(11163.2754, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11150.1923828125
tensor(11163.2754, grad_fn=<NegBackward0>) tensor(11150.1924, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11150.181640625
tensor(11150.1924, grad_fn=<NegBackward0>) tensor(11150.1816, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11150.177734375
tensor(11150.1816, grad_fn=<NegBackward0>) tensor(11150.1777, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11150.1689453125
tensor(11150.1777, grad_fn=<NegBackward0>) tensor(11150.1689, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11150.1630859375
tensor(11150.1689, grad_fn=<NegBackward0>) tensor(11150.1631, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11150.158203125
tensor(11150.1631, grad_fn=<NegBackward0>) tensor(11150.1582, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11150.15625
tensor(11150.1582, grad_fn=<NegBackward0>) tensor(11150.1562, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11150.1630859375
tensor(11150.1562, grad_fn=<NegBackward0>) tensor(11150.1631, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11150.1630859375
tensor(11150.1562, grad_fn=<NegBackward0>) tensor(11150.1631, grad_fn=<NegBackward0>)
2
Iteration 2800: Loss = -11150.1455078125
tensor(11150.1562, grad_fn=<NegBackward0>) tensor(11150.1455, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11150.1416015625
tensor(11150.1455, grad_fn=<NegBackward0>) tensor(11150.1416, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11150.140625
tensor(11150.1416, grad_fn=<NegBackward0>) tensor(11150.1406, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11150.1376953125
tensor(11150.1406, grad_fn=<NegBackward0>) tensor(11150.1377, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11150.1474609375
tensor(11150.1377, grad_fn=<NegBackward0>) tensor(11150.1475, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11150.134765625
tensor(11150.1377, grad_fn=<NegBackward0>) tensor(11150.1348, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11150.138671875
tensor(11150.1348, grad_fn=<NegBackward0>) tensor(11150.1387, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11150.1298828125
tensor(11150.1348, grad_fn=<NegBackward0>) tensor(11150.1299, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11141.0791015625
tensor(11150.1299, grad_fn=<NegBackward0>) tensor(11141.0791, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11141.078125
tensor(11141.0791, grad_fn=<NegBackward0>) tensor(11141.0781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11141.0771484375
tensor(11141.0781, grad_fn=<NegBackward0>) tensor(11141.0771, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11141.0869140625
tensor(11141.0771, grad_fn=<NegBackward0>) tensor(11141.0869, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11141.0732421875
tensor(11141.0771, grad_fn=<NegBackward0>) tensor(11141.0732, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11141.0751953125
tensor(11141.0732, grad_fn=<NegBackward0>) tensor(11141.0752, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11141.0732421875
tensor(11141.0732, grad_fn=<NegBackward0>) tensor(11141.0732, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11141.072265625
tensor(11141.0732, grad_fn=<NegBackward0>) tensor(11141.0723, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11141.0712890625
tensor(11141.0723, grad_fn=<NegBackward0>) tensor(11141.0713, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11141.0693359375
tensor(11141.0713, grad_fn=<NegBackward0>) tensor(11141.0693, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11141.078125
tensor(11141.0693, grad_fn=<NegBackward0>) tensor(11141.0781, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11141.072265625
tensor(11141.0693, grad_fn=<NegBackward0>) tensor(11141.0723, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11141.068359375
tensor(11141.0693, grad_fn=<NegBackward0>) tensor(11141.0684, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11141.068359375
tensor(11141.0684, grad_fn=<NegBackward0>) tensor(11141.0684, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11141.0673828125
tensor(11141.0684, grad_fn=<NegBackward0>) tensor(11141.0674, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11141.0673828125
tensor(11141.0674, grad_fn=<NegBackward0>) tensor(11141.0674, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11141.0673828125
tensor(11141.0674, grad_fn=<NegBackward0>) tensor(11141.0674, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11141.0712890625
tensor(11141.0674, grad_fn=<NegBackward0>) tensor(11141.0713, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11141.0654296875
tensor(11141.0674, grad_fn=<NegBackward0>) tensor(11141.0654, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11141.0654296875
tensor(11141.0654, grad_fn=<NegBackward0>) tensor(11141.0654, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11141.0654296875
tensor(11141.0654, grad_fn=<NegBackward0>) tensor(11141.0654, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11141.0654296875
tensor(11141.0654, grad_fn=<NegBackward0>) tensor(11141.0654, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11141.064453125
tensor(11141.0654, grad_fn=<NegBackward0>) tensor(11141.0645, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11141.06640625
tensor(11141.0645, grad_fn=<NegBackward0>) tensor(11141.0664, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11141.064453125
tensor(11141.0645, grad_fn=<NegBackward0>) tensor(11141.0645, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11141.064453125
tensor(11141.0645, grad_fn=<NegBackward0>) tensor(11141.0645, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11141.0634765625
tensor(11141.0645, grad_fn=<NegBackward0>) tensor(11141.0635, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11141.064453125
tensor(11141.0635, grad_fn=<NegBackward0>) tensor(11141.0645, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11141.0615234375
tensor(11141.0635, grad_fn=<NegBackward0>) tensor(11141.0615, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11141.0625
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0625, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11141.0625
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0625, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11141.0615234375
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0615, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11141.0703125
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0703, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11141.0625
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0625, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11141.0615234375
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0615, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11141.064453125
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0645, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11141.060546875
tensor(11141.0615, grad_fn=<NegBackward0>) tensor(11141.0605, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11141.0810546875
tensor(11141.0605, grad_fn=<NegBackward0>) tensor(11141.0811, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11141.06640625
tensor(11141.0605, grad_fn=<NegBackward0>) tensor(11141.0664, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11141.0849609375
tensor(11141.0605, grad_fn=<NegBackward0>) tensor(11141.0850, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11141.0703125
tensor(11141.0605, grad_fn=<NegBackward0>) tensor(11141.0703, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11141.099609375
tensor(11141.0605, grad_fn=<NegBackward0>) tensor(11141.0996, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7835, 0.2165],
        [0.2959, 0.7041]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5015, 0.4985], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1942, 0.1010],
         [0.6416, 0.3942]],

        [[0.6244, 0.0966],
         [0.5906, 0.6968]],

        [[0.6039, 0.0929],
         [0.6579, 0.7106]],

        [[0.7091, 0.0856],
         [0.5611, 0.5655]],

        [[0.6082, 0.0948],
         [0.7230, 0.5497]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
time is 3
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840312292878678
Average Adjusted Rand Index: 0.9841613032984714
[0.5525380105066097, 0.9840312292878678] [0.8029883899502426, 0.9841613032984714] [11278.6796875, 11141.099609375]
-------------------------------------
This iteration is 54
True Objective function: Loss = -11398.620989882842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20898.1171875
inf tensor(20898.1172, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12125.84375
tensor(20898.1172, grad_fn=<NegBackward0>) tensor(12125.8438, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11893.318359375
tensor(12125.8438, grad_fn=<NegBackward0>) tensor(11893.3184, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11451.3447265625
tensor(11893.3184, grad_fn=<NegBackward0>) tensor(11451.3447, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11429.943359375
tensor(11451.3447, grad_fn=<NegBackward0>) tensor(11429.9434, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11412.619140625
tensor(11429.9434, grad_fn=<NegBackward0>) tensor(11412.6191, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11412.3056640625
tensor(11412.6191, grad_fn=<NegBackward0>) tensor(11412.3057, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11412.1328125
tensor(11412.3057, grad_fn=<NegBackward0>) tensor(11412.1328, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11412.0244140625
tensor(11412.1328, grad_fn=<NegBackward0>) tensor(11412.0244, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11411.9482421875
tensor(11412.0244, grad_fn=<NegBackward0>) tensor(11411.9482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11411.8935546875
tensor(11411.9482, grad_fn=<NegBackward0>) tensor(11411.8936, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11411.8525390625
tensor(11411.8936, grad_fn=<NegBackward0>) tensor(11411.8525, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11411.8203125
tensor(11411.8525, grad_fn=<NegBackward0>) tensor(11411.8203, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11411.794921875
tensor(11411.8203, grad_fn=<NegBackward0>) tensor(11411.7949, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11411.771484375
tensor(11411.7949, grad_fn=<NegBackward0>) tensor(11411.7715, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11411.75390625
tensor(11411.7715, grad_fn=<NegBackward0>) tensor(11411.7539, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11411.7392578125
tensor(11411.7539, grad_fn=<NegBackward0>) tensor(11411.7393, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11411.7275390625
tensor(11411.7393, grad_fn=<NegBackward0>) tensor(11411.7275, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11411.716796875
tensor(11411.7275, grad_fn=<NegBackward0>) tensor(11411.7168, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11411.708984375
tensor(11411.7168, grad_fn=<NegBackward0>) tensor(11411.7090, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11411.7001953125
tensor(11411.7090, grad_fn=<NegBackward0>) tensor(11411.7002, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11411.6953125
tensor(11411.7002, grad_fn=<NegBackward0>) tensor(11411.6953, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11411.689453125
tensor(11411.6953, grad_fn=<NegBackward0>) tensor(11411.6895, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11411.6826171875
tensor(11411.6895, grad_fn=<NegBackward0>) tensor(11411.6826, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11411.6787109375
tensor(11411.6826, grad_fn=<NegBackward0>) tensor(11411.6787, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11398.0234375
tensor(11411.6787, grad_fn=<NegBackward0>) tensor(11398.0234, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11398.0
tensor(11398.0234, grad_fn=<NegBackward0>) tensor(11398., grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11397.9970703125
tensor(11398., grad_fn=<NegBackward0>) tensor(11397.9971, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11397.994140625
tensor(11397.9971, grad_fn=<NegBackward0>) tensor(11397.9941, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11397.9892578125
tensor(11397.9941, grad_fn=<NegBackward0>) tensor(11397.9893, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11397.9873046875
tensor(11397.9893, grad_fn=<NegBackward0>) tensor(11397.9873, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11397.984375
tensor(11397.9873, grad_fn=<NegBackward0>) tensor(11397.9844, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11397.9775390625
tensor(11397.9844, grad_fn=<NegBackward0>) tensor(11397.9775, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11392.056640625
tensor(11397.9775, grad_fn=<NegBackward0>) tensor(11392.0566, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11392.0546875
tensor(11392.0566, grad_fn=<NegBackward0>) tensor(11392.0547, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11392.052734375
tensor(11392.0547, grad_fn=<NegBackward0>) tensor(11392.0527, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11392.05078125
tensor(11392.0527, grad_fn=<NegBackward0>) tensor(11392.0508, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11392.0498046875
tensor(11392.0508, grad_fn=<NegBackward0>) tensor(11392.0498, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11392.048828125
tensor(11392.0498, grad_fn=<NegBackward0>) tensor(11392.0488, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11392.046875
tensor(11392.0488, grad_fn=<NegBackward0>) tensor(11392.0469, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11392.0458984375
tensor(11392.0469, grad_fn=<NegBackward0>) tensor(11392.0459, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11392.0458984375
tensor(11392.0459, grad_fn=<NegBackward0>) tensor(11392.0459, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11392.0439453125
tensor(11392.0459, grad_fn=<NegBackward0>) tensor(11392.0439, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11392.04296875
tensor(11392.0439, grad_fn=<NegBackward0>) tensor(11392.0430, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11392.0419921875
tensor(11392.0430, grad_fn=<NegBackward0>) tensor(11392.0420, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11392.04296875
tensor(11392.0420, grad_fn=<NegBackward0>) tensor(11392.0430, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11392.041015625
tensor(11392.0420, grad_fn=<NegBackward0>) tensor(11392.0410, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11392.041015625
tensor(11392.0410, grad_fn=<NegBackward0>) tensor(11392.0410, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11392.0400390625
tensor(11392.0410, grad_fn=<NegBackward0>) tensor(11392.0400, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11392.0380859375
tensor(11392.0400, grad_fn=<NegBackward0>) tensor(11392.0381, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11392.0390625
tensor(11392.0381, grad_fn=<NegBackward0>) tensor(11392.0391, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11392.037109375
tensor(11392.0381, grad_fn=<NegBackward0>) tensor(11392.0371, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11392.0361328125
tensor(11392.0371, grad_fn=<NegBackward0>) tensor(11392.0361, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11392.0087890625
tensor(11392.0361, grad_fn=<NegBackward0>) tensor(11392.0088, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11392.0078125
tensor(11392.0088, grad_fn=<NegBackward0>) tensor(11392.0078, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11392.0078125
tensor(11392.0078, grad_fn=<NegBackward0>) tensor(11392.0078, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11392.0068359375
tensor(11392.0078, grad_fn=<NegBackward0>) tensor(11392.0068, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11392.0078125
tensor(11392.0068, grad_fn=<NegBackward0>) tensor(11392.0078, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11392.005859375
tensor(11392.0068, grad_fn=<NegBackward0>) tensor(11392.0059, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11392.0068359375
tensor(11392.0059, grad_fn=<NegBackward0>) tensor(11392.0068, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11392.0048828125
tensor(11392.0059, grad_fn=<NegBackward0>) tensor(11392.0049, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11392.0068359375
tensor(11392.0049, grad_fn=<NegBackward0>) tensor(11392.0068, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11392.00390625
tensor(11392.0049, grad_fn=<NegBackward0>) tensor(11392.0039, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11392.013671875
tensor(11392.0039, grad_fn=<NegBackward0>) tensor(11392.0137, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11392.0048828125
tensor(11392.0039, grad_fn=<NegBackward0>) tensor(11392.0049, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11392.00390625
tensor(11392.0039, grad_fn=<NegBackward0>) tensor(11392.0039, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11392.00390625
tensor(11392.0039, grad_fn=<NegBackward0>) tensor(11392.0039, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11392.0029296875
tensor(11392.0039, grad_fn=<NegBackward0>) tensor(11392.0029, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11392.005859375
tensor(11392.0029, grad_fn=<NegBackward0>) tensor(11392.0059, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11392.00390625
tensor(11392.0029, grad_fn=<NegBackward0>) tensor(11392.0039, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11392.0087890625
tensor(11392.0029, grad_fn=<NegBackward0>) tensor(11392.0088, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11392.0068359375
tensor(11392.0029, grad_fn=<NegBackward0>) tensor(11392.0068, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11392.0029296875
tensor(11392.0029, grad_fn=<NegBackward0>) tensor(11392.0029, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11392.0048828125
tensor(11392.0029, grad_fn=<NegBackward0>) tensor(11392.0049, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11392.001953125
tensor(11392.0029, grad_fn=<NegBackward0>) tensor(11392.0020, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11392.001953125
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392.0020, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11392.0205078125
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392.0205, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11392.001953125
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392.0020, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11392.001953125
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392.0020, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11392.044921875
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392.0449, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11392.001953125
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392.0020, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11392.0146484375
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392.0146, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11392.0
tensor(11392.0020, grad_fn=<NegBackward0>) tensor(11392., grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11392.001953125
tensor(11392., grad_fn=<NegBackward0>) tensor(11392.0020, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11391.9873046875
tensor(11392., grad_fn=<NegBackward0>) tensor(11391.9873, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11391.9873046875
tensor(11391.9873, grad_fn=<NegBackward0>) tensor(11391.9873, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11391.9873046875
tensor(11391.9873, grad_fn=<NegBackward0>) tensor(11391.9873, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11391.990234375
tensor(11391.9873, grad_fn=<NegBackward0>) tensor(11391.9902, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11391.9873046875
tensor(11391.9873, grad_fn=<NegBackward0>) tensor(11391.9873, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11391.9873046875
tensor(11391.9873, grad_fn=<NegBackward0>) tensor(11391.9873, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11391.986328125
tensor(11391.9873, grad_fn=<NegBackward0>) tensor(11391.9863, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11391.986328125
tensor(11391.9863, grad_fn=<NegBackward0>) tensor(11391.9863, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11392.0830078125
tensor(11391.9863, grad_fn=<NegBackward0>) tensor(11392.0830, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11392.001953125
tensor(11391.9863, grad_fn=<NegBackward0>) tensor(11392.0020, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11391.9921875
tensor(11391.9863, grad_fn=<NegBackward0>) tensor(11391.9922, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11391.9814453125
tensor(11391.9863, grad_fn=<NegBackward0>) tensor(11391.9814, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11391.986328125
tensor(11391.9814, grad_fn=<NegBackward0>) tensor(11391.9863, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11391.9833984375
tensor(11391.9814, grad_fn=<NegBackward0>) tensor(11391.9834, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11392.0419921875
tensor(11391.9814, grad_fn=<NegBackward0>) tensor(11392.0420, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11391.9853515625
tensor(11391.9814, grad_fn=<NegBackward0>) tensor(11391.9854, grad_fn=<NegBackward0>)
4
pi: tensor([[0.7987, 0.2013],
        [0.2618, 0.7382]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5194, 0.4806], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1972, 0.1018],
         [0.5950, 0.4024]],

        [[0.6659, 0.1012],
         [0.7250, 0.7066]],

        [[0.5781, 0.1056],
         [0.7202, 0.7141]],

        [[0.6463, 0.0948],
         [0.5074, 0.5181]],

        [[0.6848, 0.0911],
         [0.6120, 0.6583]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9598877889442053
Global Adjusted Rand Index: 0.9919999558239977
Average Adjusted Rand Index: 0.991977557788841
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22543.65234375
inf tensor(22543.6523, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12144.76953125
tensor(22543.6523, grad_fn=<NegBackward0>) tensor(12144.7695, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12064.34765625
tensor(12144.7695, grad_fn=<NegBackward0>) tensor(12064.3477, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11614.462890625
tensor(12064.3477, grad_fn=<NegBackward0>) tensor(11614.4629, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11498.9619140625
tensor(11614.4629, grad_fn=<NegBackward0>) tensor(11498.9619, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11477.2626953125
tensor(11498.9619, grad_fn=<NegBackward0>) tensor(11477.2627, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11465.37890625
tensor(11477.2627, grad_fn=<NegBackward0>) tensor(11465.3789, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11451.849609375
tensor(11465.3789, grad_fn=<NegBackward0>) tensor(11451.8496, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11451.56640625
tensor(11451.8496, grad_fn=<NegBackward0>) tensor(11451.5664, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11444.4677734375
tensor(11451.5664, grad_fn=<NegBackward0>) tensor(11444.4678, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11424.724609375
tensor(11444.4678, grad_fn=<NegBackward0>) tensor(11424.7246, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11407.796875
tensor(11424.7246, grad_fn=<NegBackward0>) tensor(11407.7969, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11407.4716796875
tensor(11407.7969, grad_fn=<NegBackward0>) tensor(11407.4717, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11398.2626953125
tensor(11407.4717, grad_fn=<NegBackward0>) tensor(11398.2627, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11398.2138671875
tensor(11398.2627, grad_fn=<NegBackward0>) tensor(11398.2139, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11398.1728515625
tensor(11398.2139, grad_fn=<NegBackward0>) tensor(11398.1729, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11398.1357421875
tensor(11398.1729, grad_fn=<NegBackward0>) tensor(11398.1357, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11398.1103515625
tensor(11398.1357, grad_fn=<NegBackward0>) tensor(11398.1104, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11398.08984375
tensor(11398.1104, grad_fn=<NegBackward0>) tensor(11398.0898, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11398.0732421875
tensor(11398.0898, grad_fn=<NegBackward0>) tensor(11398.0732, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11398.05859375
tensor(11398.0732, grad_fn=<NegBackward0>) tensor(11398.0586, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11398.046875
tensor(11398.0586, grad_fn=<NegBackward0>) tensor(11398.0469, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11398.037109375
tensor(11398.0469, grad_fn=<NegBackward0>) tensor(11398.0371, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11398.02734375
tensor(11398.0371, grad_fn=<NegBackward0>) tensor(11398.0273, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11398.0185546875
tensor(11398.0273, grad_fn=<NegBackward0>) tensor(11398.0186, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11398.013671875
tensor(11398.0186, grad_fn=<NegBackward0>) tensor(11398.0137, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11398.0068359375
tensor(11398.0137, grad_fn=<NegBackward0>) tensor(11398.0068, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11398.0
tensor(11398.0068, grad_fn=<NegBackward0>) tensor(11398., grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11397.99609375
tensor(11398., grad_fn=<NegBackward0>) tensor(11397.9961, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11397.9912109375
tensor(11397.9961, grad_fn=<NegBackward0>) tensor(11397.9912, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11397.986328125
tensor(11397.9912, grad_fn=<NegBackward0>) tensor(11397.9863, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11397.9833984375
tensor(11397.9863, grad_fn=<NegBackward0>) tensor(11397.9834, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11397.9794921875
tensor(11397.9834, grad_fn=<NegBackward0>) tensor(11397.9795, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11397.9775390625
tensor(11397.9795, grad_fn=<NegBackward0>) tensor(11397.9775, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11397.9736328125
tensor(11397.9775, grad_fn=<NegBackward0>) tensor(11397.9736, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11397.970703125
tensor(11397.9736, grad_fn=<NegBackward0>) tensor(11397.9707, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11397.9677734375
tensor(11397.9707, grad_fn=<NegBackward0>) tensor(11397.9678, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11397.9658203125
tensor(11397.9678, grad_fn=<NegBackward0>) tensor(11397.9658, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11397.9609375
tensor(11397.9658, grad_fn=<NegBackward0>) tensor(11397.9609, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11397.9501953125
tensor(11397.9609, grad_fn=<NegBackward0>) tensor(11397.9502, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11397.947265625
tensor(11397.9502, grad_fn=<NegBackward0>) tensor(11397.9473, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11397.9443359375
tensor(11397.9473, grad_fn=<NegBackward0>) tensor(11397.9443, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11397.9443359375
tensor(11397.9443, grad_fn=<NegBackward0>) tensor(11397.9443, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11397.94140625
tensor(11397.9443, grad_fn=<NegBackward0>) tensor(11397.9414, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11397.943359375
tensor(11397.9414, grad_fn=<NegBackward0>) tensor(11397.9434, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11397.939453125
tensor(11397.9414, grad_fn=<NegBackward0>) tensor(11397.9395, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11397.939453125
tensor(11397.9395, grad_fn=<NegBackward0>) tensor(11397.9395, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11397.9384765625
tensor(11397.9395, grad_fn=<NegBackward0>) tensor(11397.9385, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11397.9365234375
tensor(11397.9385, grad_fn=<NegBackward0>) tensor(11397.9365, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11397.943359375
tensor(11397.9365, grad_fn=<NegBackward0>) tensor(11397.9434, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11397.935546875
tensor(11397.9365, grad_fn=<NegBackward0>) tensor(11397.9355, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11397.9345703125
tensor(11397.9355, grad_fn=<NegBackward0>) tensor(11397.9346, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11397.93359375
tensor(11397.9346, grad_fn=<NegBackward0>) tensor(11397.9336, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11397.93359375
tensor(11397.9336, grad_fn=<NegBackward0>) tensor(11397.9336, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11397.9345703125
tensor(11397.9336, grad_fn=<NegBackward0>) tensor(11397.9346, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11397.931640625
tensor(11397.9336, grad_fn=<NegBackward0>) tensor(11397.9316, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11397.9306640625
tensor(11397.9316, grad_fn=<NegBackward0>) tensor(11397.9307, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11397.9306640625
tensor(11397.9307, grad_fn=<NegBackward0>) tensor(11397.9307, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11397.9296875
tensor(11397.9307, grad_fn=<NegBackward0>) tensor(11397.9297, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11397.9326171875
tensor(11397.9297, grad_fn=<NegBackward0>) tensor(11397.9326, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11397.927734375
tensor(11397.9297, grad_fn=<NegBackward0>) tensor(11397.9277, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11397.9287109375
tensor(11397.9277, grad_fn=<NegBackward0>) tensor(11397.9287, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11397.927734375
tensor(11397.9277, grad_fn=<NegBackward0>) tensor(11397.9277, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11397.927734375
tensor(11397.9277, grad_fn=<NegBackward0>) tensor(11397.9277, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11397.9267578125
tensor(11397.9277, grad_fn=<NegBackward0>) tensor(11397.9268, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11397.9267578125
tensor(11397.9268, grad_fn=<NegBackward0>) tensor(11397.9268, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11397.9267578125
tensor(11397.9268, grad_fn=<NegBackward0>) tensor(11397.9268, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11397.92578125
tensor(11397.9268, grad_fn=<NegBackward0>) tensor(11397.9258, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11397.9248046875
tensor(11397.9258, grad_fn=<NegBackward0>) tensor(11397.9248, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11397.9287109375
tensor(11397.9248, grad_fn=<NegBackward0>) tensor(11397.9287, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11397.9189453125
tensor(11397.9248, grad_fn=<NegBackward0>) tensor(11397.9189, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11397.91796875
tensor(11397.9189, grad_fn=<NegBackward0>) tensor(11397.9180, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11397.916015625
tensor(11397.9180, grad_fn=<NegBackward0>) tensor(11397.9160, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11397.9169921875
tensor(11397.9160, grad_fn=<NegBackward0>) tensor(11397.9170, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11397.92578125
tensor(11397.9160, grad_fn=<NegBackward0>) tensor(11397.9258, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11397.9169921875
tensor(11397.9160, grad_fn=<NegBackward0>) tensor(11397.9170, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11397.9169921875
tensor(11397.9160, grad_fn=<NegBackward0>) tensor(11397.9170, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11397.9189453125
tensor(11397.9160, grad_fn=<NegBackward0>) tensor(11397.9189, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7321, 0.2679],
        [0.2053, 0.7947]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4782, 0.5218], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4032, 0.1019],
         [0.6310, 0.1964]],

        [[0.6962, 0.1012],
         [0.6086, 0.7031]],

        [[0.6622, 0.1090],
         [0.7060, 0.6006]],

        [[0.6085, 0.0948],
         [0.5763, 0.5995]],

        [[0.6947, 0.0912],
         [0.5155, 0.6703]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
Global Adjusted Rand Index: 0.9840319537411472
Average Adjusted Rand Index: 0.9839771189524041
[0.9919999558239977, 0.9840319537411472] [0.991977557788841, 0.9839771189524041] [11391.982421875, 11397.9189453125]
-------------------------------------
This iteration is 55
True Objective function: Loss = -11803.272069736695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22492.029296875
inf tensor(22492.0293, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12768.8388671875
tensor(22492.0293, grad_fn=<NegBackward0>) tensor(12768.8389, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12235.0107421875
tensor(12768.8389, grad_fn=<NegBackward0>) tensor(12235.0107, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12219.88671875
tensor(12235.0107, grad_fn=<NegBackward0>) tensor(12219.8867, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12219.1630859375
tensor(12219.8867, grad_fn=<NegBackward0>) tensor(12219.1631, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12218.82421875
tensor(12219.1631, grad_fn=<NegBackward0>) tensor(12218.8242, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12218.6376953125
tensor(12218.8242, grad_fn=<NegBackward0>) tensor(12218.6377, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12218.5126953125
tensor(12218.6377, grad_fn=<NegBackward0>) tensor(12218.5127, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12215.2001953125
tensor(12218.5127, grad_fn=<NegBackward0>) tensor(12215.2002, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12214.9404296875
tensor(12215.2002, grad_fn=<NegBackward0>) tensor(12214.9404, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12214.865234375
tensor(12214.9404, grad_fn=<NegBackward0>) tensor(12214.8652, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12214.8203125
tensor(12214.8652, grad_fn=<NegBackward0>) tensor(12214.8203, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12214.7880859375
tensor(12214.8203, grad_fn=<NegBackward0>) tensor(12214.7881, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12214.7626953125
tensor(12214.7881, grad_fn=<NegBackward0>) tensor(12214.7627, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12214.7412109375
tensor(12214.7627, grad_fn=<NegBackward0>) tensor(12214.7412, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12214.720703125
tensor(12214.7412, grad_fn=<NegBackward0>) tensor(12214.7207, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12214.693359375
tensor(12214.7207, grad_fn=<NegBackward0>) tensor(12214.6934, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12214.62890625
tensor(12214.6934, grad_fn=<NegBackward0>) tensor(12214.6289, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12172.96875
tensor(12214.6289, grad_fn=<NegBackward0>) tensor(12172.9688, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11945.4150390625
tensor(12172.9688, grad_fn=<NegBackward0>) tensor(11945.4150, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11841.458984375
tensor(11945.4150, grad_fn=<NegBackward0>) tensor(11841.4590, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11813.4150390625
tensor(11841.4590, grad_fn=<NegBackward0>) tensor(11813.4150, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11792.3388671875
tensor(11813.4150, grad_fn=<NegBackward0>) tensor(11792.3389, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11792.3173828125
tensor(11792.3389, grad_fn=<NegBackward0>) tensor(11792.3174, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11792.3056640625
tensor(11792.3174, grad_fn=<NegBackward0>) tensor(11792.3057, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11792.2978515625
tensor(11792.3057, grad_fn=<NegBackward0>) tensor(11792.2979, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11792.29296875
tensor(11792.2979, grad_fn=<NegBackward0>) tensor(11792.2930, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11792.2890625
tensor(11792.2930, grad_fn=<NegBackward0>) tensor(11792.2891, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11792.28515625
tensor(11792.2891, grad_fn=<NegBackward0>) tensor(11792.2852, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11792.2822265625
tensor(11792.2852, grad_fn=<NegBackward0>) tensor(11792.2822, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11792.279296875
tensor(11792.2822, grad_fn=<NegBackward0>) tensor(11792.2793, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11792.2763671875
tensor(11792.2793, grad_fn=<NegBackward0>) tensor(11792.2764, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11792.2744140625
tensor(11792.2764, grad_fn=<NegBackward0>) tensor(11792.2744, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11792.271484375
tensor(11792.2744, grad_fn=<NegBackward0>) tensor(11792.2715, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11792.2705078125
tensor(11792.2715, grad_fn=<NegBackward0>) tensor(11792.2705, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11792.26953125
tensor(11792.2705, grad_fn=<NegBackward0>) tensor(11792.2695, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11792.26953125
tensor(11792.2695, grad_fn=<NegBackward0>) tensor(11792.2695, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11792.2666015625
tensor(11792.2695, grad_fn=<NegBackward0>) tensor(11792.2666, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11792.265625
tensor(11792.2666, grad_fn=<NegBackward0>) tensor(11792.2656, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11792.2666015625
tensor(11792.2656, grad_fn=<NegBackward0>) tensor(11792.2666, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11792.2646484375
tensor(11792.2656, grad_fn=<NegBackward0>) tensor(11792.2646, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11792.263671875
tensor(11792.2646, grad_fn=<NegBackward0>) tensor(11792.2637, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11792.2626953125
tensor(11792.2637, grad_fn=<NegBackward0>) tensor(11792.2627, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11792.2626953125
tensor(11792.2627, grad_fn=<NegBackward0>) tensor(11792.2627, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11792.26171875
tensor(11792.2627, grad_fn=<NegBackward0>) tensor(11792.2617, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11792.26171875
tensor(11792.2617, grad_fn=<NegBackward0>) tensor(11792.2617, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11792.259765625
tensor(11792.2617, grad_fn=<NegBackward0>) tensor(11792.2598, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11792.259765625
tensor(11792.2598, grad_fn=<NegBackward0>) tensor(11792.2598, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11792.263671875
tensor(11792.2598, grad_fn=<NegBackward0>) tensor(11792.2637, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11792.259765625
tensor(11792.2598, grad_fn=<NegBackward0>) tensor(11792.2598, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11792.2587890625
tensor(11792.2598, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11792.259765625
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2598, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11792.2587890625
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11792.2646484375
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2646, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11792.2578125
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2578, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11792.2578125
tensor(11792.2578, grad_fn=<NegBackward0>) tensor(11792.2578, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11792.2607421875
tensor(11792.2578, grad_fn=<NegBackward0>) tensor(11792.2607, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11792.255859375
tensor(11792.2578, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11792.263671875
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2637, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11792.2568359375
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2568, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11792.255859375
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11792.259765625
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2598, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11792.2568359375
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2568, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11792.25390625
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11792.255859375
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11792.2548828125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11792.255859375
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11792.2548828125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11792.2548828125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11792.2548828125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11792.2685546875
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2686, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11792.2548828125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11792.2529296875
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2529, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11792.2578125
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2578, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11792.2529296875
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2529, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11792.25390625
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11792.25390625
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11792.2548828125
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11792.2529296875
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2529, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11792.25390625
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11792.25390625
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11792.25390625
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11792.25390625
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11792.25390625
tensor(11792.2529, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7480, 0.2520],
        [0.2882, 0.7118]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6400, 0.3600], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4112, 0.0994],
         [0.6418, 0.2003]],

        [[0.6894, 0.0890],
         [0.5800, 0.6819]],

        [[0.6857, 0.0885],
         [0.7024, 0.6838]],

        [[0.5719, 0.0938],
         [0.5922, 0.7041]],

        [[0.6155, 0.0974],
         [0.6217, 0.6192]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19786.115234375
inf tensor(19786.1152, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12259.603515625
tensor(19786.1152, grad_fn=<NegBackward0>) tensor(12259.6035, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11817.037109375
tensor(12259.6035, grad_fn=<NegBackward0>) tensor(11817.0371, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11815.6923828125
tensor(11817.0371, grad_fn=<NegBackward0>) tensor(11815.6924, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11807.55859375
tensor(11815.6924, grad_fn=<NegBackward0>) tensor(11807.5586, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11806.185546875
tensor(11807.5586, grad_fn=<NegBackward0>) tensor(11806.1855, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11806.046875
tensor(11806.1855, grad_fn=<NegBackward0>) tensor(11806.0469, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11805.958984375
tensor(11806.0469, grad_fn=<NegBackward0>) tensor(11805.9590, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11805.896484375
tensor(11805.9590, grad_fn=<NegBackward0>) tensor(11805.8965, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11805.8505859375
tensor(11805.8965, grad_fn=<NegBackward0>) tensor(11805.8506, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11805.81640625
tensor(11805.8506, grad_fn=<NegBackward0>) tensor(11805.8164, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11805.7900390625
tensor(11805.8164, grad_fn=<NegBackward0>) tensor(11805.7900, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11805.76953125
tensor(11805.7900, grad_fn=<NegBackward0>) tensor(11805.7695, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11805.7529296875
tensor(11805.7695, grad_fn=<NegBackward0>) tensor(11805.7529, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11805.73828125
tensor(11805.7529, grad_fn=<NegBackward0>) tensor(11805.7383, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11805.7265625
tensor(11805.7383, grad_fn=<NegBackward0>) tensor(11805.7266, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11805.71875
tensor(11805.7266, grad_fn=<NegBackward0>) tensor(11805.7188, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11805.708984375
tensor(11805.7188, grad_fn=<NegBackward0>) tensor(11805.7090, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11805.703125
tensor(11805.7090, grad_fn=<NegBackward0>) tensor(11805.7031, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11805.6962890625
tensor(11805.7031, grad_fn=<NegBackward0>) tensor(11805.6963, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11805.69140625
tensor(11805.6963, grad_fn=<NegBackward0>) tensor(11805.6914, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11805.6865234375
tensor(11805.6914, grad_fn=<NegBackward0>) tensor(11805.6865, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11805.68359375
tensor(11805.6865, grad_fn=<NegBackward0>) tensor(11805.6836, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11805.6796875
tensor(11805.6836, grad_fn=<NegBackward0>) tensor(11805.6797, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11805.67578125
tensor(11805.6797, grad_fn=<NegBackward0>) tensor(11805.6758, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11805.673828125
tensor(11805.6758, grad_fn=<NegBackward0>) tensor(11805.6738, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11805.6708984375
tensor(11805.6738, grad_fn=<NegBackward0>) tensor(11805.6709, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11805.6689453125
tensor(11805.6709, grad_fn=<NegBackward0>) tensor(11805.6689, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11805.6669921875
tensor(11805.6689, grad_fn=<NegBackward0>) tensor(11805.6670, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11805.6455078125
tensor(11805.6670, grad_fn=<NegBackward0>) tensor(11805.6455, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11792.275390625
tensor(11805.6455, grad_fn=<NegBackward0>) tensor(11792.2754, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11792.2734375
tensor(11792.2754, grad_fn=<NegBackward0>) tensor(11792.2734, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11792.2734375
tensor(11792.2734, grad_fn=<NegBackward0>) tensor(11792.2734, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11792.271484375
tensor(11792.2734, grad_fn=<NegBackward0>) tensor(11792.2715, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11792.2705078125
tensor(11792.2715, grad_fn=<NegBackward0>) tensor(11792.2705, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11792.2685546875
tensor(11792.2705, grad_fn=<NegBackward0>) tensor(11792.2686, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11792.2666015625
tensor(11792.2686, grad_fn=<NegBackward0>) tensor(11792.2666, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11792.265625
tensor(11792.2666, grad_fn=<NegBackward0>) tensor(11792.2656, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11792.2666015625
tensor(11792.2656, grad_fn=<NegBackward0>) tensor(11792.2666, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11792.2646484375
tensor(11792.2656, grad_fn=<NegBackward0>) tensor(11792.2646, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11792.265625
tensor(11792.2646, grad_fn=<NegBackward0>) tensor(11792.2656, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11792.2626953125
tensor(11792.2646, grad_fn=<NegBackward0>) tensor(11792.2627, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11792.263671875
tensor(11792.2627, grad_fn=<NegBackward0>) tensor(11792.2637, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11792.26171875
tensor(11792.2627, grad_fn=<NegBackward0>) tensor(11792.2617, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11792.26171875
tensor(11792.2617, grad_fn=<NegBackward0>) tensor(11792.2617, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11792.263671875
tensor(11792.2617, grad_fn=<NegBackward0>) tensor(11792.2637, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11792.2607421875
tensor(11792.2617, grad_fn=<NegBackward0>) tensor(11792.2607, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11792.26171875
tensor(11792.2607, grad_fn=<NegBackward0>) tensor(11792.2617, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11792.2587890625
tensor(11792.2607, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11792.2587890625
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11792.2646484375
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2646, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11792.2587890625
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11792.2587890625
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11792.2587890625
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11792.2568359375
tensor(11792.2588, grad_fn=<NegBackward0>) tensor(11792.2568, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11792.2587890625
tensor(11792.2568, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11792.2568359375
tensor(11792.2568, grad_fn=<NegBackward0>) tensor(11792.2568, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11792.2568359375
tensor(11792.2568, grad_fn=<NegBackward0>) tensor(11792.2568, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11792.255859375
tensor(11792.2568, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11792.2568359375
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2568, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11792.2587890625
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2588, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11792.255859375
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11792.2548828125
tensor(11792.2559, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11792.255859375
tensor(11792.2549, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11792.2548828125
tensor(11792.2549, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11792.2548828125
tensor(11792.2549, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11792.265625
tensor(11792.2549, grad_fn=<NegBackward0>) tensor(11792.2656, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11792.25390625
tensor(11792.2549, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11792.265625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2656, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11792.2666015625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2666, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11792.2548828125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2549, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11792.255859375
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11792.255859375
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2559, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11792.2646484375
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2646, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11792.275390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2754, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11792.25390625
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11792.3046875
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.3047, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11792.2578125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2578, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11792.251953125
tensor(11792.2539, grad_fn=<NegBackward0>) tensor(11792.2520, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11792.2578125
tensor(11792.2520, grad_fn=<NegBackward0>) tensor(11792.2578, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11792.2529296875
tensor(11792.2520, grad_fn=<NegBackward0>) tensor(11792.2529, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11792.2666015625
tensor(11792.2520, grad_fn=<NegBackward0>) tensor(11792.2666, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11792.25390625
tensor(11792.2520, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11792.25390625
tensor(11792.2520, grad_fn=<NegBackward0>) tensor(11792.2539, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7480, 0.2520],
        [0.2882, 0.7118]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6401, 0.3599], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4111, 0.0994],
         [0.5171, 0.2003]],

        [[0.5680, 0.0890],
         [0.5832, 0.7114]],

        [[0.5750, 0.0885],
         [0.6629, 0.6309]],

        [[0.5543, 0.0938],
         [0.7008, 0.5611]],

        [[0.5077, 0.0974],
         [0.6909, 0.7260]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11792.25390625, 11792.25390625]
-------------------------------------
This iteration is 56
True Objective function: Loss = -11990.269222144521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22026.857421875
inf tensor(22026.8574, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11988.0556640625
tensor(22026.8574, grad_fn=<NegBackward0>) tensor(11988.0557, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11985.982421875
tensor(11988.0557, grad_fn=<NegBackward0>) tensor(11985.9824, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11985.5859375
tensor(11985.9824, grad_fn=<NegBackward0>) tensor(11985.5859, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11985.408203125
tensor(11985.5859, grad_fn=<NegBackward0>) tensor(11985.4082, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11985.30859375
tensor(11985.4082, grad_fn=<NegBackward0>) tensor(11985.3086, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11985.248046875
tensor(11985.3086, grad_fn=<NegBackward0>) tensor(11985.2480, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11985.2060546875
tensor(11985.2480, grad_fn=<NegBackward0>) tensor(11985.2061, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11985.1796875
tensor(11985.2061, grad_fn=<NegBackward0>) tensor(11985.1797, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11985.1572265625
tensor(11985.1797, grad_fn=<NegBackward0>) tensor(11985.1572, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11985.142578125
tensor(11985.1572, grad_fn=<NegBackward0>) tensor(11985.1426, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11985.1298828125
tensor(11985.1426, grad_fn=<NegBackward0>) tensor(11985.1299, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11985.119140625
tensor(11985.1299, grad_fn=<NegBackward0>) tensor(11985.1191, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11985.1123046875
tensor(11985.1191, grad_fn=<NegBackward0>) tensor(11985.1123, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11985.10546875
tensor(11985.1123, grad_fn=<NegBackward0>) tensor(11985.1055, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11985.1005859375
tensor(11985.1055, grad_fn=<NegBackward0>) tensor(11985.1006, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11985.095703125
tensor(11985.1006, grad_fn=<NegBackward0>) tensor(11985.0957, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11985.091796875
tensor(11985.0957, grad_fn=<NegBackward0>) tensor(11985.0918, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11985.0888671875
tensor(11985.0918, grad_fn=<NegBackward0>) tensor(11985.0889, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11985.0849609375
tensor(11985.0889, grad_fn=<NegBackward0>) tensor(11985.0850, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11985.0849609375
tensor(11985.0850, grad_fn=<NegBackward0>) tensor(11985.0850, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11985.08203125
tensor(11985.0850, grad_fn=<NegBackward0>) tensor(11985.0820, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11985.0791015625
tensor(11985.0820, grad_fn=<NegBackward0>) tensor(11985.0791, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11985.078125
tensor(11985.0791, grad_fn=<NegBackward0>) tensor(11985.0781, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11985.076171875
tensor(11985.0781, grad_fn=<NegBackward0>) tensor(11985.0762, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11985.07421875
tensor(11985.0762, grad_fn=<NegBackward0>) tensor(11985.0742, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11985.0732421875
tensor(11985.0742, grad_fn=<NegBackward0>) tensor(11985.0732, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11985.0732421875
tensor(11985.0732, grad_fn=<NegBackward0>) tensor(11985.0732, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11985.0712890625
tensor(11985.0732, grad_fn=<NegBackward0>) tensor(11985.0713, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11985.0703125
tensor(11985.0713, grad_fn=<NegBackward0>) tensor(11985.0703, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11985.0703125
tensor(11985.0703, grad_fn=<NegBackward0>) tensor(11985.0703, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11985.068359375
tensor(11985.0703, grad_fn=<NegBackward0>) tensor(11985.0684, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11985.068359375
tensor(11985.0684, grad_fn=<NegBackward0>) tensor(11985.0684, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11985.0673828125
tensor(11985.0684, grad_fn=<NegBackward0>) tensor(11985.0674, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11985.0673828125
tensor(11985.0674, grad_fn=<NegBackward0>) tensor(11985.0674, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11985.0673828125
tensor(11985.0674, grad_fn=<NegBackward0>) tensor(11985.0674, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11985.06640625
tensor(11985.0674, grad_fn=<NegBackward0>) tensor(11985.0664, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11985.06640625
tensor(11985.0664, grad_fn=<NegBackward0>) tensor(11985.0664, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11985.0654296875
tensor(11985.0664, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11985.064453125
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0645, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11985.06640625
tensor(11985.0645, grad_fn=<NegBackward0>) tensor(11985.0664, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11985.0654296875
tensor(11985.0645, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11985.0625
tensor(11985.0645, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11985.0625
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11985.0634765625
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0635, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11985.0615234375
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0615, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11985.064453125
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0645, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11985.0634765625
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0635, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11985.0625
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11985.0625
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -11985.0625
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.7137, 0.2863],
        [0.2368, 0.7632]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4227, 0.5773], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.1006],
         [0.6353, 0.4016]],

        [[0.7180, 0.0994],
         [0.7110, 0.6883]],

        [[0.6587, 0.1033],
         [0.6409, 0.5164]],

        [[0.7136, 0.1142],
         [0.5284, 0.5056]],

        [[0.5608, 0.1013],
         [0.5824, 0.6362]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23461.8359375
inf tensor(23461.8359, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12901.232421875
tensor(23461.8359, grad_fn=<NegBackward0>) tensor(12901.2324, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12402.8173828125
tensor(12901.2324, grad_fn=<NegBackward0>) tensor(12402.8174, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12144.669921875
tensor(12402.8174, grad_fn=<NegBackward0>) tensor(12144.6699, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12108.9150390625
tensor(12144.6699, grad_fn=<NegBackward0>) tensor(12108.9150, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12081.640625
tensor(12108.9150, grad_fn=<NegBackward0>) tensor(12081.6406, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12048.00390625
tensor(12081.6406, grad_fn=<NegBackward0>) tensor(12048.0039, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12040.21484375
tensor(12048.0039, grad_fn=<NegBackward0>) tensor(12040.2148, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12027.435546875
tensor(12040.2148, grad_fn=<NegBackward0>) tensor(12027.4355, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12007.6611328125
tensor(12027.4355, grad_fn=<NegBackward0>) tensor(12007.6611, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12007.5712890625
tensor(12007.6611, grad_fn=<NegBackward0>) tensor(12007.5713, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12006.173828125
tensor(12007.5713, grad_fn=<NegBackward0>) tensor(12006.1738, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12005.998046875
tensor(12006.1738, grad_fn=<NegBackward0>) tensor(12005.9980, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12005.9365234375
tensor(12005.9980, grad_fn=<NegBackward0>) tensor(12005.9365, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11995.9443359375
tensor(12005.9365, grad_fn=<NegBackward0>) tensor(11995.9443, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11995.923828125
tensor(11995.9443, grad_fn=<NegBackward0>) tensor(11995.9238, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11995.9306640625
tensor(11995.9238, grad_fn=<NegBackward0>) tensor(11995.9307, grad_fn=<NegBackward0>)
1
Iteration 1700: Loss = -11995.892578125
tensor(11995.9238, grad_fn=<NegBackward0>) tensor(11995.8926, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11995.8798828125
tensor(11995.8926, grad_fn=<NegBackward0>) tensor(11995.8799, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11995.869140625
tensor(11995.8799, grad_fn=<NegBackward0>) tensor(11995.8691, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11995.8583984375
tensor(11995.8691, grad_fn=<NegBackward0>) tensor(11995.8584, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11995.8427734375
tensor(11995.8584, grad_fn=<NegBackward0>) tensor(11995.8428, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11985.1640625
tensor(11995.8428, grad_fn=<NegBackward0>) tensor(11985.1641, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11985.1533203125
tensor(11985.1641, grad_fn=<NegBackward0>) tensor(11985.1533, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11985.14453125
tensor(11985.1533, grad_fn=<NegBackward0>) tensor(11985.1445, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11985.1396484375
tensor(11985.1445, grad_fn=<NegBackward0>) tensor(11985.1396, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11985.1357421875
tensor(11985.1396, grad_fn=<NegBackward0>) tensor(11985.1357, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11985.12890625
tensor(11985.1357, grad_fn=<NegBackward0>) tensor(11985.1289, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11985.125
tensor(11985.1289, grad_fn=<NegBackward0>) tensor(11985.1250, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11985.1201171875
tensor(11985.1250, grad_fn=<NegBackward0>) tensor(11985.1201, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11985.125
tensor(11985.1201, grad_fn=<NegBackward0>) tensor(11985.1250, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11985.1083984375
tensor(11985.1201, grad_fn=<NegBackward0>) tensor(11985.1084, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11985.103515625
tensor(11985.1084, grad_fn=<NegBackward0>) tensor(11985.1035, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11985.099609375
tensor(11985.1035, grad_fn=<NegBackward0>) tensor(11985.0996, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11985.0986328125
tensor(11985.0996, grad_fn=<NegBackward0>) tensor(11985.0986, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11985.09765625
tensor(11985.0986, grad_fn=<NegBackward0>) tensor(11985.0977, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11985.095703125
tensor(11985.0977, grad_fn=<NegBackward0>) tensor(11985.0957, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11985.09375
tensor(11985.0957, grad_fn=<NegBackward0>) tensor(11985.0938, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11985.091796875
tensor(11985.0938, grad_fn=<NegBackward0>) tensor(11985.0918, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11985.091796875
tensor(11985.0918, grad_fn=<NegBackward0>) tensor(11985.0918, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11985.08984375
tensor(11985.0918, grad_fn=<NegBackward0>) tensor(11985.0898, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11985.08984375
tensor(11985.0898, grad_fn=<NegBackward0>) tensor(11985.0898, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11985.087890625
tensor(11985.0898, grad_fn=<NegBackward0>) tensor(11985.0879, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11985.087890625
tensor(11985.0879, grad_fn=<NegBackward0>) tensor(11985.0879, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11985.0859375
tensor(11985.0879, grad_fn=<NegBackward0>) tensor(11985.0859, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11985.0849609375
tensor(11985.0859, grad_fn=<NegBackward0>) tensor(11985.0850, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11985.083984375
tensor(11985.0850, grad_fn=<NegBackward0>) tensor(11985.0840, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11985.0830078125
tensor(11985.0840, grad_fn=<NegBackward0>) tensor(11985.0830, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11985.0986328125
tensor(11985.0830, grad_fn=<NegBackward0>) tensor(11985.0986, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11985.0810546875
tensor(11985.0830, grad_fn=<NegBackward0>) tensor(11985.0811, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11985.0947265625
tensor(11985.0811, grad_fn=<NegBackward0>) tensor(11985.0947, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11985.0810546875
tensor(11985.0811, grad_fn=<NegBackward0>) tensor(11985.0811, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11985.08203125
tensor(11985.0811, grad_fn=<NegBackward0>) tensor(11985.0820, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11985.0810546875
tensor(11985.0811, grad_fn=<NegBackward0>) tensor(11985.0811, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11985.080078125
tensor(11985.0811, grad_fn=<NegBackward0>) tensor(11985.0801, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11985.080078125
tensor(11985.0801, grad_fn=<NegBackward0>) tensor(11985.0801, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11985.0791015625
tensor(11985.0801, grad_fn=<NegBackward0>) tensor(11985.0791, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11985.0791015625
tensor(11985.0791, grad_fn=<NegBackward0>) tensor(11985.0791, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11985.078125
tensor(11985.0791, grad_fn=<NegBackward0>) tensor(11985.0781, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11985.078125
tensor(11985.0781, grad_fn=<NegBackward0>) tensor(11985.0781, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11985.078125
tensor(11985.0781, grad_fn=<NegBackward0>) tensor(11985.0781, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11985.068359375
tensor(11985.0781, grad_fn=<NegBackward0>) tensor(11985.0684, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11985.068359375
tensor(11985.0684, grad_fn=<NegBackward0>) tensor(11985.0684, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11985.06640625
tensor(11985.0684, grad_fn=<NegBackward0>) tensor(11985.0664, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11985.06640625
tensor(11985.0664, grad_fn=<NegBackward0>) tensor(11985.0664, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11985.0732421875
tensor(11985.0664, grad_fn=<NegBackward0>) tensor(11985.0732, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11985.0654296875
tensor(11985.0664, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11985.068359375
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0684, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11985.068359375
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0684, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11985.1025390625
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.1025, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11985.0947265625
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0947, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11985.0673828125
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0674, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11985.0986328125
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0986, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11985.0673828125
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0674, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11985.0830078125
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0830, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11985.072265625
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0723, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11985.068359375
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0684, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11985.0654296875
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0654, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11985.06640625
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0664, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11985.0634765625
tensor(11985.0654, grad_fn=<NegBackward0>) tensor(11985.0635, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11985.064453125
tensor(11985.0635, grad_fn=<NegBackward0>) tensor(11985.0645, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11985.0703125
tensor(11985.0635, grad_fn=<NegBackward0>) tensor(11985.0703, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11985.064453125
tensor(11985.0635, grad_fn=<NegBackward0>) tensor(11985.0645, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11985.0634765625
tensor(11985.0635, grad_fn=<NegBackward0>) tensor(11985.0635, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11985.0625
tensor(11985.0635, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11985.0625
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11985.1875
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.1875, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11985.0625
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11985.0703125
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0703, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11985.0634765625
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0635, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11985.0615234375
tensor(11985.0625, grad_fn=<NegBackward0>) tensor(11985.0615, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11985.0615234375
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0615, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11985.0625
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0625, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11985.10546875
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.1055, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11985.0712890625
tensor(11985.0615, grad_fn=<NegBackward0>) tensor(11985.0713, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7638, 0.2362],
        [0.2827, 0.7173]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5790, 0.4210], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4017, 0.1005],
         [0.6374, 0.2026]],

        [[0.5797, 0.0994],
         [0.6386, 0.6507]],

        [[0.6982, 0.1034],
         [0.7068, 0.6357]],

        [[0.5297, 0.1142],
         [0.5340, 0.6381]],

        [[0.5910, 0.1013],
         [0.6906, 0.5481]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11985.0625, 11985.06640625]
-------------------------------------
This iteration is 57
True Objective function: Loss = -11692.160926326182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22127.91796875
inf tensor(22127.9180, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12474.9990234375
tensor(22127.9180, grad_fn=<NegBackward0>) tensor(12474.9990, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12252.0791015625
tensor(12474.9990, grad_fn=<NegBackward0>) tensor(12252.0791, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11983.998046875
tensor(12252.0791, grad_fn=<NegBackward0>) tensor(11983.9980, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11914.572265625
tensor(11983.9980, grad_fn=<NegBackward0>) tensor(11914.5723, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11875.9306640625
tensor(11914.5723, grad_fn=<NegBackward0>) tensor(11875.9307, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11875.51171875
tensor(11875.9307, grad_fn=<NegBackward0>) tensor(11875.5117, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11873.01953125
tensor(11875.5117, grad_fn=<NegBackward0>) tensor(11873.0195, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11872.068359375
tensor(11873.0195, grad_fn=<NegBackward0>) tensor(11872.0684, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11871.984375
tensor(11872.0684, grad_fn=<NegBackward0>) tensor(11871.9844, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11871.931640625
tensor(11871.9844, grad_fn=<NegBackward0>) tensor(11871.9316, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11871.8857421875
tensor(11871.9316, grad_fn=<NegBackward0>) tensor(11871.8857, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11864.1279296875
tensor(11871.8857, grad_fn=<NegBackward0>) tensor(11864.1279, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11864.0361328125
tensor(11864.1279, grad_fn=<NegBackward0>) tensor(11864.0361, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11864.021484375
tensor(11864.0361, grad_fn=<NegBackward0>) tensor(11864.0215, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11864.0
tensor(11864.0215, grad_fn=<NegBackward0>) tensor(11864., grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11863.986328125
tensor(11864., grad_fn=<NegBackward0>) tensor(11863.9863, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11863.9765625
tensor(11863.9863, grad_fn=<NegBackward0>) tensor(11863.9766, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11863.966796875
tensor(11863.9766, grad_fn=<NegBackward0>) tensor(11863.9668, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11863.9609375
tensor(11863.9668, grad_fn=<NegBackward0>) tensor(11863.9609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11863.951171875
tensor(11863.9609, grad_fn=<NegBackward0>) tensor(11863.9512, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11863.9462890625
tensor(11863.9512, grad_fn=<NegBackward0>) tensor(11863.9463, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11863.9423828125
tensor(11863.9463, grad_fn=<NegBackward0>) tensor(11863.9424, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11863.9375
tensor(11863.9424, grad_fn=<NegBackward0>) tensor(11863.9375, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11863.93359375
tensor(11863.9375, grad_fn=<NegBackward0>) tensor(11863.9336, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11863.9296875
tensor(11863.9336, grad_fn=<NegBackward0>) tensor(11863.9297, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11863.9267578125
tensor(11863.9297, grad_fn=<NegBackward0>) tensor(11863.9268, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11863.923828125
tensor(11863.9268, grad_fn=<NegBackward0>) tensor(11863.9238, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11863.919921875
tensor(11863.9238, grad_fn=<NegBackward0>) tensor(11863.9199, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11863.931640625
tensor(11863.9199, grad_fn=<NegBackward0>) tensor(11863.9316, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11863.916015625
tensor(11863.9199, grad_fn=<NegBackward0>) tensor(11863.9160, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11863.9150390625
tensor(11863.9160, grad_fn=<NegBackward0>) tensor(11863.9150, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11863.9130859375
tensor(11863.9150, grad_fn=<NegBackward0>) tensor(11863.9131, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11863.912109375
tensor(11863.9131, grad_fn=<NegBackward0>) tensor(11863.9121, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11863.912109375
tensor(11863.9121, grad_fn=<NegBackward0>) tensor(11863.9121, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11863.91015625
tensor(11863.9121, grad_fn=<NegBackward0>) tensor(11863.9102, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11863.908203125
tensor(11863.9102, grad_fn=<NegBackward0>) tensor(11863.9082, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11863.90625
tensor(11863.9082, grad_fn=<NegBackward0>) tensor(11863.9062, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11863.904296875
tensor(11863.9062, grad_fn=<NegBackward0>) tensor(11863.9043, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11863.904296875
tensor(11863.9043, grad_fn=<NegBackward0>) tensor(11863.9043, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11863.9033203125
tensor(11863.9043, grad_fn=<NegBackward0>) tensor(11863.9033, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11863.9013671875
tensor(11863.9033, grad_fn=<NegBackward0>) tensor(11863.9014, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11863.9013671875
tensor(11863.9014, grad_fn=<NegBackward0>) tensor(11863.9014, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11863.8994140625
tensor(11863.9014, grad_fn=<NegBackward0>) tensor(11863.8994, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11863.8994140625
tensor(11863.8994, grad_fn=<NegBackward0>) tensor(11863.8994, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11863.900390625
tensor(11863.8994, grad_fn=<NegBackward0>) tensor(11863.9004, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11863.8984375
tensor(11863.8994, grad_fn=<NegBackward0>) tensor(11863.8984, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11863.900390625
tensor(11863.8984, grad_fn=<NegBackward0>) tensor(11863.9004, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11863.90234375
tensor(11863.8984, grad_fn=<NegBackward0>) tensor(11863.9023, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11863.8994140625
tensor(11863.8984, grad_fn=<NegBackward0>) tensor(11863.8994, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11863.8974609375
tensor(11863.8984, grad_fn=<NegBackward0>) tensor(11863.8975, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11863.8955078125
tensor(11863.8975, grad_fn=<NegBackward0>) tensor(11863.8955, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11863.896484375
tensor(11863.8955, grad_fn=<NegBackward0>) tensor(11863.8965, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11863.8955078125
tensor(11863.8955, grad_fn=<NegBackward0>) tensor(11863.8955, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11863.89453125
tensor(11863.8955, grad_fn=<NegBackward0>) tensor(11863.8945, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11863.896484375
tensor(11863.8945, grad_fn=<NegBackward0>) tensor(11863.8965, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11863.896484375
tensor(11863.8945, grad_fn=<NegBackward0>) tensor(11863.8965, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11863.8935546875
tensor(11863.8945, grad_fn=<NegBackward0>) tensor(11863.8936, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11863.892578125
tensor(11863.8936, grad_fn=<NegBackward0>) tensor(11863.8926, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11863.87890625
tensor(11863.8926, grad_fn=<NegBackward0>) tensor(11863.8789, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11863.8017578125
tensor(11863.8789, grad_fn=<NegBackward0>) tensor(11863.8018, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11863.802734375
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.8027, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11863.806640625
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.8066, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11863.8017578125
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.8018, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11863.802734375
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.8027, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11863.8017578125
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.8018, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11863.8046875
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.8047, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11863.802734375
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.8027, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11863.7998046875
tensor(11863.8018, grad_fn=<NegBackward0>) tensor(11863.7998, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11863.80078125
tensor(11863.7998, grad_fn=<NegBackward0>) tensor(11863.8008, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11863.80078125
tensor(11863.7998, grad_fn=<NegBackward0>) tensor(11863.8008, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11863.7998046875
tensor(11863.7998, grad_fn=<NegBackward0>) tensor(11863.7998, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11863.7998046875
tensor(11863.7998, grad_fn=<NegBackward0>) tensor(11863.7998, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11863.7998046875
tensor(11863.7998, grad_fn=<NegBackward0>) tensor(11863.7998, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11863.798828125
tensor(11863.7998, grad_fn=<NegBackward0>) tensor(11863.7988, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11863.7998046875
tensor(11863.7988, grad_fn=<NegBackward0>) tensor(11863.7998, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11863.8720703125
tensor(11863.7988, grad_fn=<NegBackward0>) tensor(11863.8721, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11863.798828125
tensor(11863.7988, grad_fn=<NegBackward0>) tensor(11863.7988, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11863.9697265625
tensor(11863.7988, grad_fn=<NegBackward0>) tensor(11863.9697, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11863.798828125
tensor(11863.7988, grad_fn=<NegBackward0>) tensor(11863.7988, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11863.7998046875
tensor(11863.7988, grad_fn=<NegBackward0>) tensor(11863.7998, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11863.7978515625
tensor(11863.7988, grad_fn=<NegBackward0>) tensor(11863.7979, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11863.7978515625
tensor(11863.7979, grad_fn=<NegBackward0>) tensor(11863.7979, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11863.8115234375
tensor(11863.7979, grad_fn=<NegBackward0>) tensor(11863.8115, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11863.7998046875
tensor(11863.7979, grad_fn=<NegBackward0>) tensor(11863.7998, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11863.8173828125
tensor(11863.7979, grad_fn=<NegBackward0>) tensor(11863.8174, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11863.798828125
tensor(11863.7979, grad_fn=<NegBackward0>) tensor(11863.7988, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11863.798828125
tensor(11863.7979, grad_fn=<NegBackward0>) tensor(11863.7988, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.6441, 0.3559],
        [0.4731, 0.5269]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4271, 0.5729], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2313, 0.1003],
         [0.5965, 0.3879]],

        [[0.6219, 0.0977],
         [0.6580, 0.5974]],

        [[0.7033, 0.1058],
         [0.5110, 0.5750]],

        [[0.6942, 0.0949],
         [0.5819, 0.6267]],

        [[0.6205, 0.0950],
         [0.7022, 0.5214]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 26
Adjusted Rand Index: 0.22464375523889354
time is 3
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.49460610348277506
Average Adjusted Rand Index: 0.8449287510477788
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21878.46484375
inf tensor(21878.4648, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12496.232421875
tensor(21878.4648, grad_fn=<NegBackward0>) tensor(12496.2324, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12138.099609375
tensor(12496.2324, grad_fn=<NegBackward0>) tensor(12138.0996, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11809.94140625
tensor(12138.0996, grad_fn=<NegBackward0>) tensor(11809.9414, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11733.0703125
tensor(11809.9414, grad_fn=<NegBackward0>) tensor(11733.0703, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11723.3095703125
tensor(11733.0703, grad_fn=<NegBackward0>) tensor(11723.3096, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11722.79296875
tensor(11723.3096, grad_fn=<NegBackward0>) tensor(11722.7930, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11711.888671875
tensor(11722.7930, grad_fn=<NegBackward0>) tensor(11711.8887, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11690.7880859375
tensor(11711.8887, grad_fn=<NegBackward0>) tensor(11690.7881, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11690.6708984375
tensor(11690.7881, grad_fn=<NegBackward0>) tensor(11690.6709, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11690.5927734375
tensor(11690.6709, grad_fn=<NegBackward0>) tensor(11690.5928, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11690.5361328125
tensor(11690.5928, grad_fn=<NegBackward0>) tensor(11690.5361, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11690.4912109375
tensor(11690.5361, grad_fn=<NegBackward0>) tensor(11690.4912, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11690.4560546875
tensor(11690.4912, grad_fn=<NegBackward0>) tensor(11690.4561, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11690.4306640625
tensor(11690.4561, grad_fn=<NegBackward0>) tensor(11690.4307, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11690.4091796875
tensor(11690.4307, grad_fn=<NegBackward0>) tensor(11690.4092, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11690.388671875
tensor(11690.4092, grad_fn=<NegBackward0>) tensor(11690.3887, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11690.3740234375
tensor(11690.3887, grad_fn=<NegBackward0>) tensor(11690.3740, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11690.3603515625
tensor(11690.3740, grad_fn=<NegBackward0>) tensor(11690.3604, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11690.349609375
tensor(11690.3604, grad_fn=<NegBackward0>) tensor(11690.3496, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11690.3447265625
tensor(11690.3496, grad_fn=<NegBackward0>) tensor(11690.3447, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11690.3310546875
tensor(11690.3447, grad_fn=<NegBackward0>) tensor(11690.3311, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11690.32421875
tensor(11690.3311, grad_fn=<NegBackward0>) tensor(11690.3242, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11690.318359375
tensor(11690.3242, grad_fn=<NegBackward0>) tensor(11690.3184, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11690.3134765625
tensor(11690.3184, grad_fn=<NegBackward0>) tensor(11690.3135, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11690.3076171875
tensor(11690.3135, grad_fn=<NegBackward0>) tensor(11690.3076, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11690.3037109375
tensor(11690.3076, grad_fn=<NegBackward0>) tensor(11690.3037, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11690.298828125
tensor(11690.3037, grad_fn=<NegBackward0>) tensor(11690.2988, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11690.294921875
tensor(11690.2988, grad_fn=<NegBackward0>) tensor(11690.2949, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11690.29296875
tensor(11690.2949, grad_fn=<NegBackward0>) tensor(11690.2930, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11690.2890625
tensor(11690.2930, grad_fn=<NegBackward0>) tensor(11690.2891, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11690.2861328125
tensor(11690.2891, grad_fn=<NegBackward0>) tensor(11690.2861, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11690.2841796875
tensor(11690.2861, grad_fn=<NegBackward0>) tensor(11690.2842, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11690.28125
tensor(11690.2842, grad_fn=<NegBackward0>) tensor(11690.2812, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11690.2783203125
tensor(11690.2812, grad_fn=<NegBackward0>) tensor(11690.2783, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11690.2783203125
tensor(11690.2783, grad_fn=<NegBackward0>) tensor(11690.2783, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11690.2744140625
tensor(11690.2783, grad_fn=<NegBackward0>) tensor(11690.2744, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11690.2744140625
tensor(11690.2744, grad_fn=<NegBackward0>) tensor(11690.2744, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11690.2724609375
tensor(11690.2744, grad_fn=<NegBackward0>) tensor(11690.2725, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11690.2705078125
tensor(11690.2725, grad_fn=<NegBackward0>) tensor(11690.2705, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11690.2685546875
tensor(11690.2705, grad_fn=<NegBackward0>) tensor(11690.2686, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11690.267578125
tensor(11690.2686, grad_fn=<NegBackward0>) tensor(11690.2676, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11690.2685546875
tensor(11690.2676, grad_fn=<NegBackward0>) tensor(11690.2686, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11690.2666015625
tensor(11690.2676, grad_fn=<NegBackward0>) tensor(11690.2666, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11690.2646484375
tensor(11690.2666, grad_fn=<NegBackward0>) tensor(11690.2646, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11690.265625
tensor(11690.2646, grad_fn=<NegBackward0>) tensor(11690.2656, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11690.265625
tensor(11690.2646, grad_fn=<NegBackward0>) tensor(11690.2656, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11690.2626953125
tensor(11690.2646, grad_fn=<NegBackward0>) tensor(11690.2627, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11690.26171875
tensor(11690.2627, grad_fn=<NegBackward0>) tensor(11690.2617, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11690.263671875
tensor(11690.2617, grad_fn=<NegBackward0>) tensor(11690.2637, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11690.26171875
tensor(11690.2617, grad_fn=<NegBackward0>) tensor(11690.2617, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11690.2626953125
tensor(11690.2617, grad_fn=<NegBackward0>) tensor(11690.2627, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11690.2607421875
tensor(11690.2617, grad_fn=<NegBackward0>) tensor(11690.2607, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11690.259765625
tensor(11690.2607, grad_fn=<NegBackward0>) tensor(11690.2598, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11690.2587890625
tensor(11690.2598, grad_fn=<NegBackward0>) tensor(11690.2588, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11690.2587890625
tensor(11690.2588, grad_fn=<NegBackward0>) tensor(11690.2588, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11690.2578125
tensor(11690.2588, grad_fn=<NegBackward0>) tensor(11690.2578, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11690.255859375
tensor(11690.2578, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11690.2568359375
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2568, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11690.2744140625
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2744, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11690.255859375
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11690.2578125
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2578, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11690.259765625
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2598, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11690.255859375
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11690.255859375
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11690.255859375
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11690.255859375
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11690.2529296875
tensor(11690.2559, grad_fn=<NegBackward0>) tensor(11690.2529, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11690.25390625
tensor(11690.2529, grad_fn=<NegBackward0>) tensor(11690.2539, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11690.255859375
tensor(11690.2529, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11690.2529296875
tensor(11690.2529, grad_fn=<NegBackward0>) tensor(11690.2529, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11690.2607421875
tensor(11690.2529, grad_fn=<NegBackward0>) tensor(11690.2607, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11690.251953125
tensor(11690.2529, grad_fn=<NegBackward0>) tensor(11690.2520, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11690.2529296875
tensor(11690.2520, grad_fn=<NegBackward0>) tensor(11690.2529, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11690.2587890625
tensor(11690.2520, grad_fn=<NegBackward0>) tensor(11690.2588, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11690.25390625
tensor(11690.2520, grad_fn=<NegBackward0>) tensor(11690.2539, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11690.255859375
tensor(11690.2520, grad_fn=<NegBackward0>) tensor(11690.2559, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11690.251953125
tensor(11690.2520, grad_fn=<NegBackward0>) tensor(11690.2520, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11687.130859375
tensor(11690.2520, grad_fn=<NegBackward0>) tensor(11687.1309, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11687.1318359375
tensor(11687.1309, grad_fn=<NegBackward0>) tensor(11687.1318, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11687.130859375
tensor(11687.1309, grad_fn=<NegBackward0>) tensor(11687.1309, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11687.1328125
tensor(11687.1309, grad_fn=<NegBackward0>) tensor(11687.1328, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11687.1318359375
tensor(11687.1309, grad_fn=<NegBackward0>) tensor(11687.1318, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11687.1337890625
tensor(11687.1309, grad_fn=<NegBackward0>) tensor(11687.1338, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11687.134765625
tensor(11687.1309, grad_fn=<NegBackward0>) tensor(11687.1348, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11687.1298828125
tensor(11687.1309, grad_fn=<NegBackward0>) tensor(11687.1299, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11687.134765625
tensor(11687.1299, grad_fn=<NegBackward0>) tensor(11687.1348, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11687.1328125
tensor(11687.1299, grad_fn=<NegBackward0>) tensor(11687.1328, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11687.138671875
tensor(11687.1299, grad_fn=<NegBackward0>) tensor(11687.1387, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11687.12890625
tensor(11687.1299, grad_fn=<NegBackward0>) tensor(11687.1289, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11687.1298828125
tensor(11687.1289, grad_fn=<NegBackward0>) tensor(11687.1299, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11687.2373046875
tensor(11687.1289, grad_fn=<NegBackward0>) tensor(11687.2373, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11687.12890625
tensor(11687.1289, grad_fn=<NegBackward0>) tensor(11687.1289, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11687.134765625
tensor(11687.1289, grad_fn=<NegBackward0>) tensor(11687.1348, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11687.1279296875
tensor(11687.1289, grad_fn=<NegBackward0>) tensor(11687.1279, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11687.15234375
tensor(11687.1279, grad_fn=<NegBackward0>) tensor(11687.1523, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11687.1298828125
tensor(11687.1279, grad_fn=<NegBackward0>) tensor(11687.1299, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11687.142578125
tensor(11687.1279, grad_fn=<NegBackward0>) tensor(11687.1426, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11687.1298828125
tensor(11687.1279, grad_fn=<NegBackward0>) tensor(11687.1299, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11687.1455078125
tensor(11687.1279, grad_fn=<NegBackward0>) tensor(11687.1455, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7621, 0.2379],
        [0.2522, 0.7478]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4298, 0.5702], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2084, 0.1007],
         [0.5610, 0.3961]],

        [[0.7264, 0.0978],
         [0.5086, 0.5283]],

        [[0.5714, 0.1107],
         [0.6380, 0.6113]],

        [[0.7003, 0.0950],
         [0.6144, 0.5103]],

        [[0.5156, 0.0953],
         [0.6215, 0.6335]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[0.49460610348277506, 1.0] [0.8449287510477788, 1.0] [11863.798828125, 11687.1455078125]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11507.527157787514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22564.50390625
inf tensor(22564.5039, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12201.076171875
tensor(22564.5039, grad_fn=<NegBackward0>) tensor(12201.0762, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12076.65625
tensor(12201.0762, grad_fn=<NegBackward0>) tensor(12076.6562, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11740.2431640625
tensor(12076.6562, grad_fn=<NegBackward0>) tensor(11740.2432, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11668.7353515625
tensor(11740.2432, grad_fn=<NegBackward0>) tensor(11668.7354, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11611.9716796875
tensor(11668.7354, grad_fn=<NegBackward0>) tensor(11611.9717, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11600.759765625
tensor(11611.9717, grad_fn=<NegBackward0>) tensor(11600.7598, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11578.275390625
tensor(11600.7598, grad_fn=<NegBackward0>) tensor(11578.2754, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11572.7548828125
tensor(11578.2754, grad_fn=<NegBackward0>) tensor(11572.7549, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11566.818359375
tensor(11572.7549, grad_fn=<NegBackward0>) tensor(11566.8184, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11561.1875
tensor(11566.8184, grad_fn=<NegBackward0>) tensor(11561.1875, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11546.5244140625
tensor(11561.1875, grad_fn=<NegBackward0>) tensor(11546.5244, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11546.4599609375
tensor(11546.5244, grad_fn=<NegBackward0>) tensor(11546.4600, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11546.29296875
tensor(11546.4600, grad_fn=<NegBackward0>) tensor(11546.2930, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11537.1337890625
tensor(11546.2930, grad_fn=<NegBackward0>) tensor(11537.1338, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11537.1083984375
tensor(11537.1338, grad_fn=<NegBackward0>) tensor(11537.1084, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11528.2421875
tensor(11537.1084, grad_fn=<NegBackward0>) tensor(11528.2422, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11516.84765625
tensor(11528.2422, grad_fn=<NegBackward0>) tensor(11516.8477, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11516.828125
tensor(11516.8477, grad_fn=<NegBackward0>) tensor(11516.8281, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11516.8115234375
tensor(11516.8281, grad_fn=<NegBackward0>) tensor(11516.8115, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11509.4638671875
tensor(11516.8115, grad_fn=<NegBackward0>) tensor(11509.4639, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11509.4501953125
tensor(11509.4639, grad_fn=<NegBackward0>) tensor(11509.4502, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11509.4375
tensor(11509.4502, grad_fn=<NegBackward0>) tensor(11509.4375, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11507.96484375
tensor(11509.4375, grad_fn=<NegBackward0>) tensor(11507.9648, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11505.0537109375
tensor(11507.9648, grad_fn=<NegBackward0>) tensor(11505.0537, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11505.048828125
tensor(11505.0537, grad_fn=<NegBackward0>) tensor(11505.0488, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11505.0419921875
tensor(11505.0488, grad_fn=<NegBackward0>) tensor(11505.0420, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11505.0283203125
tensor(11505.0420, grad_fn=<NegBackward0>) tensor(11505.0283, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11505.0185546875
tensor(11505.0283, grad_fn=<NegBackward0>) tensor(11505.0186, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11505.0146484375
tensor(11505.0186, grad_fn=<NegBackward0>) tensor(11505.0146, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11505.0107421875
tensor(11505.0146, grad_fn=<NegBackward0>) tensor(11505.0107, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11505.0087890625
tensor(11505.0107, grad_fn=<NegBackward0>) tensor(11505.0088, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11505.005859375
tensor(11505.0088, grad_fn=<NegBackward0>) tensor(11505.0059, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11505.00390625
tensor(11505.0059, grad_fn=<NegBackward0>) tensor(11505.0039, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11505.0029296875
tensor(11505.0039, grad_fn=<NegBackward0>) tensor(11505.0029, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11505.001953125
tensor(11505.0029, grad_fn=<NegBackward0>) tensor(11505.0020, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11504.998046875
tensor(11505.0020, grad_fn=<NegBackward0>) tensor(11504.9980, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11504.99609375
tensor(11504.9980, grad_fn=<NegBackward0>) tensor(11504.9961, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11504.9912109375
tensor(11504.9961, grad_fn=<NegBackward0>) tensor(11504.9912, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11504.9853515625
tensor(11504.9912, grad_fn=<NegBackward0>) tensor(11504.9854, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11504.9892578125
tensor(11504.9854, grad_fn=<NegBackward0>) tensor(11504.9893, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11504.982421875
tensor(11504.9854, grad_fn=<NegBackward0>) tensor(11504.9824, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11504.9794921875
tensor(11504.9824, grad_fn=<NegBackward0>) tensor(11504.9795, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11504.978515625
tensor(11504.9795, grad_fn=<NegBackward0>) tensor(11504.9785, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11504.9794921875
tensor(11504.9785, grad_fn=<NegBackward0>) tensor(11504.9795, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11504.9775390625
tensor(11504.9785, grad_fn=<NegBackward0>) tensor(11504.9775, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11504.9755859375
tensor(11504.9775, grad_fn=<NegBackward0>) tensor(11504.9756, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11504.9755859375
tensor(11504.9756, grad_fn=<NegBackward0>) tensor(11504.9756, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11504.9755859375
tensor(11504.9756, grad_fn=<NegBackward0>) tensor(11504.9756, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11504.9765625
tensor(11504.9756, grad_fn=<NegBackward0>) tensor(11504.9766, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11504.9736328125
tensor(11504.9756, grad_fn=<NegBackward0>) tensor(11504.9736, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11504.974609375
tensor(11504.9736, grad_fn=<NegBackward0>) tensor(11504.9746, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11504.9736328125
tensor(11504.9736, grad_fn=<NegBackward0>) tensor(11504.9736, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11504.9716796875
tensor(11504.9736, grad_fn=<NegBackward0>) tensor(11504.9717, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11504.9736328125
tensor(11504.9717, grad_fn=<NegBackward0>) tensor(11504.9736, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11504.974609375
tensor(11504.9717, grad_fn=<NegBackward0>) tensor(11504.9746, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11504.970703125
tensor(11504.9717, grad_fn=<NegBackward0>) tensor(11504.9707, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11504.970703125
tensor(11504.9707, grad_fn=<NegBackward0>) tensor(11504.9707, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11504.9697265625
tensor(11504.9707, grad_fn=<NegBackward0>) tensor(11504.9697, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11504.9765625
tensor(11504.9697, grad_fn=<NegBackward0>) tensor(11504.9766, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11504.9716796875
tensor(11504.9697, grad_fn=<NegBackward0>) tensor(11504.9717, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11504.970703125
tensor(11504.9697, grad_fn=<NegBackward0>) tensor(11504.9707, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11504.9697265625
tensor(11504.9697, grad_fn=<NegBackward0>) tensor(11504.9697, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11504.96875
tensor(11504.9697, grad_fn=<NegBackward0>) tensor(11504.9688, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11504.96875
tensor(11504.9688, grad_fn=<NegBackward0>) tensor(11504.9688, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11504.96875
tensor(11504.9688, grad_fn=<NegBackward0>) tensor(11504.9688, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11504.9697265625
tensor(11504.9688, grad_fn=<NegBackward0>) tensor(11504.9697, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11504.9677734375
tensor(11504.9688, grad_fn=<NegBackward0>) tensor(11504.9678, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11504.9677734375
tensor(11504.9678, grad_fn=<NegBackward0>) tensor(11504.9678, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11504.9677734375
tensor(11504.9678, grad_fn=<NegBackward0>) tensor(11504.9678, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11504.970703125
tensor(11504.9678, grad_fn=<NegBackward0>) tensor(11504.9707, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11504.96875
tensor(11504.9678, grad_fn=<NegBackward0>) tensor(11504.9688, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11504.96875
tensor(11504.9678, grad_fn=<NegBackward0>) tensor(11504.9688, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11504.9677734375
tensor(11504.9678, grad_fn=<NegBackward0>) tensor(11504.9678, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11504.966796875
tensor(11504.9678, grad_fn=<NegBackward0>) tensor(11504.9668, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11504.966796875
tensor(11504.9668, grad_fn=<NegBackward0>) tensor(11504.9668, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11504.9658203125
tensor(11504.9668, grad_fn=<NegBackward0>) tensor(11504.9658, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11504.9697265625
tensor(11504.9658, grad_fn=<NegBackward0>) tensor(11504.9697, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11504.9736328125
tensor(11504.9658, grad_fn=<NegBackward0>) tensor(11504.9736, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11504.966796875
tensor(11504.9658, grad_fn=<NegBackward0>) tensor(11504.9668, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11504.97265625
tensor(11504.9658, grad_fn=<NegBackward0>) tensor(11504.9727, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11504.9658203125
tensor(11504.9658, grad_fn=<NegBackward0>) tensor(11504.9658, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11504.96484375
tensor(11504.9658, grad_fn=<NegBackward0>) tensor(11504.9648, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11504.9599609375
tensor(11504.9648, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11504.9599609375
tensor(11504.9600, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11504.9599609375
tensor(11504.9600, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11504.9599609375
tensor(11504.9600, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11504.9599609375
tensor(11504.9600, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11504.970703125
tensor(11504.9600, grad_fn=<NegBackward0>) tensor(11504.9707, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11504.96875
tensor(11504.9600, grad_fn=<NegBackward0>) tensor(11504.9688, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11504.958984375
tensor(11504.9600, grad_fn=<NegBackward0>) tensor(11504.9590, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11504.9599609375
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11504.9638671875
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9639, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11504.970703125
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9707, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11504.9599609375
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11504.958984375
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9590, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11504.9599609375
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9600, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11504.9765625
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9766, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11504.9638671875
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9639, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11504.97265625
tensor(11504.9590, grad_fn=<NegBackward0>) tensor(11504.9727, grad_fn=<NegBackward0>)
4
pi: tensor([[0.7694, 0.2306],
        [0.2559, 0.7441]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4994, 0.5006], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1966, 0.0994],
         [0.7158, 0.3920]],

        [[0.7222, 0.1017],
         [0.6293, 0.5622]],

        [[0.7078, 0.1012],
         [0.5190, 0.6084]],

        [[0.6050, 0.1066],
         [0.6389, 0.5696]],

        [[0.6709, 0.1037],
         [0.6654, 0.5093]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22288.26953125
inf tensor(22288.2695, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12202.1015625
tensor(22288.2695, grad_fn=<NegBackward0>) tensor(12202.1016, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12126.8642578125
tensor(12202.1016, grad_fn=<NegBackward0>) tensor(12126.8643, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11932.9326171875
tensor(12126.8643, grad_fn=<NegBackward0>) tensor(11932.9326, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11761.8486328125
tensor(11932.9326, grad_fn=<NegBackward0>) tensor(11761.8486, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11759.2509765625
tensor(11761.8486, grad_fn=<NegBackward0>) tensor(11759.2510, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11758.6484375
tensor(11759.2510, grad_fn=<NegBackward0>) tensor(11758.6484, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11758.3935546875
tensor(11758.6484, grad_fn=<NegBackward0>) tensor(11758.3936, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11758.2158203125
tensor(11758.3936, grad_fn=<NegBackward0>) tensor(11758.2158, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11752.220703125
tensor(11758.2158, grad_fn=<NegBackward0>) tensor(11752.2207, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11752.166015625
tensor(11752.2207, grad_fn=<NegBackward0>) tensor(11752.1660, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11751.9658203125
tensor(11752.1660, grad_fn=<NegBackward0>) tensor(11751.9658, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11750.0029296875
tensor(11751.9658, grad_fn=<NegBackward0>) tensor(11750.0029, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11748.1171875
tensor(11750.0029, grad_fn=<NegBackward0>) tensor(11748.1172, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11747.9951171875
tensor(11748.1172, grad_fn=<NegBackward0>) tensor(11747.9951, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11747.9345703125
tensor(11747.9951, grad_fn=<NegBackward0>) tensor(11747.9346, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11747.931640625
tensor(11747.9346, grad_fn=<NegBackward0>) tensor(11747.9316, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11747.828125
tensor(11747.9316, grad_fn=<NegBackward0>) tensor(11747.8281, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11747.8212890625
tensor(11747.8281, grad_fn=<NegBackward0>) tensor(11747.8213, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11747.818359375
tensor(11747.8213, grad_fn=<NegBackward0>) tensor(11747.8184, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11747.8125
tensor(11747.8184, grad_fn=<NegBackward0>) tensor(11747.8125, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11747.787109375
tensor(11747.8125, grad_fn=<NegBackward0>) tensor(11747.7871, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11747.78515625
tensor(11747.7871, grad_fn=<NegBackward0>) tensor(11747.7852, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11747.78125
tensor(11747.7852, grad_fn=<NegBackward0>) tensor(11747.7812, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11747.783203125
tensor(11747.7812, grad_fn=<NegBackward0>) tensor(11747.7832, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11747.77734375
tensor(11747.7812, grad_fn=<NegBackward0>) tensor(11747.7773, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11747.7734375
tensor(11747.7773, grad_fn=<NegBackward0>) tensor(11747.7734, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11747.7578125
tensor(11747.7734, grad_fn=<NegBackward0>) tensor(11747.7578, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11747.7265625
tensor(11747.7578, grad_fn=<NegBackward0>) tensor(11747.7266, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11747.7041015625
tensor(11747.7266, grad_fn=<NegBackward0>) tensor(11747.7041, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11747.595703125
tensor(11747.7041, grad_fn=<NegBackward0>) tensor(11747.5957, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11747.556640625
tensor(11747.5957, grad_fn=<NegBackward0>) tensor(11747.5566, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11747.546875
tensor(11747.5566, grad_fn=<NegBackward0>) tensor(11747.5469, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11747.544921875
tensor(11747.5469, grad_fn=<NegBackward0>) tensor(11747.5449, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11747.517578125
tensor(11747.5449, grad_fn=<NegBackward0>) tensor(11747.5176, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11747.5068359375
tensor(11747.5176, grad_fn=<NegBackward0>) tensor(11747.5068, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11747.4033203125
tensor(11747.5068, grad_fn=<NegBackward0>) tensor(11747.4033, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11747.099609375
tensor(11747.4033, grad_fn=<NegBackward0>) tensor(11747.0996, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11737.275390625
tensor(11747.0996, grad_fn=<NegBackward0>) tensor(11737.2754, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11737.228515625
tensor(11737.2754, grad_fn=<NegBackward0>) tensor(11737.2285, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11737.1689453125
tensor(11737.2285, grad_fn=<NegBackward0>) tensor(11737.1689, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11737.1572265625
tensor(11737.1689, grad_fn=<NegBackward0>) tensor(11737.1572, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11737.146484375
tensor(11737.1572, grad_fn=<NegBackward0>) tensor(11737.1465, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11737.142578125
tensor(11737.1465, grad_fn=<NegBackward0>) tensor(11737.1426, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11737.1171875
tensor(11737.1426, grad_fn=<NegBackward0>) tensor(11737.1172, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11737.1162109375
tensor(11737.1172, grad_fn=<NegBackward0>) tensor(11737.1162, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11737.119140625
tensor(11737.1162, grad_fn=<NegBackward0>) tensor(11737.1191, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11737.1142578125
tensor(11737.1162, grad_fn=<NegBackward0>) tensor(11737.1143, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11737.1142578125
tensor(11737.1143, grad_fn=<NegBackward0>) tensor(11737.1143, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11737.111328125
tensor(11737.1143, grad_fn=<NegBackward0>) tensor(11737.1113, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11737.1064453125
tensor(11737.1113, grad_fn=<NegBackward0>) tensor(11737.1064, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11732.04296875
tensor(11737.1064, grad_fn=<NegBackward0>) tensor(11732.0430, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11732.041015625
tensor(11732.0430, grad_fn=<NegBackward0>) tensor(11732.0410, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11732.0419921875
tensor(11732.0410, grad_fn=<NegBackward0>) tensor(11732.0420, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11732.037109375
tensor(11732.0410, grad_fn=<NegBackward0>) tensor(11732.0371, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11732.046875
tensor(11732.0371, grad_fn=<NegBackward0>) tensor(11732.0469, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11732.021484375
tensor(11732.0371, grad_fn=<NegBackward0>) tensor(11732.0215, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11731.3291015625
tensor(11732.0215, grad_fn=<NegBackward0>) tensor(11731.3291, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11731.3291015625
tensor(11731.3291, grad_fn=<NegBackward0>) tensor(11731.3291, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11731.3291015625
tensor(11731.3291, grad_fn=<NegBackward0>) tensor(11731.3291, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11731.3349609375
tensor(11731.3291, grad_fn=<NegBackward0>) tensor(11731.3350, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11731.3291015625
tensor(11731.3291, grad_fn=<NegBackward0>) tensor(11731.3291, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11731.3291015625
tensor(11731.3291, grad_fn=<NegBackward0>) tensor(11731.3291, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11731.328125
tensor(11731.3291, grad_fn=<NegBackward0>) tensor(11731.3281, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11731.3310546875
tensor(11731.3281, grad_fn=<NegBackward0>) tensor(11731.3311, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11731.3271484375
tensor(11731.3281, grad_fn=<NegBackward0>) tensor(11731.3271, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11731.328125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3281, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11731.3330078125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3330, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11731.328125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3281, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11731.3271484375
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3271, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11731.3310546875
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3311, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11731.3291015625
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3291, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11731.328125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3281, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11731.3271484375
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3271, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11731.328125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3281, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11731.3798828125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3799, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11731.328125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3281, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11731.328125
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3281, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11731.3271484375
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3271, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11731.3271484375
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3271, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11731.326171875
tensor(11731.3271, grad_fn=<NegBackward0>) tensor(11731.3262, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11731.326171875
tensor(11731.3262, grad_fn=<NegBackward0>) tensor(11731.3262, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11731.3251953125
tensor(11731.3262, grad_fn=<NegBackward0>) tensor(11731.3252, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11731.3251953125
tensor(11731.3252, grad_fn=<NegBackward0>) tensor(11731.3252, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11731.3251953125
tensor(11731.3252, grad_fn=<NegBackward0>) tensor(11731.3252, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11730.650390625
tensor(11731.3252, grad_fn=<NegBackward0>) tensor(11730.6504, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11730.6171875
tensor(11730.6504, grad_fn=<NegBackward0>) tensor(11730.6172, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11730.6240234375
tensor(11730.6172, grad_fn=<NegBackward0>) tensor(11730.6240, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11730.6181640625
tensor(11730.6172, grad_fn=<NegBackward0>) tensor(11730.6182, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11730.6044921875
tensor(11730.6172, grad_fn=<NegBackward0>) tensor(11730.6045, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11730.60546875
tensor(11730.6045, grad_fn=<NegBackward0>) tensor(11730.6055, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11730.6083984375
tensor(11730.6045, grad_fn=<NegBackward0>) tensor(11730.6084, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11730.6064453125
tensor(11730.6045, grad_fn=<NegBackward0>) tensor(11730.6064, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11730.6064453125
tensor(11730.6045, grad_fn=<NegBackward0>) tensor(11730.6064, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11730.6240234375
tensor(11730.6045, grad_fn=<NegBackward0>) tensor(11730.6240, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.3793, 0.6207],
        [0.8293, 0.1707]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4960, 0.5040], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2432, 0.0991],
         [0.5449, 0.3462]],

        [[0.6655, 0.1010],
         [0.7249, 0.6813]],

        [[0.6935, 0.1010],
         [0.5996, 0.5912]],

        [[0.6008, 0.1006],
         [0.6303, 0.6899]],

        [[0.5226, 0.1020],
         [0.7302, 0.7236]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 17
Adjusted Rand Index: 0.4301535108653031
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 19
Adjusted Rand Index: 0.37855677651733366
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.10845235860664246
Average Adjusted Rand Index: 0.7379027461092208
[1.0, 0.10845235860664246] [1.0, 0.7379027461092208] [11504.9599609375, 11730.6240234375]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11597.024351243004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21097.169921875
inf tensor(21097.1699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12367.0595703125
tensor(21097.1699, grad_fn=<NegBackward0>) tensor(12367.0596, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12335.224609375
tensor(12367.0596, grad_fn=<NegBackward0>) tensor(12335.2246, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11987.8671875
tensor(12335.2246, grad_fn=<NegBackward0>) tensor(11987.8672, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11851.7412109375
tensor(11987.8672, grad_fn=<NegBackward0>) tensor(11851.7412, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11836.783203125
tensor(11851.7412, grad_fn=<NegBackward0>) tensor(11836.7832, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11810.615234375
tensor(11836.7832, grad_fn=<NegBackward0>) tensor(11810.6152, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11810.255859375
tensor(11810.6152, grad_fn=<NegBackward0>) tensor(11810.2559, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11794.2431640625
tensor(11810.2559, grad_fn=<NegBackward0>) tensor(11794.2432, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11783.642578125
tensor(11794.2432, grad_fn=<NegBackward0>) tensor(11783.6426, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11783.1298828125
tensor(11783.6426, grad_fn=<NegBackward0>) tensor(11783.1299, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11782.3681640625
tensor(11783.1299, grad_fn=<NegBackward0>) tensor(11782.3682, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11782.3017578125
tensor(11782.3682, grad_fn=<NegBackward0>) tensor(11782.3018, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11776.345703125
tensor(11782.3018, grad_fn=<NegBackward0>) tensor(11776.3457, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11774.865234375
tensor(11776.3457, grad_fn=<NegBackward0>) tensor(11774.8652, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11761.1962890625
tensor(11774.8652, grad_fn=<NegBackward0>) tensor(11761.1963, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11760.84765625
tensor(11761.1963, grad_fn=<NegBackward0>) tensor(11760.8477, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11760.8251953125
tensor(11760.8477, grad_fn=<NegBackward0>) tensor(11760.8252, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11760.810546875
tensor(11760.8252, grad_fn=<NegBackward0>) tensor(11760.8105, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11760.7978515625
tensor(11760.8105, grad_fn=<NegBackward0>) tensor(11760.7979, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11760.787109375
tensor(11760.7979, grad_fn=<NegBackward0>) tensor(11760.7871, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11760.77734375
tensor(11760.7871, grad_fn=<NegBackward0>) tensor(11760.7773, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11760.76953125
tensor(11760.7773, grad_fn=<NegBackward0>) tensor(11760.7695, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11760.763671875
tensor(11760.7695, grad_fn=<NegBackward0>) tensor(11760.7637, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11760.7587890625
tensor(11760.7637, grad_fn=<NegBackward0>) tensor(11760.7588, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11760.7529296875
tensor(11760.7588, grad_fn=<NegBackward0>) tensor(11760.7529, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11760.748046875
tensor(11760.7529, grad_fn=<NegBackward0>) tensor(11760.7480, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11760.744140625
tensor(11760.7480, grad_fn=<NegBackward0>) tensor(11760.7441, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11760.7373046875
tensor(11760.7441, grad_fn=<NegBackward0>) tensor(11760.7373, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11760.728515625
tensor(11760.7373, grad_fn=<NegBackward0>) tensor(11760.7285, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11760.7216796875
tensor(11760.7285, grad_fn=<NegBackward0>) tensor(11760.7217, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11760.71875
tensor(11760.7217, grad_fn=<NegBackward0>) tensor(11760.7188, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11760.7158203125
tensor(11760.7188, grad_fn=<NegBackward0>) tensor(11760.7158, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11760.712890625
tensor(11760.7158, grad_fn=<NegBackward0>) tensor(11760.7129, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11760.7109375
tensor(11760.7129, grad_fn=<NegBackward0>) tensor(11760.7109, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11760.708984375
tensor(11760.7109, grad_fn=<NegBackward0>) tensor(11760.7090, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11760.7001953125
tensor(11760.7090, grad_fn=<NegBackward0>) tensor(11760.7002, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11759.599609375
tensor(11760.7002, grad_fn=<NegBackward0>) tensor(11759.5996, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11759.5966796875
tensor(11759.5996, grad_fn=<NegBackward0>) tensor(11759.5967, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11759.5966796875
tensor(11759.5967, grad_fn=<NegBackward0>) tensor(11759.5967, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11759.5947265625
tensor(11759.5967, grad_fn=<NegBackward0>) tensor(11759.5947, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11759.5947265625
tensor(11759.5947, grad_fn=<NegBackward0>) tensor(11759.5947, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11759.59765625
tensor(11759.5947, grad_fn=<NegBackward0>) tensor(11759.5977, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11759.595703125
tensor(11759.5947, grad_fn=<NegBackward0>) tensor(11759.5957, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11759.591796875
tensor(11759.5947, grad_fn=<NegBackward0>) tensor(11759.5918, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11759.5908203125
tensor(11759.5918, grad_fn=<NegBackward0>) tensor(11759.5908, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11759.58984375
tensor(11759.5908, grad_fn=<NegBackward0>) tensor(11759.5898, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11759.587890625
tensor(11759.5898, grad_fn=<NegBackward0>) tensor(11759.5879, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11759.587890625
tensor(11759.5879, grad_fn=<NegBackward0>) tensor(11759.5879, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11759.587890625
tensor(11759.5879, grad_fn=<NegBackward0>) tensor(11759.5879, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11759.5869140625
tensor(11759.5879, grad_fn=<NegBackward0>) tensor(11759.5869, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11759.587890625
tensor(11759.5869, grad_fn=<NegBackward0>) tensor(11759.5879, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11759.5859375
tensor(11759.5869, grad_fn=<NegBackward0>) tensor(11759.5859, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11758.9814453125
tensor(11759.5859, grad_fn=<NegBackward0>) tensor(11758.9814, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11758.94921875
tensor(11758.9814, grad_fn=<NegBackward0>) tensor(11758.9492, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11758.9541015625
tensor(11758.9492, grad_fn=<NegBackward0>) tensor(11758.9541, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11758.9580078125
tensor(11758.9492, grad_fn=<NegBackward0>) tensor(11758.9580, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11758.896484375
tensor(11758.9492, grad_fn=<NegBackward0>) tensor(11758.8965, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11758.8955078125
tensor(11758.8965, grad_fn=<NegBackward0>) tensor(11758.8955, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11758.927734375
tensor(11758.8955, grad_fn=<NegBackward0>) tensor(11758.9277, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11758.89453125
tensor(11758.8955, grad_fn=<NegBackward0>) tensor(11758.8945, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11758.89453125
tensor(11758.8945, grad_fn=<NegBackward0>) tensor(11758.8945, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11758.8935546875
tensor(11758.8945, grad_fn=<NegBackward0>) tensor(11758.8936, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11758.89453125
tensor(11758.8936, grad_fn=<NegBackward0>) tensor(11758.8945, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11758.896484375
tensor(11758.8936, grad_fn=<NegBackward0>) tensor(11758.8965, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11758.89453125
tensor(11758.8936, grad_fn=<NegBackward0>) tensor(11758.8945, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11758.8916015625
tensor(11758.8936, grad_fn=<NegBackward0>) tensor(11758.8916, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11758.892578125
tensor(11758.8916, grad_fn=<NegBackward0>) tensor(11758.8926, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11758.892578125
tensor(11758.8916, grad_fn=<NegBackward0>) tensor(11758.8926, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11758.892578125
tensor(11758.8916, grad_fn=<NegBackward0>) tensor(11758.8926, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11758.892578125
tensor(11758.8916, grad_fn=<NegBackward0>) tensor(11758.8926, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11758.892578125
tensor(11758.8916, grad_fn=<NegBackward0>) tensor(11758.8926, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6025, 0.3975],
        [0.4806, 0.5194]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5099, 0.4901], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2168, 0.1048],
         [0.6862, 0.3920]],

        [[0.5281, 0.1013],
         [0.5989, 0.5074]],

        [[0.6268, 0.1041],
         [0.6673, 0.6489]],

        [[0.5751, 0.0923],
         [0.5689, 0.5034]],

        [[0.5324, 0.1080],
         [0.5461, 0.6933]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 23
Adjusted Rand Index: 0.285728951718049
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.47782347187950236
Average Adjusted Rand Index: 0.8571457903436098
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24137.98828125
inf tensor(24137.9883, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12204.6015625
tensor(24137.9883, grad_fn=<NegBackward0>) tensor(12204.6016, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11724.8779296875
tensor(12204.6016, grad_fn=<NegBackward0>) tensor(11724.8779, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11651.583984375
tensor(11724.8779, grad_fn=<NegBackward0>) tensor(11651.5840, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11645.998046875
tensor(11651.5840, grad_fn=<NegBackward0>) tensor(11645.9980, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11613.7451171875
tensor(11645.9980, grad_fn=<NegBackward0>) tensor(11613.7451, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11612.041015625
tensor(11613.7451, grad_fn=<NegBackward0>) tensor(11612.0410, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11611.970703125
tensor(11612.0410, grad_fn=<NegBackward0>) tensor(11611.9707, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11611.9267578125
tensor(11611.9707, grad_fn=<NegBackward0>) tensor(11611.9268, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11611.89453125
tensor(11611.9268, grad_fn=<NegBackward0>) tensor(11611.8945, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11611.87109375
tensor(11611.8945, grad_fn=<NegBackward0>) tensor(11611.8711, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11611.85546875
tensor(11611.8711, grad_fn=<NegBackward0>) tensor(11611.8555, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11611.8408203125
tensor(11611.8555, grad_fn=<NegBackward0>) tensor(11611.8408, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11611.8291015625
tensor(11611.8408, grad_fn=<NegBackward0>) tensor(11611.8291, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11611.8193359375
tensor(11611.8291, grad_fn=<NegBackward0>) tensor(11611.8193, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11611.810546875
tensor(11611.8193, grad_fn=<NegBackward0>) tensor(11611.8105, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11611.802734375
tensor(11611.8105, grad_fn=<NegBackward0>) tensor(11611.8027, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11611.7978515625
tensor(11611.8027, grad_fn=<NegBackward0>) tensor(11611.7979, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11611.7939453125
tensor(11611.7979, grad_fn=<NegBackward0>) tensor(11611.7939, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11611.7900390625
tensor(11611.7939, grad_fn=<NegBackward0>) tensor(11611.7900, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11611.7861328125
tensor(11611.7900, grad_fn=<NegBackward0>) tensor(11611.7861, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11611.78125
tensor(11611.7861, grad_fn=<NegBackward0>) tensor(11611.7812, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11611.77734375
tensor(11611.7812, grad_fn=<NegBackward0>) tensor(11611.7773, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11611.7744140625
tensor(11611.7773, grad_fn=<NegBackward0>) tensor(11611.7744, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11611.7724609375
tensor(11611.7744, grad_fn=<NegBackward0>) tensor(11611.7725, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11611.771484375
tensor(11611.7725, grad_fn=<NegBackward0>) tensor(11611.7715, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11611.76953125
tensor(11611.7715, grad_fn=<NegBackward0>) tensor(11611.7695, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11611.767578125
tensor(11611.7695, grad_fn=<NegBackward0>) tensor(11611.7676, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11611.765625
tensor(11611.7676, grad_fn=<NegBackward0>) tensor(11611.7656, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11611.7646484375
tensor(11611.7656, grad_fn=<NegBackward0>) tensor(11611.7646, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11611.7646484375
tensor(11611.7646, grad_fn=<NegBackward0>) tensor(11611.7646, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11611.7626953125
tensor(11611.7646, grad_fn=<NegBackward0>) tensor(11611.7627, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11611.76171875
tensor(11611.7627, grad_fn=<NegBackward0>) tensor(11611.7617, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11611.7607421875
tensor(11611.7617, grad_fn=<NegBackward0>) tensor(11611.7607, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11607.166015625
tensor(11611.7607, grad_fn=<NegBackward0>) tensor(11607.1660, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11607.068359375
tensor(11607.1660, grad_fn=<NegBackward0>) tensor(11607.0684, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11607.0576171875
tensor(11607.0684, grad_fn=<NegBackward0>) tensor(11607.0576, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11607.0693359375
tensor(11607.0576, grad_fn=<NegBackward0>) tensor(11607.0693, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11607.0576171875
tensor(11607.0576, grad_fn=<NegBackward0>) tensor(11607.0576, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11607.056640625
tensor(11607.0576, grad_fn=<NegBackward0>) tensor(11607.0566, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11607.0576171875
tensor(11607.0566, grad_fn=<NegBackward0>) tensor(11607.0576, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11607.056640625
tensor(11607.0566, grad_fn=<NegBackward0>) tensor(11607.0566, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11607.0556640625
tensor(11607.0566, grad_fn=<NegBackward0>) tensor(11607.0557, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11607.0546875
tensor(11607.0557, grad_fn=<NegBackward0>) tensor(11607.0547, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11607.0537109375
tensor(11607.0547, grad_fn=<NegBackward0>) tensor(11607.0537, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11607.0537109375
tensor(11607.0537, grad_fn=<NegBackward0>) tensor(11607.0537, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11607.0537109375
tensor(11607.0537, grad_fn=<NegBackward0>) tensor(11607.0537, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11607.0537109375
tensor(11607.0537, grad_fn=<NegBackward0>) tensor(11607.0537, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11607.0556640625
tensor(11607.0537, grad_fn=<NegBackward0>) tensor(11607.0557, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11596.673828125
tensor(11607.0537, grad_fn=<NegBackward0>) tensor(11596.6738, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11596.6337890625
tensor(11596.6738, grad_fn=<NegBackward0>) tensor(11596.6338, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11596.6337890625
tensor(11596.6338, grad_fn=<NegBackward0>) tensor(11596.6338, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11596.6328125
tensor(11596.6338, grad_fn=<NegBackward0>) tensor(11596.6328, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11596.6328125
tensor(11596.6328, grad_fn=<NegBackward0>) tensor(11596.6328, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11596.634765625
tensor(11596.6328, grad_fn=<NegBackward0>) tensor(11596.6348, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11596.630859375
tensor(11596.6328, grad_fn=<NegBackward0>) tensor(11596.6309, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11596.630859375
tensor(11596.6309, grad_fn=<NegBackward0>) tensor(11596.6309, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11596.630859375
tensor(11596.6309, grad_fn=<NegBackward0>) tensor(11596.6309, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11596.6455078125
tensor(11596.6309, grad_fn=<NegBackward0>) tensor(11596.6455, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11596.638671875
tensor(11596.6309, grad_fn=<NegBackward0>) tensor(11596.6387, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11596.630859375
tensor(11596.6309, grad_fn=<NegBackward0>) tensor(11596.6309, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11596.630859375
tensor(11596.6309, grad_fn=<NegBackward0>) tensor(11596.6309, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11596.6298828125
tensor(11596.6309, grad_fn=<NegBackward0>) tensor(11596.6299, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11596.6318359375
tensor(11596.6299, grad_fn=<NegBackward0>) tensor(11596.6318, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11596.6259765625
tensor(11596.6299, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11596.6259765625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11596.6259765625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11596.6259765625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11596.6259765625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11596.6259765625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11596.6259765625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11596.626953125
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6270, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11596.6337890625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6338, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11596.6376953125
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6377, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11596.625
tensor(11596.6260, grad_fn=<NegBackward0>) tensor(11596.6250, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11596.625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6250, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11596.6259765625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11596.6337890625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6338, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11596.625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6250, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11596.62890625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6289, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11596.625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6250, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11596.6259765625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6260, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11596.626953125
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6270, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11596.625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6250, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11596.6337890625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6338, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11596.625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6250, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11596.625
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6250, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11596.6240234375
tensor(11596.6250, grad_fn=<NegBackward0>) tensor(11596.6240, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11596.623046875
tensor(11596.6240, grad_fn=<NegBackward0>) tensor(11596.6230, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11593.7138671875
tensor(11596.6230, grad_fn=<NegBackward0>) tensor(11593.7139, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11593.720703125
tensor(11593.7139, grad_fn=<NegBackward0>) tensor(11593.7207, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11593.701171875
tensor(11593.7139, grad_fn=<NegBackward0>) tensor(11593.7012, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11593.841796875
tensor(11593.7012, grad_fn=<NegBackward0>) tensor(11593.8418, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11593.6591796875
tensor(11593.7012, grad_fn=<NegBackward0>) tensor(11593.6592, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11593.6591796875
tensor(11593.6592, grad_fn=<NegBackward0>) tensor(11593.6592, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11593.662109375
tensor(11593.6592, grad_fn=<NegBackward0>) tensor(11593.6621, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11593.662109375
tensor(11593.6592, grad_fn=<NegBackward0>) tensor(11593.6621, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11593.6630859375
tensor(11593.6592, grad_fn=<NegBackward0>) tensor(11593.6631, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11593.66015625
tensor(11593.6592, grad_fn=<NegBackward0>) tensor(11593.6602, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11593.66015625
tensor(11593.6592, grad_fn=<NegBackward0>) tensor(11593.6602, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7647, 0.2353],
        [0.2195, 0.7805]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5099, 0.4901], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.1049],
         [0.5046, 0.3956]],

        [[0.6203, 0.1009],
         [0.5782, 0.6505]],

        [[0.5850, 0.1036],
         [0.6704, 0.6483]],

        [[0.5998, 0.0923],
         [0.5914, 0.5636]],

        [[0.5124, 0.1083],
         [0.6739, 0.5137]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[0.47782347187950236, 1.0] [0.8571457903436098, 1.0] [11758.892578125, 11593.66015625]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11656.672501409359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22976.259765625
inf tensor(22976.2598, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12502.1201171875
tensor(22976.2598, grad_fn=<NegBackward0>) tensor(12502.1201, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12305.69921875
tensor(12502.1201, grad_fn=<NegBackward0>) tensor(12305.6992, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11996.5888671875
tensor(12305.6992, grad_fn=<NegBackward0>) tensor(11996.5889, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11846.56640625
tensor(11996.5889, grad_fn=<NegBackward0>) tensor(11846.5664, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11771.7294921875
tensor(11846.5664, grad_fn=<NegBackward0>) tensor(11771.7295, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11747.6044921875
tensor(11771.7295, grad_fn=<NegBackward0>) tensor(11747.6045, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11737.69921875
tensor(11747.6045, grad_fn=<NegBackward0>) tensor(11737.6992, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11737.2890625
tensor(11737.6992, grad_fn=<NegBackward0>) tensor(11737.2891, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11733.4453125
tensor(11737.2891, grad_fn=<NegBackward0>) tensor(11733.4453, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11733.21875
tensor(11733.4453, grad_fn=<NegBackward0>) tensor(11733.2188, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11723.326171875
tensor(11733.2188, grad_fn=<NegBackward0>) tensor(11723.3262, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11713.7158203125
tensor(11723.3262, grad_fn=<NegBackward0>) tensor(11713.7158, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11690.2421875
tensor(11713.7158, grad_fn=<NegBackward0>) tensor(11690.2422, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11690.1640625
tensor(11690.2422, grad_fn=<NegBackward0>) tensor(11690.1641, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11678.7041015625
tensor(11690.1641, grad_fn=<NegBackward0>) tensor(11678.7041, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11678.65625
tensor(11678.7041, grad_fn=<NegBackward0>) tensor(11678.6562, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11678.623046875
tensor(11678.6562, grad_fn=<NegBackward0>) tensor(11678.6230, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11678.5927734375
tensor(11678.6230, grad_fn=<NegBackward0>) tensor(11678.5928, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11678.5703125
tensor(11678.5928, grad_fn=<NegBackward0>) tensor(11678.5703, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11678.5478515625
tensor(11678.5703, grad_fn=<NegBackward0>) tensor(11678.5479, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11668.1025390625
tensor(11678.5479, grad_fn=<NegBackward0>) tensor(11668.1025, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11668.0693359375
tensor(11668.1025, grad_fn=<NegBackward0>) tensor(11668.0693, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11668.0556640625
tensor(11668.0693, grad_fn=<NegBackward0>) tensor(11668.0557, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11668.0498046875
tensor(11668.0557, grad_fn=<NegBackward0>) tensor(11668.0498, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11668.0361328125
tensor(11668.0498, grad_fn=<NegBackward0>) tensor(11668.0361, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11668.0263671875
tensor(11668.0361, grad_fn=<NegBackward0>) tensor(11668.0264, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11667.96484375
tensor(11668.0264, grad_fn=<NegBackward0>) tensor(11667.9648, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11666.9599609375
tensor(11667.9648, grad_fn=<NegBackward0>) tensor(11666.9600, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11658.4814453125
tensor(11666.9600, grad_fn=<NegBackward0>) tensor(11658.4814, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11658.3134765625
tensor(11658.4814, grad_fn=<NegBackward0>) tensor(11658.3135, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11658.30859375
tensor(11658.3135, grad_fn=<NegBackward0>) tensor(11658.3086, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11658.3037109375
tensor(11658.3086, grad_fn=<NegBackward0>) tensor(11658.3037, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11658.30078125
tensor(11658.3037, grad_fn=<NegBackward0>) tensor(11658.3008, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11658.3076171875
tensor(11658.3008, grad_fn=<NegBackward0>) tensor(11658.3076, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11658.2919921875
tensor(11658.3008, grad_fn=<NegBackward0>) tensor(11658.2920, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11658.2939453125
tensor(11658.2920, grad_fn=<NegBackward0>) tensor(11658.2939, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11658.287109375
tensor(11658.2920, grad_fn=<NegBackward0>) tensor(11658.2871, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11658.287109375
tensor(11658.2871, grad_fn=<NegBackward0>) tensor(11658.2871, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11658.283203125
tensor(11658.2871, grad_fn=<NegBackward0>) tensor(11658.2832, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11658.28125
tensor(11658.2832, grad_fn=<NegBackward0>) tensor(11658.2812, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11658.2939453125
tensor(11658.2812, grad_fn=<NegBackward0>) tensor(11658.2939, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11658.2783203125
tensor(11658.2812, grad_fn=<NegBackward0>) tensor(11658.2783, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11658.275390625
tensor(11658.2783, grad_fn=<NegBackward0>) tensor(11658.2754, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11658.275390625
tensor(11658.2754, grad_fn=<NegBackward0>) tensor(11658.2754, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11658.2734375
tensor(11658.2754, grad_fn=<NegBackward0>) tensor(11658.2734, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11658.2724609375
tensor(11658.2734, grad_fn=<NegBackward0>) tensor(11658.2725, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11658.2705078125
tensor(11658.2725, grad_fn=<NegBackward0>) tensor(11658.2705, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11658.28125
tensor(11658.2705, grad_fn=<NegBackward0>) tensor(11658.2812, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11658.2685546875
tensor(11658.2705, grad_fn=<NegBackward0>) tensor(11658.2686, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11658.26953125
tensor(11658.2686, grad_fn=<NegBackward0>) tensor(11658.2695, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11658.2666015625
tensor(11658.2686, grad_fn=<NegBackward0>) tensor(11658.2666, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11658.267578125
tensor(11658.2666, grad_fn=<NegBackward0>) tensor(11658.2676, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11658.2646484375
tensor(11658.2666, grad_fn=<NegBackward0>) tensor(11658.2646, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11658.265625
tensor(11658.2646, grad_fn=<NegBackward0>) tensor(11658.2656, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11658.263671875
tensor(11658.2646, grad_fn=<NegBackward0>) tensor(11658.2637, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11658.2626953125
tensor(11658.2637, grad_fn=<NegBackward0>) tensor(11658.2627, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11658.26171875
tensor(11658.2627, grad_fn=<NegBackward0>) tensor(11658.2617, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11658.2763671875
tensor(11658.2617, grad_fn=<NegBackward0>) tensor(11658.2764, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11658.2607421875
tensor(11658.2617, grad_fn=<NegBackward0>) tensor(11658.2607, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11658.259765625
tensor(11658.2607, grad_fn=<NegBackward0>) tensor(11658.2598, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11658.259765625
tensor(11658.2598, grad_fn=<NegBackward0>) tensor(11658.2598, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11658.259765625
tensor(11658.2598, grad_fn=<NegBackward0>) tensor(11658.2598, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11658.2587890625
tensor(11658.2598, grad_fn=<NegBackward0>) tensor(11658.2588, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11658.2607421875
tensor(11658.2588, grad_fn=<NegBackward0>) tensor(11658.2607, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11658.2578125
tensor(11658.2588, grad_fn=<NegBackward0>) tensor(11658.2578, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11658.2568359375
tensor(11658.2578, grad_fn=<NegBackward0>) tensor(11658.2568, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11658.255859375
tensor(11658.2568, grad_fn=<NegBackward0>) tensor(11658.2559, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11658.2568359375
tensor(11658.2559, grad_fn=<NegBackward0>) tensor(11658.2568, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11658.25390625
tensor(11658.2559, grad_fn=<NegBackward0>) tensor(11658.2539, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11658.251953125
tensor(11658.2539, grad_fn=<NegBackward0>) tensor(11658.2520, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11658.2529296875
tensor(11658.2520, grad_fn=<NegBackward0>) tensor(11658.2529, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11657.578125
tensor(11658.2520, grad_fn=<NegBackward0>) tensor(11657.5781, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11650.033203125
tensor(11657.5781, grad_fn=<NegBackward0>) tensor(11650.0332, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11650.0263671875
tensor(11650.0332, grad_fn=<NegBackward0>) tensor(11650.0264, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11650.0263671875
tensor(11650.0264, grad_fn=<NegBackward0>) tensor(11650.0264, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11650.0302734375
tensor(11650.0264, grad_fn=<NegBackward0>) tensor(11650.0303, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11650.0244140625
tensor(11650.0264, grad_fn=<NegBackward0>) tensor(11650.0244, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11650.0244140625
tensor(11650.0244, grad_fn=<NegBackward0>) tensor(11650.0244, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11650.076171875
tensor(11650.0244, grad_fn=<NegBackward0>) tensor(11650.0762, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11650.0234375
tensor(11650.0244, grad_fn=<NegBackward0>) tensor(11650.0234, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11650.0283203125
tensor(11650.0234, grad_fn=<NegBackward0>) tensor(11650.0283, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11650.0263671875
tensor(11650.0234, grad_fn=<NegBackward0>) tensor(11650.0264, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11650.025390625
tensor(11650.0234, grad_fn=<NegBackward0>) tensor(11650.0254, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11650.03125
tensor(11650.0234, grad_fn=<NegBackward0>) tensor(11650.0312, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11650.0322265625
tensor(11650.0234, grad_fn=<NegBackward0>) tensor(11650.0322, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7443, 0.2557],
        [0.2734, 0.7266]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5497, 0.4503], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3942, 0.1062],
         [0.6288, 0.2033]],

        [[0.6714, 0.0887],
         [0.5715, 0.6342]],

        [[0.5545, 0.1032],
         [0.6315, 0.5335]],

        [[0.5184, 0.0968],
         [0.5511, 0.6664]],

        [[0.6336, 0.0941],
         [0.6240, 0.7243]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21616.029296875
inf tensor(21616.0293, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12488.4287109375
tensor(21616.0293, grad_fn=<NegBackward0>) tensor(12488.4287, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12206.8798828125
tensor(12488.4287, grad_fn=<NegBackward0>) tensor(12206.8799, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11931.017578125
tensor(12206.8799, grad_fn=<NegBackward0>) tensor(11931.0176, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11785.5556640625
tensor(11931.0176, grad_fn=<NegBackward0>) tensor(11785.5557, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11773.65234375
tensor(11785.5557, grad_fn=<NegBackward0>) tensor(11773.6523, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11763.951171875
tensor(11773.6523, grad_fn=<NegBackward0>) tensor(11763.9512, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11753.0908203125
tensor(11763.9512, grad_fn=<NegBackward0>) tensor(11753.0908, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11723.8955078125
tensor(11753.0908, grad_fn=<NegBackward0>) tensor(11723.8955, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11716.873046875
tensor(11723.8955, grad_fn=<NegBackward0>) tensor(11716.8730, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11709.6689453125
tensor(11716.8730, grad_fn=<NegBackward0>) tensor(11709.6689, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11706.15234375
tensor(11709.6689, grad_fn=<NegBackward0>) tensor(11706.1523, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11698.65234375
tensor(11706.1523, grad_fn=<NegBackward0>) tensor(11698.6523, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11698.595703125
tensor(11698.6523, grad_fn=<NegBackward0>) tensor(11698.5957, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11698.513671875
tensor(11698.5957, grad_fn=<NegBackward0>) tensor(11698.5137, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11691.8369140625
tensor(11698.5137, grad_fn=<NegBackward0>) tensor(11691.8369, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11675.9150390625
tensor(11691.8369, grad_fn=<NegBackward0>) tensor(11675.9150, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11675.8837890625
tensor(11675.9150, grad_fn=<NegBackward0>) tensor(11675.8838, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11675.8623046875
tensor(11675.8838, grad_fn=<NegBackward0>) tensor(11675.8623, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11675.8271484375
tensor(11675.8623, grad_fn=<NegBackward0>) tensor(11675.8271, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11659.6015625
tensor(11675.8271, grad_fn=<NegBackward0>) tensor(11659.6016, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11659.587890625
tensor(11659.6016, grad_fn=<NegBackward0>) tensor(11659.5879, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11659.578125
tensor(11659.5879, grad_fn=<NegBackward0>) tensor(11659.5781, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11659.5673828125
tensor(11659.5781, grad_fn=<NegBackward0>) tensor(11659.5674, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11659.55859375
tensor(11659.5674, grad_fn=<NegBackward0>) tensor(11659.5586, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11659.55078125
tensor(11659.5586, grad_fn=<NegBackward0>) tensor(11659.5508, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11659.533203125
tensor(11659.5508, grad_fn=<NegBackward0>) tensor(11659.5332, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11658.8935546875
tensor(11659.5332, grad_fn=<NegBackward0>) tensor(11658.8936, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11658.888671875
tensor(11658.8936, grad_fn=<NegBackward0>) tensor(11658.8887, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11658.884765625
tensor(11658.8887, grad_fn=<NegBackward0>) tensor(11658.8848, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11658.880859375
tensor(11658.8848, grad_fn=<NegBackward0>) tensor(11658.8809, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11658.8779296875
tensor(11658.8809, grad_fn=<NegBackward0>) tensor(11658.8779, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11658.875
tensor(11658.8779, grad_fn=<NegBackward0>) tensor(11658.8750, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11658.87109375
tensor(11658.8750, grad_fn=<NegBackward0>) tensor(11658.8711, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11658.869140625
tensor(11658.8711, grad_fn=<NegBackward0>) tensor(11658.8691, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11658.8671875
tensor(11658.8691, grad_fn=<NegBackward0>) tensor(11658.8672, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11658.865234375
tensor(11658.8672, grad_fn=<NegBackward0>) tensor(11658.8652, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11658.8623046875
tensor(11658.8652, grad_fn=<NegBackward0>) tensor(11658.8623, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11658.8623046875
tensor(11658.8623, grad_fn=<NegBackward0>) tensor(11658.8623, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11658.8603515625
tensor(11658.8623, grad_fn=<NegBackward0>) tensor(11658.8604, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11658.8583984375
tensor(11658.8604, grad_fn=<NegBackward0>) tensor(11658.8584, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11658.8583984375
tensor(11658.8584, grad_fn=<NegBackward0>) tensor(11658.8584, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11658.85546875
tensor(11658.8584, grad_fn=<NegBackward0>) tensor(11658.8555, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11658.8544921875
tensor(11658.8555, grad_fn=<NegBackward0>) tensor(11658.8545, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11658.857421875
tensor(11658.8545, grad_fn=<NegBackward0>) tensor(11658.8574, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11658.8525390625
tensor(11658.8545, grad_fn=<NegBackward0>) tensor(11658.8525, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11658.8515625
tensor(11658.8525, grad_fn=<NegBackward0>) tensor(11658.8516, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11658.8515625
tensor(11658.8516, grad_fn=<NegBackward0>) tensor(11658.8516, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11658.8515625
tensor(11658.8516, grad_fn=<NegBackward0>) tensor(11658.8516, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11658.8505859375
tensor(11658.8516, grad_fn=<NegBackward0>) tensor(11658.8506, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11658.84765625
tensor(11658.8506, grad_fn=<NegBackward0>) tensor(11658.8477, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11658.8466796875
tensor(11658.8477, grad_fn=<NegBackward0>) tensor(11658.8467, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11658.84765625
tensor(11658.8467, grad_fn=<NegBackward0>) tensor(11658.8477, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11658.8466796875
tensor(11658.8467, grad_fn=<NegBackward0>) tensor(11658.8467, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11658.8515625
tensor(11658.8467, grad_fn=<NegBackward0>) tensor(11658.8516, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11658.845703125
tensor(11658.8467, grad_fn=<NegBackward0>) tensor(11658.8457, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11658.84765625
tensor(11658.8457, grad_fn=<NegBackward0>) tensor(11658.8477, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11658.84375
tensor(11658.8457, grad_fn=<NegBackward0>) tensor(11658.8438, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11658.84375
tensor(11658.8438, grad_fn=<NegBackward0>) tensor(11658.8438, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11658.8427734375
tensor(11658.8438, grad_fn=<NegBackward0>) tensor(11658.8428, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11658.84375
tensor(11658.8428, grad_fn=<NegBackward0>) tensor(11658.8438, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11658.841796875
tensor(11658.8428, grad_fn=<NegBackward0>) tensor(11658.8418, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11658.8466796875
tensor(11658.8418, grad_fn=<NegBackward0>) tensor(11658.8467, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11658.8408203125
tensor(11658.8418, grad_fn=<NegBackward0>) tensor(11658.8408, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11658.8271484375
tensor(11658.8408, grad_fn=<NegBackward0>) tensor(11658.8271, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11658.828125
tensor(11658.8271, grad_fn=<NegBackward0>) tensor(11658.8281, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11658.826171875
tensor(11658.8271, grad_fn=<NegBackward0>) tensor(11658.8262, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11658.8271484375
tensor(11658.8262, grad_fn=<NegBackward0>) tensor(11658.8271, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11658.8271484375
tensor(11658.8262, grad_fn=<NegBackward0>) tensor(11658.8271, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11658.82421875
tensor(11658.8262, grad_fn=<NegBackward0>) tensor(11658.8242, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11658.8525390625
tensor(11658.8242, grad_fn=<NegBackward0>) tensor(11658.8525, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11658.8203125
tensor(11658.8242, grad_fn=<NegBackward0>) tensor(11658.8203, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11658.8203125
tensor(11658.8203, grad_fn=<NegBackward0>) tensor(11658.8203, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11658.8388671875
tensor(11658.8203, grad_fn=<NegBackward0>) tensor(11658.8389, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11658.8271484375
tensor(11658.8203, grad_fn=<NegBackward0>) tensor(11658.8271, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11658.8193359375
tensor(11658.8203, grad_fn=<NegBackward0>) tensor(11658.8193, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11658.8193359375
tensor(11658.8193, grad_fn=<NegBackward0>) tensor(11658.8193, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11658.81640625
tensor(11658.8193, grad_fn=<NegBackward0>) tensor(11658.8164, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11658.8486328125
tensor(11658.8164, grad_fn=<NegBackward0>) tensor(11658.8486, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11658.81640625
tensor(11658.8164, grad_fn=<NegBackward0>) tensor(11658.8164, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11658.8212890625
tensor(11658.8164, grad_fn=<NegBackward0>) tensor(11658.8213, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11658.8173828125
tensor(11658.8164, grad_fn=<NegBackward0>) tensor(11658.8174, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11658.81640625
tensor(11658.8164, grad_fn=<NegBackward0>) tensor(11658.8164, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11658.81640625
tensor(11658.8164, grad_fn=<NegBackward0>) tensor(11658.8164, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11658.8154296875
tensor(11658.8164, grad_fn=<NegBackward0>) tensor(11658.8154, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11650.0234375
tensor(11658.8154, grad_fn=<NegBackward0>) tensor(11650.0234, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11650.029296875
tensor(11650.0234, grad_fn=<NegBackward0>) tensor(11650.0293, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11650.017578125
tensor(11650.0234, grad_fn=<NegBackward0>) tensor(11650.0176, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11650.0185546875
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0186, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11650.017578125
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0176, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11650.0234375
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0234, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11650.0185546875
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0186, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11650.0390625
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0391, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11650.017578125
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0176, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11650.0185546875
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0186, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11650.01953125
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0195, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11650.1162109375
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.1162, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11650.03125
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0312, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11650.017578125
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0176, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11650.0234375
tensor(11650.0176, grad_fn=<NegBackward0>) tensor(11650.0234, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7445, 0.2555],
        [0.2705, 0.7295]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5494, 0.4506], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3943, 0.1060],
         [0.6674, 0.2034]],

        [[0.7017, 0.0887],
         [0.5030, 0.5523]],

        [[0.5271, 0.1031],
         [0.6687, 0.6828]],

        [[0.6679, 0.0970],
         [0.6397, 0.5082]],

        [[0.5927, 0.0940],
         [0.7272, 0.5995]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
[0.9919999730634713, 0.9919999730634713] [0.9919992163297293, 0.9919992163297293] [11650.0322265625, 11650.0244140625]
-------------------------------------
This iteration is 61
True Objective function: Loss = -11857.678689165668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21098.6328125
inf tensor(21098.6328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12768.939453125
tensor(21098.6328, grad_fn=<NegBackward0>) tensor(12768.9395, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12353.9267578125
tensor(12768.9395, grad_fn=<NegBackward0>) tensor(12353.9268, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12253.2841796875
tensor(12353.9268, grad_fn=<NegBackward0>) tensor(12253.2842, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11851.7431640625
tensor(12253.2842, grad_fn=<NegBackward0>) tensor(11851.7432, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11851.3125
tensor(11851.7432, grad_fn=<NegBackward0>) tensor(11851.3125, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11851.1455078125
tensor(11851.3125, grad_fn=<NegBackward0>) tensor(11851.1455, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11851.0517578125
tensor(11851.1455, grad_fn=<NegBackward0>) tensor(11851.0518, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11850.994140625
tensor(11851.0518, grad_fn=<NegBackward0>) tensor(11850.9941, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11850.953125
tensor(11850.9941, grad_fn=<NegBackward0>) tensor(11850.9531, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11850.923828125
tensor(11850.9531, grad_fn=<NegBackward0>) tensor(11850.9238, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11850.90234375
tensor(11850.9238, grad_fn=<NegBackward0>) tensor(11850.9023, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11850.884765625
tensor(11850.9023, grad_fn=<NegBackward0>) tensor(11850.8848, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11850.87109375
tensor(11850.8848, grad_fn=<NegBackward0>) tensor(11850.8711, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11850.8603515625
tensor(11850.8711, grad_fn=<NegBackward0>) tensor(11850.8604, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11850.8486328125
tensor(11850.8604, grad_fn=<NegBackward0>) tensor(11850.8486, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11850.841796875
tensor(11850.8486, grad_fn=<NegBackward0>) tensor(11850.8418, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11850.833984375
tensor(11850.8418, grad_fn=<NegBackward0>) tensor(11850.8340, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11850.8291015625
tensor(11850.8340, grad_fn=<NegBackward0>) tensor(11850.8291, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11850.82421875
tensor(11850.8291, grad_fn=<NegBackward0>) tensor(11850.8242, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11850.8212890625
tensor(11850.8242, grad_fn=<NegBackward0>) tensor(11850.8213, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11850.8173828125
tensor(11850.8213, grad_fn=<NegBackward0>) tensor(11850.8174, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11850.8125
tensor(11850.8174, grad_fn=<NegBackward0>) tensor(11850.8125, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11850.810546875
tensor(11850.8125, grad_fn=<NegBackward0>) tensor(11850.8105, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11850.806640625
tensor(11850.8105, grad_fn=<NegBackward0>) tensor(11850.8066, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11850.8056640625
tensor(11850.8066, grad_fn=<NegBackward0>) tensor(11850.8057, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11850.8037109375
tensor(11850.8057, grad_fn=<NegBackward0>) tensor(11850.8037, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11850.8017578125
tensor(11850.8037, grad_fn=<NegBackward0>) tensor(11850.8018, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11850.7978515625
tensor(11850.8018, grad_fn=<NegBackward0>) tensor(11850.7979, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11850.7978515625
tensor(11850.7979, grad_fn=<NegBackward0>) tensor(11850.7979, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11850.796875
tensor(11850.7979, grad_fn=<NegBackward0>) tensor(11850.7969, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11850.794921875
tensor(11850.7969, grad_fn=<NegBackward0>) tensor(11850.7949, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11850.79296875
tensor(11850.7949, grad_fn=<NegBackward0>) tensor(11850.7930, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11850.79296875
tensor(11850.7930, grad_fn=<NegBackward0>) tensor(11850.7930, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11850.791015625
tensor(11850.7930, grad_fn=<NegBackward0>) tensor(11850.7910, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11850.7900390625
tensor(11850.7910, grad_fn=<NegBackward0>) tensor(11850.7900, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11850.7890625
tensor(11850.7900, grad_fn=<NegBackward0>) tensor(11850.7891, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11850.7890625
tensor(11850.7891, grad_fn=<NegBackward0>) tensor(11850.7891, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11850.7880859375
tensor(11850.7891, grad_fn=<NegBackward0>) tensor(11850.7881, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11850.787109375
tensor(11850.7881, grad_fn=<NegBackward0>) tensor(11850.7871, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11850.7861328125
tensor(11850.7871, grad_fn=<NegBackward0>) tensor(11850.7861, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11850.7861328125
tensor(11850.7861, grad_fn=<NegBackward0>) tensor(11850.7861, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11850.78515625
tensor(11850.7861, grad_fn=<NegBackward0>) tensor(11850.7852, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11850.78515625
tensor(11850.7852, grad_fn=<NegBackward0>) tensor(11850.7852, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11850.783203125
tensor(11850.7852, grad_fn=<NegBackward0>) tensor(11850.7832, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11850.7958984375
tensor(11850.7832, grad_fn=<NegBackward0>) tensor(11850.7959, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11850.783203125
tensor(11850.7832, grad_fn=<NegBackward0>) tensor(11850.7832, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11850.7841796875
tensor(11850.7832, grad_fn=<NegBackward0>) tensor(11850.7842, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11850.783203125
tensor(11850.7832, grad_fn=<NegBackward0>) tensor(11850.7832, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11850.783203125
tensor(11850.7832, grad_fn=<NegBackward0>) tensor(11850.7832, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11850.7822265625
tensor(11850.7832, grad_fn=<NegBackward0>) tensor(11850.7822, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11850.7822265625
tensor(11850.7822, grad_fn=<NegBackward0>) tensor(11850.7822, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11850.78125
tensor(11850.7822, grad_fn=<NegBackward0>) tensor(11850.7812, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11850.78125
tensor(11850.7812, grad_fn=<NegBackward0>) tensor(11850.7812, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11850.7822265625
tensor(11850.7812, grad_fn=<NegBackward0>) tensor(11850.7822, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11850.779296875
tensor(11850.7812, grad_fn=<NegBackward0>) tensor(11850.7793, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11850.7841796875
tensor(11850.7793, grad_fn=<NegBackward0>) tensor(11850.7842, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11850.78515625
tensor(11850.7793, grad_fn=<NegBackward0>) tensor(11850.7852, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11850.7841796875
tensor(11850.7793, grad_fn=<NegBackward0>) tensor(11850.7842, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11850.7802734375
tensor(11850.7793, grad_fn=<NegBackward0>) tensor(11850.7803, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11850.7802734375
tensor(11850.7793, grad_fn=<NegBackward0>) tensor(11850.7803, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.7585, 0.2415],
        [0.2111, 0.7889]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4045, 0.5955], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.1021],
         [0.5877, 0.4010]],

        [[0.5998, 0.1003],
         [0.5491, 0.6849]],

        [[0.5248, 0.0922],
         [0.6326, 0.6036]],

        [[0.5412, 0.0994],
         [0.5359, 0.5570]],

        [[0.5677, 0.1006],
         [0.6167, 0.6152]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919986675553487
Average Adjusted Rand Index: 0.9919996552039955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22773.28515625
inf tensor(22773.2852, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12607.7529296875
tensor(22773.2852, grad_fn=<NegBackward0>) tensor(12607.7529, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12236.5634765625
tensor(12607.7529, grad_fn=<NegBackward0>) tensor(12236.5635, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12140.0234375
tensor(12236.5635, grad_fn=<NegBackward0>) tensor(12140.0234, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12114.9873046875
tensor(12140.0234, grad_fn=<NegBackward0>) tensor(12114.9873, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12096.05859375
tensor(12114.9873, grad_fn=<NegBackward0>) tensor(12096.0586, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12095.9365234375
tensor(12096.0586, grad_fn=<NegBackward0>) tensor(12095.9365, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12095.8662109375
tensor(12095.9365, grad_fn=<NegBackward0>) tensor(12095.8662, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12095.8203125
tensor(12095.8662, grad_fn=<NegBackward0>) tensor(12095.8203, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12095.7880859375
tensor(12095.8203, grad_fn=<NegBackward0>) tensor(12095.7881, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12095.7666015625
tensor(12095.7881, grad_fn=<NegBackward0>) tensor(12095.7666, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12095.748046875
tensor(12095.7666, grad_fn=<NegBackward0>) tensor(12095.7480, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12095.7353515625
tensor(12095.7480, grad_fn=<NegBackward0>) tensor(12095.7354, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12095.724609375
tensor(12095.7354, grad_fn=<NegBackward0>) tensor(12095.7246, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12095.7158203125
tensor(12095.7246, grad_fn=<NegBackward0>) tensor(12095.7158, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12095.7080078125
tensor(12095.7158, grad_fn=<NegBackward0>) tensor(12095.7080, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12095.7021484375
tensor(12095.7080, grad_fn=<NegBackward0>) tensor(12095.7021, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12095.6962890625
tensor(12095.7021, grad_fn=<NegBackward0>) tensor(12095.6963, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12095.69140625
tensor(12095.6963, grad_fn=<NegBackward0>) tensor(12095.6914, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12095.6884765625
tensor(12095.6914, grad_fn=<NegBackward0>) tensor(12095.6885, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12095.685546875
tensor(12095.6885, grad_fn=<NegBackward0>) tensor(12095.6855, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12095.681640625
tensor(12095.6855, grad_fn=<NegBackward0>) tensor(12095.6816, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12095.6796875
tensor(12095.6816, grad_fn=<NegBackward0>) tensor(12095.6797, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12095.6787109375
tensor(12095.6797, grad_fn=<NegBackward0>) tensor(12095.6787, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12095.6767578125
tensor(12095.6787, grad_fn=<NegBackward0>) tensor(12095.6768, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12095.673828125
tensor(12095.6768, grad_fn=<NegBackward0>) tensor(12095.6738, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12095.6708984375
tensor(12095.6738, grad_fn=<NegBackward0>) tensor(12095.6709, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12095.669921875
tensor(12095.6709, grad_fn=<NegBackward0>) tensor(12095.6699, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12095.66796875
tensor(12095.6699, grad_fn=<NegBackward0>) tensor(12095.6680, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12095.6669921875
tensor(12095.6680, grad_fn=<NegBackward0>) tensor(12095.6670, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12095.666015625
tensor(12095.6670, grad_fn=<NegBackward0>) tensor(12095.6660, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12095.6630859375
tensor(12095.6660, grad_fn=<NegBackward0>) tensor(12095.6631, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12095.6630859375
tensor(12095.6631, grad_fn=<NegBackward0>) tensor(12095.6631, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12095.6630859375
tensor(12095.6631, grad_fn=<NegBackward0>) tensor(12095.6631, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12095.662109375
tensor(12095.6631, grad_fn=<NegBackward0>) tensor(12095.6621, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12095.6611328125
tensor(12095.6621, grad_fn=<NegBackward0>) tensor(12095.6611, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12095.6591796875
tensor(12095.6611, grad_fn=<NegBackward0>) tensor(12095.6592, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12095.66015625
tensor(12095.6592, grad_fn=<NegBackward0>) tensor(12095.6602, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12095.6591796875
tensor(12095.6592, grad_fn=<NegBackward0>) tensor(12095.6592, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12095.658203125
tensor(12095.6592, grad_fn=<NegBackward0>) tensor(12095.6582, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12095.6591796875
tensor(12095.6582, grad_fn=<NegBackward0>) tensor(12095.6592, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12095.658203125
tensor(12095.6582, grad_fn=<NegBackward0>) tensor(12095.6582, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12095.65625
tensor(12095.6582, grad_fn=<NegBackward0>) tensor(12095.6562, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12095.66015625
tensor(12095.6562, grad_fn=<NegBackward0>) tensor(12095.6602, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12095.65625
tensor(12095.6562, grad_fn=<NegBackward0>) tensor(12095.6562, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12095.65625
tensor(12095.6562, grad_fn=<NegBackward0>) tensor(12095.6562, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12095.6572265625
tensor(12095.6562, grad_fn=<NegBackward0>) tensor(12095.6572, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12095.6650390625
tensor(12095.6562, grad_fn=<NegBackward0>) tensor(12095.6650, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -12095.6611328125
tensor(12095.6562, grad_fn=<NegBackward0>) tensor(12095.6611, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -12095.6552734375
tensor(12095.6562, grad_fn=<NegBackward0>) tensor(12095.6553, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12095.6552734375
tensor(12095.6553, grad_fn=<NegBackward0>) tensor(12095.6553, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12095.65625
tensor(12095.6553, grad_fn=<NegBackward0>) tensor(12095.6562, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12095.65625
tensor(12095.6553, grad_fn=<NegBackward0>) tensor(12095.6562, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -12095.654296875
tensor(12095.6553, grad_fn=<NegBackward0>) tensor(12095.6543, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12095.654296875
tensor(12095.6543, grad_fn=<NegBackward0>) tensor(12095.6543, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12095.65234375
tensor(12095.6543, grad_fn=<NegBackward0>) tensor(12095.6523, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12095.65625
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6562, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12095.65234375
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6523, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12095.65234375
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6523, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12095.6640625
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6641, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12095.65234375
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6523, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12095.6533203125
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6533, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12095.6533203125
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6533, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12095.6513671875
tensor(12095.6523, grad_fn=<NegBackward0>) tensor(12095.6514, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12095.658203125
tensor(12095.6514, grad_fn=<NegBackward0>) tensor(12095.6582, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12095.6533203125
tensor(12095.6514, grad_fn=<NegBackward0>) tensor(12095.6533, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12095.65234375
tensor(12095.6514, grad_fn=<NegBackward0>) tensor(12095.6523, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12093.3662109375
tensor(12095.6514, grad_fn=<NegBackward0>) tensor(12093.3662, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12093.3515625
tensor(12093.3662, grad_fn=<NegBackward0>) tensor(12093.3516, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12093.34765625
tensor(12093.3516, grad_fn=<NegBackward0>) tensor(12093.3477, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12093.33984375
tensor(12093.3477, grad_fn=<NegBackward0>) tensor(12093.3398, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12093.34375
tensor(12093.3398, grad_fn=<NegBackward0>) tensor(12093.3438, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12093.3408203125
tensor(12093.3398, grad_fn=<NegBackward0>) tensor(12093.3408, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12093.33984375
tensor(12093.3398, grad_fn=<NegBackward0>) tensor(12093.3398, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12093.3388671875
tensor(12093.3398, grad_fn=<NegBackward0>) tensor(12093.3389, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12093.337890625
tensor(12093.3389, grad_fn=<NegBackward0>) tensor(12093.3379, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12093.33984375
tensor(12093.3379, grad_fn=<NegBackward0>) tensor(12093.3398, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12093.35546875
tensor(12093.3379, grad_fn=<NegBackward0>) tensor(12093.3555, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12093.3984375
tensor(12093.3379, grad_fn=<NegBackward0>) tensor(12093.3984, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -12093.34375
tensor(12093.3379, grad_fn=<NegBackward0>) tensor(12093.3438, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -12093.341796875
tensor(12093.3379, grad_fn=<NegBackward0>) tensor(12093.3418, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.5588, 0.4412],
        [0.4888, 0.5112]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4012, 0.5988], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2515, 0.1008],
         [0.5893, 0.3874]],

        [[0.6959, 0.1001],
         [0.6706, 0.6575]],

        [[0.6831, 0.0872],
         [0.6281, 0.6773]],

        [[0.5337, 0.0994],
         [0.5971, 0.5980]],

        [[0.6306, 0.1006],
         [0.5596, 0.5153]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 13
Adjusted Rand Index: 0.5411047126675903
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.41357761106285085
Average Adjusted Rand Index: 0.892215108819492
[0.9919986675553487, 0.41357761106285085] [0.9919996552039955, 0.892215108819492] [11850.7802734375, 12093.341796875]
-------------------------------------
This iteration is 62
True Objective function: Loss = -11579.283973201182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21717.658203125
inf tensor(21717.6582, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12495.8876953125
tensor(21717.6582, grad_fn=<NegBackward0>) tensor(12495.8877, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12060.0439453125
tensor(12495.8877, grad_fn=<NegBackward0>) tensor(12060.0439, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11832.048828125
tensor(12060.0439, grad_fn=<NegBackward0>) tensor(11832.0488, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11767.1728515625
tensor(11832.0488, grad_fn=<NegBackward0>) tensor(11767.1729, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11738.125
tensor(11767.1729, grad_fn=<NegBackward0>) tensor(11738.1250, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11700.12890625
tensor(11738.1250, grad_fn=<NegBackward0>) tensor(11700.1289, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11692.595703125
tensor(11700.1289, grad_fn=<NegBackward0>) tensor(11692.5957, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11645.9306640625
tensor(11692.5957, grad_fn=<NegBackward0>) tensor(11645.9307, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11626.7158203125
tensor(11645.9307, grad_fn=<NegBackward0>) tensor(11626.7158, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11615.7109375
tensor(11626.7158, grad_fn=<NegBackward0>) tensor(11615.7109, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11602.6201171875
tensor(11615.7109, grad_fn=<NegBackward0>) tensor(11602.6201, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11584.451171875
tensor(11602.6201, grad_fn=<NegBackward0>) tensor(11584.4512, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11584.3984375
tensor(11584.4512, grad_fn=<NegBackward0>) tensor(11584.3984, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11584.3623046875
tensor(11584.3984, grad_fn=<NegBackward0>) tensor(11584.3623, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11584.3349609375
tensor(11584.3623, grad_fn=<NegBackward0>) tensor(11584.3350, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11584.310546875
tensor(11584.3350, grad_fn=<NegBackward0>) tensor(11584.3105, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11584.2890625
tensor(11584.3105, grad_fn=<NegBackward0>) tensor(11584.2891, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11584.140625
tensor(11584.2891, grad_fn=<NegBackward0>) tensor(11584.1406, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11577.4794921875
tensor(11584.1406, grad_fn=<NegBackward0>) tensor(11577.4795, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11577.46875
tensor(11577.4795, grad_fn=<NegBackward0>) tensor(11577.4688, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11577.458984375
tensor(11577.4688, grad_fn=<NegBackward0>) tensor(11577.4590, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11577.4482421875
tensor(11577.4590, grad_fn=<NegBackward0>) tensor(11577.4482, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11577.4375
tensor(11577.4482, grad_fn=<NegBackward0>) tensor(11577.4375, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11575.3681640625
tensor(11577.4375, grad_fn=<NegBackward0>) tensor(11575.3682, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11574.546875
tensor(11575.3682, grad_fn=<NegBackward0>) tensor(11574.5469, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11574.5458984375
tensor(11574.5469, grad_fn=<NegBackward0>) tensor(11574.5459, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11574.53515625
tensor(11574.5459, grad_fn=<NegBackward0>) tensor(11574.5352, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11574.5263671875
tensor(11574.5352, grad_fn=<NegBackward0>) tensor(11574.5264, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11574.48828125
tensor(11574.5264, grad_fn=<NegBackward0>) tensor(11574.4883, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11574.482421875
tensor(11574.4883, grad_fn=<NegBackward0>) tensor(11574.4824, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11574.4794921875
tensor(11574.4824, grad_fn=<NegBackward0>) tensor(11574.4795, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11574.4775390625
tensor(11574.4795, grad_fn=<NegBackward0>) tensor(11574.4775, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11574.4755859375
tensor(11574.4775, grad_fn=<NegBackward0>) tensor(11574.4756, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11574.4765625
tensor(11574.4756, grad_fn=<NegBackward0>) tensor(11574.4766, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11574.470703125
tensor(11574.4756, grad_fn=<NegBackward0>) tensor(11574.4707, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11574.4697265625
tensor(11574.4707, grad_fn=<NegBackward0>) tensor(11574.4697, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11574.46875
tensor(11574.4697, grad_fn=<NegBackward0>) tensor(11574.4688, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11574.466796875
tensor(11574.4688, grad_fn=<NegBackward0>) tensor(11574.4668, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11574.4658203125
tensor(11574.4668, grad_fn=<NegBackward0>) tensor(11574.4658, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11574.4638671875
tensor(11574.4658, grad_fn=<NegBackward0>) tensor(11574.4639, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11574.4619140625
tensor(11574.4639, grad_fn=<NegBackward0>) tensor(11574.4619, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11574.46484375
tensor(11574.4619, grad_fn=<NegBackward0>) tensor(11574.4648, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11574.4599609375
tensor(11574.4619, grad_fn=<NegBackward0>) tensor(11574.4600, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11574.4599609375
tensor(11574.4600, grad_fn=<NegBackward0>) tensor(11574.4600, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11574.462890625
tensor(11574.4600, grad_fn=<NegBackward0>) tensor(11574.4629, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11574.45703125
tensor(11574.4600, grad_fn=<NegBackward0>) tensor(11574.4570, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11574.4580078125
tensor(11574.4570, grad_fn=<NegBackward0>) tensor(11574.4580, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11574.4560546875
tensor(11574.4570, grad_fn=<NegBackward0>) tensor(11574.4561, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11574.4560546875
tensor(11574.4561, grad_fn=<NegBackward0>) tensor(11574.4561, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11574.4560546875
tensor(11574.4561, grad_fn=<NegBackward0>) tensor(11574.4561, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11574.4541015625
tensor(11574.4561, grad_fn=<NegBackward0>) tensor(11574.4541, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11574.453125
tensor(11574.4541, grad_fn=<NegBackward0>) tensor(11574.4531, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11574.4130859375
tensor(11574.4531, grad_fn=<NegBackward0>) tensor(11574.4131, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11574.40625
tensor(11574.4131, grad_fn=<NegBackward0>) tensor(11574.4062, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11574.40625
tensor(11574.4062, grad_fn=<NegBackward0>) tensor(11574.4062, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11574.4052734375
tensor(11574.4062, grad_fn=<NegBackward0>) tensor(11574.4053, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11574.4091796875
tensor(11574.4053, grad_fn=<NegBackward0>) tensor(11574.4092, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11574.404296875
tensor(11574.4053, grad_fn=<NegBackward0>) tensor(11574.4043, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11574.404296875
tensor(11574.4043, grad_fn=<NegBackward0>) tensor(11574.4043, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11574.404296875
tensor(11574.4043, grad_fn=<NegBackward0>) tensor(11574.4043, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11574.404296875
tensor(11574.4043, grad_fn=<NegBackward0>) tensor(11574.4043, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11574.40234375
tensor(11574.4043, grad_fn=<NegBackward0>) tensor(11574.4023, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11574.4033203125
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4033, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11574.41015625
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4102, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11574.4052734375
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4053, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11574.40234375
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4023, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11574.40234375
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4023, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11574.40234375
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4023, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11574.40234375
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4023, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11574.4013671875
tensor(11574.4023, grad_fn=<NegBackward0>) tensor(11574.4014, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11574.400390625
tensor(11574.4014, grad_fn=<NegBackward0>) tensor(11574.4004, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11574.404296875
tensor(11574.4004, grad_fn=<NegBackward0>) tensor(11574.4043, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11574.400390625
tensor(11574.4004, grad_fn=<NegBackward0>) tensor(11574.4004, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11574.4033203125
tensor(11574.4004, grad_fn=<NegBackward0>) tensor(11574.4033, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11574.400390625
tensor(11574.4004, grad_fn=<NegBackward0>) tensor(11574.4004, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11574.4677734375
tensor(11574.4004, grad_fn=<NegBackward0>) tensor(11574.4678, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11574.400390625
tensor(11574.4004, grad_fn=<NegBackward0>) tensor(11574.4004, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11574.3994140625
tensor(11574.4004, grad_fn=<NegBackward0>) tensor(11574.3994, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11574.41015625
tensor(11574.3994, grad_fn=<NegBackward0>) tensor(11574.4102, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11574.3984375
tensor(11574.3994, grad_fn=<NegBackward0>) tensor(11574.3984, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11574.3984375
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.3984, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11574.4013671875
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.4014, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11574.3984375
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.3984, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11574.3984375
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.3984, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11574.400390625
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.4004, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11574.400390625
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.4004, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11574.4150390625
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.4150, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11574.46484375
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.4648, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11574.3994140625
tensor(11574.3984, grad_fn=<NegBackward0>) tensor(11574.3994, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.7636, 0.2364],
        [0.2545, 0.7455]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5476, 0.4524], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4019, 0.0988],
         [0.6015, 0.2022]],

        [[0.6372, 0.1010],
         [0.7280, 0.5241]],

        [[0.6124, 0.0975],
         [0.7188, 0.7126]],

        [[0.6894, 0.0873],
         [0.6133, 0.5151]],

        [[0.6680, 0.0904],
         [0.5868, 0.7242]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21389.0625
inf tensor(21389.0625, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12501.3369140625
tensor(21389.0625, grad_fn=<NegBackward0>) tensor(12501.3369, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12039.724609375
tensor(12501.3369, grad_fn=<NegBackward0>) tensor(12039.7246, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11884.6689453125
tensor(12039.7246, grad_fn=<NegBackward0>) tensor(11884.6689, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11818.298828125
tensor(11884.6689, grad_fn=<NegBackward0>) tensor(11818.2988, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11796.564453125
tensor(11818.2988, grad_fn=<NegBackward0>) tensor(11796.5645, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11789.14453125
tensor(11796.5645, grad_fn=<NegBackward0>) tensor(11789.1445, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11788.9619140625
tensor(11789.1445, grad_fn=<NegBackward0>) tensor(11788.9619, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11774.126953125
tensor(11788.9619, grad_fn=<NegBackward0>) tensor(11774.1270, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11773.5517578125
tensor(11774.1270, grad_fn=<NegBackward0>) tensor(11773.5518, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11772.8232421875
tensor(11773.5518, grad_fn=<NegBackward0>) tensor(11772.8232, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11770.2587890625
tensor(11772.8232, grad_fn=<NegBackward0>) tensor(11770.2588, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11764.9208984375
tensor(11770.2588, grad_fn=<NegBackward0>) tensor(11764.9209, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11764.8486328125
tensor(11764.9209, grad_fn=<NegBackward0>) tensor(11764.8486, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11764.818359375
tensor(11764.8486, grad_fn=<NegBackward0>) tensor(11764.8184, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11764.7890625
tensor(11764.8184, grad_fn=<NegBackward0>) tensor(11764.7891, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11764.771484375
tensor(11764.7891, grad_fn=<NegBackward0>) tensor(11764.7715, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11764.7607421875
tensor(11764.7715, grad_fn=<NegBackward0>) tensor(11764.7607, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11764.7509765625
tensor(11764.7607, grad_fn=<NegBackward0>) tensor(11764.7510, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11764.7421875
tensor(11764.7510, grad_fn=<NegBackward0>) tensor(11764.7422, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11764.7353515625
tensor(11764.7422, grad_fn=<NegBackward0>) tensor(11764.7354, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11764.7294921875
tensor(11764.7354, grad_fn=<NegBackward0>) tensor(11764.7295, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11764.728515625
tensor(11764.7295, grad_fn=<NegBackward0>) tensor(11764.7285, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11764.720703125
tensor(11764.7285, grad_fn=<NegBackward0>) tensor(11764.7207, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11764.7158203125
tensor(11764.7207, grad_fn=<NegBackward0>) tensor(11764.7158, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11764.7099609375
tensor(11764.7158, grad_fn=<NegBackward0>) tensor(11764.7100, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11764.705078125
tensor(11764.7100, grad_fn=<NegBackward0>) tensor(11764.7051, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11764.701171875
tensor(11764.7051, grad_fn=<NegBackward0>) tensor(11764.7012, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11764.697265625
tensor(11764.7012, grad_fn=<NegBackward0>) tensor(11764.6973, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11764.6953125
tensor(11764.6973, grad_fn=<NegBackward0>) tensor(11764.6953, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11764.6923828125
tensor(11764.6953, grad_fn=<NegBackward0>) tensor(11764.6924, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11764.6904296875
tensor(11764.6924, grad_fn=<NegBackward0>) tensor(11764.6904, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11764.6865234375
tensor(11764.6904, grad_fn=<NegBackward0>) tensor(11764.6865, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11764.6865234375
tensor(11764.6865, grad_fn=<NegBackward0>) tensor(11764.6865, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11764.6826171875
tensor(11764.6865, grad_fn=<NegBackward0>) tensor(11764.6826, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11764.6806640625
tensor(11764.6826, grad_fn=<NegBackward0>) tensor(11764.6807, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11764.681640625
tensor(11764.6807, grad_fn=<NegBackward0>) tensor(11764.6816, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11764.6796875
tensor(11764.6807, grad_fn=<NegBackward0>) tensor(11764.6797, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11764.6796875
tensor(11764.6797, grad_fn=<NegBackward0>) tensor(11764.6797, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11764.6787109375
tensor(11764.6797, grad_fn=<NegBackward0>) tensor(11764.6787, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11764.6787109375
tensor(11764.6787, grad_fn=<NegBackward0>) tensor(11764.6787, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11764.6875
tensor(11764.6787, grad_fn=<NegBackward0>) tensor(11764.6875, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11764.6748046875
tensor(11764.6787, grad_fn=<NegBackward0>) tensor(11764.6748, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11764.6748046875
tensor(11764.6748, grad_fn=<NegBackward0>) tensor(11764.6748, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11764.673828125
tensor(11764.6748, grad_fn=<NegBackward0>) tensor(11764.6738, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11764.6728515625
tensor(11764.6738, grad_fn=<NegBackward0>) tensor(11764.6729, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11764.671875
tensor(11764.6729, grad_fn=<NegBackward0>) tensor(11764.6719, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11764.671875
tensor(11764.6719, grad_fn=<NegBackward0>) tensor(11764.6719, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11764.6796875
tensor(11764.6719, grad_fn=<NegBackward0>) tensor(11764.6797, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11764.671875
tensor(11764.6719, grad_fn=<NegBackward0>) tensor(11764.6719, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11764.6708984375
tensor(11764.6719, grad_fn=<NegBackward0>) tensor(11764.6709, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11764.671875
tensor(11764.6709, grad_fn=<NegBackward0>) tensor(11764.6719, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11764.669921875
tensor(11764.6709, grad_fn=<NegBackward0>) tensor(11764.6699, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11764.669921875
tensor(11764.6699, grad_fn=<NegBackward0>) tensor(11764.6699, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11764.669921875
tensor(11764.6699, grad_fn=<NegBackward0>) tensor(11764.6699, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11764.669921875
tensor(11764.6699, grad_fn=<NegBackward0>) tensor(11764.6699, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11764.66796875
tensor(11764.6699, grad_fn=<NegBackward0>) tensor(11764.6680, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11764.6689453125
tensor(11764.6680, grad_fn=<NegBackward0>) tensor(11764.6689, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11764.66796875
tensor(11764.6680, grad_fn=<NegBackward0>) tensor(11764.6680, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11764.677734375
tensor(11764.6680, grad_fn=<NegBackward0>) tensor(11764.6777, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11764.474609375
tensor(11764.6680, grad_fn=<NegBackward0>) tensor(11764.4746, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11764.48046875
tensor(11764.4746, grad_fn=<NegBackward0>) tensor(11764.4805, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11764.4755859375
tensor(11764.4746, grad_fn=<NegBackward0>) tensor(11764.4756, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11764.4736328125
tensor(11764.4746, grad_fn=<NegBackward0>) tensor(11764.4736, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11764.47265625
tensor(11764.4736, grad_fn=<NegBackward0>) tensor(11764.4727, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11764.47265625
tensor(11764.4727, grad_fn=<NegBackward0>) tensor(11764.4727, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11764.4716796875
tensor(11764.4727, grad_fn=<NegBackward0>) tensor(11764.4717, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11764.4716796875
tensor(11764.4717, grad_fn=<NegBackward0>) tensor(11764.4717, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11764.4716796875
tensor(11764.4717, grad_fn=<NegBackward0>) tensor(11764.4717, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11764.47265625
tensor(11764.4717, grad_fn=<NegBackward0>) tensor(11764.4727, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11764.470703125
tensor(11764.4717, grad_fn=<NegBackward0>) tensor(11764.4707, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11764.470703125
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4707, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11764.4755859375
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4756, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11764.4775390625
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4775, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11764.55078125
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.5508, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11764.470703125
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4707, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11764.470703125
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4707, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11764.4912109375
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4912, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11764.474609375
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4746, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11764.46875
tensor(11764.4707, grad_fn=<NegBackward0>) tensor(11764.4688, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11764.470703125
tensor(11764.4688, grad_fn=<NegBackward0>) tensor(11764.4707, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11764.46875
tensor(11764.4688, grad_fn=<NegBackward0>) tensor(11764.4688, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11764.5439453125
tensor(11764.4688, grad_fn=<NegBackward0>) tensor(11764.5439, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11764.470703125
tensor(11764.4688, grad_fn=<NegBackward0>) tensor(11764.4707, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11764.4716796875
tensor(11764.4688, grad_fn=<NegBackward0>) tensor(11764.4717, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11764.4697265625
tensor(11764.4688, grad_fn=<NegBackward0>) tensor(11764.4697, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11764.4697265625
tensor(11764.4688, grad_fn=<NegBackward0>) tensor(11764.4697, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.6142, 0.3858],
        [0.3200, 0.6800]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5478, 0.4522], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3887, 0.0987],
         [0.6740, 0.2327]],

        [[0.6489, 0.1007],
         [0.6490, 0.5251]],

        [[0.5274, 0.0970],
         [0.5736, 0.5684]],

        [[0.6305, 0.0873],
         [0.7285, 0.7275]],

        [[0.5378, 0.0911],
         [0.6655, 0.6487]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 82
Adjusted Rand Index: 0.4044204067079371
Global Adjusted Rand Index: 0.450486986701918
Average Adjusted Rand Index: 0.8808840813415874
[1.0, 0.450486986701918] [1.0, 0.8808840813415874] [11574.3994140625, 11764.4697265625]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11628.100174212863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21403.845703125
inf tensor(21403.8457, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12391.8466796875
tensor(21403.8457, grad_fn=<NegBackward0>) tensor(12391.8467, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11891.5576171875
tensor(12391.8467, grad_fn=<NegBackward0>) tensor(11891.5576, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11841.71484375
tensor(11891.5576, grad_fn=<NegBackward0>) tensor(11841.7148, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11811.94921875
tensor(11841.7148, grad_fn=<NegBackward0>) tensor(11811.9492, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11792.544921875
tensor(11811.9492, grad_fn=<NegBackward0>) tensor(11792.5449, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11773.4833984375
tensor(11792.5449, grad_fn=<NegBackward0>) tensor(11773.4834, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11724.2783203125
tensor(11773.4834, grad_fn=<NegBackward0>) tensor(11724.2783, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11666.9775390625
tensor(11724.2783, grad_fn=<NegBackward0>) tensor(11666.9775, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11652.39453125
tensor(11666.9775, grad_fn=<NegBackward0>) tensor(11652.3945, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11652.2333984375
tensor(11652.3945, grad_fn=<NegBackward0>) tensor(11652.2334, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11639.4931640625
tensor(11652.2334, grad_fn=<NegBackward0>) tensor(11639.4932, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11620.521484375
tensor(11639.4932, grad_fn=<NegBackward0>) tensor(11620.5215, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11620.4580078125
tensor(11620.5215, grad_fn=<NegBackward0>) tensor(11620.4580, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11620.419921875
tensor(11620.4580, grad_fn=<NegBackward0>) tensor(11620.4199, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11620.3994140625
tensor(11620.4199, grad_fn=<NegBackward0>) tensor(11620.3994, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11620.38671875
tensor(11620.3994, grad_fn=<NegBackward0>) tensor(11620.3867, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11620.373046875
tensor(11620.3867, grad_fn=<NegBackward0>) tensor(11620.3730, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11620.36328125
tensor(11620.3730, grad_fn=<NegBackward0>) tensor(11620.3633, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11620.35546875
tensor(11620.3633, grad_fn=<NegBackward0>) tensor(11620.3555, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11620.3486328125
tensor(11620.3555, grad_fn=<NegBackward0>) tensor(11620.3486, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11620.3427734375
tensor(11620.3486, grad_fn=<NegBackward0>) tensor(11620.3428, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11620.3369140625
tensor(11620.3428, grad_fn=<NegBackward0>) tensor(11620.3369, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11620.3330078125
tensor(11620.3369, grad_fn=<NegBackward0>) tensor(11620.3330, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11620.330078125
tensor(11620.3330, grad_fn=<NegBackward0>) tensor(11620.3301, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11620.3251953125
tensor(11620.3301, grad_fn=<NegBackward0>) tensor(11620.3252, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11620.3232421875
tensor(11620.3252, grad_fn=<NegBackward0>) tensor(11620.3232, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11620.3251953125
tensor(11620.3232, grad_fn=<NegBackward0>) tensor(11620.3252, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11620.3173828125
tensor(11620.3232, grad_fn=<NegBackward0>) tensor(11620.3174, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11620.3154296875
tensor(11620.3174, grad_fn=<NegBackward0>) tensor(11620.3154, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11620.3154296875
tensor(11620.3154, grad_fn=<NegBackward0>) tensor(11620.3154, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11620.3125
tensor(11620.3154, grad_fn=<NegBackward0>) tensor(11620.3125, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11620.310546875
tensor(11620.3125, grad_fn=<NegBackward0>) tensor(11620.3105, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11620.3095703125
tensor(11620.3105, grad_fn=<NegBackward0>) tensor(11620.3096, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11620.30859375
tensor(11620.3096, grad_fn=<NegBackward0>) tensor(11620.3086, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11620.3056640625
tensor(11620.3086, grad_fn=<NegBackward0>) tensor(11620.3057, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11620.306640625
tensor(11620.3057, grad_fn=<NegBackward0>) tensor(11620.3066, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11620.3046875
tensor(11620.3057, grad_fn=<NegBackward0>) tensor(11620.3047, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11620.306640625
tensor(11620.3047, grad_fn=<NegBackward0>) tensor(11620.3066, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11620.3037109375
tensor(11620.3047, grad_fn=<NegBackward0>) tensor(11620.3037, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11620.3017578125
tensor(11620.3037, grad_fn=<NegBackward0>) tensor(11620.3018, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11620.3017578125
tensor(11620.3018, grad_fn=<NegBackward0>) tensor(11620.3018, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11620.30078125
tensor(11620.3018, grad_fn=<NegBackward0>) tensor(11620.3008, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11620.30078125
tensor(11620.3008, grad_fn=<NegBackward0>) tensor(11620.3008, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11620.2998046875
tensor(11620.3008, grad_fn=<NegBackward0>) tensor(11620.2998, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11620.298828125
tensor(11620.2998, grad_fn=<NegBackward0>) tensor(11620.2988, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11620.2978515625
tensor(11620.2988, grad_fn=<NegBackward0>) tensor(11620.2979, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11620.296875
tensor(11620.2979, grad_fn=<NegBackward0>) tensor(11620.2969, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11620.296875
tensor(11620.2969, grad_fn=<NegBackward0>) tensor(11620.2969, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11620.296875
tensor(11620.2969, grad_fn=<NegBackward0>) tensor(11620.2969, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11620.3017578125
tensor(11620.2969, grad_fn=<NegBackward0>) tensor(11620.3018, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11620.296875
tensor(11620.2969, grad_fn=<NegBackward0>) tensor(11620.2969, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11620.2958984375
tensor(11620.2969, grad_fn=<NegBackward0>) tensor(11620.2959, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11620.2978515625
tensor(11620.2959, grad_fn=<NegBackward0>) tensor(11620.2979, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11620.294921875
tensor(11620.2959, grad_fn=<NegBackward0>) tensor(11620.2949, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11620.2978515625
tensor(11620.2949, grad_fn=<NegBackward0>) tensor(11620.2979, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11620.294921875
tensor(11620.2949, grad_fn=<NegBackward0>) tensor(11620.2949, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11620.294921875
tensor(11620.2949, grad_fn=<NegBackward0>) tensor(11620.2949, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11620.2939453125
tensor(11620.2949, grad_fn=<NegBackward0>) tensor(11620.2939, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11620.2939453125
tensor(11620.2939, grad_fn=<NegBackward0>) tensor(11620.2939, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11620.29296875
tensor(11620.2939, grad_fn=<NegBackward0>) tensor(11620.2930, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11620.2939453125
tensor(11620.2930, grad_fn=<NegBackward0>) tensor(11620.2939, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11620.294921875
tensor(11620.2930, grad_fn=<NegBackward0>) tensor(11620.2949, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11620.2919921875
tensor(11620.2930, grad_fn=<NegBackward0>) tensor(11620.2920, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11620.2939453125
tensor(11620.2920, grad_fn=<NegBackward0>) tensor(11620.2939, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11620.29296875
tensor(11620.2920, grad_fn=<NegBackward0>) tensor(11620.2930, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11620.2919921875
tensor(11620.2920, grad_fn=<NegBackward0>) tensor(11620.2920, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11620.291015625
tensor(11620.2920, grad_fn=<NegBackward0>) tensor(11620.2910, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11620.2900390625
tensor(11620.2910, grad_fn=<NegBackward0>) tensor(11620.2900, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11620.3388671875
tensor(11620.2900, grad_fn=<NegBackward0>) tensor(11620.3389, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11620.291015625
tensor(11620.2900, grad_fn=<NegBackward0>) tensor(11620.2910, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11620.33203125
tensor(11620.2900, grad_fn=<NegBackward0>) tensor(11620.3320, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11620.2919921875
tensor(11620.2900, grad_fn=<NegBackward0>) tensor(11620.2920, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11620.4111328125
tensor(11620.2900, grad_fn=<NegBackward0>) tensor(11620.4111, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7263, 0.2737],
        [0.2608, 0.7392]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1977, 0.1017],
         [0.5039, 0.4169]],

        [[0.5835, 0.0984],
         [0.6292, 0.5907]],

        [[0.7246, 0.0876],
         [0.6410, 0.6645]],

        [[0.6088, 0.0976],
         [0.5967, 0.5978]],

        [[0.5901, 0.1083],
         [0.5414, 0.5993]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22247.11328125
inf tensor(22247.1133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12515.4521484375
tensor(22247.1133, grad_fn=<NegBackward0>) tensor(12515.4521, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11977.5048828125
tensor(12515.4521, grad_fn=<NegBackward0>) tensor(11977.5049, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11854.0458984375
tensor(11977.5049, grad_fn=<NegBackward0>) tensor(11854.0459, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11798.5693359375
tensor(11854.0459, grad_fn=<NegBackward0>) tensor(11798.5693, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11797.8251953125
tensor(11798.5693, grad_fn=<NegBackward0>) tensor(11797.8252, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11794.396484375
tensor(11797.8252, grad_fn=<NegBackward0>) tensor(11794.3965, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11793.1220703125
tensor(11794.3965, grad_fn=<NegBackward0>) tensor(11793.1221, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11792.9619140625
tensor(11793.1221, grad_fn=<NegBackward0>) tensor(11792.9619, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11792.865234375
tensor(11792.9619, grad_fn=<NegBackward0>) tensor(11792.8652, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11792.794921875
tensor(11792.8652, grad_fn=<NegBackward0>) tensor(11792.7949, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11792.7412109375
tensor(11792.7949, grad_fn=<NegBackward0>) tensor(11792.7412, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11792.69921875
tensor(11792.7412, grad_fn=<NegBackward0>) tensor(11792.6992, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11792.6669921875
tensor(11792.6992, grad_fn=<NegBackward0>) tensor(11792.6670, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11792.6396484375
tensor(11792.6670, grad_fn=<NegBackward0>) tensor(11792.6396, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11792.619140625
tensor(11792.6396, grad_fn=<NegBackward0>) tensor(11792.6191, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11792.599609375
tensor(11792.6191, grad_fn=<NegBackward0>) tensor(11792.5996, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11792.5859375
tensor(11792.5996, grad_fn=<NegBackward0>) tensor(11792.5859, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11792.572265625
tensor(11792.5859, grad_fn=<NegBackward0>) tensor(11792.5723, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11792.560546875
tensor(11792.5723, grad_fn=<NegBackward0>) tensor(11792.5605, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11792.5517578125
tensor(11792.5605, grad_fn=<NegBackward0>) tensor(11792.5518, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11792.54296875
tensor(11792.5518, grad_fn=<NegBackward0>) tensor(11792.5430, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11792.53515625
tensor(11792.5430, grad_fn=<NegBackward0>) tensor(11792.5352, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11792.5302734375
tensor(11792.5352, grad_fn=<NegBackward0>) tensor(11792.5303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11792.5234375
tensor(11792.5303, grad_fn=<NegBackward0>) tensor(11792.5234, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11792.517578125
tensor(11792.5234, grad_fn=<NegBackward0>) tensor(11792.5176, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11792.513671875
tensor(11792.5176, grad_fn=<NegBackward0>) tensor(11792.5137, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11792.5087890625
tensor(11792.5137, grad_fn=<NegBackward0>) tensor(11792.5088, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11792.5048828125
tensor(11792.5088, grad_fn=<NegBackward0>) tensor(11792.5049, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11792.501953125
tensor(11792.5049, grad_fn=<NegBackward0>) tensor(11792.5020, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11792.5
tensor(11792.5020, grad_fn=<NegBackward0>) tensor(11792.5000, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11792.4951171875
tensor(11792.5000, grad_fn=<NegBackward0>) tensor(11792.4951, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11792.494140625
tensor(11792.4951, grad_fn=<NegBackward0>) tensor(11792.4941, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11792.4921875
tensor(11792.4941, grad_fn=<NegBackward0>) tensor(11792.4922, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11792.4892578125
tensor(11792.4922, grad_fn=<NegBackward0>) tensor(11792.4893, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11792.4873046875
tensor(11792.4893, grad_fn=<NegBackward0>) tensor(11792.4873, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11792.4853515625
tensor(11792.4873, grad_fn=<NegBackward0>) tensor(11792.4854, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11792.484375
tensor(11792.4854, grad_fn=<NegBackward0>) tensor(11792.4844, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11792.482421875
tensor(11792.4844, grad_fn=<NegBackward0>) tensor(11792.4824, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11792.4814453125
tensor(11792.4824, grad_fn=<NegBackward0>) tensor(11792.4814, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11792.4794921875
tensor(11792.4814, grad_fn=<NegBackward0>) tensor(11792.4795, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11792.478515625
tensor(11792.4795, grad_fn=<NegBackward0>) tensor(11792.4785, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11792.478515625
tensor(11792.4785, grad_fn=<NegBackward0>) tensor(11792.4785, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11792.4755859375
tensor(11792.4785, grad_fn=<NegBackward0>) tensor(11792.4756, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11792.4755859375
tensor(11792.4756, grad_fn=<NegBackward0>) tensor(11792.4756, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11792.474609375
tensor(11792.4756, grad_fn=<NegBackward0>) tensor(11792.4746, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11792.4736328125
tensor(11792.4746, grad_fn=<NegBackward0>) tensor(11792.4736, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11792.4716796875
tensor(11792.4736, grad_fn=<NegBackward0>) tensor(11792.4717, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11792.4599609375
tensor(11792.4717, grad_fn=<NegBackward0>) tensor(11792.4600, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11792.4521484375
tensor(11792.4600, grad_fn=<NegBackward0>) tensor(11792.4521, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11792.451171875
tensor(11792.4521, grad_fn=<NegBackward0>) tensor(11792.4512, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11792.44921875
tensor(11792.4512, grad_fn=<NegBackward0>) tensor(11792.4492, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11792.4482421875
tensor(11792.4492, grad_fn=<NegBackward0>) tensor(11792.4482, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11792.447265625
tensor(11792.4482, grad_fn=<NegBackward0>) tensor(11792.4473, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11792.4599609375
tensor(11792.4473, grad_fn=<NegBackward0>) tensor(11792.4600, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11792.4423828125
tensor(11792.4473, grad_fn=<NegBackward0>) tensor(11792.4424, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11792.4306640625
tensor(11792.4424, grad_fn=<NegBackward0>) tensor(11792.4307, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11786.314453125
tensor(11792.4307, grad_fn=<NegBackward0>) tensor(11786.3145, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11754.677734375
tensor(11786.3145, grad_fn=<NegBackward0>) tensor(11754.6777, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11730.1396484375
tensor(11754.6777, grad_fn=<NegBackward0>) tensor(11730.1396, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11707.3994140625
tensor(11730.1396, grad_fn=<NegBackward0>) tensor(11707.3994, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11696.970703125
tensor(11707.3994, grad_fn=<NegBackward0>) tensor(11696.9707, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11685.0361328125
tensor(11696.9707, grad_fn=<NegBackward0>) tensor(11685.0361, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11675.53125
tensor(11685.0361, grad_fn=<NegBackward0>) tensor(11675.5312, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11675.5302734375
tensor(11675.5312, grad_fn=<NegBackward0>) tensor(11675.5303, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11675.5302734375
tensor(11675.5303, grad_fn=<NegBackward0>) tensor(11675.5303, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11675.5283203125
tensor(11675.5303, grad_fn=<NegBackward0>) tensor(11675.5283, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11675.53515625
tensor(11675.5283, grad_fn=<NegBackward0>) tensor(11675.5352, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11666.2236328125
tensor(11675.5283, grad_fn=<NegBackward0>) tensor(11666.2236, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11666.216796875
tensor(11666.2236, grad_fn=<NegBackward0>) tensor(11666.2168, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11666.2119140625
tensor(11666.2168, grad_fn=<NegBackward0>) tensor(11666.2119, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11666.1826171875
tensor(11666.2119, grad_fn=<NegBackward0>) tensor(11666.1826, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11650.4921875
tensor(11666.1826, grad_fn=<NegBackward0>) tensor(11650.4922, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11650.490234375
tensor(11650.4922, grad_fn=<NegBackward0>) tensor(11650.4902, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11650.4931640625
tensor(11650.4902, grad_fn=<NegBackward0>) tensor(11650.4932, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11650.50390625
tensor(11650.4902, grad_fn=<NegBackward0>) tensor(11650.5039, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11650.501953125
tensor(11650.4902, grad_fn=<NegBackward0>) tensor(11650.5020, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11650.48828125
tensor(11650.4902, grad_fn=<NegBackward0>) tensor(11650.4883, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11631.2001953125
tensor(11650.4883, grad_fn=<NegBackward0>) tensor(11631.2002, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11631.1962890625
tensor(11631.2002, grad_fn=<NegBackward0>) tensor(11631.1963, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11631.1962890625
tensor(11631.1963, grad_fn=<NegBackward0>) tensor(11631.1963, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11631.2158203125
tensor(11631.1963, grad_fn=<NegBackward0>) tensor(11631.2158, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11631.1953125
tensor(11631.1963, grad_fn=<NegBackward0>) tensor(11631.1953, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11631.1943359375
tensor(11631.1953, grad_fn=<NegBackward0>) tensor(11631.1943, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11631.193359375
tensor(11631.1943, grad_fn=<NegBackward0>) tensor(11631.1934, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11631.193359375
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.1934, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11631.193359375
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.1934, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11631.193359375
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.1934, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11631.1953125
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.1953, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11631.1943359375
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.1943, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11631.2099609375
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.2100, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11631.205078125
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.2051, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11631.205078125
tensor(11631.1934, grad_fn=<NegBackward0>) tensor(11631.2051, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.7363, 0.2637],
        [0.2823, 0.7177]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4795, 0.5205], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4148, 0.1067],
         [0.7161, 0.1965]],

        [[0.6117, 0.0984],
         [0.6134, 0.6113]],

        [[0.6922, 0.0876],
         [0.6021, 0.6736]],

        [[0.5156, 0.0976],
         [0.5552, 0.5929]],

        [[0.5685, 0.1083],
         [0.5993, 0.6268]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999860917265
Average Adjusted Rand Index: 0.9919993417272899
[1.0, 0.9919999860917265] [1.0, 0.9919993417272899] [11620.4111328125, 11631.205078125]
-------------------------------------
This iteration is 64
True Objective function: Loss = -11519.68922611485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23068.2421875
inf tensor(23068.2422, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12163.9541015625
tensor(23068.2422, grad_fn=<NegBackward0>) tensor(12163.9541, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12157.982421875
tensor(12163.9541, grad_fn=<NegBackward0>) tensor(12157.9824, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11938.015625
tensor(12157.9824, grad_fn=<NegBackward0>) tensor(11938.0156, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11835.0068359375
tensor(11938.0156, grad_fn=<NegBackward0>) tensor(11835.0068, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11809.6142578125
tensor(11835.0068, grad_fn=<NegBackward0>) tensor(11809.6143, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11765.966796875
tensor(11809.6143, grad_fn=<NegBackward0>) tensor(11765.9668, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11763.9609375
tensor(11765.9668, grad_fn=<NegBackward0>) tensor(11763.9609, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11763.6533203125
tensor(11763.9609, grad_fn=<NegBackward0>) tensor(11763.6533, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11762.9970703125
tensor(11763.6533, grad_fn=<NegBackward0>) tensor(11762.9971, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11762.904296875
tensor(11762.9971, grad_fn=<NegBackward0>) tensor(11762.9043, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11762.58984375
tensor(11762.9043, grad_fn=<NegBackward0>) tensor(11762.5898, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11761.150390625
tensor(11762.5898, grad_fn=<NegBackward0>) tensor(11761.1504, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11757.48828125
tensor(11761.1504, grad_fn=<NegBackward0>) tensor(11757.4883, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11757.2900390625
tensor(11757.4883, grad_fn=<NegBackward0>) tensor(11757.2900, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11755.4228515625
tensor(11757.2900, grad_fn=<NegBackward0>) tensor(11755.4229, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11755.400390625
tensor(11755.4229, grad_fn=<NegBackward0>) tensor(11755.4004, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11755.375
tensor(11755.4004, grad_fn=<NegBackward0>) tensor(11755.3750, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11752.0068359375
tensor(11755.3750, grad_fn=<NegBackward0>) tensor(11752.0068, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11751.9892578125
tensor(11752.0068, grad_fn=<NegBackward0>) tensor(11751.9893, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11751.26171875
tensor(11751.9893, grad_fn=<NegBackward0>) tensor(11751.2617, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11751.23828125
tensor(11751.2617, grad_fn=<NegBackward0>) tensor(11751.2383, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11751.224609375
tensor(11751.2383, grad_fn=<NegBackward0>) tensor(11751.2246, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11751.2158203125
tensor(11751.2246, grad_fn=<NegBackward0>) tensor(11751.2158, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11751.2080078125
tensor(11751.2158, grad_fn=<NegBackward0>) tensor(11751.2080, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11751.189453125
tensor(11751.2080, grad_fn=<NegBackward0>) tensor(11751.1895, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11751.13671875
tensor(11751.1895, grad_fn=<NegBackward0>) tensor(11751.1367, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11751.1328125
tensor(11751.1367, grad_fn=<NegBackward0>) tensor(11751.1328, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11751.1298828125
tensor(11751.1328, grad_fn=<NegBackward0>) tensor(11751.1299, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11751.126953125
tensor(11751.1299, grad_fn=<NegBackward0>) tensor(11751.1270, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11751.125
tensor(11751.1270, grad_fn=<NegBackward0>) tensor(11751.1250, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11751.1240234375
tensor(11751.1250, grad_fn=<NegBackward0>) tensor(11751.1240, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11751.1220703125
tensor(11751.1240, grad_fn=<NegBackward0>) tensor(11751.1221, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11751.1220703125
tensor(11751.1221, grad_fn=<NegBackward0>) tensor(11751.1221, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11751.119140625
tensor(11751.1221, grad_fn=<NegBackward0>) tensor(11751.1191, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11751.1181640625
tensor(11751.1191, grad_fn=<NegBackward0>) tensor(11751.1182, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11751.1201171875
tensor(11751.1182, grad_fn=<NegBackward0>) tensor(11751.1201, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11751.1171875
tensor(11751.1182, grad_fn=<NegBackward0>) tensor(11751.1172, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11751.115234375
tensor(11751.1172, grad_fn=<NegBackward0>) tensor(11751.1152, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11751.1181640625
tensor(11751.1152, grad_fn=<NegBackward0>) tensor(11751.1182, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11751.1142578125
tensor(11751.1152, grad_fn=<NegBackward0>) tensor(11751.1143, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11751.1162109375
tensor(11751.1143, grad_fn=<NegBackward0>) tensor(11751.1162, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11751.1123046875
tensor(11751.1143, grad_fn=<NegBackward0>) tensor(11751.1123, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11751.1083984375
tensor(11751.1123, grad_fn=<NegBackward0>) tensor(11751.1084, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11751.1083984375
tensor(11751.1084, grad_fn=<NegBackward0>) tensor(11751.1084, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11751.1064453125
tensor(11751.1084, grad_fn=<NegBackward0>) tensor(11751.1064, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11751.1064453125
tensor(11751.1064, grad_fn=<NegBackward0>) tensor(11751.1064, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11751.10546875
tensor(11751.1064, grad_fn=<NegBackward0>) tensor(11751.1055, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11751.1103515625
tensor(11751.1055, grad_fn=<NegBackward0>) tensor(11751.1104, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11751.1044921875
tensor(11751.1055, grad_fn=<NegBackward0>) tensor(11751.1045, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11751.1044921875
tensor(11751.1045, grad_fn=<NegBackward0>) tensor(11751.1045, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11751.1123046875
tensor(11751.1045, grad_fn=<NegBackward0>) tensor(11751.1123, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11751.10546875
tensor(11751.1045, grad_fn=<NegBackward0>) tensor(11751.1055, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11751.103515625
tensor(11751.1045, grad_fn=<NegBackward0>) tensor(11751.1035, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11751.10546875
tensor(11751.1035, grad_fn=<NegBackward0>) tensor(11751.1055, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11751.103515625
tensor(11751.1035, grad_fn=<NegBackward0>) tensor(11751.1035, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11751.1015625
tensor(11751.1035, grad_fn=<NegBackward0>) tensor(11751.1016, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11751.1015625
tensor(11751.1016, grad_fn=<NegBackward0>) tensor(11751.1016, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11751.1015625
tensor(11751.1016, grad_fn=<NegBackward0>) tensor(11751.1016, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11751.1015625
tensor(11751.1016, grad_fn=<NegBackward0>) tensor(11751.1016, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11751.111328125
tensor(11751.1016, grad_fn=<NegBackward0>) tensor(11751.1113, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11751.1015625
tensor(11751.1016, grad_fn=<NegBackward0>) tensor(11751.1016, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11751.1005859375
tensor(11751.1016, grad_fn=<NegBackward0>) tensor(11751.1006, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11751.109375
tensor(11751.1006, grad_fn=<NegBackward0>) tensor(11751.1094, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11751.0986328125
tensor(11751.1006, grad_fn=<NegBackward0>) tensor(11751.0986, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11751.099609375
tensor(11751.0986, grad_fn=<NegBackward0>) tensor(11751.0996, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11751.0986328125
tensor(11751.0986, grad_fn=<NegBackward0>) tensor(11751.0986, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11751.0986328125
tensor(11751.0986, grad_fn=<NegBackward0>) tensor(11751.0986, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11751.0986328125
tensor(11751.0986, grad_fn=<NegBackward0>) tensor(11751.0986, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11751.1044921875
tensor(11751.0986, grad_fn=<NegBackward0>) tensor(11751.1045, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11751.0986328125
tensor(11751.0986, grad_fn=<NegBackward0>) tensor(11751.0986, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11751.09765625
tensor(11751.0986, grad_fn=<NegBackward0>) tensor(11751.0977, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11751.103515625
tensor(11751.0977, grad_fn=<NegBackward0>) tensor(11751.1035, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11751.0986328125
tensor(11751.0977, grad_fn=<NegBackward0>) tensor(11751.0986, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11751.09765625
tensor(11751.0977, grad_fn=<NegBackward0>) tensor(11751.0977, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11751.0966796875
tensor(11751.0977, grad_fn=<NegBackward0>) tensor(11751.0967, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11751.0986328125
tensor(11751.0967, grad_fn=<NegBackward0>) tensor(11751.0986, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11751.103515625
tensor(11751.0967, grad_fn=<NegBackward0>) tensor(11751.1035, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11751.1015625
tensor(11751.0967, grad_fn=<NegBackward0>) tensor(11751.1016, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11751.099609375
tensor(11751.0967, grad_fn=<NegBackward0>) tensor(11751.0996, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11751.0947265625
tensor(11751.0967, grad_fn=<NegBackward0>) tensor(11751.0947, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11751.095703125
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0957, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11751.095703125
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0957, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11751.09765625
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0977, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11751.0947265625
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0947, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11751.0947265625
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0947, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11751.11328125
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.1133, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11751.0947265625
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0947, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11751.0947265625
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0947, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11751.0947265625
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0947, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11751.095703125
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0957, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11751.0927734375
tensor(11751.0947, grad_fn=<NegBackward0>) tensor(11751.0928, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11751.0947265625
tensor(11751.0928, grad_fn=<NegBackward0>) tensor(11751.0947, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11751.09375
tensor(11751.0928, grad_fn=<NegBackward0>) tensor(11751.0938, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11751.1044921875
tensor(11751.0928, grad_fn=<NegBackward0>) tensor(11751.1045, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -11751.09375
tensor(11751.0928, grad_fn=<NegBackward0>) tensor(11751.0938, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -11751.3017578125
tensor(11751.0928, grad_fn=<NegBackward0>) tensor(11751.3018, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.3967, 0.6033],
        [0.2642, 0.7358]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5262, 0.4738], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4052, 0.1043],
         [0.6204, 0.2098]],

        [[0.6546, 0.0973],
         [0.5880, 0.6789]],

        [[0.6827, 0.1158],
         [0.7243, 0.6286]],

        [[0.5872, 0.1173],
         [0.5331, 0.6308]],

        [[0.7277, 0.1006],
         [0.6519, 0.6756]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.07344715387708843
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.03527450460349335
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207352941176471
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.2557176593177502
Average Adjusted Rand Index: 0.568266437163097
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21506.76171875
inf tensor(21506.7617, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12147.8134765625
tensor(21506.7617, grad_fn=<NegBackward0>) tensor(12147.8135, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11992.083984375
tensor(12147.8135, grad_fn=<NegBackward0>) tensor(11992.0840, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11987.17578125
tensor(11992.0840, grad_fn=<NegBackward0>) tensor(11987.1758, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11919.3232421875
tensor(11987.1758, grad_fn=<NegBackward0>) tensor(11919.3232, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11825.0263671875
tensor(11919.3232, grad_fn=<NegBackward0>) tensor(11825.0264, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11791.017578125
tensor(11825.0264, grad_fn=<NegBackward0>) tensor(11791.0176, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11790.6796875
tensor(11791.0176, grad_fn=<NegBackward0>) tensor(11790.6797, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11790.5634765625
tensor(11790.6797, grad_fn=<NegBackward0>) tensor(11790.5635, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11778.7724609375
tensor(11790.5635, grad_fn=<NegBackward0>) tensor(11778.7725, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11778.73046875
tensor(11778.7725, grad_fn=<NegBackward0>) tensor(11778.7305, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11778.7041015625
tensor(11778.7305, grad_fn=<NegBackward0>) tensor(11778.7041, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11778.6826171875
tensor(11778.7041, grad_fn=<NegBackward0>) tensor(11778.6826, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11778.66796875
tensor(11778.6826, grad_fn=<NegBackward0>) tensor(11778.6680, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11778.658203125
tensor(11778.6680, grad_fn=<NegBackward0>) tensor(11778.6582, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11778.6484375
tensor(11778.6582, grad_fn=<NegBackward0>) tensor(11778.6484, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11778.642578125
tensor(11778.6484, grad_fn=<NegBackward0>) tensor(11778.6426, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11778.63671875
tensor(11778.6426, grad_fn=<NegBackward0>) tensor(11778.6367, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11778.630859375
tensor(11778.6367, grad_fn=<NegBackward0>) tensor(11778.6309, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11778.626953125
tensor(11778.6309, grad_fn=<NegBackward0>) tensor(11778.6270, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11778.62109375
tensor(11778.6270, grad_fn=<NegBackward0>) tensor(11778.6211, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11778.619140625
tensor(11778.6211, grad_fn=<NegBackward0>) tensor(11778.6191, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11778.615234375
tensor(11778.6191, grad_fn=<NegBackward0>) tensor(11778.6152, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11778.6123046875
tensor(11778.6152, grad_fn=<NegBackward0>) tensor(11778.6123, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11778.609375
tensor(11778.6123, grad_fn=<NegBackward0>) tensor(11778.6094, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11778.607421875
tensor(11778.6094, grad_fn=<NegBackward0>) tensor(11778.6074, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11778.60546875
tensor(11778.6074, grad_fn=<NegBackward0>) tensor(11778.6055, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11778.6044921875
tensor(11778.6055, grad_fn=<NegBackward0>) tensor(11778.6045, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11778.6015625
tensor(11778.6045, grad_fn=<NegBackward0>) tensor(11778.6016, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11778.6005859375
tensor(11778.6016, grad_fn=<NegBackward0>) tensor(11778.6006, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11778.5986328125
tensor(11778.6006, grad_fn=<NegBackward0>) tensor(11778.5986, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11778.5986328125
tensor(11778.5986, grad_fn=<NegBackward0>) tensor(11778.5986, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11778.5966796875
tensor(11778.5986, grad_fn=<NegBackward0>) tensor(11778.5967, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11778.595703125
tensor(11778.5967, grad_fn=<NegBackward0>) tensor(11778.5957, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11778.5947265625
tensor(11778.5957, grad_fn=<NegBackward0>) tensor(11778.5947, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11778.59375
tensor(11778.5947, grad_fn=<NegBackward0>) tensor(11778.5938, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11778.5927734375
tensor(11778.5938, grad_fn=<NegBackward0>) tensor(11778.5928, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11778.5927734375
tensor(11778.5928, grad_fn=<NegBackward0>) tensor(11778.5928, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11778.5908203125
tensor(11778.5928, grad_fn=<NegBackward0>) tensor(11778.5908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11778.591796875
tensor(11778.5908, grad_fn=<NegBackward0>) tensor(11778.5918, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11778.5908203125
tensor(11778.5908, grad_fn=<NegBackward0>) tensor(11778.5908, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11778.5888671875
tensor(11778.5908, grad_fn=<NegBackward0>) tensor(11778.5889, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11778.5888671875
tensor(11778.5889, grad_fn=<NegBackward0>) tensor(11778.5889, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11778.587890625
tensor(11778.5889, grad_fn=<NegBackward0>) tensor(11778.5879, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11778.5908203125
tensor(11778.5879, grad_fn=<NegBackward0>) tensor(11778.5908, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11778.587890625
tensor(11778.5879, grad_fn=<NegBackward0>) tensor(11778.5879, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11778.583984375
tensor(11778.5879, grad_fn=<NegBackward0>) tensor(11778.5840, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11778.5166015625
tensor(11778.5840, grad_fn=<NegBackward0>) tensor(11778.5166, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11778.5166015625
tensor(11778.5166, grad_fn=<NegBackward0>) tensor(11778.5166, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11778.515625
tensor(11778.5166, grad_fn=<NegBackward0>) tensor(11778.5156, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11778.5166015625
tensor(11778.5156, grad_fn=<NegBackward0>) tensor(11778.5166, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11778.515625
tensor(11778.5156, grad_fn=<NegBackward0>) tensor(11778.5156, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11778.5146484375
tensor(11778.5156, grad_fn=<NegBackward0>) tensor(11778.5146, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11778.51953125
tensor(11778.5146, grad_fn=<NegBackward0>) tensor(11778.5195, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11778.515625
tensor(11778.5146, grad_fn=<NegBackward0>) tensor(11778.5156, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11778.513671875
tensor(11778.5146, grad_fn=<NegBackward0>) tensor(11778.5137, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11778.517578125
tensor(11778.5137, grad_fn=<NegBackward0>) tensor(11778.5176, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11778.5146484375
tensor(11778.5137, grad_fn=<NegBackward0>) tensor(11778.5146, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11778.515625
tensor(11778.5137, grad_fn=<NegBackward0>) tensor(11778.5156, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11778.5146484375
tensor(11778.5137, grad_fn=<NegBackward0>) tensor(11778.5146, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11778.513671875
tensor(11778.5137, grad_fn=<NegBackward0>) tensor(11778.5137, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11778.51171875
tensor(11778.5137, grad_fn=<NegBackward0>) tensor(11778.5117, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11778.5126953125
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5127, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11778.51171875
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5117, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11778.515625
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5156, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11778.513671875
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5137, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11778.513671875
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5137, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11778.517578125
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5176, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -11778.51171875
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5117, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11778.51171875
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5117, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11778.51171875
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5117, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11778.5107421875
tensor(11778.5117, grad_fn=<NegBackward0>) tensor(11778.5107, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11778.51171875
tensor(11778.5107, grad_fn=<NegBackward0>) tensor(11778.5117, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11778.5107421875
tensor(11778.5107, grad_fn=<NegBackward0>) tensor(11778.5107, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11778.51171875
tensor(11778.5107, grad_fn=<NegBackward0>) tensor(11778.5117, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11778.513671875
tensor(11778.5107, grad_fn=<NegBackward0>) tensor(11778.5137, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11778.44140625
tensor(11778.5107, grad_fn=<NegBackward0>) tensor(11778.4414, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11778.439453125
tensor(11778.4414, grad_fn=<NegBackward0>) tensor(11778.4395, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11778.4404296875
tensor(11778.4395, grad_fn=<NegBackward0>) tensor(11778.4404, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11778.3330078125
tensor(11778.4395, grad_fn=<NegBackward0>) tensor(11778.3330, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11778.3330078125
tensor(11778.3330, grad_fn=<NegBackward0>) tensor(11778.3330, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11778.33984375
tensor(11778.3330, grad_fn=<NegBackward0>) tensor(11778.3398, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11778.33203125
tensor(11778.3330, grad_fn=<NegBackward0>) tensor(11778.3320, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11778.33203125
tensor(11778.3320, grad_fn=<NegBackward0>) tensor(11778.3320, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11778.33203125
tensor(11778.3320, grad_fn=<NegBackward0>) tensor(11778.3320, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11778.33203125
tensor(11778.3320, grad_fn=<NegBackward0>) tensor(11778.3320, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11778.40625
tensor(11778.3320, grad_fn=<NegBackward0>) tensor(11778.4062, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11778.33203125
tensor(11778.3320, grad_fn=<NegBackward0>) tensor(11778.3320, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11778.3408203125
tensor(11778.3320, grad_fn=<NegBackward0>) tensor(11778.3408, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11778.3310546875
tensor(11778.3320, grad_fn=<NegBackward0>) tensor(11778.3311, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11778.33203125
tensor(11778.3311, grad_fn=<NegBackward0>) tensor(11778.3320, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11778.3369140625
tensor(11778.3311, grad_fn=<NegBackward0>) tensor(11778.3369, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11778.3310546875
tensor(11778.3311, grad_fn=<NegBackward0>) tensor(11778.3311, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11778.37109375
tensor(11778.3311, grad_fn=<NegBackward0>) tensor(11778.3711, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11778.328125
tensor(11778.3311, grad_fn=<NegBackward0>) tensor(11778.3281, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11778.328125
tensor(11778.3281, grad_fn=<NegBackward0>) tensor(11778.3281, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11778.328125
tensor(11778.3281, grad_fn=<NegBackward0>) tensor(11778.3281, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11778.328125
tensor(11778.3281, grad_fn=<NegBackward0>) tensor(11778.3281, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11778.328125
tensor(11778.3281, grad_fn=<NegBackward0>) tensor(11778.3281, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11778.326171875
tensor(11778.3281, grad_fn=<NegBackward0>) tensor(11778.3262, grad_fn=<NegBackward0>)
pi: tensor([[0.6064, 0.3936],
        [0.7527, 0.2473]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4716, 0.5284], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2284, 0.1042],
         [0.5648, 0.3773]],

        [[0.5318, 0.1046],
         [0.7147, 0.5317]],

        [[0.6608, 0.1022],
         [0.5788, 0.7082]],

        [[0.6168, 0.1145],
         [0.6759, 0.6382]],

        [[0.7256, 0.0959],
         [0.6751, 0.5349]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 23
Adjusted Rand Index: 0.2854942233632863
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9202875629043853
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 28
Adjusted Rand Index: 0.18545454545454546
Global Adjusted Rand Index: 0.1451427547319202
Average Adjusted Rand Index: 0.6544008771919694
[0.2557176593177502, 0.1451427547319202] [0.568266437163097, 0.6544008771919694] [11751.3017578125, 11771.185546875]
-------------------------------------
This iteration is 65
True Objective function: Loss = -11555.58082767735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21097.46875
inf tensor(21097.4688, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12488.2724609375
tensor(21097.4688, grad_fn=<NegBackward0>) tensor(12488.2725, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12116.5556640625
tensor(12488.2725, grad_fn=<NegBackward0>) tensor(12116.5557, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11591.5244140625
tensor(12116.5557, grad_fn=<NegBackward0>) tensor(11591.5244, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11570.6787109375
tensor(11591.5244, grad_fn=<NegBackward0>) tensor(11570.6787, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11569.7509765625
tensor(11570.6787, grad_fn=<NegBackward0>) tensor(11569.7510, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11563.33203125
tensor(11569.7510, grad_fn=<NegBackward0>) tensor(11563.3320, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11554.5947265625
tensor(11563.3320, grad_fn=<NegBackward0>) tensor(11554.5947, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11554.2890625
tensor(11554.5947, grad_fn=<NegBackward0>) tensor(11554.2891, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11554.169921875
tensor(11554.2891, grad_fn=<NegBackward0>) tensor(11554.1699, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11554.0859375
tensor(11554.1699, grad_fn=<NegBackward0>) tensor(11554.0859, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11554.0224609375
tensor(11554.0859, grad_fn=<NegBackward0>) tensor(11554.0225, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11553.97265625
tensor(11554.0225, grad_fn=<NegBackward0>) tensor(11553.9727, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11553.9345703125
tensor(11553.9727, grad_fn=<NegBackward0>) tensor(11553.9346, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11553.9033203125
tensor(11553.9346, grad_fn=<NegBackward0>) tensor(11553.9033, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11553.876953125
tensor(11553.9033, grad_fn=<NegBackward0>) tensor(11553.8770, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11553.8564453125
tensor(11553.8770, grad_fn=<NegBackward0>) tensor(11553.8564, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11553.837890625
tensor(11553.8564, grad_fn=<NegBackward0>) tensor(11553.8379, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11553.822265625
tensor(11553.8379, grad_fn=<NegBackward0>) tensor(11553.8223, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11553.80859375
tensor(11553.8223, grad_fn=<NegBackward0>) tensor(11553.8086, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11553.798828125
tensor(11553.8086, grad_fn=<NegBackward0>) tensor(11553.7988, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11553.7890625
tensor(11553.7988, grad_fn=<NegBackward0>) tensor(11553.7891, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11553.7802734375
tensor(11553.7891, grad_fn=<NegBackward0>) tensor(11553.7803, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11553.771484375
tensor(11553.7803, grad_fn=<NegBackward0>) tensor(11553.7715, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11553.7646484375
tensor(11553.7715, grad_fn=<NegBackward0>) tensor(11553.7646, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11553.7578125
tensor(11553.7646, grad_fn=<NegBackward0>) tensor(11553.7578, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11553.75390625
tensor(11553.7578, grad_fn=<NegBackward0>) tensor(11553.7539, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11553.748046875
tensor(11553.7539, grad_fn=<NegBackward0>) tensor(11553.7480, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11553.7421875
tensor(11553.7480, grad_fn=<NegBackward0>) tensor(11553.7422, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11546.248046875
tensor(11553.7422, grad_fn=<NegBackward0>) tensor(11546.2480, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11546.23828125
tensor(11546.2480, grad_fn=<NegBackward0>) tensor(11546.2383, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11546.2333984375
tensor(11546.2383, grad_fn=<NegBackward0>) tensor(11546.2334, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11546.23046875
tensor(11546.2334, grad_fn=<NegBackward0>) tensor(11546.2305, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11546.228515625
tensor(11546.2305, grad_fn=<NegBackward0>) tensor(11546.2285, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11546.224609375
tensor(11546.2285, grad_fn=<NegBackward0>) tensor(11546.2246, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11546.220703125
tensor(11546.2246, grad_fn=<NegBackward0>) tensor(11546.2207, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11546.2158203125
tensor(11546.2207, grad_fn=<NegBackward0>) tensor(11546.2158, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11546.1845703125
tensor(11546.2158, grad_fn=<NegBackward0>) tensor(11546.1846, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11546.181640625
tensor(11546.1846, grad_fn=<NegBackward0>) tensor(11546.1816, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11546.1806640625
tensor(11546.1816, grad_fn=<NegBackward0>) tensor(11546.1807, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11546.1796875
tensor(11546.1807, grad_fn=<NegBackward0>) tensor(11546.1797, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11546.1748046875
tensor(11546.1797, grad_fn=<NegBackward0>) tensor(11546.1748, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11546.1748046875
tensor(11546.1748, grad_fn=<NegBackward0>) tensor(11546.1748, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11546.1728515625
tensor(11546.1748, grad_fn=<NegBackward0>) tensor(11546.1729, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11546.1728515625
tensor(11546.1729, grad_fn=<NegBackward0>) tensor(11546.1729, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11546.169921875
tensor(11546.1729, grad_fn=<NegBackward0>) tensor(11546.1699, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11546.169921875
tensor(11546.1699, grad_fn=<NegBackward0>) tensor(11546.1699, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11546.171875
tensor(11546.1699, grad_fn=<NegBackward0>) tensor(11546.1719, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11546.16796875
tensor(11546.1699, grad_fn=<NegBackward0>) tensor(11546.1680, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11546.1669921875
tensor(11546.1680, grad_fn=<NegBackward0>) tensor(11546.1670, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11546.166015625
tensor(11546.1670, grad_fn=<NegBackward0>) tensor(11546.1660, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11546.1650390625
tensor(11546.1660, grad_fn=<NegBackward0>) tensor(11546.1650, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11546.1640625
tensor(11546.1650, grad_fn=<NegBackward0>) tensor(11546.1641, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11546.1630859375
tensor(11546.1641, grad_fn=<NegBackward0>) tensor(11546.1631, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11546.1630859375
tensor(11546.1631, grad_fn=<NegBackward0>) tensor(11546.1631, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11546.166015625
tensor(11546.1631, grad_fn=<NegBackward0>) tensor(11546.1660, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11546.162109375
tensor(11546.1631, grad_fn=<NegBackward0>) tensor(11546.1621, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11546.1630859375
tensor(11546.1621, grad_fn=<NegBackward0>) tensor(11546.1631, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11546.1611328125
tensor(11546.1621, grad_fn=<NegBackward0>) tensor(11546.1611, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11546.162109375
tensor(11546.1611, grad_fn=<NegBackward0>) tensor(11546.1621, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11546.1650390625
tensor(11546.1611, grad_fn=<NegBackward0>) tensor(11546.1650, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11546.16015625
tensor(11546.1611, grad_fn=<NegBackward0>) tensor(11546.1602, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11546.1591796875
tensor(11546.1602, grad_fn=<NegBackward0>) tensor(11546.1592, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11546.1591796875
tensor(11546.1592, grad_fn=<NegBackward0>) tensor(11546.1592, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11546.1591796875
tensor(11546.1592, grad_fn=<NegBackward0>) tensor(11546.1592, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11546.16015625
tensor(11546.1592, grad_fn=<NegBackward0>) tensor(11546.1602, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11546.1572265625
tensor(11546.1592, grad_fn=<NegBackward0>) tensor(11546.1572, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11546.1572265625
tensor(11546.1572, grad_fn=<NegBackward0>) tensor(11546.1572, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11546.158203125
tensor(11546.1572, grad_fn=<NegBackward0>) tensor(11546.1582, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11546.15625
tensor(11546.1572, grad_fn=<NegBackward0>) tensor(11546.1562, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11546.16015625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1602, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11546.15625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1562, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11546.1572265625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1572, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11546.1572265625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1572, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11546.15625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1562, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11546.1572265625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1572, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11546.1572265625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1572, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11546.2783203125
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.2783, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11546.15625
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1562, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11546.1552734375
tensor(11546.1562, grad_fn=<NegBackward0>) tensor(11546.1553, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11546.1552734375
tensor(11546.1553, grad_fn=<NegBackward0>) tensor(11546.1553, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11546.154296875
tensor(11546.1553, grad_fn=<NegBackward0>) tensor(11546.1543, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11546.16015625
tensor(11546.1543, grad_fn=<NegBackward0>) tensor(11546.1602, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11546.169921875
tensor(11546.1543, grad_fn=<NegBackward0>) tensor(11546.1699, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11546.169921875
tensor(11546.1543, grad_fn=<NegBackward0>) tensor(11546.1699, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11546.1552734375
tensor(11546.1543, grad_fn=<NegBackward0>) tensor(11546.1553, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11546.22265625
tensor(11546.1543, grad_fn=<NegBackward0>) tensor(11546.2227, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7559, 0.2441],
        [0.2430, 0.7570]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4598, 0.5402], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.1039],
         [0.6537, 0.4180]],

        [[0.5034, 0.0911],
         [0.6402, 0.5759]],

        [[0.5403, 0.1052],
         [0.6629, 0.5587]],

        [[0.6030, 0.0996],
         [0.5494, 0.5624]],

        [[0.5826, 0.0913],
         [0.5881, 0.7097]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22274.166015625
inf tensor(22274.1660, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12152.69921875
tensor(22274.1660, grad_fn=<NegBackward0>) tensor(12152.6992, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11673.59765625
tensor(12152.6992, grad_fn=<NegBackward0>) tensor(11673.5977, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11645.74609375
tensor(11673.5977, grad_fn=<NegBackward0>) tensor(11645.7461, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11634.7578125
tensor(11645.7461, grad_fn=<NegBackward0>) tensor(11634.7578, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11618.951171875
tensor(11634.7578, grad_fn=<NegBackward0>) tensor(11618.9512, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11618.736328125
tensor(11618.9512, grad_fn=<NegBackward0>) tensor(11618.7363, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11605.6279296875
tensor(11618.7363, grad_fn=<NegBackward0>) tensor(11605.6279, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11593.8642578125
tensor(11605.6279, grad_fn=<NegBackward0>) tensor(11593.8643, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11579.9248046875
tensor(11593.8643, grad_fn=<NegBackward0>) tensor(11579.9248, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11579.873046875
tensor(11579.9248, grad_fn=<NegBackward0>) tensor(11579.8730, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11579.8349609375
tensor(11579.8730, grad_fn=<NegBackward0>) tensor(11579.8350, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11579.8046875
tensor(11579.8350, grad_fn=<NegBackward0>) tensor(11579.8047, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11579.779296875
tensor(11579.8047, grad_fn=<NegBackward0>) tensor(11579.7793, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11579.7587890625
tensor(11579.7793, grad_fn=<NegBackward0>) tensor(11579.7588, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11572.546875
tensor(11579.7588, grad_fn=<NegBackward0>) tensor(11572.5469, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11572.521484375
tensor(11572.5469, grad_fn=<NegBackward0>) tensor(11572.5215, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11572.5107421875
tensor(11572.5215, grad_fn=<NegBackward0>) tensor(11572.5107, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11572.501953125
tensor(11572.5107, grad_fn=<NegBackward0>) tensor(11572.5020, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11572.4931640625
tensor(11572.5020, grad_fn=<NegBackward0>) tensor(11572.4932, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11572.4853515625
tensor(11572.4932, grad_fn=<NegBackward0>) tensor(11572.4854, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11561.134765625
tensor(11572.4854, grad_fn=<NegBackward0>) tensor(11561.1348, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11561.126953125
tensor(11561.1348, grad_fn=<NegBackward0>) tensor(11561.1270, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11561.12109375
tensor(11561.1270, grad_fn=<NegBackward0>) tensor(11561.1211, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11561.1162109375
tensor(11561.1211, grad_fn=<NegBackward0>) tensor(11561.1162, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11561.111328125
tensor(11561.1162, grad_fn=<NegBackward0>) tensor(11561.1113, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11561.107421875
tensor(11561.1113, grad_fn=<NegBackward0>) tensor(11561.1074, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11552.43359375
tensor(11561.1074, grad_fn=<NegBackward0>) tensor(11552.4336, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11552.421875
tensor(11552.4336, grad_fn=<NegBackward0>) tensor(11552.4219, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11552.419921875
tensor(11552.4219, grad_fn=<NegBackward0>) tensor(11552.4199, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11552.41796875
tensor(11552.4199, grad_fn=<NegBackward0>) tensor(11552.4180, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11552.416015625
tensor(11552.4180, grad_fn=<NegBackward0>) tensor(11552.4160, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11552.4140625
tensor(11552.4160, grad_fn=<NegBackward0>) tensor(11552.4141, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11552.4130859375
tensor(11552.4141, grad_fn=<NegBackward0>) tensor(11552.4131, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11552.4111328125
tensor(11552.4131, grad_fn=<NegBackward0>) tensor(11552.4111, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11552.4111328125
tensor(11552.4111, grad_fn=<NegBackward0>) tensor(11552.4111, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11552.4091796875
tensor(11552.4111, grad_fn=<NegBackward0>) tensor(11552.4092, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11552.4072265625
tensor(11552.4092, grad_fn=<NegBackward0>) tensor(11552.4072, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11552.4072265625
tensor(11552.4072, grad_fn=<NegBackward0>) tensor(11552.4072, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11552.40625
tensor(11552.4072, grad_fn=<NegBackward0>) tensor(11552.4062, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11552.4033203125
tensor(11552.4062, grad_fn=<NegBackward0>) tensor(11552.4033, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11552.4033203125
tensor(11552.4033, grad_fn=<NegBackward0>) tensor(11552.4033, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11546.2080078125
tensor(11552.4033, grad_fn=<NegBackward0>) tensor(11546.2080, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11546.203125
tensor(11546.2080, grad_fn=<NegBackward0>) tensor(11546.2031, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11546.203125
tensor(11546.2031, grad_fn=<NegBackward0>) tensor(11546.2031, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11546.203125
tensor(11546.2031, grad_fn=<NegBackward0>) tensor(11546.2031, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11546.2021484375
tensor(11546.2031, grad_fn=<NegBackward0>) tensor(11546.2021, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11546.2021484375
tensor(11546.2021, grad_fn=<NegBackward0>) tensor(11546.2021, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11546.201171875
tensor(11546.2021, grad_fn=<NegBackward0>) tensor(11546.2012, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11546.2001953125
tensor(11546.2012, grad_fn=<NegBackward0>) tensor(11546.2002, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11546.201171875
tensor(11546.2002, grad_fn=<NegBackward0>) tensor(11546.2012, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11546.19921875
tensor(11546.2002, grad_fn=<NegBackward0>) tensor(11546.1992, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11546.19921875
tensor(11546.1992, grad_fn=<NegBackward0>) tensor(11546.1992, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11546.2001953125
tensor(11546.1992, grad_fn=<NegBackward0>) tensor(11546.2002, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11546.203125
tensor(11546.1992, grad_fn=<NegBackward0>) tensor(11546.2031, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11546.1982421875
tensor(11546.1992, grad_fn=<NegBackward0>) tensor(11546.1982, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11546.1982421875
tensor(11546.1982, grad_fn=<NegBackward0>) tensor(11546.1982, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11546.1982421875
tensor(11546.1982, grad_fn=<NegBackward0>) tensor(11546.1982, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11546.1982421875
tensor(11546.1982, grad_fn=<NegBackward0>) tensor(11546.1982, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11546.19921875
tensor(11546.1982, grad_fn=<NegBackward0>) tensor(11546.1992, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11546.197265625
tensor(11546.1982, grad_fn=<NegBackward0>) tensor(11546.1973, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11546.2041015625
tensor(11546.1973, grad_fn=<NegBackward0>) tensor(11546.2041, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11546.1962890625
tensor(11546.1973, grad_fn=<NegBackward0>) tensor(11546.1963, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11546.197265625
tensor(11546.1963, grad_fn=<NegBackward0>) tensor(11546.1973, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11546.1962890625
tensor(11546.1963, grad_fn=<NegBackward0>) tensor(11546.1963, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11546.1953125
tensor(11546.1963, grad_fn=<NegBackward0>) tensor(11546.1953, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11546.1962890625
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1963, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11546.1962890625
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1963, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11546.1953125
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1953, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11546.1953125
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1953, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11546.1962890625
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1963, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11546.1953125
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1953, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11546.205078125
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.2051, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11546.1962890625
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1963, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11546.1943359375
tensor(11546.1953, grad_fn=<NegBackward0>) tensor(11546.1943, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11546.197265625
tensor(11546.1943, grad_fn=<NegBackward0>) tensor(11546.1973, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11546.203125
tensor(11546.1943, grad_fn=<NegBackward0>) tensor(11546.2031, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11546.1943359375
tensor(11546.1943, grad_fn=<NegBackward0>) tensor(11546.1943, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11546.193359375
tensor(11546.1943, grad_fn=<NegBackward0>) tensor(11546.1934, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11546.1943359375
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1943, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11546.193359375
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1934, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11546.1943359375
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1943, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11546.193359375
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1934, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11546.1962890625
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1963, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11546.193359375
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1934, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11546.1943359375
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1943, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11546.193359375
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1934, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11546.1953125
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1953, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11546.1923828125
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11546.1924, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11546.1923828125
tensor(11546.1924, grad_fn=<NegBackward0>) tensor(11546.1924, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11546.1943359375
tensor(11546.1924, grad_fn=<NegBackward0>) tensor(11546.1943, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11546.208984375
tensor(11546.1924, grad_fn=<NegBackward0>) tensor(11546.2090, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11546.1923828125
tensor(11546.1924, grad_fn=<NegBackward0>) tensor(11546.1924, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11546.1865234375
tensor(11546.1924, grad_fn=<NegBackward0>) tensor(11546.1865, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11546.2041015625
tensor(11546.1865, grad_fn=<NegBackward0>) tensor(11546.2041, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11546.185546875
tensor(11546.1865, grad_fn=<NegBackward0>) tensor(11546.1855, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11546.1875
tensor(11546.1855, grad_fn=<NegBackward0>) tensor(11546.1875, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11546.185546875
tensor(11546.1855, grad_fn=<NegBackward0>) tensor(11546.1855, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11546.185546875
tensor(11546.1855, grad_fn=<NegBackward0>) tensor(11546.1855, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11546.1884765625
tensor(11546.1855, grad_fn=<NegBackward0>) tensor(11546.1885, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7563, 0.2437],
        [0.2487, 0.7513]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4603, 0.5397], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.1040],
         [0.5162, 0.4171]],

        [[0.5979, 0.0914],
         [0.5067, 0.5058]],

        [[0.6614, 0.1053],
         [0.6514, 0.5511]],

        [[0.6589, 0.0996],
         [0.7072, 0.6661]],

        [[0.7220, 0.0913],
         [0.5180, 0.5315]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919993417272899
[0.9919999775871758, 0.9919999775871758] [0.9919993417272899, 0.9919993417272899] [11546.22265625, 11546.2314453125]
-------------------------------------
This iteration is 66
True Objective function: Loss = -11682.475543641836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21212.953125
inf tensor(21212.9531, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12555.564453125
tensor(21212.9531, grad_fn=<NegBackward0>) tensor(12555.5645, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12050.1220703125
tensor(12555.5645, grad_fn=<NegBackward0>) tensor(12050.1221, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11876.2578125
tensor(12050.1221, grad_fn=<NegBackward0>) tensor(11876.2578, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11838.900390625
tensor(11876.2578, grad_fn=<NegBackward0>) tensor(11838.9004, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11778.04296875
tensor(11838.9004, grad_fn=<NegBackward0>) tensor(11778.0430, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11741.2802734375
tensor(11778.0430, grad_fn=<NegBackward0>) tensor(11741.2803, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11726.4365234375
tensor(11741.2803, grad_fn=<NegBackward0>) tensor(11726.4365, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11726.3134765625
tensor(11726.4365, grad_fn=<NegBackward0>) tensor(11726.3135, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11726.23828125
tensor(11726.3135, grad_fn=<NegBackward0>) tensor(11726.2383, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11719.3095703125
tensor(11726.2383, grad_fn=<NegBackward0>) tensor(11719.3096, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11719.271484375
tensor(11719.3096, grad_fn=<NegBackward0>) tensor(11719.2715, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11719.2451171875
tensor(11719.2715, grad_fn=<NegBackward0>) tensor(11719.2451, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11719.2216796875
tensor(11719.2451, grad_fn=<NegBackward0>) tensor(11719.2217, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11719.2060546875
tensor(11719.2217, grad_fn=<NegBackward0>) tensor(11719.2061, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11719.19140625
tensor(11719.2061, grad_fn=<NegBackward0>) tensor(11719.1914, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11719.1796875
tensor(11719.1914, grad_fn=<NegBackward0>) tensor(11719.1797, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11719.1494140625
tensor(11719.1797, grad_fn=<NegBackward0>) tensor(11719.1494, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11712.6220703125
tensor(11719.1494, grad_fn=<NegBackward0>) tensor(11712.6221, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11712.6142578125
tensor(11712.6221, grad_fn=<NegBackward0>) tensor(11712.6143, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11712.607421875
tensor(11712.6143, grad_fn=<NegBackward0>) tensor(11712.6074, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11712.6015625
tensor(11712.6074, grad_fn=<NegBackward0>) tensor(11712.6016, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11712.59765625
tensor(11712.6016, grad_fn=<NegBackward0>) tensor(11712.5977, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11712.59375
tensor(11712.5977, grad_fn=<NegBackward0>) tensor(11712.5938, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11712.5888671875
tensor(11712.5938, grad_fn=<NegBackward0>) tensor(11712.5889, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11695.4677734375
tensor(11712.5889, grad_fn=<NegBackward0>) tensor(11695.4678, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11695.455078125
tensor(11695.4678, grad_fn=<NegBackward0>) tensor(11695.4551, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11682.8125
tensor(11695.4551, grad_fn=<NegBackward0>) tensor(11682.8125, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11682.732421875
tensor(11682.8125, grad_fn=<NegBackward0>) tensor(11682.7324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11682.7265625
tensor(11682.7324, grad_fn=<NegBackward0>) tensor(11682.7266, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11682.681640625
tensor(11682.7266, grad_fn=<NegBackward0>) tensor(11682.6816, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11682.6796875
tensor(11682.6816, grad_fn=<NegBackward0>) tensor(11682.6797, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11682.6787109375
tensor(11682.6797, grad_fn=<NegBackward0>) tensor(11682.6787, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11682.677734375
tensor(11682.6787, grad_fn=<NegBackward0>) tensor(11682.6777, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11682.67578125
tensor(11682.6777, grad_fn=<NegBackward0>) tensor(11682.6758, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11682.673828125
tensor(11682.6758, grad_fn=<NegBackward0>) tensor(11682.6738, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11682.671875
tensor(11682.6738, grad_fn=<NegBackward0>) tensor(11682.6719, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11682.671875
tensor(11682.6719, grad_fn=<NegBackward0>) tensor(11682.6719, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11682.6875
tensor(11682.6719, grad_fn=<NegBackward0>) tensor(11682.6875, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11682.6708984375
tensor(11682.6719, grad_fn=<NegBackward0>) tensor(11682.6709, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11682.66796875
tensor(11682.6709, grad_fn=<NegBackward0>) tensor(11682.6680, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11682.66796875
tensor(11682.6680, grad_fn=<NegBackward0>) tensor(11682.6680, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11682.6669921875
tensor(11682.6680, grad_fn=<NegBackward0>) tensor(11682.6670, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11682.6669921875
tensor(11682.6670, grad_fn=<NegBackward0>) tensor(11682.6670, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11678.0185546875
tensor(11682.6670, grad_fn=<NegBackward0>) tensor(11678.0186, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11678.0166015625
tensor(11678.0186, grad_fn=<NegBackward0>) tensor(11678.0166, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11678.017578125
tensor(11678.0166, grad_fn=<NegBackward0>) tensor(11678.0176, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11678.015625
tensor(11678.0166, grad_fn=<NegBackward0>) tensor(11678.0156, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11678.015625
tensor(11678.0156, grad_fn=<NegBackward0>) tensor(11678.0156, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11678.015625
tensor(11678.0156, grad_fn=<NegBackward0>) tensor(11678.0156, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11678.0166015625
tensor(11678.0156, grad_fn=<NegBackward0>) tensor(11678.0166, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11678.013671875
tensor(11678.0156, grad_fn=<NegBackward0>) tensor(11678.0137, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11678.0126953125
tensor(11678.0137, grad_fn=<NegBackward0>) tensor(11678.0127, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11678.013671875
tensor(11678.0127, grad_fn=<NegBackward0>) tensor(11678.0137, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11678.0107421875
tensor(11678.0127, grad_fn=<NegBackward0>) tensor(11678.0107, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11678.01171875
tensor(11678.0107, grad_fn=<NegBackward0>) tensor(11678.0117, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11678.0107421875
tensor(11678.0107, grad_fn=<NegBackward0>) tensor(11678.0107, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11678.0107421875
tensor(11678.0107, grad_fn=<NegBackward0>) tensor(11678.0107, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11678.0107421875
tensor(11678.0107, grad_fn=<NegBackward0>) tensor(11678.0107, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11678.009765625
tensor(11678.0107, grad_fn=<NegBackward0>) tensor(11678.0098, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11678.0087890625
tensor(11678.0098, grad_fn=<NegBackward0>) tensor(11678.0088, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11678.009765625
tensor(11678.0088, grad_fn=<NegBackward0>) tensor(11678.0098, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11678.0087890625
tensor(11678.0088, grad_fn=<NegBackward0>) tensor(11678.0088, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11678.0078125
tensor(11678.0088, grad_fn=<NegBackward0>) tensor(11678.0078, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11678.0087890625
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0088, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11678.009765625
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0098, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11678.01171875
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0117, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11678.0078125
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0078, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11678.0078125
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0078, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11678.0078125
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0078, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11678.0087890625
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0088, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11678.0068359375
tensor(11678.0078, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11678.0068359375
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11678.0068359375
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11678.0068359375
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11678.0078125
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0078, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11678.013671875
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0137, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11678.0087890625
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0088, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11678.005859375
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0059, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11678.0166015625
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0166, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11678.009765625
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0098, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11678.0068359375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11678.0068359375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11678.005859375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0059, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11678.005859375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0059, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11678.005859375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0059, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11678.01171875
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0117, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11678.0078125
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0078, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11678.0205078125
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0205, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11678.01171875
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0117, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11678.01171875
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0117, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7508, 0.2492],
        [0.2023, 0.7977]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5399, 0.4601], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2036, 0.1002],
         [0.5193, 0.4095]],

        [[0.7117, 0.1064],
         [0.6715, 0.6686]],

        [[0.5459, 0.0943],
         [0.7174, 0.5441]],

        [[0.6490, 0.1003],
         [0.6325, 0.6193]],

        [[0.7067, 0.1027],
         [0.6787, 0.5670]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22166.232421875
inf tensor(22166.2324, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12570.3720703125
tensor(22166.2324, grad_fn=<NegBackward0>) tensor(12570.3721, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12296.142578125
tensor(12570.3721, grad_fn=<NegBackward0>) tensor(12296.1426, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11849.5048828125
tensor(12296.1426, grad_fn=<NegBackward0>) tensor(11849.5049, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11754.0166015625
tensor(11849.5049, grad_fn=<NegBackward0>) tensor(11754.0166, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11722.171875
tensor(11754.0166, grad_fn=<NegBackward0>) tensor(11722.1719, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11706.88671875
tensor(11722.1719, grad_fn=<NegBackward0>) tensor(11706.8867, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11698.8212890625
tensor(11706.8867, grad_fn=<NegBackward0>) tensor(11698.8213, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11692.0
tensor(11698.8213, grad_fn=<NegBackward0>) tensor(11692., grad_fn=<NegBackward0>)
Iteration 900: Loss = -11691.9501953125
tensor(11692., grad_fn=<NegBackward0>) tensor(11691.9502, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11691.916015625
tensor(11691.9502, grad_fn=<NegBackward0>) tensor(11691.9160, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11691.8857421875
tensor(11691.9160, grad_fn=<NegBackward0>) tensor(11691.8857, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11691.8623046875
tensor(11691.8857, grad_fn=<NegBackward0>) tensor(11691.8623, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11682.7470703125
tensor(11691.8623, grad_fn=<NegBackward0>) tensor(11682.7471, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11682.732421875
tensor(11682.7471, grad_fn=<NegBackward0>) tensor(11682.7324, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11682.72265625
tensor(11682.7324, grad_fn=<NegBackward0>) tensor(11682.7227, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11682.7138671875
tensor(11682.7227, grad_fn=<NegBackward0>) tensor(11682.7139, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11682.7080078125
tensor(11682.7139, grad_fn=<NegBackward0>) tensor(11682.7080, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11682.701171875
tensor(11682.7080, grad_fn=<NegBackward0>) tensor(11682.7012, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11682.697265625
tensor(11682.7012, grad_fn=<NegBackward0>) tensor(11682.6973, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11682.6943359375
tensor(11682.6973, grad_fn=<NegBackward0>) tensor(11682.6943, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11682.6904296875
tensor(11682.6943, grad_fn=<NegBackward0>) tensor(11682.6904, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11682.6875
tensor(11682.6904, grad_fn=<NegBackward0>) tensor(11682.6875, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11682.6845703125
tensor(11682.6875, grad_fn=<NegBackward0>) tensor(11682.6846, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11682.681640625
tensor(11682.6846, grad_fn=<NegBackward0>) tensor(11682.6816, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11682.6796875
tensor(11682.6816, grad_fn=<NegBackward0>) tensor(11682.6797, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11682.6826171875
tensor(11682.6797, grad_fn=<NegBackward0>) tensor(11682.6826, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11682.67578125
tensor(11682.6797, grad_fn=<NegBackward0>) tensor(11682.6758, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11682.6748046875
tensor(11682.6758, grad_fn=<NegBackward0>) tensor(11682.6748, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11682.671875
tensor(11682.6748, grad_fn=<NegBackward0>) tensor(11682.6719, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11682.68359375
tensor(11682.6719, grad_fn=<NegBackward0>) tensor(11682.6836, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11682.6708984375
tensor(11682.6719, grad_fn=<NegBackward0>) tensor(11682.6709, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11682.6689453125
tensor(11682.6709, grad_fn=<NegBackward0>) tensor(11682.6689, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11682.669921875
tensor(11682.6689, grad_fn=<NegBackward0>) tensor(11682.6699, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11682.6669921875
tensor(11682.6689, grad_fn=<NegBackward0>) tensor(11682.6670, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11682.66796875
tensor(11682.6670, grad_fn=<NegBackward0>) tensor(11682.6680, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11682.6650390625
tensor(11682.6670, grad_fn=<NegBackward0>) tensor(11682.6650, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11682.6689453125
tensor(11682.6650, grad_fn=<NegBackward0>) tensor(11682.6689, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11682.666015625
tensor(11682.6650, grad_fn=<NegBackward0>) tensor(11682.6660, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11682.6640625
tensor(11682.6650, grad_fn=<NegBackward0>) tensor(11682.6641, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11682.6640625
tensor(11682.6641, grad_fn=<NegBackward0>) tensor(11682.6641, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11682.6630859375
tensor(11682.6641, grad_fn=<NegBackward0>) tensor(11682.6631, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11682.662109375
tensor(11682.6631, grad_fn=<NegBackward0>) tensor(11682.6621, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11682.662109375
tensor(11682.6621, grad_fn=<NegBackward0>) tensor(11682.6621, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11682.6611328125
tensor(11682.6621, grad_fn=<NegBackward0>) tensor(11682.6611, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11682.662109375
tensor(11682.6611, grad_fn=<NegBackward0>) tensor(11682.6621, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11682.66015625
tensor(11682.6611, grad_fn=<NegBackward0>) tensor(11682.6602, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11682.6611328125
tensor(11682.6602, grad_fn=<NegBackward0>) tensor(11682.6611, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11682.6611328125
tensor(11682.6602, grad_fn=<NegBackward0>) tensor(11682.6611, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11682.66015625
tensor(11682.6602, grad_fn=<NegBackward0>) tensor(11682.6602, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11682.66015625
tensor(11682.6602, grad_fn=<NegBackward0>) tensor(11682.6602, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11682.6611328125
tensor(11682.6602, grad_fn=<NegBackward0>) tensor(11682.6611, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11682.6591796875
tensor(11682.6602, grad_fn=<NegBackward0>) tensor(11682.6592, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11682.658203125
tensor(11682.6592, grad_fn=<NegBackward0>) tensor(11682.6582, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11682.6591796875
tensor(11682.6582, grad_fn=<NegBackward0>) tensor(11682.6592, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11682.66015625
tensor(11682.6582, grad_fn=<NegBackward0>) tensor(11682.6602, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11682.658203125
tensor(11682.6582, grad_fn=<NegBackward0>) tensor(11682.6582, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11682.662109375
tensor(11682.6582, grad_fn=<NegBackward0>) tensor(11682.6621, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11682.6572265625
tensor(11682.6582, grad_fn=<NegBackward0>) tensor(11682.6572, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11682.65625
tensor(11682.6572, grad_fn=<NegBackward0>) tensor(11682.6562, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11682.66015625
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6602, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11682.6572265625
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6572, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11682.658203125
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6582, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -11682.65625
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6562, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11682.6611328125
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6611, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11682.6572265625
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6572, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11682.65625
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6562, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11682.6591796875
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6592, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11682.6552734375
tensor(11682.6562, grad_fn=<NegBackward0>) tensor(11682.6553, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11682.65625
tensor(11682.6553, grad_fn=<NegBackward0>) tensor(11682.6562, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11678.0068359375
tensor(11682.6553, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11678.005859375
tensor(11678.0068, grad_fn=<NegBackward0>) tensor(11678.0059, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11678.0107421875
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0107, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11678.0078125
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0078, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11678.005859375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0059, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11678.0458984375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0459, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11678.01171875
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0117, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11678.0126953125
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0127, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11678.0068359375
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0068, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11678.015625
tensor(11678.0059, grad_fn=<NegBackward0>) tensor(11678.0156, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7482, 0.2518],
        [0.2015, 0.7985]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5399, 0.4601], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2036, 0.1002],
         [0.6991, 0.4094]],

        [[0.5278, 0.1064],
         [0.6818, 0.5856]],

        [[0.7035, 0.0943],
         [0.6525, 0.7138]],

        [[0.6113, 0.1002],
         [0.6406, 0.5617]],

        [[0.6347, 0.1027],
         [0.5915, 0.6351]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11678.01171875, 11678.015625]
-------------------------------------
This iteration is 67
True Objective function: Loss = -11539.452856942185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23263.818359375
inf tensor(23263.8184, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12155.3857421875
tensor(23263.8184, grad_fn=<NegBackward0>) tensor(12155.3857, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11681.9189453125
tensor(12155.3857, grad_fn=<NegBackward0>) tensor(11681.9189, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11581.365234375
tensor(11681.9189, grad_fn=<NegBackward0>) tensor(11581.3652, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11568.9111328125
tensor(11581.3652, grad_fn=<NegBackward0>) tensor(11568.9111, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11562.67578125
tensor(11568.9111, grad_fn=<NegBackward0>) tensor(11562.6758, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11552.1298828125
tensor(11562.6758, grad_fn=<NegBackward0>) tensor(11552.1299, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11551.97265625
tensor(11552.1299, grad_fn=<NegBackward0>) tensor(11551.9727, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11540.97265625
tensor(11551.9727, grad_fn=<NegBackward0>) tensor(11540.9727, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11540.9150390625
tensor(11540.9727, grad_fn=<NegBackward0>) tensor(11540.9150, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11540.875
tensor(11540.9150, grad_fn=<NegBackward0>) tensor(11540.8750, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11540.83984375
tensor(11540.8750, grad_fn=<NegBackward0>) tensor(11540.8398, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11540.3583984375
tensor(11540.8398, grad_fn=<NegBackward0>) tensor(11540.3584, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11534.318359375
tensor(11540.3584, grad_fn=<NegBackward0>) tensor(11534.3184, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11534.298828125
tensor(11534.3184, grad_fn=<NegBackward0>) tensor(11534.2988, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11534.2744140625
tensor(11534.2988, grad_fn=<NegBackward0>) tensor(11534.2744, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11534.1630859375
tensor(11534.2744, grad_fn=<NegBackward0>) tensor(11534.1631, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11534.1552734375
tensor(11534.1631, grad_fn=<NegBackward0>) tensor(11534.1553, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11534.1455078125
tensor(11534.1553, grad_fn=<NegBackward0>) tensor(11534.1455, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11534.1357421875
tensor(11534.1455, grad_fn=<NegBackward0>) tensor(11534.1357, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11534.126953125
tensor(11534.1357, grad_fn=<NegBackward0>) tensor(11534.1270, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11534.111328125
tensor(11534.1270, grad_fn=<NegBackward0>) tensor(11534.1113, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11530.96484375
tensor(11534.1113, grad_fn=<NegBackward0>) tensor(11530.9648, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11530.9609375
tensor(11530.9648, grad_fn=<NegBackward0>) tensor(11530.9609, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11530.9560546875
tensor(11530.9609, grad_fn=<NegBackward0>) tensor(11530.9561, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11530.9541015625
tensor(11530.9561, grad_fn=<NegBackward0>) tensor(11530.9541, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11530.951171875
tensor(11530.9541, grad_fn=<NegBackward0>) tensor(11530.9512, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11530.94921875
tensor(11530.9512, grad_fn=<NegBackward0>) tensor(11530.9492, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11530.9462890625
tensor(11530.9492, grad_fn=<NegBackward0>) tensor(11530.9463, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11530.9443359375
tensor(11530.9463, grad_fn=<NegBackward0>) tensor(11530.9443, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11530.9443359375
tensor(11530.9443, grad_fn=<NegBackward0>) tensor(11530.9443, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11530.94140625
tensor(11530.9443, grad_fn=<NegBackward0>) tensor(11530.9414, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11530.939453125
tensor(11530.9414, grad_fn=<NegBackward0>) tensor(11530.9395, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11530.9384765625
tensor(11530.9395, grad_fn=<NegBackward0>) tensor(11530.9385, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11530.9375
tensor(11530.9385, grad_fn=<NegBackward0>) tensor(11530.9375, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11530.9345703125
tensor(11530.9375, grad_fn=<NegBackward0>) tensor(11530.9346, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11530.935546875
tensor(11530.9346, grad_fn=<NegBackward0>) tensor(11530.9355, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11530.935546875
tensor(11530.9346, grad_fn=<NegBackward0>) tensor(11530.9355, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11530.9326171875
tensor(11530.9346, grad_fn=<NegBackward0>) tensor(11530.9326, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11530.931640625
tensor(11530.9326, grad_fn=<NegBackward0>) tensor(11530.9316, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11530.931640625
tensor(11530.9316, grad_fn=<NegBackward0>) tensor(11530.9316, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11530.9287109375
tensor(11530.9316, grad_fn=<NegBackward0>) tensor(11530.9287, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11530.927734375
tensor(11530.9287, grad_fn=<NegBackward0>) tensor(11530.9277, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11530.9150390625
tensor(11530.9277, grad_fn=<NegBackward0>) tensor(11530.9150, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11530.9033203125
tensor(11530.9150, grad_fn=<NegBackward0>) tensor(11530.9033, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11530.904296875
tensor(11530.9033, grad_fn=<NegBackward0>) tensor(11530.9043, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11530.9033203125
tensor(11530.9033, grad_fn=<NegBackward0>) tensor(11530.9033, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11530.9033203125
tensor(11530.9033, grad_fn=<NegBackward0>) tensor(11530.9033, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11530.90234375
tensor(11530.9033, grad_fn=<NegBackward0>) tensor(11530.9023, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11530.9150390625
tensor(11530.9023, grad_fn=<NegBackward0>) tensor(11530.9150, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11530.9013671875
tensor(11530.9023, grad_fn=<NegBackward0>) tensor(11530.9014, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11530.908203125
tensor(11530.9014, grad_fn=<NegBackward0>) tensor(11530.9082, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11530.900390625
tensor(11530.9014, grad_fn=<NegBackward0>) tensor(11530.9004, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11530.9013671875
tensor(11530.9004, grad_fn=<NegBackward0>) tensor(11530.9014, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11530.900390625
tensor(11530.9004, grad_fn=<NegBackward0>) tensor(11530.9004, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11530.900390625
tensor(11530.9004, grad_fn=<NegBackward0>) tensor(11530.9004, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11530.900390625
tensor(11530.9004, grad_fn=<NegBackward0>) tensor(11530.9004, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11530.900390625
tensor(11530.9004, grad_fn=<NegBackward0>) tensor(11530.9004, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11530.8994140625
tensor(11530.9004, grad_fn=<NegBackward0>) tensor(11530.8994, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11530.9013671875
tensor(11530.8994, grad_fn=<NegBackward0>) tensor(11530.9014, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11530.9052734375
tensor(11530.8994, grad_fn=<NegBackward0>) tensor(11530.9053, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11530.9052734375
tensor(11530.8994, grad_fn=<NegBackward0>) tensor(11530.9053, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11530.884765625
tensor(11530.8994, grad_fn=<NegBackward0>) tensor(11530.8848, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11530.8837890625
tensor(11530.8848, grad_fn=<NegBackward0>) tensor(11530.8838, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11530.8828125
tensor(11530.8838, grad_fn=<NegBackward0>) tensor(11530.8828, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11530.8828125
tensor(11530.8828, grad_fn=<NegBackward0>) tensor(11530.8828, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11530.8837890625
tensor(11530.8828, grad_fn=<NegBackward0>) tensor(11530.8838, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11530.8828125
tensor(11530.8828, grad_fn=<NegBackward0>) tensor(11530.8828, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11530.8828125
tensor(11530.8828, grad_fn=<NegBackward0>) tensor(11530.8828, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11530.880859375
tensor(11530.8828, grad_fn=<NegBackward0>) tensor(11530.8809, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11530.8818359375
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8818, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11530.8818359375
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8818, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11530.8828125
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8828, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11530.8828125
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8828, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11530.8984375
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8984, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.7457, 0.2543],
        [0.2606, 0.7394]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4569, 0.5431], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1963, 0.1057],
         [0.5477, 0.3988]],

        [[0.5268, 0.0924],
         [0.7083, 0.7134]],

        [[0.5906, 0.1102],
         [0.5734, 0.5358]],

        [[0.6288, 0.0888],
         [0.6260, 0.6988]],

        [[0.5711, 0.0995],
         [0.5151, 0.7147]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21804.529296875
inf tensor(21804.5293, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12356.1220703125
tensor(21804.5293, grad_fn=<NegBackward0>) tensor(12356.1221, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12212.3935546875
tensor(12356.1221, grad_fn=<NegBackward0>) tensor(12212.3936, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11799.2802734375
tensor(12212.3936, grad_fn=<NegBackward0>) tensor(11799.2803, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11677.4853515625
tensor(11799.2803, grad_fn=<NegBackward0>) tensor(11677.4854, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11560.9130859375
tensor(11677.4854, grad_fn=<NegBackward0>) tensor(11560.9131, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11555.8115234375
tensor(11560.9131, grad_fn=<NegBackward0>) tensor(11555.8115, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11543.00390625
tensor(11555.8115, grad_fn=<NegBackward0>) tensor(11543.0039, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11531.650390625
tensor(11543.0039, grad_fn=<NegBackward0>) tensor(11531.6504, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11531.48828125
tensor(11531.6504, grad_fn=<NegBackward0>) tensor(11531.4883, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11531.3759765625
tensor(11531.4883, grad_fn=<NegBackward0>) tensor(11531.3760, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11531.2958984375
tensor(11531.3760, grad_fn=<NegBackward0>) tensor(11531.2959, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11531.234375
tensor(11531.2959, grad_fn=<NegBackward0>) tensor(11531.2344, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11531.1875
tensor(11531.2344, grad_fn=<NegBackward0>) tensor(11531.1875, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11531.150390625
tensor(11531.1875, grad_fn=<NegBackward0>) tensor(11531.1504, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11531.119140625
tensor(11531.1504, grad_fn=<NegBackward0>) tensor(11531.1191, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11531.09375
tensor(11531.1191, grad_fn=<NegBackward0>) tensor(11531.0938, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11531.072265625
tensor(11531.0938, grad_fn=<NegBackward0>) tensor(11531.0723, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11531.0546875
tensor(11531.0723, grad_fn=<NegBackward0>) tensor(11531.0547, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11531.0400390625
tensor(11531.0547, grad_fn=<NegBackward0>) tensor(11531.0400, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11531.02734375
tensor(11531.0400, grad_fn=<NegBackward0>) tensor(11531.0273, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11531.015625
tensor(11531.0273, grad_fn=<NegBackward0>) tensor(11531.0156, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11531.0048828125
tensor(11531.0156, grad_fn=<NegBackward0>) tensor(11531.0049, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11530.9951171875
tensor(11531.0049, grad_fn=<NegBackward0>) tensor(11530.9951, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11530.9892578125
tensor(11530.9951, grad_fn=<NegBackward0>) tensor(11530.9893, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11530.9814453125
tensor(11530.9893, grad_fn=<NegBackward0>) tensor(11530.9814, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11530.9755859375
tensor(11530.9814, grad_fn=<NegBackward0>) tensor(11530.9756, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11530.96875
tensor(11530.9756, grad_fn=<NegBackward0>) tensor(11530.9688, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11530.9658203125
tensor(11530.9688, grad_fn=<NegBackward0>) tensor(11530.9658, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11530.9599609375
tensor(11530.9658, grad_fn=<NegBackward0>) tensor(11530.9600, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11530.955078125
tensor(11530.9600, grad_fn=<NegBackward0>) tensor(11530.9551, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11530.951171875
tensor(11530.9551, grad_fn=<NegBackward0>) tensor(11530.9512, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11530.9482421875
tensor(11530.9512, grad_fn=<NegBackward0>) tensor(11530.9482, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11530.943359375
tensor(11530.9482, grad_fn=<NegBackward0>) tensor(11530.9434, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11530.94140625
tensor(11530.9434, grad_fn=<NegBackward0>) tensor(11530.9414, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11530.9365234375
tensor(11530.9414, grad_fn=<NegBackward0>) tensor(11530.9365, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11530.931640625
tensor(11530.9365, grad_fn=<NegBackward0>) tensor(11530.9316, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11530.923828125
tensor(11530.9316, grad_fn=<NegBackward0>) tensor(11530.9238, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11530.9208984375
tensor(11530.9238, grad_fn=<NegBackward0>) tensor(11530.9209, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11530.91796875
tensor(11530.9209, grad_fn=<NegBackward0>) tensor(11530.9180, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11530.91015625
tensor(11530.9180, grad_fn=<NegBackward0>) tensor(11530.9102, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11530.900390625
tensor(11530.9102, grad_fn=<NegBackward0>) tensor(11530.9004, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11530.8994140625
tensor(11530.9004, grad_fn=<NegBackward0>) tensor(11530.8994, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11530.896484375
tensor(11530.8994, grad_fn=<NegBackward0>) tensor(11530.8965, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11530.89453125
tensor(11530.8965, grad_fn=<NegBackward0>) tensor(11530.8945, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11530.8935546875
tensor(11530.8945, grad_fn=<NegBackward0>) tensor(11530.8936, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11530.8935546875
tensor(11530.8936, grad_fn=<NegBackward0>) tensor(11530.8936, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11530.8916015625
tensor(11530.8936, grad_fn=<NegBackward0>) tensor(11530.8916, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11530.8896484375
tensor(11530.8916, grad_fn=<NegBackward0>) tensor(11530.8896, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11530.892578125
tensor(11530.8896, grad_fn=<NegBackward0>) tensor(11530.8926, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11530.888671875
tensor(11530.8896, grad_fn=<NegBackward0>) tensor(11530.8887, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11530.888671875
tensor(11530.8887, grad_fn=<NegBackward0>) tensor(11530.8887, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11530.88671875
tensor(11530.8887, grad_fn=<NegBackward0>) tensor(11530.8867, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11530.8857421875
tensor(11530.8867, grad_fn=<NegBackward0>) tensor(11530.8857, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11530.88671875
tensor(11530.8857, grad_fn=<NegBackward0>) tensor(11530.8867, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11530.884765625
tensor(11530.8857, grad_fn=<NegBackward0>) tensor(11530.8848, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11530.884765625
tensor(11530.8848, grad_fn=<NegBackward0>) tensor(11530.8848, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11530.8837890625
tensor(11530.8848, grad_fn=<NegBackward0>) tensor(11530.8838, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11530.884765625
tensor(11530.8838, grad_fn=<NegBackward0>) tensor(11530.8848, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11530.8828125
tensor(11530.8838, grad_fn=<NegBackward0>) tensor(11530.8828, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11530.8896484375
tensor(11530.8828, grad_fn=<NegBackward0>) tensor(11530.8896, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11530.880859375
tensor(11530.8828, grad_fn=<NegBackward0>) tensor(11530.8809, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11530.8818359375
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8818, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11530.880859375
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8809, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11530.8798828125
tensor(11530.8809, grad_fn=<NegBackward0>) tensor(11530.8799, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11530.880859375
tensor(11530.8799, grad_fn=<NegBackward0>) tensor(11530.8809, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11530.880859375
tensor(11530.8799, grad_fn=<NegBackward0>) tensor(11530.8809, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11530.880859375
tensor(11530.8799, grad_fn=<NegBackward0>) tensor(11530.8809, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11530.87890625
tensor(11530.8799, grad_fn=<NegBackward0>) tensor(11530.8789, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11530.8818359375
tensor(11530.8789, grad_fn=<NegBackward0>) tensor(11530.8818, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11530.8857421875
tensor(11530.8789, grad_fn=<NegBackward0>) tensor(11530.8857, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11530.87890625
tensor(11530.8789, grad_fn=<NegBackward0>) tensor(11530.8789, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11530.8798828125
tensor(11530.8789, grad_fn=<NegBackward0>) tensor(11530.8799, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11530.87890625
tensor(11530.8789, grad_fn=<NegBackward0>) tensor(11530.8789, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11530.884765625
tensor(11530.8789, grad_fn=<NegBackward0>) tensor(11530.8848, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11530.876953125
tensor(11530.8789, grad_fn=<NegBackward0>) tensor(11530.8770, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11530.87890625
tensor(11530.8770, grad_fn=<NegBackward0>) tensor(11530.8789, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11530.8798828125
tensor(11530.8770, grad_fn=<NegBackward0>) tensor(11530.8799, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11530.8857421875
tensor(11530.8770, grad_fn=<NegBackward0>) tensor(11530.8857, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11530.8779296875
tensor(11530.8770, grad_fn=<NegBackward0>) tensor(11530.8779, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11530.8759765625
tensor(11530.8770, grad_fn=<NegBackward0>) tensor(11530.8760, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11530.87890625
tensor(11530.8760, grad_fn=<NegBackward0>) tensor(11530.8789, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11530.87890625
tensor(11530.8760, grad_fn=<NegBackward0>) tensor(11530.8789, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11530.88671875
tensor(11530.8760, grad_fn=<NegBackward0>) tensor(11530.8867, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11530.8857421875
tensor(11530.8760, grad_fn=<NegBackward0>) tensor(11530.8857, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11530.875
tensor(11530.8760, grad_fn=<NegBackward0>) tensor(11530.8750, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11530.875
tensor(11530.8750, grad_fn=<NegBackward0>) tensor(11530.8750, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11530.8759765625
tensor(11530.8750, grad_fn=<NegBackward0>) tensor(11530.8760, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11530.8779296875
tensor(11530.8750, grad_fn=<NegBackward0>) tensor(11530.8779, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11530.8740234375
tensor(11530.8750, grad_fn=<NegBackward0>) tensor(11530.8740, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11530.876953125
tensor(11530.8740, grad_fn=<NegBackward0>) tensor(11530.8770, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11530.8876953125
tensor(11530.8740, grad_fn=<NegBackward0>) tensor(11530.8877, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11530.8740234375
tensor(11530.8740, grad_fn=<NegBackward0>) tensor(11530.8740, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11530.8779296875
tensor(11530.8740, grad_fn=<NegBackward0>) tensor(11530.8779, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11530.873046875
tensor(11530.8740, grad_fn=<NegBackward0>) tensor(11530.8730, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11530.875
tensor(11530.8730, grad_fn=<NegBackward0>) tensor(11530.8750, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11530.873046875
tensor(11530.8730, grad_fn=<NegBackward0>) tensor(11530.8730, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11530.875
tensor(11530.8730, grad_fn=<NegBackward0>) tensor(11530.8750, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11530.8740234375
tensor(11530.8730, grad_fn=<NegBackward0>) tensor(11530.8740, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11530.8740234375
tensor(11530.8730, grad_fn=<NegBackward0>) tensor(11530.8740, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7398, 0.2602],
        [0.2530, 0.7470]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5440, 0.4560], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3999, 0.1057],
         [0.5074, 0.1964]],

        [[0.7151, 0.0926],
         [0.6041, 0.7173]],

        [[0.5264, 0.1101],
         [0.7052, 0.6137]],

        [[0.5986, 0.0881],
         [0.5803, 0.5404]],

        [[0.5903, 0.0995],
         [0.6977, 0.6818]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919992163297293
[0.9919999740011123, 0.9919999740011123] [0.9919992163297293, 0.9919992163297293] [11530.8984375, 11530.880859375]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11484.355200692185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24766.314453125
inf tensor(24766.3145, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12175.990234375
tensor(24766.3145, grad_fn=<NegBackward0>) tensor(12175.9902, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11644.9013671875
tensor(12175.9902, grad_fn=<NegBackward0>) tensor(11644.9014, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11545.48828125
tensor(11644.9014, grad_fn=<NegBackward0>) tensor(11545.4883, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11528.2158203125
tensor(11545.4883, grad_fn=<NegBackward0>) tensor(11528.2158, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11523.935546875
tensor(11528.2158, grad_fn=<NegBackward0>) tensor(11523.9355, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11504.4736328125
tensor(11523.9355, grad_fn=<NegBackward0>) tensor(11504.4736, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11504.0126953125
tensor(11504.4736, grad_fn=<NegBackward0>) tensor(11504.0127, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11502.8232421875
tensor(11504.0127, grad_fn=<NegBackward0>) tensor(11502.8232, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11495.3359375
tensor(11502.8232, grad_fn=<NegBackward0>) tensor(11495.3359, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11495.2939453125
tensor(11495.3359, grad_fn=<NegBackward0>) tensor(11495.2939, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11495.26171875
tensor(11495.2939, grad_fn=<NegBackward0>) tensor(11495.2617, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11487.578125
tensor(11495.2617, grad_fn=<NegBackward0>) tensor(11487.5781, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11487.5205078125
tensor(11487.5781, grad_fn=<NegBackward0>) tensor(11487.5205, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11487.5029296875
tensor(11487.5205, grad_fn=<NegBackward0>) tensor(11487.5029, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11487.48046875
tensor(11487.5029, grad_fn=<NegBackward0>) tensor(11487.4805, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11482.3271484375
tensor(11487.4805, grad_fn=<NegBackward0>) tensor(11482.3271, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11482.3095703125
tensor(11482.3271, grad_fn=<NegBackward0>) tensor(11482.3096, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11474.6494140625
tensor(11482.3096, grad_fn=<NegBackward0>) tensor(11474.6494, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11474.640625
tensor(11474.6494, grad_fn=<NegBackward0>) tensor(11474.6406, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11474.634765625
tensor(11474.6406, grad_fn=<NegBackward0>) tensor(11474.6348, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11474.6279296875
tensor(11474.6348, grad_fn=<NegBackward0>) tensor(11474.6279, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11474.625
tensor(11474.6279, grad_fn=<NegBackward0>) tensor(11474.6250, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11474.62109375
tensor(11474.6250, grad_fn=<NegBackward0>) tensor(11474.6211, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11474.6181640625
tensor(11474.6211, grad_fn=<NegBackward0>) tensor(11474.6182, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11474.615234375
tensor(11474.6182, grad_fn=<NegBackward0>) tensor(11474.6152, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11474.6123046875
tensor(11474.6152, grad_fn=<NegBackward0>) tensor(11474.6123, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11474.609375
tensor(11474.6123, grad_fn=<NegBackward0>) tensor(11474.6094, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11474.6064453125
tensor(11474.6094, grad_fn=<NegBackward0>) tensor(11474.6064, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11474.60546875
tensor(11474.6064, grad_fn=<NegBackward0>) tensor(11474.6055, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11474.6015625
tensor(11474.6055, grad_fn=<NegBackward0>) tensor(11474.6016, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11474.599609375
tensor(11474.6016, grad_fn=<NegBackward0>) tensor(11474.5996, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11474.595703125
tensor(11474.5996, grad_fn=<NegBackward0>) tensor(11474.5957, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11474.5947265625
tensor(11474.5957, grad_fn=<NegBackward0>) tensor(11474.5947, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11474.5947265625
tensor(11474.5947, grad_fn=<NegBackward0>) tensor(11474.5947, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11474.5966796875
tensor(11474.5947, grad_fn=<NegBackward0>) tensor(11474.5967, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11474.591796875
tensor(11474.5947, grad_fn=<NegBackward0>) tensor(11474.5918, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11474.591796875
tensor(11474.5918, grad_fn=<NegBackward0>) tensor(11474.5918, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11474.5908203125
tensor(11474.5918, grad_fn=<NegBackward0>) tensor(11474.5908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11474.5908203125
tensor(11474.5908, grad_fn=<NegBackward0>) tensor(11474.5908, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11474.58984375
tensor(11474.5908, grad_fn=<NegBackward0>) tensor(11474.5898, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11474.5888671875
tensor(11474.5898, grad_fn=<NegBackward0>) tensor(11474.5889, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11474.587890625
tensor(11474.5889, grad_fn=<NegBackward0>) tensor(11474.5879, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11474.5927734375
tensor(11474.5879, grad_fn=<NegBackward0>) tensor(11474.5928, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11474.5869140625
tensor(11474.5879, grad_fn=<NegBackward0>) tensor(11474.5869, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11474.5869140625
tensor(11474.5869, grad_fn=<NegBackward0>) tensor(11474.5869, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11474.583984375
tensor(11474.5869, grad_fn=<NegBackward0>) tensor(11474.5840, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11474.580078125
tensor(11474.5840, grad_fn=<NegBackward0>) tensor(11474.5801, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11474.580078125
tensor(11474.5801, grad_fn=<NegBackward0>) tensor(11474.5801, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11474.578125
tensor(11474.5801, grad_fn=<NegBackward0>) tensor(11474.5781, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11474.578125
tensor(11474.5781, grad_fn=<NegBackward0>) tensor(11474.5781, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11474.578125
tensor(11474.5781, grad_fn=<NegBackward0>) tensor(11474.5781, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11474.578125
tensor(11474.5781, grad_fn=<NegBackward0>) tensor(11474.5781, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11474.5771484375
tensor(11474.5781, grad_fn=<NegBackward0>) tensor(11474.5771, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11474.58203125
tensor(11474.5771, grad_fn=<NegBackward0>) tensor(11474.5820, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11474.578125
tensor(11474.5771, grad_fn=<NegBackward0>) tensor(11474.5781, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11474.5771484375
tensor(11474.5771, grad_fn=<NegBackward0>) tensor(11474.5771, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11474.5771484375
tensor(11474.5771, grad_fn=<NegBackward0>) tensor(11474.5771, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11474.576171875
tensor(11474.5771, grad_fn=<NegBackward0>) tensor(11474.5762, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11474.5791015625
tensor(11474.5762, grad_fn=<NegBackward0>) tensor(11474.5791, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11474.5751953125
tensor(11474.5762, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11474.5751953125
tensor(11474.5752, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11474.5751953125
tensor(11474.5752, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11474.5751953125
tensor(11474.5752, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11474.57421875
tensor(11474.5752, grad_fn=<NegBackward0>) tensor(11474.5742, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11474.5751953125
tensor(11474.5742, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11474.5751953125
tensor(11474.5742, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11474.5732421875
tensor(11474.5742, grad_fn=<NegBackward0>) tensor(11474.5732, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11474.57421875
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5742, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11474.57421875
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5742, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11474.5751953125
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11474.57421875
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5742, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11474.5751953125
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.7707, 0.2293],
        [0.2731, 0.7269]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4746, 0.5254], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1929, 0.0940],
         [0.5206, 0.4017]],

        [[0.5302, 0.1098],
         [0.7022, 0.7162]],

        [[0.5141, 0.1156],
         [0.5625, 0.5544]],

        [[0.5160, 0.0945],
         [0.5208, 0.7047]],

        [[0.5142, 0.0918],
         [0.7160, 0.5252]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22161.63671875
inf tensor(22161.6367, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12237.26171875
tensor(22161.6367, grad_fn=<NegBackward0>) tensor(12237.2617, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12047.7705078125
tensor(12237.2617, grad_fn=<NegBackward0>) tensor(12047.7705, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11622.419921875
tensor(12047.7705, grad_fn=<NegBackward0>) tensor(11622.4199, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11537.2119140625
tensor(11622.4199, grad_fn=<NegBackward0>) tensor(11537.2119, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11519.138671875
tensor(11537.2119, grad_fn=<NegBackward0>) tensor(11519.1387, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11518.7138671875
tensor(11519.1387, grad_fn=<NegBackward0>) tensor(11518.7139, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11518.525390625
tensor(11518.7139, grad_fn=<NegBackward0>) tensor(11518.5254, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11515.8505859375
tensor(11518.5254, grad_fn=<NegBackward0>) tensor(11515.8506, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11515.732421875
tensor(11515.8506, grad_fn=<NegBackward0>) tensor(11515.7324, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11507.2275390625
tensor(11515.7324, grad_fn=<NegBackward0>) tensor(11507.2275, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11507.1796875
tensor(11507.2275, grad_fn=<NegBackward0>) tensor(11507.1797, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11507.1435546875
tensor(11507.1797, grad_fn=<NegBackward0>) tensor(11507.1436, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11507.037109375
tensor(11507.1436, grad_fn=<NegBackward0>) tensor(11507.0371, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11501.7001953125
tensor(11507.0371, grad_fn=<NegBackward0>) tensor(11501.7002, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11501.681640625
tensor(11501.7002, grad_fn=<NegBackward0>) tensor(11501.6816, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11501.6650390625
tensor(11501.6816, grad_fn=<NegBackward0>) tensor(11501.6650, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11501.654296875
tensor(11501.6650, grad_fn=<NegBackward0>) tensor(11501.6543, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11501.6435546875
tensor(11501.6543, grad_fn=<NegBackward0>) tensor(11501.6436, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11501.634765625
tensor(11501.6436, grad_fn=<NegBackward0>) tensor(11501.6348, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11501.626953125
tensor(11501.6348, grad_fn=<NegBackward0>) tensor(11501.6270, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11501.619140625
tensor(11501.6270, grad_fn=<NegBackward0>) tensor(11501.6191, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11501.61328125
tensor(11501.6191, grad_fn=<NegBackward0>) tensor(11501.6133, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11501.60546875
tensor(11501.6133, grad_fn=<NegBackward0>) tensor(11501.6055, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11496.466796875
tensor(11501.6055, grad_fn=<NegBackward0>) tensor(11496.4668, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11496.45703125
tensor(11496.4668, grad_fn=<NegBackward0>) tensor(11496.4570, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11496.451171875
tensor(11496.4570, grad_fn=<NegBackward0>) tensor(11496.4512, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11482.6982421875
tensor(11496.4512, grad_fn=<NegBackward0>) tensor(11482.6982, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11482.6962890625
tensor(11482.6982, grad_fn=<NegBackward0>) tensor(11482.6963, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11482.693359375
tensor(11482.6963, grad_fn=<NegBackward0>) tensor(11482.6934, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11482.6904296875
tensor(11482.6934, grad_fn=<NegBackward0>) tensor(11482.6904, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11482.6875
tensor(11482.6904, grad_fn=<NegBackward0>) tensor(11482.6875, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11482.6875
tensor(11482.6875, grad_fn=<NegBackward0>) tensor(11482.6875, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11482.6845703125
tensor(11482.6875, grad_fn=<NegBackward0>) tensor(11482.6846, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11482.681640625
tensor(11482.6846, grad_fn=<NegBackward0>) tensor(11482.6816, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11482.6796875
tensor(11482.6816, grad_fn=<NegBackward0>) tensor(11482.6797, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11482.6796875
tensor(11482.6797, grad_fn=<NegBackward0>) tensor(11482.6797, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11482.6806640625
tensor(11482.6797, grad_fn=<NegBackward0>) tensor(11482.6807, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11482.6748046875
tensor(11482.6797, grad_fn=<NegBackward0>) tensor(11482.6748, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11482.6708984375
tensor(11482.6748, grad_fn=<NegBackward0>) tensor(11482.6709, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11482.66796875
tensor(11482.6709, grad_fn=<NegBackward0>) tensor(11482.6680, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11475.0244140625
tensor(11482.6680, grad_fn=<NegBackward0>) tensor(11475.0244, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11475.0263671875
tensor(11475.0244, grad_fn=<NegBackward0>) tensor(11475.0264, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11475.0224609375
tensor(11475.0244, grad_fn=<NegBackward0>) tensor(11475.0225, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11475.0205078125
tensor(11475.0225, grad_fn=<NegBackward0>) tensor(11475.0205, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11475.0205078125
tensor(11475.0205, grad_fn=<NegBackward0>) tensor(11475.0205, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11475.01953125
tensor(11475.0205, grad_fn=<NegBackward0>) tensor(11475.0195, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11475.0185546875
tensor(11475.0195, grad_fn=<NegBackward0>) tensor(11475.0186, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11475.017578125
tensor(11475.0186, grad_fn=<NegBackward0>) tensor(11475.0176, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11475.017578125
tensor(11475.0176, grad_fn=<NegBackward0>) tensor(11475.0176, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11475.017578125
tensor(11475.0176, grad_fn=<NegBackward0>) tensor(11475.0176, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11475.0166015625
tensor(11475.0176, grad_fn=<NegBackward0>) tensor(11475.0166, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11475.0166015625
tensor(11475.0166, grad_fn=<NegBackward0>) tensor(11475.0166, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11475.0146484375
tensor(11475.0166, grad_fn=<NegBackward0>) tensor(11475.0146, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11474.580078125
tensor(11475.0146, grad_fn=<NegBackward0>) tensor(11474.5801, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11474.5732421875
tensor(11474.5801, grad_fn=<NegBackward0>) tensor(11474.5732, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11474.5751953125
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11474.5751953125
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5752, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11474.572265625
tensor(11474.5732, grad_fn=<NegBackward0>) tensor(11474.5723, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11474.5712890625
tensor(11474.5723, grad_fn=<NegBackward0>) tensor(11474.5713, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11474.57421875
tensor(11474.5713, grad_fn=<NegBackward0>) tensor(11474.5742, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11474.556640625
tensor(11474.5713, grad_fn=<NegBackward0>) tensor(11474.5566, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11474.5478515625
tensor(11474.5566, grad_fn=<NegBackward0>) tensor(11474.5479, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11474.55078125
tensor(11474.5479, grad_fn=<NegBackward0>) tensor(11474.5508, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11474.560546875
tensor(11474.5479, grad_fn=<NegBackward0>) tensor(11474.5605, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11474.5478515625
tensor(11474.5479, grad_fn=<NegBackward0>) tensor(11474.5479, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11474.548828125
tensor(11474.5479, grad_fn=<NegBackward0>) tensor(11474.5488, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11474.546875
tensor(11474.5479, grad_fn=<NegBackward0>) tensor(11474.5469, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11474.5478515625
tensor(11474.5469, grad_fn=<NegBackward0>) tensor(11474.5479, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11474.5478515625
tensor(11474.5469, grad_fn=<NegBackward0>) tensor(11474.5479, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11474.546875
tensor(11474.5469, grad_fn=<NegBackward0>) tensor(11474.5469, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11474.546875
tensor(11474.5469, grad_fn=<NegBackward0>) tensor(11474.5469, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11474.5458984375
tensor(11474.5469, grad_fn=<NegBackward0>) tensor(11474.5459, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11474.5458984375
tensor(11474.5459, grad_fn=<NegBackward0>) tensor(11474.5459, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11474.5390625
tensor(11474.5459, grad_fn=<NegBackward0>) tensor(11474.5391, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11474.541015625
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5410, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11474.544921875
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5449, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11474.5390625
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5391, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11474.5419921875
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5420, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11474.5400390625
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5400, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11474.5419921875
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5420, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11474.5458984375
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5459, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11474.560546875
tensor(11474.5391, grad_fn=<NegBackward0>) tensor(11474.5605, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.7291, 0.2709],
        [0.2267, 0.7733]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5270, 0.4730], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4010, 0.0940],
         [0.6468, 0.1929]],

        [[0.6647, 0.1098],
         [0.7194, 0.6247]],

        [[0.5789, 0.1157],
         [0.5768, 0.5623]],

        [[0.5090, 0.0945],
         [0.7080, 0.5638]],

        [[0.5421, 0.0918],
         [0.5805, 0.7105]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[1.0, 1.0] [1.0, 1.0] [11474.5751953125, 11474.560546875]
-------------------------------------
This iteration is 69
True Objective function: Loss = -11281.93303835854
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23371.005859375
inf tensor(23371.0059, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12037.1533203125
tensor(23371.0059, grad_fn=<NegBackward0>) tensor(12037.1533, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11873.5712890625
tensor(12037.1533, grad_fn=<NegBackward0>) tensor(11873.5713, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11521.40234375
tensor(11873.5713, grad_fn=<NegBackward0>) tensor(11521.4023, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11446.3837890625
tensor(11521.4023, grad_fn=<NegBackward0>) tensor(11446.3838, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11391.2431640625
tensor(11446.3838, grad_fn=<NegBackward0>) tensor(11391.2432, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11333.2646484375
tensor(11391.2432, grad_fn=<NegBackward0>) tensor(11333.2646, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11319.0302734375
tensor(11333.2646, grad_fn=<NegBackward0>) tensor(11319.0303, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11318.7578125
tensor(11319.0303, grad_fn=<NegBackward0>) tensor(11318.7578, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11318.5869140625
tensor(11318.7578, grad_fn=<NegBackward0>) tensor(11318.5869, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11304.8662109375
tensor(11318.5869, grad_fn=<NegBackward0>) tensor(11304.8662, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11283.3330078125
tensor(11304.8662, grad_fn=<NegBackward0>) tensor(11283.3330, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11283.2158203125
tensor(11283.3330, grad_fn=<NegBackward0>) tensor(11283.2158, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11283.1591796875
tensor(11283.2158, grad_fn=<NegBackward0>) tensor(11283.1592, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11283.1123046875
tensor(11283.1592, grad_fn=<NegBackward0>) tensor(11283.1123, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11283.0390625
tensor(11283.1123, grad_fn=<NegBackward0>) tensor(11283.0391, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11280.3125
tensor(11283.0391, grad_fn=<NegBackward0>) tensor(11280.3125, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11280.2587890625
tensor(11280.3125, grad_fn=<NegBackward0>) tensor(11280.2588, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11280.23828125
tensor(11280.2588, grad_fn=<NegBackward0>) tensor(11280.2383, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11280.22265625
tensor(11280.2383, grad_fn=<NegBackward0>) tensor(11280.2227, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11280.2080078125
tensor(11280.2227, grad_fn=<NegBackward0>) tensor(11280.2080, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11280.197265625
tensor(11280.2080, grad_fn=<NegBackward0>) tensor(11280.1973, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11280.1875
tensor(11280.1973, grad_fn=<NegBackward0>) tensor(11280.1875, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11280.1796875
tensor(11280.1875, grad_fn=<NegBackward0>) tensor(11280.1797, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11280.1708984375
tensor(11280.1797, grad_fn=<NegBackward0>) tensor(11280.1709, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11280.1650390625
tensor(11280.1709, grad_fn=<NegBackward0>) tensor(11280.1650, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11280.158203125
tensor(11280.1650, grad_fn=<NegBackward0>) tensor(11280.1582, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11280.1533203125
tensor(11280.1582, grad_fn=<NegBackward0>) tensor(11280.1533, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11280.146484375
tensor(11280.1533, grad_fn=<NegBackward0>) tensor(11280.1465, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11280.142578125
tensor(11280.1465, grad_fn=<NegBackward0>) tensor(11280.1426, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11280.138671875
tensor(11280.1426, grad_fn=<NegBackward0>) tensor(11280.1387, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11280.134765625
tensor(11280.1387, grad_fn=<NegBackward0>) tensor(11280.1348, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11280.1328125
tensor(11280.1348, grad_fn=<NegBackward0>) tensor(11280.1328, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11280.1298828125
tensor(11280.1328, grad_fn=<NegBackward0>) tensor(11280.1299, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11280.126953125
tensor(11280.1299, grad_fn=<NegBackward0>) tensor(11280.1270, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11280.130859375
tensor(11280.1270, grad_fn=<NegBackward0>) tensor(11280.1309, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11280.123046875
tensor(11280.1270, grad_fn=<NegBackward0>) tensor(11280.1230, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11280.12109375
tensor(11280.1230, grad_fn=<NegBackward0>) tensor(11280.1211, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11280.1396484375
tensor(11280.1211, grad_fn=<NegBackward0>) tensor(11280.1396, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11280.1181640625
tensor(11280.1211, grad_fn=<NegBackward0>) tensor(11280.1182, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11280.115234375
tensor(11280.1182, grad_fn=<NegBackward0>) tensor(11280.1152, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11280.11328125
tensor(11280.1152, grad_fn=<NegBackward0>) tensor(11280.1133, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11280.11328125
tensor(11280.1133, grad_fn=<NegBackward0>) tensor(11280.1133, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11280.1142578125
tensor(11280.1133, grad_fn=<NegBackward0>) tensor(11280.1143, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11280.1103515625
tensor(11280.1133, grad_fn=<NegBackward0>) tensor(11280.1104, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11280.1083984375
tensor(11280.1104, grad_fn=<NegBackward0>) tensor(11280.1084, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11280.1064453125
tensor(11280.1084, grad_fn=<NegBackward0>) tensor(11280.1064, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11280.1064453125
tensor(11280.1064, grad_fn=<NegBackward0>) tensor(11280.1064, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11280.10546875
tensor(11280.1064, grad_fn=<NegBackward0>) tensor(11280.1055, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11280.10546875
tensor(11280.1055, grad_fn=<NegBackward0>) tensor(11280.1055, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11280.1044921875
tensor(11280.1055, grad_fn=<NegBackward0>) tensor(11280.1045, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11280.1025390625
tensor(11280.1045, grad_fn=<NegBackward0>) tensor(11280.1025, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11280.10546875
tensor(11280.1025, grad_fn=<NegBackward0>) tensor(11280.1055, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11280.099609375
tensor(11280.1025, grad_fn=<NegBackward0>) tensor(11280.0996, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11280.099609375
tensor(11280.0996, grad_fn=<NegBackward0>) tensor(11280.0996, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11280.0986328125
tensor(11280.0996, grad_fn=<NegBackward0>) tensor(11280.0986, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11280.09765625
tensor(11280.0986, grad_fn=<NegBackward0>) tensor(11280.0977, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11280.0966796875
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 70%|███████   | 70/100 [17:29:53<7:35:54, 911.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 71%|███████   | 71/100 [17:45:38<7:25:26, 921.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 72%|███████▏  | 72/100 [18:00:23<7:04:59, 910.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 73%|███████▎  | 73/100 [18:16:41<6:58:52, 930.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 74%|███████▍  | 74/100 [18:34:28<7:01:05, 971.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 75%|███████▌  | 75/100 [18:51:42<6:52:36, 990.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 76%|███████▌  | 76/100 [19:08:24<6:37:36, 994.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 77%|███████▋  | 77/100 [19:23:57<6:14:00, 975.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 78%|███████▊  | 78/100 [19:36:43<5:34:39, 912.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 79%|███████▉  | 79/100 [19:52:13<5:21:17, 917.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 80%|████████  | 80/100 [20:07:54<5:08:15, 924.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 81%|████████  | 81/100 [20:22:46<4:49:42, 914.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 82%|████████▏ | 82/100 [20:35:48<4:22:33, 875.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 83%|████████▎ | 83/100 [20:53:01<4:21:23, 922.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 84%|████████▍ | 84/100 [21:07:39<4:02:24, 909.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 85%|████████▌ | 85/100 [21:20:58<3:39:03, 876.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 86%|████████▌ | 86/100 [21:36:17<3:27:24, 888.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 87%|████████▋ | 87/100 [21:52:52<3:19:28, 920.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 88%|████████▊ | 88/100 [22:10:10<3:11:10, 955.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 89%|████████▉ | 89/100 [22:25:40<2:53:50, 948.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 90%|█████████ | 90/100 [22:38:31<2:29:09, 894.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 91%|█████████ | 91/100 [22:54:10<2:16:12, 908.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 92%|█████████▏| 92/100 [23:06:51<1:55:13, 864.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 93%|█████████▎| 93/100 [23:21:38<1:41:35, 870.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 94%|█████████▍| 94/100 [23:36:54<1:28:27, 884.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 95%|█████████▌| 95/100 [23:50:46<1:12:23, 868.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 96%|█████████▌| 96/100 [24:07:00<1:00:00, 900.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 97%|█████████▋| 97/100 [24:23:59<46:47, 935.99s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 98%|█████████▊| 98/100 [24:37:38<30:01, 900.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 99%|█████████▉| 99/100 [24:54:59<15:42, 942.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
100%|██████████| 100/100 [25:11:43<00:00, 961.05s/it]100%|██████████| 100/100 [25:11:43<00:00, 907.03s/it]
tensor(11280.0977, grad_fn=<NegBackward0>) tensor(11280.0967, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11280.09765625
tensor(11280.0967, grad_fn=<NegBackward0>) tensor(11280.0977, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11280.10546875
tensor(11280.0967, grad_fn=<NegBackward0>) tensor(11280.1055, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11280.095703125
tensor(11280.0967, grad_fn=<NegBackward0>) tensor(11280.0957, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11280.095703125
tensor(11280.0957, grad_fn=<NegBackward0>) tensor(11280.0957, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11280.0947265625
tensor(11280.0957, grad_fn=<NegBackward0>) tensor(11280.0947, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11280.107421875
tensor(11280.0947, grad_fn=<NegBackward0>) tensor(11280.1074, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11280.0947265625
tensor(11280.0947, grad_fn=<NegBackward0>) tensor(11280.0947, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11280.09375
tensor(11280.0947, grad_fn=<NegBackward0>) tensor(11280.0938, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11280.09375
tensor(11280.0938, grad_fn=<NegBackward0>) tensor(11280.0938, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11280.1103515625
tensor(11280.0938, grad_fn=<NegBackward0>) tensor(11280.1104, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11280.091796875
tensor(11280.0938, grad_fn=<NegBackward0>) tensor(11280.0918, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11280.0927734375
tensor(11280.0918, grad_fn=<NegBackward0>) tensor(11280.0928, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11280.091796875
tensor(11280.0918, grad_fn=<NegBackward0>) tensor(11280.0918, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11280.0947265625
tensor(11280.0918, grad_fn=<NegBackward0>) tensor(11280.0947, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11280.0908203125
tensor(11280.0918, grad_fn=<NegBackward0>) tensor(11280.0908, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11280.0908203125
tensor(11280.0908, grad_fn=<NegBackward0>) tensor(11280.0908, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11267.9169921875
tensor(11280.0908, grad_fn=<NegBackward0>) tensor(11267.9170, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11267.916015625
tensor(11267.9170, grad_fn=<NegBackward0>) tensor(11267.9160, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11267.9345703125
tensor(11267.9160, grad_fn=<NegBackward0>) tensor(11267.9346, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11267.916015625
tensor(11267.9160, grad_fn=<NegBackward0>) tensor(11267.9160, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11267.9150390625
tensor(11267.9160, grad_fn=<NegBackward0>) tensor(11267.9150, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11267.9130859375
tensor(11267.9150, grad_fn=<NegBackward0>) tensor(11267.9131, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11267.912109375
tensor(11267.9131, grad_fn=<NegBackward0>) tensor(11267.9121, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11267.927734375
tensor(11267.9121, grad_fn=<NegBackward0>) tensor(11267.9277, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11267.9111328125
tensor(11267.9121, grad_fn=<NegBackward0>) tensor(11267.9111, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11267.912109375
tensor(11267.9111, grad_fn=<NegBackward0>) tensor(11267.9121, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11267.9091796875
tensor(11267.9111, grad_fn=<NegBackward0>) tensor(11267.9092, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11267.9560546875
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9561, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11267.919921875
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9199, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11267.919921875
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9199, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11267.9091796875
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9092, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11267.91015625
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9102, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11267.990234375
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9902, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11267.9091796875
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9092, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11267.9111328125
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9111, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11267.91015625
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9102, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11267.9091796875
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9092, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11267.912109375
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9121, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11267.916015625
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9160, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11267.908203125
tensor(11267.9092, grad_fn=<NegBackward0>) tensor(11267.9082, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11267.908203125
tensor(11267.9082, grad_fn=<NegBackward0>) tensor(11267.9082, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11267.9091796875
tensor(11267.9082, grad_fn=<NegBackward0>) tensor(11267.9092, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7421, 0.2579],
        [0.3225, 0.6775]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4536, 0.5464], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1930, 0.0985],
         [0.6955, 0.3962]],

        [[0.6546, 0.0875],
         [0.5598, 0.6992]],

        [[0.6059, 0.0813],
         [0.6758, 0.5399]],

        [[0.6736, 0.0961],
         [0.5419, 0.5136]],

        [[0.6609, 0.1080],
         [0.6327, 0.5428]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22474.38671875
inf tensor(22474.3867, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12028.00390625
tensor(22474.3867, grad_fn=<NegBackward0>) tensor(12028.0039, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11753.09765625
tensor(12028.0039, grad_fn=<NegBackward0>) tensor(11753.0977, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11469.037109375
tensor(11753.0977, grad_fn=<NegBackward0>) tensor(11469.0371, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11369.5654296875
tensor(11469.0371, grad_fn=<NegBackward0>) tensor(11369.5654, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11314.6240234375
tensor(11369.5654, grad_fn=<NegBackward0>) tensor(11314.6240, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11281.255859375
tensor(11314.6240, grad_fn=<NegBackward0>) tensor(11281.2559, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11280.8896484375
tensor(11281.2559, grad_fn=<NegBackward0>) tensor(11280.8896, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11280.69140625
tensor(11280.8896, grad_fn=<NegBackward0>) tensor(11280.6914, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11280.5634765625
tensor(11280.6914, grad_fn=<NegBackward0>) tensor(11280.5635, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11280.4716796875
tensor(11280.5635, grad_fn=<NegBackward0>) tensor(11280.4717, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11280.39453125
tensor(11280.4717, grad_fn=<NegBackward0>) tensor(11280.3945, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11268.1953125
tensor(11280.3945, grad_fn=<NegBackward0>) tensor(11268.1953, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11268.1533203125
tensor(11268.1953, grad_fn=<NegBackward0>) tensor(11268.1533, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11268.12109375
tensor(11268.1533, grad_fn=<NegBackward0>) tensor(11268.1211, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11268.09375
tensor(11268.1211, grad_fn=<NegBackward0>) tensor(11268.0938, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11268.072265625
tensor(11268.0938, grad_fn=<NegBackward0>) tensor(11268.0723, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11268.0546875
tensor(11268.0723, grad_fn=<NegBackward0>) tensor(11268.0547, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11268.0390625
tensor(11268.0547, grad_fn=<NegBackward0>) tensor(11268.0391, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11268.025390625
tensor(11268.0391, grad_fn=<NegBackward0>) tensor(11268.0254, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11268.0146484375
tensor(11268.0254, grad_fn=<NegBackward0>) tensor(11268.0146, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11268.0029296875
tensor(11268.0146, grad_fn=<NegBackward0>) tensor(11268.0029, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11267.99609375
tensor(11268.0029, grad_fn=<NegBackward0>) tensor(11267.9961, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11267.9892578125
tensor(11267.9961, grad_fn=<NegBackward0>) tensor(11267.9893, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11267.982421875
tensor(11267.9893, grad_fn=<NegBackward0>) tensor(11267.9824, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11267.9765625
tensor(11267.9824, grad_fn=<NegBackward0>) tensor(11267.9766, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11267.97265625
tensor(11267.9766, grad_fn=<NegBackward0>) tensor(11267.9727, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11267.966796875
tensor(11267.9727, grad_fn=<NegBackward0>) tensor(11267.9668, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11267.962890625
tensor(11267.9668, grad_fn=<NegBackward0>) tensor(11267.9629, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11267.958984375
tensor(11267.9629, grad_fn=<NegBackward0>) tensor(11267.9590, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11267.9677734375
tensor(11267.9590, grad_fn=<NegBackward0>) tensor(11267.9678, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11267.9521484375
tensor(11267.9590, grad_fn=<NegBackward0>) tensor(11267.9521, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11267.9501953125
tensor(11267.9521, grad_fn=<NegBackward0>) tensor(11267.9502, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11267.947265625
tensor(11267.9502, grad_fn=<NegBackward0>) tensor(11267.9473, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11267.9453125
tensor(11267.9473, grad_fn=<NegBackward0>) tensor(11267.9453, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11267.943359375
tensor(11267.9453, grad_fn=<NegBackward0>) tensor(11267.9434, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11267.9423828125
tensor(11267.9434, grad_fn=<NegBackward0>) tensor(11267.9424, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11267.9384765625
tensor(11267.9424, grad_fn=<NegBackward0>) tensor(11267.9385, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11267.935546875
tensor(11267.9385, grad_fn=<NegBackward0>) tensor(11267.9355, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11267.94140625
tensor(11267.9355, grad_fn=<NegBackward0>) tensor(11267.9414, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11267.9345703125
tensor(11267.9355, grad_fn=<NegBackward0>) tensor(11267.9346, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11267.93359375
tensor(11267.9346, grad_fn=<NegBackward0>) tensor(11267.9336, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11267.9326171875
tensor(11267.9336, grad_fn=<NegBackward0>) tensor(11267.9326, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11267.93359375
tensor(11267.9326, grad_fn=<NegBackward0>) tensor(11267.9336, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11267.9296875
tensor(11267.9326, grad_fn=<NegBackward0>) tensor(11267.9297, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11267.9306640625
tensor(11267.9297, grad_fn=<NegBackward0>) tensor(11267.9307, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11267.927734375
tensor(11267.9297, grad_fn=<NegBackward0>) tensor(11267.9277, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11267.9267578125
tensor(11267.9277, grad_fn=<NegBackward0>) tensor(11267.9268, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11267.9306640625
tensor(11267.9268, grad_fn=<NegBackward0>) tensor(11267.9307, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11267.9248046875
tensor(11267.9268, grad_fn=<NegBackward0>) tensor(11267.9248, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11267.923828125
tensor(11267.9248, grad_fn=<NegBackward0>) tensor(11267.9238, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11267.9228515625
tensor(11267.9238, grad_fn=<NegBackward0>) tensor(11267.9229, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11267.921875
tensor(11267.9229, grad_fn=<NegBackward0>) tensor(11267.9219, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11267.919921875
tensor(11267.9219, grad_fn=<NegBackward0>) tensor(11267.9199, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11267.92578125
tensor(11267.9199, grad_fn=<NegBackward0>) tensor(11267.9258, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11267.919921875
tensor(11267.9199, grad_fn=<NegBackward0>) tensor(11267.9199, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11267.9189453125
tensor(11267.9199, grad_fn=<NegBackward0>) tensor(11267.9189, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11267.9169921875
tensor(11267.9189, grad_fn=<NegBackward0>) tensor(11267.9170, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11267.91796875
tensor(11267.9170, grad_fn=<NegBackward0>) tensor(11267.9180, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11267.916015625
tensor(11267.9170, grad_fn=<NegBackward0>) tensor(11267.9160, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11267.9150390625
tensor(11267.9160, grad_fn=<NegBackward0>) tensor(11267.9150, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11267.91796875
tensor(11267.9150, grad_fn=<NegBackward0>) tensor(11267.9180, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11267.9130859375
tensor(11267.9150, grad_fn=<NegBackward0>) tensor(11267.9131, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11267.9140625
tensor(11267.9131, grad_fn=<NegBackward0>) tensor(11267.9141, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11267.9189453125
tensor(11267.9131, grad_fn=<NegBackward0>) tensor(11267.9189, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11267.916015625
tensor(11267.9131, grad_fn=<NegBackward0>) tensor(11267.9160, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11267.9140625
tensor(11267.9131, grad_fn=<NegBackward0>) tensor(11267.9141, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11267.9189453125
tensor(11267.9131, grad_fn=<NegBackward0>) tensor(11267.9189, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.7393, 0.2607],
        [0.3223, 0.6777]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4527, 0.5473], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1933, 0.0983],
         [0.6913, 0.3963]],

        [[0.6649, 0.0875],
         [0.5362, 0.6211]],

        [[0.6267, 0.0813],
         [0.5288, 0.6160]],

        [[0.6959, 0.0961],
         [0.6824, 0.7029]],

        [[0.7216, 0.1079],
         [0.6091, 0.5542]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.992
[0.9919999997943784, 0.9919999997943784] [0.992, 0.992] [11267.9091796875, 11267.9189453125]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11661.857235912514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22451.537109375
inf tensor(22451.5371, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12582.001953125
tensor(22451.5371, grad_fn=<NegBackward0>) tensor(12582.0020, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12046.388671875
tensor(12582.0020, grad_fn=<NegBackward0>) tensor(12046.3887, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11845.65625
tensor(12046.3887, grad_fn=<NegBackward0>) tensor(11845.6562, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11822.2529296875
tensor(11845.6562, grad_fn=<NegBackward0>) tensor(11822.2529, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11773.291015625
tensor(11822.2529, grad_fn=<NegBackward0>) tensor(11773.2910, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11748.84765625
tensor(11773.2910, grad_fn=<NegBackward0>) tensor(11748.8477, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11742.7939453125
tensor(11748.8477, grad_fn=<NegBackward0>) tensor(11742.7939, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11739.09375
tensor(11742.7939, grad_fn=<NegBackward0>) tensor(11739.0938, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11738.00390625
tensor(11739.0938, grad_fn=<NegBackward0>) tensor(11738.0039, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11704.2470703125
tensor(11738.0039, grad_fn=<NegBackward0>) tensor(11704.2471, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11690.20703125
tensor(11704.2471, grad_fn=<NegBackward0>) tensor(11690.2070, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11669.6015625
tensor(11690.2070, grad_fn=<NegBackward0>) tensor(11669.6016, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11666.0283203125
tensor(11669.6016, grad_fn=<NegBackward0>) tensor(11666.0283, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11665.986328125
tensor(11666.0283, grad_fn=<NegBackward0>) tensor(11665.9863, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11661.189453125
tensor(11665.9863, grad_fn=<NegBackward0>) tensor(11661.1895, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11661.099609375
tensor(11661.1895, grad_fn=<NegBackward0>) tensor(11661.0996, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11658.125
tensor(11661.0996, grad_fn=<NegBackward0>) tensor(11658.1250, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11658.109375
tensor(11658.1250, grad_fn=<NegBackward0>) tensor(11658.1094, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11658.0986328125
tensor(11658.1094, grad_fn=<NegBackward0>) tensor(11658.0986, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11658.087890625
tensor(11658.0986, grad_fn=<NegBackward0>) tensor(11658.0879, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11658.08203125
tensor(11658.0879, grad_fn=<NegBackward0>) tensor(11658.0820, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11658.0712890625
tensor(11658.0820, grad_fn=<NegBackward0>) tensor(11658.0713, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11658.064453125
tensor(11658.0713, grad_fn=<NegBackward0>) tensor(11658.0645, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11658.0576171875
tensor(11658.0645, grad_fn=<NegBackward0>) tensor(11658.0576, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11658.048828125
tensor(11658.0576, grad_fn=<NegBackward0>) tensor(11658.0488, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11658.0107421875
tensor(11658.0488, grad_fn=<NegBackward0>) tensor(11658.0107, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11658.0048828125
tensor(11658.0107, grad_fn=<NegBackward0>) tensor(11658.0049, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11658.001953125
tensor(11658.0049, grad_fn=<NegBackward0>) tensor(11658.0020, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11657.998046875
tensor(11658.0020, grad_fn=<NegBackward0>) tensor(11657.9980, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11657.994140625
tensor(11657.9980, grad_fn=<NegBackward0>) tensor(11657.9941, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11657.9931640625
tensor(11657.9941, grad_fn=<NegBackward0>) tensor(11657.9932, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11657.9912109375
tensor(11657.9932, grad_fn=<NegBackward0>) tensor(11657.9912, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11657.9873046875
tensor(11657.9912, grad_fn=<NegBackward0>) tensor(11657.9873, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11657.9951171875
tensor(11657.9873, grad_fn=<NegBackward0>) tensor(11657.9951, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11657.9814453125
tensor(11657.9873, grad_fn=<NegBackward0>) tensor(11657.9814, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11657.9765625
tensor(11657.9814, grad_fn=<NegBackward0>) tensor(11657.9766, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11657.9775390625
tensor(11657.9766, grad_fn=<NegBackward0>) tensor(11657.9775, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11657.974609375
tensor(11657.9766, grad_fn=<NegBackward0>) tensor(11657.9746, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11657.9736328125
tensor(11657.9746, grad_fn=<NegBackward0>) tensor(11657.9736, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11657.97265625
tensor(11657.9736, grad_fn=<NegBackward0>) tensor(11657.9727, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11657.970703125
tensor(11657.9727, grad_fn=<NegBackward0>) tensor(11657.9707, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11657.9775390625
tensor(11657.9707, grad_fn=<NegBackward0>) tensor(11657.9775, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11657.96875
tensor(11657.9707, grad_fn=<NegBackward0>) tensor(11657.9688, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11657.96875
tensor(11657.9688, grad_fn=<NegBackward0>) tensor(11657.9688, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11657.966796875
tensor(11657.9688, grad_fn=<NegBackward0>) tensor(11657.9668, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11657.966796875
tensor(11657.9668, grad_fn=<NegBackward0>) tensor(11657.9668, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11657.9658203125
tensor(11657.9668, grad_fn=<NegBackward0>) tensor(11657.9658, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11657.96484375
tensor(11657.9658, grad_fn=<NegBackward0>) tensor(11657.9648, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11657.96484375
tensor(11657.9648, grad_fn=<NegBackward0>) tensor(11657.9648, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11657.9638671875
tensor(11657.9648, grad_fn=<NegBackward0>) tensor(11657.9639, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11657.9130859375
tensor(11657.9639, grad_fn=<NegBackward0>) tensor(11657.9131, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11657.904296875
tensor(11657.9131, grad_fn=<NegBackward0>) tensor(11657.9043, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11657.892578125
tensor(11657.9043, grad_fn=<NegBackward0>) tensor(11657.8926, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11657.892578125
tensor(11657.8926, grad_fn=<NegBackward0>) tensor(11657.8926, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11657.8916015625
tensor(11657.8926, grad_fn=<NegBackward0>) tensor(11657.8916, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11657.890625
tensor(11657.8916, grad_fn=<NegBackward0>) tensor(11657.8906, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11657.892578125
tensor(11657.8906, grad_fn=<NegBackward0>) tensor(11657.8926, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11657.8896484375
tensor(11657.8906, grad_fn=<NegBackward0>) tensor(11657.8896, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11657.90234375
tensor(11657.8896, grad_fn=<NegBackward0>) tensor(11657.9023, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11657.888671875
tensor(11657.8896, grad_fn=<NegBackward0>) tensor(11657.8887, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11657.8896484375
tensor(11657.8887, grad_fn=<NegBackward0>) tensor(11657.8896, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11657.8876953125
tensor(11657.8887, grad_fn=<NegBackward0>) tensor(11657.8877, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11657.888671875
tensor(11657.8877, grad_fn=<NegBackward0>) tensor(11657.8887, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11657.8984375
tensor(11657.8877, grad_fn=<NegBackward0>) tensor(11657.8984, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11657.8876953125
tensor(11657.8877, grad_fn=<NegBackward0>) tensor(11657.8877, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11657.8876953125
tensor(11657.8877, grad_fn=<NegBackward0>) tensor(11657.8877, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11657.8857421875
tensor(11657.8877, grad_fn=<NegBackward0>) tensor(11657.8857, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11657.8876953125
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8877, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11657.88671875
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8867, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11657.8857421875
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8857, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11657.8857421875
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8857, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11657.8896484375
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8896, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11657.8857421875
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8857, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11657.8916015625
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8916, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11657.884765625
tensor(11657.8857, grad_fn=<NegBackward0>) tensor(11657.8848, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11657.890625
tensor(11657.8848, grad_fn=<NegBackward0>) tensor(11657.8906, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11657.8837890625
tensor(11657.8848, grad_fn=<NegBackward0>) tensor(11657.8838, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11658.0634765625
tensor(11657.8838, grad_fn=<NegBackward0>) tensor(11658.0635, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11657.8916015625
tensor(11657.8838, grad_fn=<NegBackward0>) tensor(11657.8916, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11657.8818359375
tensor(11657.8838, grad_fn=<NegBackward0>) tensor(11657.8818, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11657.88671875
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8867, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11657.8818359375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8818, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11657.89453125
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8945, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11657.8837890625
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8838, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11658.0234375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11658.0234, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11657.8828125
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8828, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11657.8818359375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8818, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11657.8828125
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8828, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11657.8818359375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8818, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11657.8818359375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8818, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11657.8828125
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8828, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11657.8828125
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8828, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11657.912109375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.9121, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11657.8818359375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8818, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11657.8837890625
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8838, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11657.880859375
tensor(11657.8818, grad_fn=<NegBackward0>) tensor(11657.8809, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11657.8916015625
tensor(11657.8809, grad_fn=<NegBackward0>) tensor(11657.8916, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11657.88671875
tensor(11657.8809, grad_fn=<NegBackward0>) tensor(11657.8867, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11657.8857421875
tensor(11657.8809, grad_fn=<NegBackward0>) tensor(11657.8857, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7351, 0.2649],
        [0.2209, 0.7791]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4796, 0.5204], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1953, 0.1066],
         [0.5360, 0.4107]],

        [[0.7055, 0.0927],
         [0.5552, 0.6460]],

        [[0.7174, 0.1026],
         [0.6199, 0.6516]],

        [[0.5588, 0.0982],
         [0.7230, 0.7247]],

        [[0.6034, 0.0983],
         [0.6876, 0.5134]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22421.03515625
inf tensor(22421.0352, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12583.275390625
tensor(22421.0352, grad_fn=<NegBackward0>) tensor(12583.2754, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12139.1826171875
tensor(12583.2754, grad_fn=<NegBackward0>) tensor(12139.1826, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12045.0625
tensor(12139.1826, grad_fn=<NegBackward0>) tensor(12045.0625, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12029.9814453125
tensor(12045.0625, grad_fn=<NegBackward0>) tensor(12029.9814, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12016.986328125
tensor(12029.9814, grad_fn=<NegBackward0>) tensor(12016.9863, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12014.587890625
tensor(12016.9863, grad_fn=<NegBackward0>) tensor(12014.5879, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12014.30859375
tensor(12014.5879, grad_fn=<NegBackward0>) tensor(12014.3086, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12014.01953125
tensor(12014.3086, grad_fn=<NegBackward0>) tensor(12014.0195, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12008.1806640625
tensor(12014.0195, grad_fn=<NegBackward0>) tensor(12008.1807, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12006.0546875
tensor(12008.1807, grad_fn=<NegBackward0>) tensor(12006.0547, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12004.96484375
tensor(12006.0547, grad_fn=<NegBackward0>) tensor(12004.9648, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12004.2685546875
tensor(12004.9648, grad_fn=<NegBackward0>) tensor(12004.2686, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12003.9013671875
tensor(12004.2686, grad_fn=<NegBackward0>) tensor(12003.9014, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12003.8818359375
tensor(12003.9014, grad_fn=<NegBackward0>) tensor(12003.8818, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12003.8681640625
tensor(12003.8818, grad_fn=<NegBackward0>) tensor(12003.8682, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12003.8583984375
tensor(12003.8682, grad_fn=<NegBackward0>) tensor(12003.8584, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12003.84765625
tensor(12003.8584, grad_fn=<NegBackward0>) tensor(12003.8477, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12003.576171875
tensor(12003.8477, grad_fn=<NegBackward0>) tensor(12003.5762, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12002.697265625
tensor(12003.5762, grad_fn=<NegBackward0>) tensor(12002.6973, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12002.685546875
tensor(12002.6973, grad_fn=<NegBackward0>) tensor(12002.6855, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12002.6240234375
tensor(12002.6855, grad_fn=<NegBackward0>) tensor(12002.6240, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12002.6123046875
tensor(12002.6240, grad_fn=<NegBackward0>) tensor(12002.6123, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12002.607421875
tensor(12002.6123, grad_fn=<NegBackward0>) tensor(12002.6074, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12002.603515625
tensor(12002.6074, grad_fn=<NegBackward0>) tensor(12002.6035, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12002.595703125
tensor(12002.6035, grad_fn=<NegBackward0>) tensor(12002.5957, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12002.568359375
tensor(12002.5957, grad_fn=<NegBackward0>) tensor(12002.5684, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12002.564453125
tensor(12002.5684, grad_fn=<NegBackward0>) tensor(12002.5645, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12002.5615234375
tensor(12002.5645, grad_fn=<NegBackward0>) tensor(12002.5615, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12002.5556640625
tensor(12002.5615, grad_fn=<NegBackward0>) tensor(12002.5557, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12001.8876953125
tensor(12002.5557, grad_fn=<NegBackward0>) tensor(12001.8877, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12000.111328125
tensor(12001.8877, grad_fn=<NegBackward0>) tensor(12000.1113, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12000.0380859375
tensor(12000.1113, grad_fn=<NegBackward0>) tensor(12000.0381, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12000.0234375
tensor(12000.0381, grad_fn=<NegBackward0>) tensor(12000.0234, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12000.0234375
tensor(12000.0234, grad_fn=<NegBackward0>) tensor(12000.0234, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12000.0224609375
tensor(12000.0234, grad_fn=<NegBackward0>) tensor(12000.0225, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12000.01953125
tensor(12000.0225, grad_fn=<NegBackward0>) tensor(12000.0195, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12000.0185546875
tensor(12000.0195, grad_fn=<NegBackward0>) tensor(12000.0186, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12000.01953125
tensor(12000.0186, grad_fn=<NegBackward0>) tensor(12000.0195, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12000.0224609375
tensor(12000.0186, grad_fn=<NegBackward0>) tensor(12000.0225, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -12000.0185546875
tensor(12000.0186, grad_fn=<NegBackward0>) tensor(12000.0186, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12000.017578125
tensor(12000.0186, grad_fn=<NegBackward0>) tensor(12000.0176, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12000.0166015625
tensor(12000.0176, grad_fn=<NegBackward0>) tensor(12000.0166, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12000.015625
tensor(12000.0166, grad_fn=<NegBackward0>) tensor(12000.0156, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12000.0146484375
tensor(12000.0156, grad_fn=<NegBackward0>) tensor(12000.0146, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12000.0205078125
tensor(12000.0146, grad_fn=<NegBackward0>) tensor(12000.0205, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12000.0146484375
tensor(12000.0146, grad_fn=<NegBackward0>) tensor(12000.0146, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12000.0146484375
tensor(12000.0146, grad_fn=<NegBackward0>) tensor(12000.0146, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12000.013671875
tensor(12000.0146, grad_fn=<NegBackward0>) tensor(12000.0137, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12000.0126953125
tensor(12000.0137, grad_fn=<NegBackward0>) tensor(12000.0127, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12000.01171875
tensor(12000.0127, grad_fn=<NegBackward0>) tensor(12000.0117, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11993.7626953125
tensor(12000.0117, grad_fn=<NegBackward0>) tensor(11993.7627, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11993.0771484375
tensor(11993.7627, grad_fn=<NegBackward0>) tensor(11993.0771, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11993.07421875
tensor(11993.0771, grad_fn=<NegBackward0>) tensor(11993.0742, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11993.0732421875
tensor(11993.0742, grad_fn=<NegBackward0>) tensor(11993.0732, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11989.3447265625
tensor(11993.0732, grad_fn=<NegBackward0>) tensor(11989.3447, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11986.07421875
tensor(11989.3447, grad_fn=<NegBackward0>) tensor(11986.0742, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11986.0693359375
tensor(11986.0742, grad_fn=<NegBackward0>) tensor(11986.0693, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11986.076171875
tensor(11986.0693, grad_fn=<NegBackward0>) tensor(11986.0762, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11986.0673828125
tensor(11986.0693, grad_fn=<NegBackward0>) tensor(11986.0674, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11986.0625
tensor(11986.0674, grad_fn=<NegBackward0>) tensor(11986.0625, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11986.05859375
tensor(11986.0625, grad_fn=<NegBackward0>) tensor(11986.0586, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11986.05859375
tensor(11986.0586, grad_fn=<NegBackward0>) tensor(11986.0586, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11986.0634765625
tensor(11986.0586, grad_fn=<NegBackward0>) tensor(11986.0635, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11986.0576171875
tensor(11986.0586, grad_fn=<NegBackward0>) tensor(11986.0576, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11986.0576171875
tensor(11986.0576, grad_fn=<NegBackward0>) tensor(11986.0576, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11986.05859375
tensor(11986.0576, grad_fn=<NegBackward0>) tensor(11986.0586, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11986.0595703125
tensor(11986.0576, grad_fn=<NegBackward0>) tensor(11986.0596, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11986.0634765625
tensor(11986.0576, grad_fn=<NegBackward0>) tensor(11986.0635, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11986.0556640625
tensor(11986.0576, grad_fn=<NegBackward0>) tensor(11986.0557, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11986.0517578125
tensor(11986.0557, grad_fn=<NegBackward0>) tensor(11986.0518, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11986.048828125
tensor(11986.0518, grad_fn=<NegBackward0>) tensor(11986.0488, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11986.0556640625
tensor(11986.0488, grad_fn=<NegBackward0>) tensor(11986.0557, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11986.056640625
tensor(11986.0488, grad_fn=<NegBackward0>) tensor(11986.0566, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11986.0498046875
tensor(11986.0488, grad_fn=<NegBackward0>) tensor(11986.0498, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11986.0517578125
tensor(11986.0488, grad_fn=<NegBackward0>) tensor(11986.0518, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11986.1171875
tensor(11986.0488, grad_fn=<NegBackward0>) tensor(11986.1172, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.4125, 0.5875],
        [0.6541, 0.3459]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6685, 0.3315], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2643, 0.1020],
         [0.5988, 0.3692]],

        [[0.6794, 0.0918],
         [0.5358, 0.5909]],

        [[0.6440, 0.1007],
         [0.5596, 0.6040]],

        [[0.6976, 0.1017],
         [0.5686, 0.7166]],

        [[0.5637, 0.0982],
         [0.6852, 0.6733]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 14
Adjusted Rand Index: 0.5139202893869905
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 10
Adjusted Rand Index: 0.6364549732959653
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.08110160097574139
Average Adjusted Rand Index: 0.8140736105936103
[1.0, 0.08110160097574139] [1.0, 0.8140736105936103] [11657.90234375, 11986.1171875]
-------------------------------------
This iteration is 71
True Objective function: Loss = -11342.886327549195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21932.744140625
inf tensor(21932.7441, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11902.31640625
tensor(21932.7441, grad_fn=<NegBackward0>) tensor(11902.3164, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11896.0869140625
tensor(11902.3164, grad_fn=<NegBackward0>) tensor(11896.0869, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11891.0185546875
tensor(11896.0869, grad_fn=<NegBackward0>) tensor(11891.0186, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11715.6845703125
tensor(11891.0186, grad_fn=<NegBackward0>) tensor(11715.6846, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11536.888671875
tensor(11715.6846, grad_fn=<NegBackward0>) tensor(11536.8887, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11496.0390625
tensor(11536.8887, grad_fn=<NegBackward0>) tensor(11496.0391, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11465.7080078125
tensor(11496.0391, grad_fn=<NegBackward0>) tensor(11465.7080, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11457.7685546875
tensor(11465.7080, grad_fn=<NegBackward0>) tensor(11457.7686, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11457.5224609375
tensor(11457.7686, grad_fn=<NegBackward0>) tensor(11457.5225, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11457.3798828125
tensor(11457.5225, grad_fn=<NegBackward0>) tensor(11457.3799, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11457.28125
tensor(11457.3799, grad_fn=<NegBackward0>) tensor(11457.2812, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11457.169921875
tensor(11457.2812, grad_fn=<NegBackward0>) tensor(11457.1699, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11448.7822265625
tensor(11457.1699, grad_fn=<NegBackward0>) tensor(11448.7822, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11448.740234375
tensor(11448.7822, grad_fn=<NegBackward0>) tensor(11448.7402, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11448.7080078125
tensor(11448.7402, grad_fn=<NegBackward0>) tensor(11448.7080, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11448.6806640625
tensor(11448.7080, grad_fn=<NegBackward0>) tensor(11448.6807, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11448.6435546875
tensor(11448.6807, grad_fn=<NegBackward0>) tensor(11448.6436, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11447.84765625
tensor(11448.6436, grad_fn=<NegBackward0>) tensor(11447.8477, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11447.8330078125
tensor(11447.8477, grad_fn=<NegBackward0>) tensor(11447.8330, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11447.8212890625
tensor(11447.8330, grad_fn=<NegBackward0>) tensor(11447.8213, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11447.8095703125
tensor(11447.8213, grad_fn=<NegBackward0>) tensor(11447.8096, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11447.80078125
tensor(11447.8096, grad_fn=<NegBackward0>) tensor(11447.8008, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11447.791015625
tensor(11447.8008, grad_fn=<NegBackward0>) tensor(11447.7910, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11447.783203125
tensor(11447.7910, grad_fn=<NegBackward0>) tensor(11447.7832, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11447.775390625
tensor(11447.7832, grad_fn=<NegBackward0>) tensor(11447.7754, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11447.767578125
tensor(11447.7754, grad_fn=<NegBackward0>) tensor(11447.7676, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11447.76171875
tensor(11447.7676, grad_fn=<NegBackward0>) tensor(11447.7617, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11447.755859375
tensor(11447.7617, grad_fn=<NegBackward0>) tensor(11447.7559, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11447.751953125
tensor(11447.7559, grad_fn=<NegBackward0>) tensor(11447.7520, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11447.7470703125
tensor(11447.7520, grad_fn=<NegBackward0>) tensor(11447.7471, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11447.74609375
tensor(11447.7471, grad_fn=<NegBackward0>) tensor(11447.7461, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11447.7421875
tensor(11447.7461, grad_fn=<NegBackward0>) tensor(11447.7422, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11447.7392578125
tensor(11447.7422, grad_fn=<NegBackward0>) tensor(11447.7393, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11447.7373046875
tensor(11447.7393, grad_fn=<NegBackward0>) tensor(11447.7373, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11447.7353515625
tensor(11447.7373, grad_fn=<NegBackward0>) tensor(11447.7354, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11447.732421875
tensor(11447.7354, grad_fn=<NegBackward0>) tensor(11447.7324, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11447.73046875
tensor(11447.7324, grad_fn=<NegBackward0>) tensor(11447.7305, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11447.728515625
tensor(11447.7305, grad_fn=<NegBackward0>) tensor(11447.7285, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11447.7255859375
tensor(11447.7285, grad_fn=<NegBackward0>) tensor(11447.7256, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11447.7255859375
tensor(11447.7256, grad_fn=<NegBackward0>) tensor(11447.7256, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11447.7236328125
tensor(11447.7256, grad_fn=<NegBackward0>) tensor(11447.7236, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11447.7216796875
tensor(11447.7236, grad_fn=<NegBackward0>) tensor(11447.7217, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11447.720703125
tensor(11447.7217, grad_fn=<NegBackward0>) tensor(11447.7207, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11447.7216796875
tensor(11447.7207, grad_fn=<NegBackward0>) tensor(11447.7217, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11447.71875
tensor(11447.7207, grad_fn=<NegBackward0>) tensor(11447.7188, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11447.7177734375
tensor(11447.7188, grad_fn=<NegBackward0>) tensor(11447.7178, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11447.716796875
tensor(11447.7178, grad_fn=<NegBackward0>) tensor(11447.7168, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11447.7158203125
tensor(11447.7168, grad_fn=<NegBackward0>) tensor(11447.7158, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11447.71484375
tensor(11447.7158, grad_fn=<NegBackward0>) tensor(11447.7148, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11447.7138671875
tensor(11447.7148, grad_fn=<NegBackward0>) tensor(11447.7139, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11447.7138671875
tensor(11447.7139, grad_fn=<NegBackward0>) tensor(11447.7139, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11447.7109375
tensor(11447.7139, grad_fn=<NegBackward0>) tensor(11447.7109, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11447.7109375
tensor(11447.7109, grad_fn=<NegBackward0>) tensor(11447.7109, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11447.708984375
tensor(11447.7109, grad_fn=<NegBackward0>) tensor(11447.7090, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11447.703125
tensor(11447.7090, grad_fn=<NegBackward0>) tensor(11447.7031, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11447.6943359375
tensor(11447.7031, grad_fn=<NegBackward0>) tensor(11447.6943, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11447.4228515625
tensor(11447.6943, grad_fn=<NegBackward0>) tensor(11447.4229, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11445.4111328125
tensor(11447.4229, grad_fn=<NegBackward0>) tensor(11445.4111, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11440.86328125
tensor(11445.4111, grad_fn=<NegBackward0>) tensor(11440.8633, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11440.8134765625
tensor(11440.8633, grad_fn=<NegBackward0>) tensor(11440.8135, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11440.7998046875
tensor(11440.8135, grad_fn=<NegBackward0>) tensor(11440.7998, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11440.76953125
tensor(11440.7998, grad_fn=<NegBackward0>) tensor(11440.7695, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11440.7578125
tensor(11440.7695, grad_fn=<NegBackward0>) tensor(11440.7578, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11440.755859375
tensor(11440.7578, grad_fn=<NegBackward0>) tensor(11440.7559, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11440.7607421875
tensor(11440.7559, grad_fn=<NegBackward0>) tensor(11440.7607, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11440.2236328125
tensor(11440.7559, grad_fn=<NegBackward0>) tensor(11440.2236, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11440.2109375
tensor(11440.2236, grad_fn=<NegBackward0>) tensor(11440.2109, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11440.2060546875
tensor(11440.2109, grad_fn=<NegBackward0>) tensor(11440.2061, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11440.205078125
tensor(11440.2061, grad_fn=<NegBackward0>) tensor(11440.2051, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11440.197265625
tensor(11440.2051, grad_fn=<NegBackward0>) tensor(11440.1973, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11440.1806640625
tensor(11440.1973, grad_fn=<NegBackward0>) tensor(11440.1807, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11440.1728515625
tensor(11440.1807, grad_fn=<NegBackward0>) tensor(11440.1729, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11440.1044921875
tensor(11440.1729, grad_fn=<NegBackward0>) tensor(11440.1045, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11440.1044921875
tensor(11440.1045, grad_fn=<NegBackward0>) tensor(11440.1045, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11440.1025390625
tensor(11440.1045, grad_fn=<NegBackward0>) tensor(11440.1025, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11440.1015625
tensor(11440.1025, grad_fn=<NegBackward0>) tensor(11440.1016, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11440.1015625
tensor(11440.1016, grad_fn=<NegBackward0>) tensor(11440.1016, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11440.1083984375
tensor(11440.1016, grad_fn=<NegBackward0>) tensor(11440.1084, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11440.1083984375
tensor(11440.1016, grad_fn=<NegBackward0>) tensor(11440.1084, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11440.099609375
tensor(11440.1016, grad_fn=<NegBackward0>) tensor(11440.0996, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11440.1015625
tensor(11440.0996, grad_fn=<NegBackward0>) tensor(11440.1016, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11440.1044921875
tensor(11440.0996, grad_fn=<NegBackward0>) tensor(11440.1045, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11440.12109375
tensor(11440.0996, grad_fn=<NegBackward0>) tensor(11440.1211, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11440.1171875
tensor(11440.0996, grad_fn=<NegBackward0>) tensor(11440.1172, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11440.1103515625
tensor(11440.0996, grad_fn=<NegBackward0>) tensor(11440.1104, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.6878, 0.3122],
        [0.3241, 0.6759]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9166, 0.0834], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2043, 0.1007],
         [0.6817, 0.3853]],

        [[0.6674, 0.1020],
         [0.7285, 0.6435]],

        [[0.6865, 0.0999],
         [0.5989, 0.5775]],

        [[0.5585, 0.0938],
         [0.7126, 0.6870]],

        [[0.5271, 0.0970],
         [0.6542, 0.6899]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.016623577451856865
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6134503706011325
Average Adjusted Rand Index: 0.7728293341935916
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19934.63671875
inf tensor(19934.6367, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11902.5615234375
tensor(19934.6367, grad_fn=<NegBackward0>) tensor(11902.5615, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11895.5185546875
tensor(11902.5615, grad_fn=<NegBackward0>) tensor(11895.5186, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11853.865234375
tensor(11895.5186, grad_fn=<NegBackward0>) tensor(11853.8652, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11492.1298828125
tensor(11853.8652, grad_fn=<NegBackward0>) tensor(11492.1299, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11460.5126953125
tensor(11492.1299, grad_fn=<NegBackward0>) tensor(11460.5127, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11457.228515625
tensor(11460.5127, grad_fn=<NegBackward0>) tensor(11457.2285, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11448.0908203125
tensor(11457.2285, grad_fn=<NegBackward0>) tensor(11448.0908, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11447.9775390625
tensor(11448.0908, grad_fn=<NegBackward0>) tensor(11447.9775, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11447.9072265625
tensor(11447.9775, grad_fn=<NegBackward0>) tensor(11447.9072, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11447.8583984375
tensor(11447.9072, grad_fn=<NegBackward0>) tensor(11447.8584, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11447.822265625
tensor(11447.8584, grad_fn=<NegBackward0>) tensor(11447.8223, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11447.79296875
tensor(11447.8223, grad_fn=<NegBackward0>) tensor(11447.7930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11447.767578125
tensor(11447.7930, grad_fn=<NegBackward0>) tensor(11447.7676, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11447.7451171875
tensor(11447.7676, grad_fn=<NegBackward0>) tensor(11447.7451, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11447.716796875
tensor(11447.7451, grad_fn=<NegBackward0>) tensor(11447.7168, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11447.67578125
tensor(11447.7168, grad_fn=<NegBackward0>) tensor(11447.6758, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11447.5439453125
tensor(11447.6758, grad_fn=<NegBackward0>) tensor(11447.5439, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11442.1845703125
tensor(11447.5439, grad_fn=<NegBackward0>) tensor(11442.1846, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11440.09375
tensor(11442.1846, grad_fn=<NegBackward0>) tensor(11440.0938, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11440.017578125
tensor(11440.0938, grad_fn=<NegBackward0>) tensor(11440.0176, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11439.9970703125
tensor(11440.0176, grad_fn=<NegBackward0>) tensor(11439.9971, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11439.986328125
tensor(11439.9971, grad_fn=<NegBackward0>) tensor(11439.9863, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11439.98046875
tensor(11439.9863, grad_fn=<NegBackward0>) tensor(11439.9805, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11439.9755859375
tensor(11439.9805, grad_fn=<NegBackward0>) tensor(11439.9756, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11439.9716796875
tensor(11439.9756, grad_fn=<NegBackward0>) tensor(11439.9717, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11439.96875
tensor(11439.9717, grad_fn=<NegBackward0>) tensor(11439.9688, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11439.9658203125
tensor(11439.9688, grad_fn=<NegBackward0>) tensor(11439.9658, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11439.9619140625
tensor(11439.9658, grad_fn=<NegBackward0>) tensor(11439.9619, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11439.958984375
tensor(11439.9619, grad_fn=<NegBackward0>) tensor(11439.9590, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11439.955078125
tensor(11439.9590, grad_fn=<NegBackward0>) tensor(11439.9551, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11439.9541015625
tensor(11439.9551, grad_fn=<NegBackward0>) tensor(11439.9541, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11439.9521484375
tensor(11439.9541, grad_fn=<NegBackward0>) tensor(11439.9521, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11439.951171875
tensor(11439.9521, grad_fn=<NegBackward0>) tensor(11439.9512, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11439.9501953125
tensor(11439.9512, grad_fn=<NegBackward0>) tensor(11439.9502, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11439.9482421875
tensor(11439.9502, grad_fn=<NegBackward0>) tensor(11439.9482, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11439.947265625
tensor(11439.9482, grad_fn=<NegBackward0>) tensor(11439.9473, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11439.9482421875
tensor(11439.9473, grad_fn=<NegBackward0>) tensor(11439.9482, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11439.9462890625
tensor(11439.9473, grad_fn=<NegBackward0>) tensor(11439.9463, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11439.9462890625
tensor(11439.9463, grad_fn=<NegBackward0>) tensor(11439.9463, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11439.9462890625
tensor(11439.9463, grad_fn=<NegBackward0>) tensor(11439.9463, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11439.9453125
tensor(11439.9463, grad_fn=<NegBackward0>) tensor(11439.9453, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11439.9443359375
tensor(11439.9453, grad_fn=<NegBackward0>) tensor(11439.9443, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11439.943359375
tensor(11439.9443, grad_fn=<NegBackward0>) tensor(11439.9434, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11439.943359375
tensor(11439.9434, grad_fn=<NegBackward0>) tensor(11439.9434, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11439.943359375
tensor(11439.9434, grad_fn=<NegBackward0>) tensor(11439.9434, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11439.9423828125
tensor(11439.9434, grad_fn=<NegBackward0>) tensor(11439.9424, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11439.94140625
tensor(11439.9424, grad_fn=<NegBackward0>) tensor(11439.9414, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11439.94140625
tensor(11439.9414, grad_fn=<NegBackward0>) tensor(11439.9414, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11439.94140625
tensor(11439.9414, grad_fn=<NegBackward0>) tensor(11439.9414, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11439.9404296875
tensor(11439.9414, grad_fn=<NegBackward0>) tensor(11439.9404, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11439.9404296875
tensor(11439.9404, grad_fn=<NegBackward0>) tensor(11439.9404, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11439.939453125
tensor(11439.9404, grad_fn=<NegBackward0>) tensor(11439.9395, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11439.9423828125
tensor(11439.9395, grad_fn=<NegBackward0>) tensor(11439.9424, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11439.9404296875
tensor(11439.9395, grad_fn=<NegBackward0>) tensor(11439.9404, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11439.9404296875
tensor(11439.9395, grad_fn=<NegBackward0>) tensor(11439.9404, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11439.939453125
tensor(11439.9395, grad_fn=<NegBackward0>) tensor(11439.9395, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11439.939453125
tensor(11439.9395, grad_fn=<NegBackward0>) tensor(11439.9395, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11439.9384765625
tensor(11439.9395, grad_fn=<NegBackward0>) tensor(11439.9385, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11439.939453125
tensor(11439.9385, grad_fn=<NegBackward0>) tensor(11439.9395, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11439.9375
tensor(11439.9385, grad_fn=<NegBackward0>) tensor(11439.9375, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11439.9384765625
tensor(11439.9375, grad_fn=<NegBackward0>) tensor(11439.9385, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11439.9375
tensor(11439.9375, grad_fn=<NegBackward0>) tensor(11439.9375, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11439.9404296875
tensor(11439.9375, grad_fn=<NegBackward0>) tensor(11439.9404, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11439.947265625
tensor(11439.9375, grad_fn=<NegBackward0>) tensor(11439.9473, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11439.9384765625
tensor(11439.9375, grad_fn=<NegBackward0>) tensor(11439.9385, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11439.9365234375
tensor(11439.9375, grad_fn=<NegBackward0>) tensor(11439.9365, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11439.9365234375
tensor(11439.9365, grad_fn=<NegBackward0>) tensor(11439.9365, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11439.9443359375
tensor(11439.9365, grad_fn=<NegBackward0>) tensor(11439.9443, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11439.935546875
tensor(11439.9365, grad_fn=<NegBackward0>) tensor(11439.9355, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11439.9365234375
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9365, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11439.9375
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9375, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11439.9375
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9375, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11439.935546875
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9355, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11439.9365234375
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9365, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11439.9453125
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9453, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11439.9365234375
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9365, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11439.939453125
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9395, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11439.9365234375
tensor(11439.9355, grad_fn=<NegBackward0>) tensor(11439.9365, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.6763, 0.3237],
        [0.3113, 0.6887]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0831, 0.9169], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3844, 0.1008],
         [0.7083, 0.2039]],

        [[0.5710, 0.1020],
         [0.6943, 0.6908]],

        [[0.6032, 0.0999],
         [0.5336, 0.5743]],

        [[0.6524, 0.0937],
         [0.6320, 0.5923]],

        [[0.5935, 0.0970],
         [0.6704, 0.6605]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.016623577451856865
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6134503706011325
Average Adjusted Rand Index: 0.7728293341935916
[0.6134503706011325, 0.6134503706011325] [0.7728293341935916, 0.7728293341935916] [11440.1103515625, 11439.9365234375]
-------------------------------------
This iteration is 72
True Objective function: Loss = -11368.076982998846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22623.35546875
inf tensor(22623.3555, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12071.0947265625
tensor(22623.3555, grad_fn=<NegBackward0>) tensor(12071.0947, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11550.29296875
tensor(12071.0947, grad_fn=<NegBackward0>) tensor(11550.2930, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11491.3447265625
tensor(11550.2930, grad_fn=<NegBackward0>) tensor(11491.3447, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11490.09375
tensor(11491.3447, grad_fn=<NegBackward0>) tensor(11490.0938, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11489.8681640625
tensor(11490.0938, grad_fn=<NegBackward0>) tensor(11489.8682, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11489.7490234375
tensor(11489.8682, grad_fn=<NegBackward0>) tensor(11489.7490, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11489.6767578125
tensor(11489.7490, grad_fn=<NegBackward0>) tensor(11489.6768, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11488.4287109375
tensor(11489.6768, grad_fn=<NegBackward0>) tensor(11488.4287, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11488.2783203125
tensor(11488.4287, grad_fn=<NegBackward0>) tensor(11488.2783, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11488.2548828125
tensor(11488.2783, grad_fn=<NegBackward0>) tensor(11488.2549, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11488.2353515625
tensor(11488.2549, grad_fn=<NegBackward0>) tensor(11488.2354, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11488.2275390625
tensor(11488.2354, grad_fn=<NegBackward0>) tensor(11488.2275, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11488.2099609375
tensor(11488.2275, grad_fn=<NegBackward0>) tensor(11488.2100, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11488.19921875
tensor(11488.2100, grad_fn=<NegBackward0>) tensor(11488.1992, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11488.1923828125
tensor(11488.1992, grad_fn=<NegBackward0>) tensor(11488.1924, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11488.18359375
tensor(11488.1924, grad_fn=<NegBackward0>) tensor(11488.1836, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11488.1767578125
tensor(11488.1836, grad_fn=<NegBackward0>) tensor(11488.1768, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11488.171875
tensor(11488.1768, grad_fn=<NegBackward0>) tensor(11488.1719, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11488.1708984375
tensor(11488.1719, grad_fn=<NegBackward0>) tensor(11488.1709, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11488.1640625
tensor(11488.1709, grad_fn=<NegBackward0>) tensor(11488.1641, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11488.1611328125
tensor(11488.1641, grad_fn=<NegBackward0>) tensor(11488.1611, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11488.158203125
tensor(11488.1611, grad_fn=<NegBackward0>) tensor(11488.1582, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11488.15234375
tensor(11488.1582, grad_fn=<NegBackward0>) tensor(11488.1523, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11488.1533203125
tensor(11488.1523, grad_fn=<NegBackward0>) tensor(11488.1533, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11488.1455078125
tensor(11488.1523, grad_fn=<NegBackward0>) tensor(11488.1455, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11488.142578125
tensor(11488.1455, grad_fn=<NegBackward0>) tensor(11488.1426, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11488.1396484375
tensor(11488.1426, grad_fn=<NegBackward0>) tensor(11488.1396, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11488.134765625
tensor(11488.1396, grad_fn=<NegBackward0>) tensor(11488.1348, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11488.138671875
tensor(11488.1348, grad_fn=<NegBackward0>) tensor(11488.1387, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11488.1181640625
tensor(11488.1348, grad_fn=<NegBackward0>) tensor(11488.1182, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11488.107421875
tensor(11488.1182, grad_fn=<NegBackward0>) tensor(11488.1074, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11488.1044921875
tensor(11488.1074, grad_fn=<NegBackward0>) tensor(11488.1045, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11488.1005859375
tensor(11488.1045, grad_fn=<NegBackward0>) tensor(11488.1006, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11488.0986328125
tensor(11488.1006, grad_fn=<NegBackward0>) tensor(11488.0986, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11488.09765625
tensor(11488.0986, grad_fn=<NegBackward0>) tensor(11488.0977, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11488.0947265625
tensor(11488.0977, grad_fn=<NegBackward0>) tensor(11488.0947, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11488.0947265625
tensor(11488.0947, grad_fn=<NegBackward0>) tensor(11488.0947, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11488.0927734375
tensor(11488.0947, grad_fn=<NegBackward0>) tensor(11488.0928, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11488.0927734375
tensor(11488.0928, grad_fn=<NegBackward0>) tensor(11488.0928, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11488.0908203125
tensor(11488.0928, grad_fn=<NegBackward0>) tensor(11488.0908, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11488.0908203125
tensor(11488.0908, grad_fn=<NegBackward0>) tensor(11488.0908, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11488.103515625
tensor(11488.0908, grad_fn=<NegBackward0>) tensor(11488.1035, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11488.08984375
tensor(11488.0908, grad_fn=<NegBackward0>) tensor(11488.0898, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11488.0908203125
tensor(11488.0898, grad_fn=<NegBackward0>) tensor(11488.0908, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11488.0888671875
tensor(11488.0898, grad_fn=<NegBackward0>) tensor(11488.0889, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11488.0888671875
tensor(11488.0889, grad_fn=<NegBackward0>) tensor(11488.0889, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11488.08984375
tensor(11488.0889, grad_fn=<NegBackward0>) tensor(11488.0898, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11488.087890625
tensor(11488.0889, grad_fn=<NegBackward0>) tensor(11488.0879, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11488.087890625
tensor(11488.0879, grad_fn=<NegBackward0>) tensor(11488.0879, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11488.087890625
tensor(11488.0879, grad_fn=<NegBackward0>) tensor(11488.0879, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11488.087890625
tensor(11488.0879, grad_fn=<NegBackward0>) tensor(11488.0879, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11488.0869140625
tensor(11488.0879, grad_fn=<NegBackward0>) tensor(11488.0869, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11488.0869140625
tensor(11488.0869, grad_fn=<NegBackward0>) tensor(11488.0869, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11488.0888671875
tensor(11488.0869, grad_fn=<NegBackward0>) tensor(11488.0889, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11488.0869140625
tensor(11488.0869, grad_fn=<NegBackward0>) tensor(11488.0869, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11488.0869140625
tensor(11488.0869, grad_fn=<NegBackward0>) tensor(11488.0869, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11488.0869140625
tensor(11488.0869, grad_fn=<NegBackward0>) tensor(11488.0869, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11488.0859375
tensor(11488.0869, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11488.0859375
tensor(11488.0859, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11488.087890625
tensor(11488.0859, grad_fn=<NegBackward0>) tensor(11488.0879, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11488.0859375
tensor(11488.0859, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11488.087890625
tensor(11488.0859, grad_fn=<NegBackward0>) tensor(11488.0879, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11488.0859375
tensor(11488.0859, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11488.0859375
tensor(11488.0859, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11488.0849609375
tensor(11488.0859, grad_fn=<NegBackward0>) tensor(11488.0850, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11488.0849609375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0850, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11488.0849609375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0850, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11488.0859375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11488.0859375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11488.0859375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11488.0849609375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0850, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11488.0849609375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0850, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11488.0849609375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0850, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11488.0859375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11488.083984375
tensor(11488.0850, grad_fn=<NegBackward0>) tensor(11488.0840, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11488.083984375
tensor(11488.0840, grad_fn=<NegBackward0>) tensor(11488.0840, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11488.083984375
tensor(11488.0840, grad_fn=<NegBackward0>) tensor(11488.0840, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11488.0830078125
tensor(11488.0840, grad_fn=<NegBackward0>) tensor(11488.0830, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11488.0888671875
tensor(11488.0830, grad_fn=<NegBackward0>) tensor(11488.0889, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11488.083984375
tensor(11488.0830, grad_fn=<NegBackward0>) tensor(11488.0840, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11488.0859375
tensor(11488.0830, grad_fn=<NegBackward0>) tensor(11488.0859, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11488.083984375
tensor(11488.0830, grad_fn=<NegBackward0>) tensor(11488.0840, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11488.0908203125
tensor(11488.0830, grad_fn=<NegBackward0>) tensor(11488.0908, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.7536, 0.2464],
        [0.3338, 0.6662]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0051, 0.9949], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3994, 0.1031],
         [0.5744, 0.1837]],

        [[0.5535, 0.1026],
         [0.5114, 0.7088]],

        [[0.6252, 0.1070],
         [0.5049, 0.5524]],

        [[0.6800, 0.0936],
         [0.6652, 0.6931]],

        [[0.5604, 0.1088],
         [0.6730, 0.5845]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.6849207656422429
Average Adjusted Rand Index: 0.7761598197715858
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22301.142578125
inf tensor(22301.1426, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12060.8017578125
tensor(22301.1426, grad_fn=<NegBackward0>) tensor(12060.8018, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11875.037109375
tensor(12060.8018, grad_fn=<NegBackward0>) tensor(11875.0371, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11766.4970703125
tensor(11875.0371, grad_fn=<NegBackward0>) tensor(11766.4971, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11631.3076171875
tensor(11766.4971, grad_fn=<NegBackward0>) tensor(11631.3076, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11588.13671875
tensor(11631.3076, grad_fn=<NegBackward0>) tensor(11588.1367, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11555.08203125
tensor(11588.1367, grad_fn=<NegBackward0>) tensor(11555.0820, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11543.5703125
tensor(11555.0820, grad_fn=<NegBackward0>) tensor(11543.5703, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11538.5205078125
tensor(11543.5703, grad_fn=<NegBackward0>) tensor(11538.5205, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11538.2255859375
tensor(11538.5205, grad_fn=<NegBackward0>) tensor(11538.2256, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11537.6875
tensor(11538.2256, grad_fn=<NegBackward0>) tensor(11537.6875, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11529.4716796875
tensor(11537.6875, grad_fn=<NegBackward0>) tensor(11529.4717, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11523.7783203125
tensor(11529.4717, grad_fn=<NegBackward0>) tensor(11523.7783, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11523.4501953125
tensor(11523.7783, grad_fn=<NegBackward0>) tensor(11523.4502, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11523.421875
tensor(11523.4502, grad_fn=<NegBackward0>) tensor(11523.4219, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11522.2939453125
tensor(11523.4219, grad_fn=<NegBackward0>) tensor(11522.2939, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11522.0576171875
tensor(11522.2939, grad_fn=<NegBackward0>) tensor(11522.0576, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11522.0390625
tensor(11522.0576, grad_fn=<NegBackward0>) tensor(11522.0391, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11521.91015625
tensor(11522.0391, grad_fn=<NegBackward0>) tensor(11521.9102, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11520.0576171875
tensor(11521.9102, grad_fn=<NegBackward0>) tensor(11520.0576, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11520.044921875
tensor(11520.0576, grad_fn=<NegBackward0>) tensor(11520.0449, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11520.0234375
tensor(11520.0449, grad_fn=<NegBackward0>) tensor(11520.0234, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11520.01953125
tensor(11520.0234, grad_fn=<NegBackward0>) tensor(11520.0195, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11520.015625
tensor(11520.0195, grad_fn=<NegBackward0>) tensor(11520.0156, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11520.01171875
tensor(11520.0156, grad_fn=<NegBackward0>) tensor(11520.0117, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11520.0029296875
tensor(11520.0117, grad_fn=<NegBackward0>) tensor(11520.0029, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11520.0
tensor(11520.0029, grad_fn=<NegBackward0>) tensor(11520., grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11519.9951171875
tensor(11520., grad_fn=<NegBackward0>) tensor(11519.9951, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11519.98046875
tensor(11519.9951, grad_fn=<NegBackward0>) tensor(11519.9805, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11519.9775390625
tensor(11519.9805, grad_fn=<NegBackward0>) tensor(11519.9775, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11519.9775390625
tensor(11519.9775, grad_fn=<NegBackward0>) tensor(11519.9775, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11519.974609375
tensor(11519.9775, grad_fn=<NegBackward0>) tensor(11519.9746, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11519.9736328125
tensor(11519.9746, grad_fn=<NegBackward0>) tensor(11519.9736, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11519.9716796875
tensor(11519.9736, grad_fn=<NegBackward0>) tensor(11519.9717, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11519.974609375
tensor(11519.9717, grad_fn=<NegBackward0>) tensor(11519.9746, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11519.966796875
tensor(11519.9717, grad_fn=<NegBackward0>) tensor(11519.9668, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11519.9658203125
tensor(11519.9668, grad_fn=<NegBackward0>) tensor(11519.9658, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11519.9658203125
tensor(11519.9658, grad_fn=<NegBackward0>) tensor(11519.9658, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11519.96484375
tensor(11519.9658, grad_fn=<NegBackward0>) tensor(11519.9648, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11519.962890625
tensor(11519.9648, grad_fn=<NegBackward0>) tensor(11519.9629, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11519.96484375
tensor(11519.9629, grad_fn=<NegBackward0>) tensor(11519.9648, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11519.962890625
tensor(11519.9629, grad_fn=<NegBackward0>) tensor(11519.9629, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11519.96484375
tensor(11519.9629, grad_fn=<NegBackward0>) tensor(11519.9648, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11519.962890625
tensor(11519.9629, grad_fn=<NegBackward0>) tensor(11519.9629, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11519.9619140625
tensor(11519.9629, grad_fn=<NegBackward0>) tensor(11519.9619, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11519.962890625
tensor(11519.9619, grad_fn=<NegBackward0>) tensor(11519.9629, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11519.966796875
tensor(11519.9619, grad_fn=<NegBackward0>) tensor(11519.9668, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11519.955078125
tensor(11519.9619, grad_fn=<NegBackward0>) tensor(11519.9551, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11519.943359375
tensor(11519.9551, grad_fn=<NegBackward0>) tensor(11519.9434, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11519.943359375
tensor(11519.9434, grad_fn=<NegBackward0>) tensor(11519.9434, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11519.9443359375
tensor(11519.9434, grad_fn=<NegBackward0>) tensor(11519.9443, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11519.9296875
tensor(11519.9434, grad_fn=<NegBackward0>) tensor(11519.9297, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11519.927734375
tensor(11519.9297, grad_fn=<NegBackward0>) tensor(11519.9277, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11519.93359375
tensor(11519.9277, grad_fn=<NegBackward0>) tensor(11519.9336, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11519.9267578125
tensor(11519.9277, grad_fn=<NegBackward0>) tensor(11519.9268, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11519.923828125
tensor(11519.9268, grad_fn=<NegBackward0>) tensor(11519.9238, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11519.9248046875
tensor(11519.9238, grad_fn=<NegBackward0>) tensor(11519.9248, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11519.9248046875
tensor(11519.9238, grad_fn=<NegBackward0>) tensor(11519.9248, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11519.921875
tensor(11519.9238, grad_fn=<NegBackward0>) tensor(11519.9219, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11519.919921875
tensor(11519.9219, grad_fn=<NegBackward0>) tensor(11519.9199, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11519.919921875
tensor(11519.9199, grad_fn=<NegBackward0>) tensor(11519.9199, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11519.923828125
tensor(11519.9199, grad_fn=<NegBackward0>) tensor(11519.9238, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11519.919921875
tensor(11519.9199, grad_fn=<NegBackward0>) tensor(11519.9199, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11519.919921875
tensor(11519.9199, grad_fn=<NegBackward0>) tensor(11519.9199, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11519.9189453125
tensor(11519.9199, grad_fn=<NegBackward0>) tensor(11519.9189, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11519.919921875
tensor(11519.9189, grad_fn=<NegBackward0>) tensor(11519.9199, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11519.9189453125
tensor(11519.9189, grad_fn=<NegBackward0>) tensor(11519.9189, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11519.919921875
tensor(11519.9189, grad_fn=<NegBackward0>) tensor(11519.9199, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11519.9189453125
tensor(11519.9189, grad_fn=<NegBackward0>) tensor(11519.9189, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11519.91796875
tensor(11519.9189, grad_fn=<NegBackward0>) tensor(11519.9180, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11519.927734375
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9277, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11519.9189453125
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9189, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11519.9189453125
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9189, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11519.91796875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9180, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11519.91796875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9180, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11519.9189453125
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9189, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11519.91796875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9180, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11519.9248046875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9248, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11519.9248046875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9248, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11519.91796875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9180, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11519.9560546875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9561, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11519.9169921875
tensor(11519.9180, grad_fn=<NegBackward0>) tensor(11519.9170, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11519.923828125
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9238, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11519.9228515625
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9229, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11519.91796875
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9180, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11519.919921875
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9199, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11519.9169921875
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9170, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11519.9228515625
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9229, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11519.9169921875
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9170, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11519.9228515625
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9229, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11519.9169921875
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9170, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11519.9208984375
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9209, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11519.9404296875
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9404, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11519.916015625
tensor(11519.9170, grad_fn=<NegBackward0>) tensor(11519.9160, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11519.9169921875
tensor(11519.9160, grad_fn=<NegBackward0>) tensor(11519.9170, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11519.93359375
tensor(11519.9160, grad_fn=<NegBackward0>) tensor(11519.9336, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11519.9169921875
tensor(11519.9160, grad_fn=<NegBackward0>) tensor(11519.9170, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11519.931640625
tensor(11519.9160, grad_fn=<NegBackward0>) tensor(11519.9316, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11519.9169921875
tensor(11519.9160, grad_fn=<NegBackward0>) tensor(11519.9170, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.6212, 0.3788],
        [0.4705, 0.5295]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5994, 0.4006], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2163, 0.0858],
         [0.6494, 0.3830]],

        [[0.6859, 0.0998],
         [0.5341, 0.7181]],

        [[0.6544, 0.1026],
         [0.7225, 0.5974]],

        [[0.6529, 0.0936],
         [0.6643, 0.6868]],

        [[0.6527, 0.1080],
         [0.5122, 0.5349]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 23
Adjusted Rand Index: 0.28514463947159197
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.4613063391292412
Average Adjusted Rand Index: 0.8331901417529679
[0.6849207656422429, 0.4613063391292412] [0.7761598197715858, 0.8331901417529679] [11488.0908203125, 11519.9169921875]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11825.493893091018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22184.248046875
inf tensor(22184.2480, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12708.4970703125
tensor(22184.2480, grad_fn=<NegBackward0>) tensor(12708.4971, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12105.1005859375
tensor(12708.4971, grad_fn=<NegBackward0>) tensor(12105.1006, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11883.353515625
tensor(12105.1006, grad_fn=<NegBackward0>) tensor(11883.3535, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11860.57421875
tensor(11883.3535, grad_fn=<NegBackward0>) tensor(11860.5742, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11846.951171875
tensor(11860.5742, grad_fn=<NegBackward0>) tensor(11846.9512, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11843.484375
tensor(11846.9512, grad_fn=<NegBackward0>) tensor(11843.4844, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11843.1875
tensor(11843.4844, grad_fn=<NegBackward0>) tensor(11843.1875, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11826.7041015625
tensor(11843.1875, grad_fn=<NegBackward0>) tensor(11826.7041, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11826.53125
tensor(11826.7041, grad_fn=<NegBackward0>) tensor(11826.5312, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11826.458984375
tensor(11826.5312, grad_fn=<NegBackward0>) tensor(11826.4590, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11826.4130859375
tensor(11826.4590, grad_fn=<NegBackward0>) tensor(11826.4131, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11826.3740234375
tensor(11826.4131, grad_fn=<NegBackward0>) tensor(11826.3740, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11826.34375
tensor(11826.3740, grad_fn=<NegBackward0>) tensor(11826.3438, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11826.3203125
tensor(11826.3438, grad_fn=<NegBackward0>) tensor(11826.3203, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11826.30078125
tensor(11826.3203, grad_fn=<NegBackward0>) tensor(11826.3008, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11826.283203125
tensor(11826.3008, grad_fn=<NegBackward0>) tensor(11826.2832, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11826.265625
tensor(11826.2832, grad_fn=<NegBackward0>) tensor(11826.2656, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11819.1689453125
tensor(11826.2656, grad_fn=<NegBackward0>) tensor(11819.1689, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11819.15625
tensor(11819.1689, grad_fn=<NegBackward0>) tensor(11819.1562, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11819.146484375
tensor(11819.1562, grad_fn=<NegBackward0>) tensor(11819.1465, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11819.1376953125
tensor(11819.1465, grad_fn=<NegBackward0>) tensor(11819.1377, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11819.1298828125
tensor(11819.1377, grad_fn=<NegBackward0>) tensor(11819.1299, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11819.125
tensor(11819.1299, grad_fn=<NegBackward0>) tensor(11819.1250, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11819.119140625
tensor(11819.1250, grad_fn=<NegBackward0>) tensor(11819.1191, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11819.1142578125
tensor(11819.1191, grad_fn=<NegBackward0>) tensor(11819.1143, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11819.111328125
tensor(11819.1143, grad_fn=<NegBackward0>) tensor(11819.1113, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11819.1064453125
tensor(11819.1113, grad_fn=<NegBackward0>) tensor(11819.1064, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11819.103515625
tensor(11819.1064, grad_fn=<NegBackward0>) tensor(11819.1035, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11819.099609375
tensor(11819.1035, grad_fn=<NegBackward0>) tensor(11819.0996, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11819.09765625
tensor(11819.0996, grad_fn=<NegBackward0>) tensor(11819.0977, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11819.0947265625
tensor(11819.0977, grad_fn=<NegBackward0>) tensor(11819.0947, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11819.09375
tensor(11819.0947, grad_fn=<NegBackward0>) tensor(11819.0938, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11819.08984375
tensor(11819.0938, grad_fn=<NegBackward0>) tensor(11819.0898, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11819.08984375
tensor(11819.0898, grad_fn=<NegBackward0>) tensor(11819.0898, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11819.087890625
tensor(11819.0898, grad_fn=<NegBackward0>) tensor(11819.0879, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11819.0849609375
tensor(11819.0879, grad_fn=<NegBackward0>) tensor(11819.0850, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11819.0859375
tensor(11819.0850, grad_fn=<NegBackward0>) tensor(11819.0859, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11819.08203125
tensor(11819.0850, grad_fn=<NegBackward0>) tensor(11819.0820, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11819.08203125
tensor(11819.0820, grad_fn=<NegBackward0>) tensor(11819.0820, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11819.080078125
tensor(11819.0820, grad_fn=<NegBackward0>) tensor(11819.0801, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11819.080078125
tensor(11819.0801, grad_fn=<NegBackward0>) tensor(11819.0801, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11819.0791015625
tensor(11819.0801, grad_fn=<NegBackward0>) tensor(11819.0791, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11819.080078125
tensor(11819.0791, grad_fn=<NegBackward0>) tensor(11819.0801, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11819.0771484375
tensor(11819.0791, grad_fn=<NegBackward0>) tensor(11819.0771, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11819.076171875
tensor(11819.0771, grad_fn=<NegBackward0>) tensor(11819.0762, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11819.076171875
tensor(11819.0762, grad_fn=<NegBackward0>) tensor(11819.0762, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11819.0732421875
tensor(11819.0762, grad_fn=<NegBackward0>) tensor(11819.0732, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11819.0732421875
tensor(11819.0732, grad_fn=<NegBackward0>) tensor(11819.0732, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11819.0732421875
tensor(11819.0732, grad_fn=<NegBackward0>) tensor(11819.0732, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11819.0712890625
tensor(11819.0732, grad_fn=<NegBackward0>) tensor(11819.0713, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11819.072265625
tensor(11819.0713, grad_fn=<NegBackward0>) tensor(11819.0723, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11819.0712890625
tensor(11819.0713, grad_fn=<NegBackward0>) tensor(11819.0713, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11819.0703125
tensor(11819.0713, grad_fn=<NegBackward0>) tensor(11819.0703, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11819.0703125
tensor(11819.0703, grad_fn=<NegBackward0>) tensor(11819.0703, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11819.0703125
tensor(11819.0703, grad_fn=<NegBackward0>) tensor(11819.0703, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11819.068359375
tensor(11819.0703, grad_fn=<NegBackward0>) tensor(11819.0684, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11819.0703125
tensor(11819.0684, grad_fn=<NegBackward0>) tensor(11819.0703, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11819.08203125
tensor(11819.0684, grad_fn=<NegBackward0>) tensor(11819.0820, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11819.068359375
tensor(11819.0684, grad_fn=<NegBackward0>) tensor(11819.0684, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11819.072265625
tensor(11819.0684, grad_fn=<NegBackward0>) tensor(11819.0723, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11819.068359375
tensor(11819.0684, grad_fn=<NegBackward0>) tensor(11819.0684, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11819.06640625
tensor(11819.0684, grad_fn=<NegBackward0>) tensor(11819.0664, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11819.0673828125
tensor(11819.0664, grad_fn=<NegBackward0>) tensor(11819.0674, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11819.068359375
tensor(11819.0664, grad_fn=<NegBackward0>) tensor(11819.0684, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11819.0654296875
tensor(11819.0664, grad_fn=<NegBackward0>) tensor(11819.0654, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11819.064453125
tensor(11819.0654, grad_fn=<NegBackward0>) tensor(11819.0645, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11819.0615234375
tensor(11819.0645, grad_fn=<NegBackward0>) tensor(11819.0615, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11819.0625
tensor(11819.0615, grad_fn=<NegBackward0>) tensor(11819.0625, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11819.1142578125
tensor(11819.0615, grad_fn=<NegBackward0>) tensor(11819.1143, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11819.1513671875
tensor(11819.0615, grad_fn=<NegBackward0>) tensor(11819.1514, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11819.0615234375
tensor(11819.0615, grad_fn=<NegBackward0>) tensor(11819.0615, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11819.0625
tensor(11819.0615, grad_fn=<NegBackward0>) tensor(11819.0625, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11819.064453125
tensor(11819.0615, grad_fn=<NegBackward0>) tensor(11819.0645, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11819.060546875
tensor(11819.0615, grad_fn=<NegBackward0>) tensor(11819.0605, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11819.0615234375
tensor(11819.0605, grad_fn=<NegBackward0>) tensor(11819.0615, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11819.0615234375
tensor(11819.0605, grad_fn=<NegBackward0>) tensor(11819.0615, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11819.0625
tensor(11819.0605, grad_fn=<NegBackward0>) tensor(11819.0625, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11819.1181640625
tensor(11819.0605, grad_fn=<NegBackward0>) tensor(11819.1182, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11819.060546875
tensor(11819.0605, grad_fn=<NegBackward0>) tensor(11819.0605, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11818.826171875
tensor(11819.0605, grad_fn=<NegBackward0>) tensor(11818.8262, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11818.890625
tensor(11818.8262, grad_fn=<NegBackward0>) tensor(11818.8906, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11818.8232421875
tensor(11818.8262, grad_fn=<NegBackward0>) tensor(11818.8232, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11818.9970703125
tensor(11818.8232, grad_fn=<NegBackward0>) tensor(11818.9971, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11818.82421875
tensor(11818.8232, grad_fn=<NegBackward0>) tensor(11818.8242, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11818.9951171875
tensor(11818.8232, grad_fn=<NegBackward0>) tensor(11818.9951, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11818.8232421875
tensor(11818.8232, grad_fn=<NegBackward0>) tensor(11818.8232, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11818.8232421875
tensor(11818.8232, grad_fn=<NegBackward0>) tensor(11818.8232, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11818.82421875
tensor(11818.8232, grad_fn=<NegBackward0>) tensor(11818.8242, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11818.822265625
tensor(11818.8232, grad_fn=<NegBackward0>) tensor(11818.8223, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11818.8251953125
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8252, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11818.8232421875
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8232, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11818.822265625
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8223, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11818.822265625
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8223, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11818.8330078125
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8330, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11818.822265625
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8223, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11818.8251953125
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8252, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11818.822265625
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8223, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11818.8251953125
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8252, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11818.822265625
tensor(11818.8223, grad_fn=<NegBackward0>) tensor(11818.8223, grad_fn=<NegBackward0>)
pi: tensor([[0.7561, 0.2439],
        [0.2539, 0.7461]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5829, 0.4171], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4072, 0.0930],
         [0.6158, 0.2010]],

        [[0.6959, 0.1044],
         [0.6043, 0.5532]],

        [[0.5477, 0.1030],
         [0.5652, 0.6598]],

        [[0.6635, 0.1024],
         [0.7292, 0.5246]],

        [[0.6551, 0.1057],
         [0.7291, 0.5459]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318395231936
Average Adjusted Rand Index: 0.9841581403596603
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21248.671875
inf tensor(21248.6719, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12724.9638671875
tensor(21248.6719, grad_fn=<NegBackward0>) tensor(12724.9639, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12208.0791015625
tensor(12724.9639, grad_fn=<NegBackward0>) tensor(12208.0791, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11937.2607421875
tensor(12208.0791, grad_fn=<NegBackward0>) tensor(11937.2607, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11876.65625
tensor(11937.2607, grad_fn=<NegBackward0>) tensor(11876.6562, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11849.484375
tensor(11876.6562, grad_fn=<NegBackward0>) tensor(11849.4844, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11838.900390625
tensor(11849.4844, grad_fn=<NegBackward0>) tensor(11838.9004, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11830.45703125
tensor(11838.9004, grad_fn=<NegBackward0>) tensor(11830.4570, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11830.2431640625
tensor(11830.4570, grad_fn=<NegBackward0>) tensor(11830.2432, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11830.125
tensor(11830.2432, grad_fn=<NegBackward0>) tensor(11830.1250, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11830.0205078125
tensor(11830.1250, grad_fn=<NegBackward0>) tensor(11830.0205, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11829.9619140625
tensor(11830.0205, grad_fn=<NegBackward0>) tensor(11829.9619, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11829.9169921875
tensor(11829.9619, grad_fn=<NegBackward0>) tensor(11829.9170, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11829.880859375
tensor(11829.9170, grad_fn=<NegBackward0>) tensor(11829.8809, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11829.861328125
tensor(11829.8809, grad_fn=<NegBackward0>) tensor(11829.8613, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11829.830078125
tensor(11829.8613, grad_fn=<NegBackward0>) tensor(11829.8301, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11829.810546875
tensor(11829.8301, grad_fn=<NegBackward0>) tensor(11829.8105, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11829.79296875
tensor(11829.8105, grad_fn=<NegBackward0>) tensor(11829.7930, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11829.779296875
tensor(11829.7930, grad_fn=<NegBackward0>) tensor(11829.7793, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11829.7685546875
tensor(11829.7793, grad_fn=<NegBackward0>) tensor(11829.7686, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11829.755859375
tensor(11829.7686, grad_fn=<NegBackward0>) tensor(11829.7559, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11829.744140625
tensor(11829.7559, grad_fn=<NegBackward0>) tensor(11829.7441, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11829.619140625
tensor(11829.7441, grad_fn=<NegBackward0>) tensor(11829.6191, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11820.34765625
tensor(11829.6191, grad_fn=<NegBackward0>) tensor(11820.3477, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11820.3310546875
tensor(11820.3477, grad_fn=<NegBackward0>) tensor(11820.3311, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11820.326171875
tensor(11820.3311, grad_fn=<NegBackward0>) tensor(11820.3262, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11820.3193359375
tensor(11820.3262, grad_fn=<NegBackward0>) tensor(11820.3193, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11820.3154296875
tensor(11820.3193, grad_fn=<NegBackward0>) tensor(11820.3154, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11820.310546875
tensor(11820.3154, grad_fn=<NegBackward0>) tensor(11820.3105, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11820.306640625
tensor(11820.3105, grad_fn=<NegBackward0>) tensor(11820.3066, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11820.3046875
tensor(11820.3066, grad_fn=<NegBackward0>) tensor(11820.3047, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11820.3017578125
tensor(11820.3047, grad_fn=<NegBackward0>) tensor(11820.3018, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11820.298828125
tensor(11820.3018, grad_fn=<NegBackward0>) tensor(11820.2988, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11820.2958984375
tensor(11820.2988, grad_fn=<NegBackward0>) tensor(11820.2959, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11820.2998046875
tensor(11820.2959, grad_fn=<NegBackward0>) tensor(11820.2998, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11820.291015625
tensor(11820.2959, grad_fn=<NegBackward0>) tensor(11820.2910, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11820.29296875
tensor(11820.2910, grad_fn=<NegBackward0>) tensor(11820.2930, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11820.2880859375
tensor(11820.2910, grad_fn=<NegBackward0>) tensor(11820.2881, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11820.287109375
tensor(11820.2881, grad_fn=<NegBackward0>) tensor(11820.2871, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11820.28515625
tensor(11820.2871, grad_fn=<NegBackward0>) tensor(11820.2852, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11820.283203125
tensor(11820.2852, grad_fn=<NegBackward0>) tensor(11820.2832, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11820.2822265625
tensor(11820.2832, grad_fn=<NegBackward0>) tensor(11820.2822, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11820.28125
tensor(11820.2822, grad_fn=<NegBackward0>) tensor(11820.2812, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11820.279296875
tensor(11820.2812, grad_fn=<NegBackward0>) tensor(11820.2793, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11820.2783203125
tensor(11820.2793, grad_fn=<NegBackward0>) tensor(11820.2783, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11820.27734375
tensor(11820.2783, grad_fn=<NegBackward0>) tensor(11820.2773, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11820.275390625
tensor(11820.2773, grad_fn=<NegBackward0>) tensor(11820.2754, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11820.275390625
tensor(11820.2754, grad_fn=<NegBackward0>) tensor(11820.2754, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11820.275390625
tensor(11820.2754, grad_fn=<NegBackward0>) tensor(11820.2754, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11820.2734375
tensor(11820.2754, grad_fn=<NegBackward0>) tensor(11820.2734, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11820.2724609375
tensor(11820.2734, grad_fn=<NegBackward0>) tensor(11820.2725, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11820.2734375
tensor(11820.2725, grad_fn=<NegBackward0>) tensor(11820.2734, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11820.271484375
tensor(11820.2725, grad_fn=<NegBackward0>) tensor(11820.2715, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11820.271484375
tensor(11820.2715, grad_fn=<NegBackward0>) tensor(11820.2715, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11820.271484375
tensor(11820.2715, grad_fn=<NegBackward0>) tensor(11820.2715, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11820.271484375
tensor(11820.2715, grad_fn=<NegBackward0>) tensor(11820.2715, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11820.271484375
tensor(11820.2715, grad_fn=<NegBackward0>) tensor(11820.2715, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11820.267578125
tensor(11820.2715, grad_fn=<NegBackward0>) tensor(11820.2676, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11820.2724609375
tensor(11820.2676, grad_fn=<NegBackward0>) tensor(11820.2725, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11820.2685546875
tensor(11820.2676, grad_fn=<NegBackward0>) tensor(11820.2686, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11820.267578125
tensor(11820.2676, grad_fn=<NegBackward0>) tensor(11820.2676, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11820.2734375
tensor(11820.2676, grad_fn=<NegBackward0>) tensor(11820.2734, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11820.2685546875
tensor(11820.2676, grad_fn=<NegBackward0>) tensor(11820.2686, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11820.267578125
tensor(11820.2676, grad_fn=<NegBackward0>) tensor(11820.2676, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11820.265625
tensor(11820.2676, grad_fn=<NegBackward0>) tensor(11820.2656, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11820.267578125
tensor(11820.2656, grad_fn=<NegBackward0>) tensor(11820.2676, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11820.2666015625
tensor(11820.2656, grad_fn=<NegBackward0>) tensor(11820.2666, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11820.2646484375
tensor(11820.2656, grad_fn=<NegBackward0>) tensor(11820.2646, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11820.2646484375
tensor(11820.2646, grad_fn=<NegBackward0>) tensor(11820.2646, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11820.2646484375
tensor(11820.2646, grad_fn=<NegBackward0>) tensor(11820.2646, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11820.2646484375
tensor(11820.2646, grad_fn=<NegBackward0>) tensor(11820.2646, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11820.291015625
tensor(11820.2646, grad_fn=<NegBackward0>) tensor(11820.2910, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11820.2646484375
tensor(11820.2646, grad_fn=<NegBackward0>) tensor(11820.2646, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11820.2705078125
tensor(11820.2646, grad_fn=<NegBackward0>) tensor(11820.2705, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11820.263671875
tensor(11820.2646, grad_fn=<NegBackward0>) tensor(11820.2637, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11820.263671875
tensor(11820.2637, grad_fn=<NegBackward0>) tensor(11820.2637, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11820.2626953125
tensor(11820.2637, grad_fn=<NegBackward0>) tensor(11820.2627, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11820.2685546875
tensor(11820.2627, grad_fn=<NegBackward0>) tensor(11820.2686, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11820.265625
tensor(11820.2627, grad_fn=<NegBackward0>) tensor(11820.2656, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11820.26953125
tensor(11820.2627, grad_fn=<NegBackward0>) tensor(11820.2695, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11820.26171875
tensor(11820.2627, grad_fn=<NegBackward0>) tensor(11820.2617, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11820.27734375
tensor(11820.2617, grad_fn=<NegBackward0>) tensor(11820.2773, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11820.26171875
tensor(11820.2617, grad_fn=<NegBackward0>) tensor(11820.2617, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11820.26171875
tensor(11820.2617, grad_fn=<NegBackward0>) tensor(11820.2617, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11820.259765625
tensor(11820.2617, grad_fn=<NegBackward0>) tensor(11820.2598, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11820.2607421875
tensor(11820.2598, grad_fn=<NegBackward0>) tensor(11820.2607, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11820.2744140625
tensor(11820.2598, grad_fn=<NegBackward0>) tensor(11820.2744, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11820.2587890625
tensor(11820.2598, grad_fn=<NegBackward0>) tensor(11820.2588, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11820.2587890625
tensor(11820.2588, grad_fn=<NegBackward0>) tensor(11820.2588, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11820.2587890625
tensor(11820.2588, grad_fn=<NegBackward0>) tensor(11820.2588, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11819.0537109375
tensor(11820.2588, grad_fn=<NegBackward0>) tensor(11819.0537, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11818.8251953125
tensor(11819.0537, grad_fn=<NegBackward0>) tensor(11818.8252, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11818.826171875
tensor(11818.8252, grad_fn=<NegBackward0>) tensor(11818.8262, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11818.8251953125
tensor(11818.8252, grad_fn=<NegBackward0>) tensor(11818.8252, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11818.82421875
tensor(11818.8252, grad_fn=<NegBackward0>) tensor(11818.8242, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11818.8291015625
tensor(11818.8242, grad_fn=<NegBackward0>) tensor(11818.8291, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11818.8369140625
tensor(11818.8242, grad_fn=<NegBackward0>) tensor(11818.8369, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11818.8408203125
tensor(11818.8242, grad_fn=<NegBackward0>) tensor(11818.8408, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11818.8251953125
tensor(11818.8242, grad_fn=<NegBackward0>) tensor(11818.8252, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11818.826171875
tensor(11818.8242, grad_fn=<NegBackward0>) tensor(11818.8262, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7564, 0.2436],
        [0.2535, 0.7465]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5836, 0.4164], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4069, 0.0929],
         [0.5519, 0.2010]],

        [[0.6347, 0.1042],
         [0.6420, 0.7015]],

        [[0.6510, 0.1030],
         [0.7043, 0.5582]],

        [[0.5536, 0.1021],
         [0.6175, 0.6851]],

        [[0.7043, 0.1057],
         [0.6379, 0.6999]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318395231936
Average Adjusted Rand Index: 0.9841581403596603
[0.9840318395231936, 0.9840318395231936] [0.9841581403596603, 0.9841581403596603] [11818.828125, 11818.826171875]
-------------------------------------
This iteration is 74
True Objective function: Loss = -11977.255714460178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20967.4140625
inf tensor(20967.4141, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12907.7861328125
tensor(20967.4141, grad_fn=<NegBackward0>) tensor(12907.7861, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12873.6884765625
tensor(12907.7861, grad_fn=<NegBackward0>) tensor(12873.6885, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12463.2490234375
tensor(12873.6885, grad_fn=<NegBackward0>) tensor(12463.2490, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12322.9326171875
tensor(12463.2490, grad_fn=<NegBackward0>) tensor(12322.9326, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12305.2861328125
tensor(12322.9326, grad_fn=<NegBackward0>) tensor(12305.2861, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12298.95703125
tensor(12305.2861, grad_fn=<NegBackward0>) tensor(12298.9570, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12279.248046875
tensor(12298.9570, grad_fn=<NegBackward0>) tensor(12279.2480, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12278.798828125
tensor(12279.2480, grad_fn=<NegBackward0>) tensor(12278.7988, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12278.6201171875
tensor(12278.7988, grad_fn=<NegBackward0>) tensor(12278.6201, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12278.521484375
tensor(12278.6201, grad_fn=<NegBackward0>) tensor(12278.5215, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12278.4404296875
tensor(12278.5215, grad_fn=<NegBackward0>) tensor(12278.4404, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12278.3369140625
tensor(12278.4404, grad_fn=<NegBackward0>) tensor(12278.3369, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12278.28515625
tensor(12278.3369, grad_fn=<NegBackward0>) tensor(12278.2852, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12276.365234375
tensor(12278.2852, grad_fn=<NegBackward0>) tensor(12276.3652, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12263.783203125
tensor(12276.3652, grad_fn=<NegBackward0>) tensor(12263.7832, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12262.498046875
tensor(12263.7832, grad_fn=<NegBackward0>) tensor(12262.4980, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12262.3447265625
tensor(12262.4980, grad_fn=<NegBackward0>) tensor(12262.3447, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12256.3701171875
tensor(12262.3447, grad_fn=<NegBackward0>) tensor(12256.3701, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12256.1982421875
tensor(12256.3701, grad_fn=<NegBackward0>) tensor(12256.1982, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12256.1220703125
tensor(12256.1982, grad_fn=<NegBackward0>) tensor(12256.1221, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12256.0771484375
tensor(12256.1221, grad_fn=<NegBackward0>) tensor(12256.0771, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12255.9501953125
tensor(12256.0771, grad_fn=<NegBackward0>) tensor(12255.9502, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12255.9375
tensor(12255.9502, grad_fn=<NegBackward0>) tensor(12255.9375, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12255.9267578125
tensor(12255.9375, grad_fn=<NegBackward0>) tensor(12255.9268, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12255.9130859375
tensor(12255.9268, grad_fn=<NegBackward0>) tensor(12255.9131, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12255.8984375
tensor(12255.9131, grad_fn=<NegBackward0>) tensor(12255.8984, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12255.8916015625
tensor(12255.8984, grad_fn=<NegBackward0>) tensor(12255.8916, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12255.8837890625
tensor(12255.8916, grad_fn=<NegBackward0>) tensor(12255.8838, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12255.876953125
tensor(12255.8838, grad_fn=<NegBackward0>) tensor(12255.8770, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12255.8662109375
tensor(12255.8770, grad_fn=<NegBackward0>) tensor(12255.8662, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12255.8359375
tensor(12255.8662, grad_fn=<NegBackward0>) tensor(12255.8359, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12255.8291015625
tensor(12255.8359, grad_fn=<NegBackward0>) tensor(12255.8291, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12255.826171875
tensor(12255.8291, grad_fn=<NegBackward0>) tensor(12255.8262, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12255.8251953125
tensor(12255.8262, grad_fn=<NegBackward0>) tensor(12255.8252, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12255.8203125
tensor(12255.8252, grad_fn=<NegBackward0>) tensor(12255.8203, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12255.8154296875
tensor(12255.8203, grad_fn=<NegBackward0>) tensor(12255.8154, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12255.810546875
tensor(12255.8154, grad_fn=<NegBackward0>) tensor(12255.8105, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12255.80859375
tensor(12255.8105, grad_fn=<NegBackward0>) tensor(12255.8086, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12255.806640625
tensor(12255.8086, grad_fn=<NegBackward0>) tensor(12255.8066, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12255.8046875
tensor(12255.8066, grad_fn=<NegBackward0>) tensor(12255.8047, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12255.8046875
tensor(12255.8047, grad_fn=<NegBackward0>) tensor(12255.8047, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12255.8017578125
tensor(12255.8047, grad_fn=<NegBackward0>) tensor(12255.8018, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12255.80078125
tensor(12255.8018, grad_fn=<NegBackward0>) tensor(12255.8008, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12255.7998046875
tensor(12255.8008, grad_fn=<NegBackward0>) tensor(12255.7998, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12255.798828125
tensor(12255.7998, grad_fn=<NegBackward0>) tensor(12255.7988, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12255.798828125
tensor(12255.7988, grad_fn=<NegBackward0>) tensor(12255.7988, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12255.7919921875
tensor(12255.7988, grad_fn=<NegBackward0>) tensor(12255.7920, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12255.79296875
tensor(12255.7920, grad_fn=<NegBackward0>) tensor(12255.7930, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12255.7880859375
tensor(12255.7920, grad_fn=<NegBackward0>) tensor(12255.7881, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12255.798828125
tensor(12255.7881, grad_fn=<NegBackward0>) tensor(12255.7988, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12255.787109375
tensor(12255.7881, grad_fn=<NegBackward0>) tensor(12255.7871, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12255.7998046875
tensor(12255.7871, grad_fn=<NegBackward0>) tensor(12255.7998, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12255.7861328125
tensor(12255.7871, grad_fn=<NegBackward0>) tensor(12255.7861, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12255.78515625
tensor(12255.7861, grad_fn=<NegBackward0>) tensor(12255.7852, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12255.78515625
tensor(12255.7852, grad_fn=<NegBackward0>) tensor(12255.7852, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12255.7841796875
tensor(12255.7852, grad_fn=<NegBackward0>) tensor(12255.7842, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12255.783203125
tensor(12255.7842, grad_fn=<NegBackward0>) tensor(12255.7832, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12255.7841796875
tensor(12255.7832, grad_fn=<NegBackward0>) tensor(12255.7842, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12255.7890625
tensor(12255.7832, grad_fn=<NegBackward0>) tensor(12255.7891, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12255.7841796875
tensor(12255.7832, grad_fn=<NegBackward0>) tensor(12255.7842, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -12255.78515625
tensor(12255.7832, grad_fn=<NegBackward0>) tensor(12255.7852, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -12255.7802734375
tensor(12255.7832, grad_fn=<NegBackward0>) tensor(12255.7803, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12255.78125
tensor(12255.7803, grad_fn=<NegBackward0>) tensor(12255.7812, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12255.78125
tensor(12255.7803, grad_fn=<NegBackward0>) tensor(12255.7812, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12255.7802734375
tensor(12255.7803, grad_fn=<NegBackward0>) tensor(12255.7803, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12255.783203125
tensor(12255.7803, grad_fn=<NegBackward0>) tensor(12255.7832, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12255.78515625
tensor(12255.7803, grad_fn=<NegBackward0>) tensor(12255.7852, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12255.779296875
tensor(12255.7803, grad_fn=<NegBackward0>) tensor(12255.7793, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12255.7783203125
tensor(12255.7793, grad_fn=<NegBackward0>) tensor(12255.7783, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12255.7783203125
tensor(12255.7783, grad_fn=<NegBackward0>) tensor(12255.7783, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12255.7783203125
tensor(12255.7783, grad_fn=<NegBackward0>) tensor(12255.7783, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12255.7783203125
tensor(12255.7783, grad_fn=<NegBackward0>) tensor(12255.7783, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12255.77734375
tensor(12255.7783, grad_fn=<NegBackward0>) tensor(12255.7773, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12255.7763671875
tensor(12255.7773, grad_fn=<NegBackward0>) tensor(12255.7764, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12255.7763671875
tensor(12255.7764, grad_fn=<NegBackward0>) tensor(12255.7764, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12254.8603515625
tensor(12255.7764, grad_fn=<NegBackward0>) tensor(12254.8604, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12254.8095703125
tensor(12254.8604, grad_fn=<NegBackward0>) tensor(12254.8096, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12254.677734375
tensor(12254.8096, grad_fn=<NegBackward0>) tensor(12254.6777, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12254.77734375
tensor(12254.6777, grad_fn=<NegBackward0>) tensor(12254.7773, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12254.67578125
tensor(12254.6777, grad_fn=<NegBackward0>) tensor(12254.6758, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12254.67578125
tensor(12254.6758, grad_fn=<NegBackward0>) tensor(12254.6758, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12254.67578125
tensor(12254.6758, grad_fn=<NegBackward0>) tensor(12254.6758, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12254.67578125
tensor(12254.6758, grad_fn=<NegBackward0>) tensor(12254.6758, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12254.7763671875
tensor(12254.6758, grad_fn=<NegBackward0>) tensor(12254.7764, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12254.67578125
tensor(12254.6758, grad_fn=<NegBackward0>) tensor(12254.6758, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12254.67578125
tensor(12254.6758, grad_fn=<NegBackward0>) tensor(12254.6758, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12254.673828125
tensor(12254.6758, grad_fn=<NegBackward0>) tensor(12254.6738, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12254.6748046875
tensor(12254.6738, grad_fn=<NegBackward0>) tensor(12254.6748, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12254.6787109375
tensor(12254.6738, grad_fn=<NegBackward0>) tensor(12254.6787, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -12254.6748046875
tensor(12254.6738, grad_fn=<NegBackward0>) tensor(12254.6748, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -12254.6748046875
tensor(12254.6738, grad_fn=<NegBackward0>) tensor(12254.6748, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -12254.701171875
tensor(12254.6738, grad_fn=<NegBackward0>) tensor(12254.7012, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.3789, 0.6211],
        [0.6173, 0.3827]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6490, 0.3510], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3472, 0.0894],
         [0.6347, 0.2952]],

        [[0.5572, 0.0956],
         [0.7101, 0.5481]],

        [[0.6935, 0.1061],
         [0.6963, 0.7008]],

        [[0.5969, 0.1148],
         [0.5099, 0.7284]],

        [[0.6814, 0.0993],
         [0.6622, 0.5410]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8823523358261945
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448275862068966
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
Global Adjusted Rand Index: 0.04649399666893103
Average Adjusted Rand Index: 0.91440762646591
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22669.255859375
inf tensor(22669.2559, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12860.32421875
tensor(22669.2559, grad_fn=<NegBackward0>) tensor(12860.3242, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12569.615234375
tensor(12860.3242, grad_fn=<NegBackward0>) tensor(12569.6152, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12280.08984375
tensor(12569.6152, grad_fn=<NegBackward0>) tensor(12280.0898, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12271.2958984375
tensor(12280.0898, grad_fn=<NegBackward0>) tensor(12271.2959, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12248.0048828125
tensor(12271.2959, grad_fn=<NegBackward0>) tensor(12248.0049, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12247.4892578125
tensor(12248.0049, grad_fn=<NegBackward0>) tensor(12247.4893, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12246.630859375
tensor(12247.4893, grad_fn=<NegBackward0>) tensor(12246.6309, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12246.455078125
tensor(12246.6309, grad_fn=<NegBackward0>) tensor(12246.4551, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12245.6904296875
tensor(12246.4551, grad_fn=<NegBackward0>) tensor(12245.6904, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12245.6103515625
tensor(12245.6904, grad_fn=<NegBackward0>) tensor(12245.6104, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12245.51171875
tensor(12245.6104, grad_fn=<NegBackward0>) tensor(12245.5117, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12245.486328125
tensor(12245.5117, grad_fn=<NegBackward0>) tensor(12245.4863, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12245.46484375
tensor(12245.4863, grad_fn=<NegBackward0>) tensor(12245.4648, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12245.4482421875
tensor(12245.4648, grad_fn=<NegBackward0>) tensor(12245.4482, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12245.4345703125
tensor(12245.4482, grad_fn=<NegBackward0>) tensor(12245.4346, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12245.4228515625
tensor(12245.4346, grad_fn=<NegBackward0>) tensor(12245.4229, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12245.40625
tensor(12245.4229, grad_fn=<NegBackward0>) tensor(12245.4062, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12245.375
tensor(12245.4062, grad_fn=<NegBackward0>) tensor(12245.3750, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12245.251953125
tensor(12245.3750, grad_fn=<NegBackward0>) tensor(12245.2520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12245.244140625
tensor(12245.2520, grad_fn=<NegBackward0>) tensor(12245.2441, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12245.2412109375
tensor(12245.2441, grad_fn=<NegBackward0>) tensor(12245.2412, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12245.2353515625
tensor(12245.2412, grad_fn=<NegBackward0>) tensor(12245.2354, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12245.2294921875
tensor(12245.2354, grad_fn=<NegBackward0>) tensor(12245.2295, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12245.2255859375
tensor(12245.2295, grad_fn=<NegBackward0>) tensor(12245.2256, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12245.22265625
tensor(12245.2256, grad_fn=<NegBackward0>) tensor(12245.2227, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12245.220703125
tensor(12245.2227, grad_fn=<NegBackward0>) tensor(12245.2207, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12245.21875
tensor(12245.2207, grad_fn=<NegBackward0>) tensor(12245.2188, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12245.2158203125
tensor(12245.2188, grad_fn=<NegBackward0>) tensor(12245.2158, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12245.21484375
tensor(12245.2158, grad_fn=<NegBackward0>) tensor(12245.2148, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12245.21484375
tensor(12245.2148, grad_fn=<NegBackward0>) tensor(12245.2148, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12245.2109375
tensor(12245.2148, grad_fn=<NegBackward0>) tensor(12245.2109, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12245.208984375
tensor(12245.2109, grad_fn=<NegBackward0>) tensor(12245.2090, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12245.2099609375
tensor(12245.2090, grad_fn=<NegBackward0>) tensor(12245.2100, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12245.2080078125
tensor(12245.2090, grad_fn=<NegBackward0>) tensor(12245.2080, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12245.20703125
tensor(12245.2080, grad_fn=<NegBackward0>) tensor(12245.2070, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12245.205078125
tensor(12245.2070, grad_fn=<NegBackward0>) tensor(12245.2051, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12245.203125
tensor(12245.2051, grad_fn=<NegBackward0>) tensor(12245.2031, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12245.212890625
tensor(12245.2031, grad_fn=<NegBackward0>) tensor(12245.2129, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12245.2001953125
tensor(12245.2031, grad_fn=<NegBackward0>) tensor(12245.2002, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12245.2001953125
tensor(12245.2002, grad_fn=<NegBackward0>) tensor(12245.2002, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12245.201171875
tensor(12245.2002, grad_fn=<NegBackward0>) tensor(12245.2012, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12245.19921875
tensor(12245.2002, grad_fn=<NegBackward0>) tensor(12245.1992, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12245.19921875
tensor(12245.1992, grad_fn=<NegBackward0>) tensor(12245.1992, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12245.1982421875
tensor(12245.1992, grad_fn=<NegBackward0>) tensor(12245.1982, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12245.1982421875
tensor(12245.1982, grad_fn=<NegBackward0>) tensor(12245.1982, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12245.1982421875
tensor(12245.1982, grad_fn=<NegBackward0>) tensor(12245.1982, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12245.197265625
tensor(12245.1982, grad_fn=<NegBackward0>) tensor(12245.1973, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12245.197265625
tensor(12245.1973, grad_fn=<NegBackward0>) tensor(12245.1973, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12245.197265625
tensor(12245.1973, grad_fn=<NegBackward0>) tensor(12245.1973, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12245.1962890625
tensor(12245.1973, grad_fn=<NegBackward0>) tensor(12245.1963, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12245.1982421875
tensor(12245.1963, grad_fn=<NegBackward0>) tensor(12245.1982, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12245.1943359375
tensor(12245.1963, grad_fn=<NegBackward0>) tensor(12245.1943, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12245.1943359375
tensor(12245.1943, grad_fn=<NegBackward0>) tensor(12245.1943, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12245.1962890625
tensor(12245.1943, grad_fn=<NegBackward0>) tensor(12245.1963, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12245.193359375
tensor(12245.1943, grad_fn=<NegBackward0>) tensor(12245.1934, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12245.19140625
tensor(12245.1934, grad_fn=<NegBackward0>) tensor(12245.1914, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12245.1982421875
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1982, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12245.193359375
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1934, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12245.193359375
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1934, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -12245.1923828125
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1924, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -12245.19140625
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1914, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12245.19140625
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1914, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12245.19140625
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1914, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12245.1923828125
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1924, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12245.1904296875
tensor(12245.1914, grad_fn=<NegBackward0>) tensor(12245.1904, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12245.1953125
tensor(12245.1904, grad_fn=<NegBackward0>) tensor(12245.1953, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12245.1953125
tensor(12245.1904, grad_fn=<NegBackward0>) tensor(12245.1953, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12245.1884765625
tensor(12245.1904, grad_fn=<NegBackward0>) tensor(12245.1885, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12245.1865234375
tensor(12245.1885, grad_fn=<NegBackward0>) tensor(12245.1865, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12245.1884765625
tensor(12245.1865, grad_fn=<NegBackward0>) tensor(12245.1885, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12245.1865234375
tensor(12245.1865, grad_fn=<NegBackward0>) tensor(12245.1865, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12245.1865234375
tensor(12245.1865, grad_fn=<NegBackward0>) tensor(12245.1865, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12245.1875
tensor(12245.1865, grad_fn=<NegBackward0>) tensor(12245.1875, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12245.185546875
tensor(12245.1865, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12245.185546875
tensor(12245.1855, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12245.2216796875
tensor(12245.1855, grad_fn=<NegBackward0>) tensor(12245.2217, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12245.1865234375
tensor(12245.1855, grad_fn=<NegBackward0>) tensor(12245.1865, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12245.185546875
tensor(12245.1855, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12245.1845703125
tensor(12245.1855, grad_fn=<NegBackward0>) tensor(12245.1846, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12245.185546875
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12245.248046875
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.2480, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -12245.185546875
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -12245.1845703125
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1846, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12245.185546875
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12245.185546875
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12245.203125
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.2031, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -12245.185546875
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -12245.1845703125
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1846, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12245.1845703125
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1846, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12245.2021484375
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.2021, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12245.185546875
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12245.2900390625
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.2900, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -12245.1845703125
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1846, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12245.28515625
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.2852, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12245.18359375
tensor(12245.1846, grad_fn=<NegBackward0>) tensor(12245.1836, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12245.185546875
tensor(12245.1836, grad_fn=<NegBackward0>) tensor(12245.1855, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12245.20703125
tensor(12245.1836, grad_fn=<NegBackward0>) tensor(12245.2070, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -12245.1845703125
tensor(12245.1836, grad_fn=<NegBackward0>) tensor(12245.1846, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -12245.193359375
tensor(12245.1836, grad_fn=<NegBackward0>) tensor(12245.1934, grad_fn=<NegBackward0>)
4
pi: tensor([[0.3640, 0.6360],
        [0.5742, 0.4258]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6441, 0.3559], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3548, 0.0901],
         [0.5874, 0.2908]],

        [[0.5774, 0.0914],
         [0.6959, 0.7035]],

        [[0.7104, 0.1062],
         [0.6831, 0.6319]],

        [[0.7226, 0.1148],
         [0.6606, 0.5054]],

        [[0.7193, 0.0995],
         [0.7199, 0.5715]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7366343430876169
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
Global Adjusted Rand Index: 0.05768226657231091
Average Adjusted Rand Index: 0.9162985106768152
[0.04649399666893103, 0.05768226657231091] [0.91440762646591, 0.9162985106768152] [12254.701171875, 12245.1845703125]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11548.157020076182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21877.142578125
inf tensor(21877.1426, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11984.8779296875
tensor(21877.1426, grad_fn=<NegBackward0>) tensor(11984.8779, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11610.7421875
tensor(11984.8779, grad_fn=<NegBackward0>) tensor(11610.7422, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11537.0185546875
tensor(11610.7422, grad_fn=<NegBackward0>) tensor(11537.0186, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11530.6875
tensor(11537.0186, grad_fn=<NegBackward0>) tensor(11530.6875, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11530.248046875
tensor(11530.6875, grad_fn=<NegBackward0>) tensor(11530.2480, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11530.1162109375
tensor(11530.2480, grad_fn=<NegBackward0>) tensor(11530.1162, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11530.037109375
tensor(11530.1162, grad_fn=<NegBackward0>) tensor(11530.0371, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11529.986328125
tensor(11530.0371, grad_fn=<NegBackward0>) tensor(11529.9863, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11529.9482421875
tensor(11529.9863, grad_fn=<NegBackward0>) tensor(11529.9482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11529.9208984375
tensor(11529.9482, grad_fn=<NegBackward0>) tensor(11529.9209, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11529.900390625
tensor(11529.9209, grad_fn=<NegBackward0>) tensor(11529.9004, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11529.8857421875
tensor(11529.9004, grad_fn=<NegBackward0>) tensor(11529.8857, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11529.87109375
tensor(11529.8857, grad_fn=<NegBackward0>) tensor(11529.8711, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11529.8603515625
tensor(11529.8711, grad_fn=<NegBackward0>) tensor(11529.8604, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11529.8515625
tensor(11529.8604, grad_fn=<NegBackward0>) tensor(11529.8516, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11529.845703125
tensor(11529.8516, grad_fn=<NegBackward0>) tensor(11529.8457, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11529.837890625
tensor(11529.8457, grad_fn=<NegBackward0>) tensor(11529.8379, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11529.8330078125
tensor(11529.8379, grad_fn=<NegBackward0>) tensor(11529.8330, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11529.8291015625
tensor(11529.8330, grad_fn=<NegBackward0>) tensor(11529.8291, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11529.8251953125
tensor(11529.8291, grad_fn=<NegBackward0>) tensor(11529.8252, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11529.8212890625
tensor(11529.8252, grad_fn=<NegBackward0>) tensor(11529.8213, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11529.8193359375
tensor(11529.8213, grad_fn=<NegBackward0>) tensor(11529.8193, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11529.8154296875
tensor(11529.8193, grad_fn=<NegBackward0>) tensor(11529.8154, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11529.814453125
tensor(11529.8154, grad_fn=<NegBackward0>) tensor(11529.8145, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11529.8115234375
tensor(11529.8145, grad_fn=<NegBackward0>) tensor(11529.8115, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11529.80859375
tensor(11529.8115, grad_fn=<NegBackward0>) tensor(11529.8086, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11529.8076171875
tensor(11529.8086, grad_fn=<NegBackward0>) tensor(11529.8076, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11529.8076171875
tensor(11529.8076, grad_fn=<NegBackward0>) tensor(11529.8076, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11529.8037109375
tensor(11529.8076, grad_fn=<NegBackward0>) tensor(11529.8037, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11529.8037109375
tensor(11529.8037, grad_fn=<NegBackward0>) tensor(11529.8037, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11529.8017578125
tensor(11529.8037, grad_fn=<NegBackward0>) tensor(11529.8018, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11529.8017578125
tensor(11529.8018, grad_fn=<NegBackward0>) tensor(11529.8018, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11529.7998046875
tensor(11529.8018, grad_fn=<NegBackward0>) tensor(11529.7998, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11529.7998046875
tensor(11529.7998, grad_fn=<NegBackward0>) tensor(11529.7998, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11529.7978515625
tensor(11529.7998, grad_fn=<NegBackward0>) tensor(11529.7979, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11529.7939453125
tensor(11529.7979, grad_fn=<NegBackward0>) tensor(11529.7939, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11529.79296875
tensor(11529.7939, grad_fn=<NegBackward0>) tensor(11529.7930, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11529.791015625
tensor(11529.7930, grad_fn=<NegBackward0>) tensor(11529.7910, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11529.7900390625
tensor(11529.7910, grad_fn=<NegBackward0>) tensor(11529.7900, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11529.791015625
tensor(11529.7900, grad_fn=<NegBackward0>) tensor(11529.7910, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11529.7890625
tensor(11529.7900, grad_fn=<NegBackward0>) tensor(11529.7891, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11529.787109375
tensor(11529.7891, grad_fn=<NegBackward0>) tensor(11529.7871, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11529.787109375
tensor(11529.7871, grad_fn=<NegBackward0>) tensor(11529.7871, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11529.787109375
tensor(11529.7871, grad_fn=<NegBackward0>) tensor(11529.7871, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11529.78515625
tensor(11529.7871, grad_fn=<NegBackward0>) tensor(11529.7852, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11529.78515625
tensor(11529.7852, grad_fn=<NegBackward0>) tensor(11529.7852, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11529.5927734375
tensor(11529.7852, grad_fn=<NegBackward0>) tensor(11529.5928, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11529.5927734375
tensor(11529.5928, grad_fn=<NegBackward0>) tensor(11529.5928, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11529.5908203125
tensor(11529.5928, grad_fn=<NegBackward0>) tensor(11529.5908, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11529.591796875
tensor(11529.5908, grad_fn=<NegBackward0>) tensor(11529.5918, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11529.5908203125
tensor(11529.5908, grad_fn=<NegBackward0>) tensor(11529.5908, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11529.5888671875
tensor(11529.5908, grad_fn=<NegBackward0>) tensor(11529.5889, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11529.58984375
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5898, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11529.5927734375
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5928, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11529.607421875
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.6074, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11529.5888671875
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5889, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11529.5888671875
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5889, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11529.5888671875
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5889, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11529.58984375
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5898, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11529.591796875
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5918, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11529.587890625
tensor(11529.5889, grad_fn=<NegBackward0>) tensor(11529.5879, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11529.5869140625
tensor(11529.5879, grad_fn=<NegBackward0>) tensor(11529.5869, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11529.587890625
tensor(11529.5869, grad_fn=<NegBackward0>) tensor(11529.5879, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11529.587890625
tensor(11529.5869, grad_fn=<NegBackward0>) tensor(11529.5879, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11529.591796875
tensor(11529.5869, grad_fn=<NegBackward0>) tensor(11529.5918, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11529.587890625
tensor(11529.5869, grad_fn=<NegBackward0>) tensor(11529.5879, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11529.5869140625
tensor(11529.5869, grad_fn=<NegBackward0>) tensor(11529.5869, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11529.5869140625
tensor(11529.5869, grad_fn=<NegBackward0>) tensor(11529.5869, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11529.5859375
tensor(11529.5869, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11529.5859375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11529.591796875
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5918, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11529.5859375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11529.587890625
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5879, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11529.5869140625
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5869, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11529.609375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.6094, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11529.5859375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11529.5986328125
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5986, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11529.5859375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11529.6455078125
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.6455, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11529.5859375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11529.6015625
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.6016, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11529.5859375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11529.5849609375
tensor(11529.5859, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11529.591796875
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5918, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11529.5859375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11529.609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.6094, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11529.5859375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5859, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11529.7236328125
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.7236, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11529.5869140625
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5869, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11529.5869140625
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5869, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11529.5849609375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5850, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11529.583984375
tensor(11529.5850, grad_fn=<NegBackward0>) tensor(11529.5840, grad_fn=<NegBackward0>)
pi: tensor([[0.7779, 0.2221],
        [0.2781, 0.7219]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4494, 0.5506], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1928, 0.1012],
         [0.7302, 0.3992]],

        [[0.5475, 0.1026],
         [0.7308, 0.6162]],

        [[0.5523, 0.1047],
         [0.5739, 0.5654]],

        [[0.7094, 0.1033],
         [0.5718, 0.5270]],

        [[0.6554, 0.1096],
         [0.6195, 0.5635]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8443214938060299
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603206327564754
Average Adjusted Rand Index: 0.9608642987612059
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20257.107421875
inf tensor(20257.1074, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12199.978515625
tensor(20257.1074, grad_fn=<NegBackward0>) tensor(12199.9785, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11898.337890625
tensor(12199.9785, grad_fn=<NegBackward0>) tensor(11898.3379, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11687.8173828125
tensor(11898.3379, grad_fn=<NegBackward0>) tensor(11687.8174, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11631.3076171875
tensor(11687.8174, grad_fn=<NegBackward0>) tensor(11631.3076, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11608.3076171875
tensor(11631.3076, grad_fn=<NegBackward0>) tensor(11608.3076, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11579.302734375
tensor(11608.3076, grad_fn=<NegBackward0>) tensor(11579.3027, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11555.1201171875
tensor(11579.3027, grad_fn=<NegBackward0>) tensor(11555.1201, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11554.904296875
tensor(11555.1201, grad_fn=<NegBackward0>) tensor(11554.9043, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11554.8408203125
tensor(11554.9043, grad_fn=<NegBackward0>) tensor(11554.8408, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11554.7958984375
tensor(11554.8408, grad_fn=<NegBackward0>) tensor(11554.7959, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11554.76171875
tensor(11554.7959, grad_fn=<NegBackward0>) tensor(11554.7617, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11541.8681640625
tensor(11554.7617, grad_fn=<NegBackward0>) tensor(11541.8682, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11541.83984375
tensor(11541.8682, grad_fn=<NegBackward0>) tensor(11541.8398, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11541.822265625
tensor(11541.8398, grad_fn=<NegBackward0>) tensor(11541.8223, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11541.810546875
tensor(11541.8223, grad_fn=<NegBackward0>) tensor(11541.8105, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11541.7978515625
tensor(11541.8105, grad_fn=<NegBackward0>) tensor(11541.7979, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11541.7900390625
tensor(11541.7979, grad_fn=<NegBackward0>) tensor(11541.7900, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11541.779296875
tensor(11541.7900, grad_fn=<NegBackward0>) tensor(11541.7793, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11530.1083984375
tensor(11541.7793, grad_fn=<NegBackward0>) tensor(11530.1084, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11530.1015625
tensor(11530.1084, grad_fn=<NegBackward0>) tensor(11530.1016, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11530.0966796875
tensor(11530.1016, grad_fn=<NegBackward0>) tensor(11530.0967, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11530.0908203125
tensor(11530.0967, grad_fn=<NegBackward0>) tensor(11530.0908, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11530.0869140625
tensor(11530.0908, grad_fn=<NegBackward0>) tensor(11530.0869, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11530.083984375
tensor(11530.0869, grad_fn=<NegBackward0>) tensor(11530.0840, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11530.0810546875
tensor(11530.0840, grad_fn=<NegBackward0>) tensor(11530.0811, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11530.078125
tensor(11530.0811, grad_fn=<NegBackward0>) tensor(11530.0781, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11530.076171875
tensor(11530.0781, grad_fn=<NegBackward0>) tensor(11530.0762, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11530.072265625
tensor(11530.0762, grad_fn=<NegBackward0>) tensor(11530.0723, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11530.072265625
tensor(11530.0723, grad_fn=<NegBackward0>) tensor(11530.0723, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11530.0703125
tensor(11530.0723, grad_fn=<NegBackward0>) tensor(11530.0703, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11530.068359375
tensor(11530.0703, grad_fn=<NegBackward0>) tensor(11530.0684, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11530.0654296875
tensor(11530.0684, grad_fn=<NegBackward0>) tensor(11530.0654, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11530.064453125
tensor(11530.0654, grad_fn=<NegBackward0>) tensor(11530.0645, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11530.06640625
tensor(11530.0645, grad_fn=<NegBackward0>) tensor(11530.0664, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11530.0634765625
tensor(11530.0645, grad_fn=<NegBackward0>) tensor(11530.0635, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11530.060546875
tensor(11530.0635, grad_fn=<NegBackward0>) tensor(11530.0605, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11530.0556640625
tensor(11530.0605, grad_fn=<NegBackward0>) tensor(11530.0557, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11530.056640625
tensor(11530.0557, grad_fn=<NegBackward0>) tensor(11530.0566, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11530.0576171875
tensor(11530.0557, grad_fn=<NegBackward0>) tensor(11530.0576, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11530.0546875
tensor(11530.0557, grad_fn=<NegBackward0>) tensor(11530.0547, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11530.0537109375
tensor(11530.0547, grad_fn=<NegBackward0>) tensor(11530.0537, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11530.0615234375
tensor(11530.0537, grad_fn=<NegBackward0>) tensor(11530.0615, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11530.0517578125
tensor(11530.0537, grad_fn=<NegBackward0>) tensor(11530.0518, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11530.05078125
tensor(11530.0518, grad_fn=<NegBackward0>) tensor(11530.0508, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11530.0380859375
tensor(11530.0508, grad_fn=<NegBackward0>) tensor(11530.0381, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11530.001953125
tensor(11530.0381, grad_fn=<NegBackward0>) tensor(11530.0020, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11530.00390625
tensor(11530.0020, grad_fn=<NegBackward0>) tensor(11530.0039, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11530.001953125
tensor(11530.0020, grad_fn=<NegBackward0>) tensor(11530.0020, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11530.0009765625
tensor(11530.0020, grad_fn=<NegBackward0>) tensor(11530.0010, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11530.0009765625
tensor(11530.0010, grad_fn=<NegBackward0>) tensor(11530.0010, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11530.0
tensor(11530.0010, grad_fn=<NegBackward0>) tensor(11530., grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11530.0
tensor(11530., grad_fn=<NegBackward0>) tensor(11530., grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11529.998046875
tensor(11530., grad_fn=<NegBackward0>) tensor(11529.9980, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11530.0078125
tensor(11529.9980, grad_fn=<NegBackward0>) tensor(11530.0078, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11529.998046875
tensor(11529.9980, grad_fn=<NegBackward0>) tensor(11529.9980, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11529.998046875
tensor(11529.9980, grad_fn=<NegBackward0>) tensor(11529.9980, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11529.998046875
tensor(11529.9980, grad_fn=<NegBackward0>) tensor(11529.9980, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11529.998046875
tensor(11529.9980, grad_fn=<NegBackward0>) tensor(11529.9980, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11529.99609375
tensor(11529.9980, grad_fn=<NegBackward0>) tensor(11529.9961, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11530.00390625
tensor(11529.9961, grad_fn=<NegBackward0>) tensor(11530.0039, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11529.99609375
tensor(11529.9961, grad_fn=<NegBackward0>) tensor(11529.9961, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11529.9951171875
tensor(11529.9961, grad_fn=<NegBackward0>) tensor(11529.9951, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11529.9951171875
tensor(11529.9951, grad_fn=<NegBackward0>) tensor(11529.9951, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11529.9970703125
tensor(11529.9951, grad_fn=<NegBackward0>) tensor(11529.9971, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11529.994140625
tensor(11529.9951, grad_fn=<NegBackward0>) tensor(11529.9941, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11529.9921875
tensor(11529.9941, grad_fn=<NegBackward0>) tensor(11529.9922, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11529.98828125
tensor(11529.9922, grad_fn=<NegBackward0>) tensor(11529.9883, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11529.9912109375
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9912, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11529.9970703125
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9971, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11529.98828125
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9883, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11529.98828125
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9883, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11529.98828125
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9883, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11529.98828125
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9883, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11529.98828125
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9883, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11529.9931640625
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9932, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11530.060546875
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11530.0605, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11529.9892578125
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9893, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11529.9873046875
tensor(11529.9883, grad_fn=<NegBackward0>) tensor(11529.9873, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11529.984375
tensor(11529.9873, grad_fn=<NegBackward0>) tensor(11529.9844, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11529.9814453125
tensor(11529.9844, grad_fn=<NegBackward0>) tensor(11529.9814, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11529.984375
tensor(11529.9814, grad_fn=<NegBackward0>) tensor(11529.9844, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11529.98828125
tensor(11529.9814, grad_fn=<NegBackward0>) tensor(11529.9883, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11529.982421875
tensor(11529.9814, grad_fn=<NegBackward0>) tensor(11529.9824, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11530.00390625
tensor(11529.9814, grad_fn=<NegBackward0>) tensor(11530.0039, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11529.982421875
tensor(11529.9814, grad_fn=<NegBackward0>) tensor(11529.9824, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.7227, 0.2773],
        [0.2218, 0.7782]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5496, 0.4504], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3993, 0.1012],
         [0.6441, 0.1926]],

        [[0.6977, 0.1028],
         [0.5082, 0.5436]],

        [[0.5234, 0.1050],
         [0.5829, 0.6440]],

        [[0.6481, 0.1033],
         [0.5175, 0.6637]],

        [[0.5853, 0.1095],
         [0.6487, 0.7199]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8443214938060299
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603206327564754
Average Adjusted Rand Index: 0.9608642987612059
[0.9603206327564754, 0.9603206327564754] [0.9608642987612059, 0.9608642987612059] [11529.5908203125, 11529.982421875]
-------------------------------------
This iteration is 76
True Objective function: Loss = -11363.099341317185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21274.220703125
inf tensor(21274.2207, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12170.845703125
tensor(21274.2207, grad_fn=<NegBackward0>) tensor(12170.8457, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12165.0498046875
tensor(12170.8457, grad_fn=<NegBackward0>) tensor(12165.0498, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11706.248046875
tensor(12165.0498, grad_fn=<NegBackward0>) tensor(11706.2480, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11402.5244140625
tensor(11706.2480, grad_fn=<NegBackward0>) tensor(11402.5244, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11393.1865234375
tensor(11402.5244, grad_fn=<NegBackward0>) tensor(11393.1865, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11367.9501953125
tensor(11393.1865, grad_fn=<NegBackward0>) tensor(11367.9502, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11358.5986328125
tensor(11367.9502, grad_fn=<NegBackward0>) tensor(11358.5986, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11358.4208984375
tensor(11358.5986, grad_fn=<NegBackward0>) tensor(11358.4209, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11358.3115234375
tensor(11358.4209, grad_fn=<NegBackward0>) tensor(11358.3115, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11358.2353515625
tensor(11358.3115, grad_fn=<NegBackward0>) tensor(11358.2354, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11358.1806640625
tensor(11358.2354, grad_fn=<NegBackward0>) tensor(11358.1807, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11358.1396484375
tensor(11358.1807, grad_fn=<NegBackward0>) tensor(11358.1396, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11358.107421875
tensor(11358.1396, grad_fn=<NegBackward0>) tensor(11358.1074, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11358.0810546875
tensor(11358.1074, grad_fn=<NegBackward0>) tensor(11358.0811, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11358.0595703125
tensor(11358.0811, grad_fn=<NegBackward0>) tensor(11358.0596, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11358.04296875
tensor(11358.0596, grad_fn=<NegBackward0>) tensor(11358.0430, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11358.0283203125
tensor(11358.0430, grad_fn=<NegBackward0>) tensor(11358.0283, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11358.015625
tensor(11358.0283, grad_fn=<NegBackward0>) tensor(11358.0156, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11358.0048828125
tensor(11358.0156, grad_fn=<NegBackward0>) tensor(11358.0049, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11357.99609375
tensor(11358.0049, grad_fn=<NegBackward0>) tensor(11357.9961, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11357.98828125
tensor(11357.9961, grad_fn=<NegBackward0>) tensor(11357.9883, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11357.98828125
tensor(11357.9883, grad_fn=<NegBackward0>) tensor(11357.9883, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11357.9755859375
tensor(11357.9883, grad_fn=<NegBackward0>) tensor(11357.9756, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11357.96875
tensor(11357.9756, grad_fn=<NegBackward0>) tensor(11357.9688, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11357.9658203125
tensor(11357.9688, grad_fn=<NegBackward0>) tensor(11357.9658, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11357.9619140625
tensor(11357.9658, grad_fn=<NegBackward0>) tensor(11357.9619, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11357.9619140625
tensor(11357.9619, grad_fn=<NegBackward0>) tensor(11357.9619, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11357.953125
tensor(11357.9619, grad_fn=<NegBackward0>) tensor(11357.9531, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11357.9501953125
tensor(11357.9531, grad_fn=<NegBackward0>) tensor(11357.9502, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11357.9482421875
tensor(11357.9502, grad_fn=<NegBackward0>) tensor(11357.9482, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11357.943359375
tensor(11357.9482, grad_fn=<NegBackward0>) tensor(11357.9434, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11357.943359375
tensor(11357.9434, grad_fn=<NegBackward0>) tensor(11357.9434, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11357.9404296875
tensor(11357.9434, grad_fn=<NegBackward0>) tensor(11357.9404, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11357.9384765625
tensor(11357.9404, grad_fn=<NegBackward0>) tensor(11357.9385, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11357.935546875
tensor(11357.9385, grad_fn=<NegBackward0>) tensor(11357.9355, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11357.9326171875
tensor(11357.9355, grad_fn=<NegBackward0>) tensor(11357.9326, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11357.9306640625
tensor(11357.9326, grad_fn=<NegBackward0>) tensor(11357.9307, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11357.9296875
tensor(11357.9307, grad_fn=<NegBackward0>) tensor(11357.9297, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11357.9287109375
tensor(11357.9297, grad_fn=<NegBackward0>) tensor(11357.9287, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11357.9267578125
tensor(11357.9287, grad_fn=<NegBackward0>) tensor(11357.9268, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11357.9267578125
tensor(11357.9268, grad_fn=<NegBackward0>) tensor(11357.9268, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11357.9228515625
tensor(11357.9268, grad_fn=<NegBackward0>) tensor(11357.9229, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11357.9228515625
tensor(11357.9229, grad_fn=<NegBackward0>) tensor(11357.9229, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11357.921875
tensor(11357.9229, grad_fn=<NegBackward0>) tensor(11357.9219, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11357.919921875
tensor(11357.9219, grad_fn=<NegBackward0>) tensor(11357.9199, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11357.9208984375
tensor(11357.9199, grad_fn=<NegBackward0>) tensor(11357.9209, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11357.9189453125
tensor(11357.9199, grad_fn=<NegBackward0>) tensor(11357.9189, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11357.87109375
tensor(11357.9189, grad_fn=<NegBackward0>) tensor(11357.8711, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11357.861328125
tensor(11357.8711, grad_fn=<NegBackward0>) tensor(11357.8613, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11357.861328125
tensor(11357.8613, grad_fn=<NegBackward0>) tensor(11357.8613, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11357.861328125
tensor(11357.8613, grad_fn=<NegBackward0>) tensor(11357.8613, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11357.8603515625
tensor(11357.8613, grad_fn=<NegBackward0>) tensor(11357.8604, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11357.859375
tensor(11357.8604, grad_fn=<NegBackward0>) tensor(11357.8594, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11357.8583984375
tensor(11357.8594, grad_fn=<NegBackward0>) tensor(11357.8584, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11357.8583984375
tensor(11357.8584, grad_fn=<NegBackward0>) tensor(11357.8584, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11357.8603515625
tensor(11357.8584, grad_fn=<NegBackward0>) tensor(11357.8604, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11357.8603515625
tensor(11357.8584, grad_fn=<NegBackward0>) tensor(11357.8604, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11357.85546875
tensor(11357.8584, grad_fn=<NegBackward0>) tensor(11357.8555, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11357.859375
tensor(11357.8555, grad_fn=<NegBackward0>) tensor(11357.8594, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11357.85546875
tensor(11357.8555, grad_fn=<NegBackward0>) tensor(11357.8555, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11357.85546875
tensor(11357.8555, grad_fn=<NegBackward0>) tensor(11357.8555, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11357.8583984375
tensor(11357.8555, grad_fn=<NegBackward0>) tensor(11357.8584, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11357.85546875
tensor(11357.8555, grad_fn=<NegBackward0>) tensor(11357.8555, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11357.8544921875
tensor(11357.8555, grad_fn=<NegBackward0>) tensor(11357.8545, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11357.8544921875
tensor(11357.8545, grad_fn=<NegBackward0>) tensor(11357.8545, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11357.853515625
tensor(11357.8545, grad_fn=<NegBackward0>) tensor(11357.8535, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11357.853515625
tensor(11357.8535, grad_fn=<NegBackward0>) tensor(11357.8535, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11357.853515625
tensor(11357.8535, grad_fn=<NegBackward0>) tensor(11357.8535, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11357.853515625
tensor(11357.8535, grad_fn=<NegBackward0>) tensor(11357.8535, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11357.8544921875
tensor(11357.8535, grad_fn=<NegBackward0>) tensor(11357.8545, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11357.8525390625
tensor(11357.8535, grad_fn=<NegBackward0>) tensor(11357.8525, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11357.8544921875
tensor(11357.8525, grad_fn=<NegBackward0>) tensor(11357.8545, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11357.853515625
tensor(11357.8525, grad_fn=<NegBackward0>) tensor(11357.8535, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11357.8515625
tensor(11357.8525, grad_fn=<NegBackward0>) tensor(11357.8516, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11357.8525390625
tensor(11357.8516, grad_fn=<NegBackward0>) tensor(11357.8525, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11357.861328125
tensor(11357.8516, grad_fn=<NegBackward0>) tensor(11357.8613, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11357.8515625
tensor(11357.8516, grad_fn=<NegBackward0>) tensor(11357.8516, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11357.8515625
tensor(11357.8516, grad_fn=<NegBackward0>) tensor(11357.8516, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11357.8505859375
tensor(11357.8516, grad_fn=<NegBackward0>) tensor(11357.8506, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11357.8544921875
tensor(11357.8506, grad_fn=<NegBackward0>) tensor(11357.8545, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11358.005859375
tensor(11357.8506, grad_fn=<NegBackward0>) tensor(11358.0059, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11357.8505859375
tensor(11357.8506, grad_fn=<NegBackward0>) tensor(11357.8506, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11357.849609375
tensor(11357.8506, grad_fn=<NegBackward0>) tensor(11357.8496, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11357.8505859375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8506, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11357.859375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8594, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11357.853515625
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8535, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11357.8505859375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8506, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11357.849609375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8496, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11357.8505859375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8506, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11357.8505859375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8506, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11357.8505859375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8506, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11357.857421875
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8574, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11357.849609375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8496, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11357.849609375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8496, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11357.8583984375
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8584, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11357.861328125
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8613, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11357.8525390625
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8525, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11357.875
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8750, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11357.853515625
tensor(11357.8496, grad_fn=<NegBackward0>) tensor(11357.8535, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7210, 0.2790],
        [0.2295, 0.7705]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5062, 0.4938], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4074, 0.0923],
         [0.5526, 0.2012]],

        [[0.6814, 0.0979],
         [0.7062, 0.5144]],

        [[0.6259, 0.1035],
         [0.6122, 0.5958]],

        [[0.6453, 0.0948],
         [0.5991, 0.5883]],

        [[0.6604, 0.0918],
         [0.7167, 0.5446]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21098.046875
inf tensor(21098.0469, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12157.900390625
tensor(21098.0469, grad_fn=<NegBackward0>) tensor(12157.9004, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12116.28125
tensor(12157.9004, grad_fn=<NegBackward0>) tensor(12116.2812, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11645.8212890625
tensor(12116.2812, grad_fn=<NegBackward0>) tensor(11645.8213, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11558.47265625
tensor(11645.8213, grad_fn=<NegBackward0>) tensor(11558.4727, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11545.3125
tensor(11558.4727, grad_fn=<NegBackward0>) tensor(11545.3125, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11544.8837890625
tensor(11545.3125, grad_fn=<NegBackward0>) tensor(11544.8838, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11544.7529296875
tensor(11544.8838, grad_fn=<NegBackward0>) tensor(11544.7529, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11544.673828125
tensor(11544.7529, grad_fn=<NegBackward0>) tensor(11544.6738, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11544.572265625
tensor(11544.6738, grad_fn=<NegBackward0>) tensor(11544.5723, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11544.5263671875
tensor(11544.5723, grad_fn=<NegBackward0>) tensor(11544.5264, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11544.486328125
tensor(11544.5264, grad_fn=<NegBackward0>) tensor(11544.4863, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11544.447265625
tensor(11544.4863, grad_fn=<NegBackward0>) tensor(11544.4473, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11544.4375
tensor(11544.4473, grad_fn=<NegBackward0>) tensor(11544.4375, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11544.4140625
tensor(11544.4375, grad_fn=<NegBackward0>) tensor(11544.4141, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11544.404296875
tensor(11544.4141, grad_fn=<NegBackward0>) tensor(11544.4043, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11544.392578125
tensor(11544.4043, grad_fn=<NegBackward0>) tensor(11544.3926, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11544.384765625
tensor(11544.3926, grad_fn=<NegBackward0>) tensor(11544.3848, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11544.3798828125
tensor(11544.3848, grad_fn=<NegBackward0>) tensor(11544.3799, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11544.373046875
tensor(11544.3799, grad_fn=<NegBackward0>) tensor(11544.3730, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11544.3671875
tensor(11544.3730, grad_fn=<NegBackward0>) tensor(11544.3672, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11544.361328125
tensor(11544.3672, grad_fn=<NegBackward0>) tensor(11544.3613, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11544.359375
tensor(11544.3613, grad_fn=<NegBackward0>) tensor(11544.3594, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11544.35546875
tensor(11544.3594, grad_fn=<NegBackward0>) tensor(11544.3555, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11544.3525390625
tensor(11544.3555, grad_fn=<NegBackward0>) tensor(11544.3525, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11544.349609375
tensor(11544.3525, grad_fn=<NegBackward0>) tensor(11544.3496, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11544.3466796875
tensor(11544.3496, grad_fn=<NegBackward0>) tensor(11544.3467, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11544.345703125
tensor(11544.3467, grad_fn=<NegBackward0>) tensor(11544.3457, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11544.34375
tensor(11544.3457, grad_fn=<NegBackward0>) tensor(11544.3438, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11544.3408203125
tensor(11544.3438, grad_fn=<NegBackward0>) tensor(11544.3408, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11544.337890625
tensor(11544.3408, grad_fn=<NegBackward0>) tensor(11544.3379, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11544.3369140625
tensor(11544.3379, grad_fn=<NegBackward0>) tensor(11544.3369, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11544.3359375
tensor(11544.3369, grad_fn=<NegBackward0>) tensor(11544.3359, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11544.3349609375
tensor(11544.3359, grad_fn=<NegBackward0>) tensor(11544.3350, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11544.3349609375
tensor(11544.3350, grad_fn=<NegBackward0>) tensor(11544.3350, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11544.3330078125
tensor(11544.3350, grad_fn=<NegBackward0>) tensor(11544.3330, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11544.3310546875
tensor(11544.3330, grad_fn=<NegBackward0>) tensor(11544.3311, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11544.333984375
tensor(11544.3311, grad_fn=<NegBackward0>) tensor(11544.3340, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11544.3291015625
tensor(11544.3311, grad_fn=<NegBackward0>) tensor(11544.3291, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11544.34375
tensor(11544.3291, grad_fn=<NegBackward0>) tensor(11544.3438, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11544.3291015625
tensor(11544.3291, grad_fn=<NegBackward0>) tensor(11544.3291, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11544.328125
tensor(11544.3291, grad_fn=<NegBackward0>) tensor(11544.3281, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11544.3271484375
tensor(11544.3281, grad_fn=<NegBackward0>) tensor(11544.3271, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11544.3271484375
tensor(11544.3271, grad_fn=<NegBackward0>) tensor(11544.3271, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11544.326171875
tensor(11544.3271, grad_fn=<NegBackward0>) tensor(11544.3262, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11544.326171875
tensor(11544.3262, grad_fn=<NegBackward0>) tensor(11544.3262, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11544.3271484375
tensor(11544.3262, grad_fn=<NegBackward0>) tensor(11544.3271, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11544.3251953125
tensor(11544.3262, grad_fn=<NegBackward0>) tensor(11544.3252, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11544.3251953125
tensor(11544.3252, grad_fn=<NegBackward0>) tensor(11544.3252, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11544.326171875
tensor(11544.3252, grad_fn=<NegBackward0>) tensor(11544.3262, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11544.3271484375
tensor(11544.3252, grad_fn=<NegBackward0>) tensor(11544.3271, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11544.32421875
tensor(11544.3252, grad_fn=<NegBackward0>) tensor(11544.3242, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11544.3232421875
tensor(11544.3242, grad_fn=<NegBackward0>) tensor(11544.3232, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11544.3232421875
tensor(11544.3232, grad_fn=<NegBackward0>) tensor(11544.3232, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11544.3212890625
tensor(11544.3232, grad_fn=<NegBackward0>) tensor(11544.3213, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11544.32421875
tensor(11544.3213, grad_fn=<NegBackward0>) tensor(11544.3242, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11544.3232421875
tensor(11544.3213, grad_fn=<NegBackward0>) tensor(11544.3232, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11544.3212890625
tensor(11544.3213, grad_fn=<NegBackward0>) tensor(11544.3213, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11544.3232421875
tensor(11544.3213, grad_fn=<NegBackward0>) tensor(11544.3232, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11544.3212890625
tensor(11544.3213, grad_fn=<NegBackward0>) tensor(11544.3213, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11544.3212890625
tensor(11544.3213, grad_fn=<NegBackward0>) tensor(11544.3213, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11544.3203125
tensor(11544.3213, grad_fn=<NegBackward0>) tensor(11544.3203, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11544.3212890625
tensor(11544.3203, grad_fn=<NegBackward0>) tensor(11544.3213, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11544.3203125
tensor(11544.3203, grad_fn=<NegBackward0>) tensor(11544.3203, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11544.3203125
tensor(11544.3203, grad_fn=<NegBackward0>) tensor(11544.3203, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11544.3193359375
tensor(11544.3203, grad_fn=<NegBackward0>) tensor(11544.3193, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11544.326171875
tensor(11544.3193, grad_fn=<NegBackward0>) tensor(11544.3262, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11544.318359375
tensor(11544.3193, grad_fn=<NegBackward0>) tensor(11544.3184, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11544.318359375
tensor(11544.3184, grad_fn=<NegBackward0>) tensor(11544.3184, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11544.3193359375
tensor(11544.3184, grad_fn=<NegBackward0>) tensor(11544.3193, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11544.3173828125
tensor(11544.3184, grad_fn=<NegBackward0>) tensor(11544.3174, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11544.318359375
tensor(11544.3174, grad_fn=<NegBackward0>) tensor(11544.3184, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11544.318359375
tensor(11544.3174, grad_fn=<NegBackward0>) tensor(11544.3184, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11544.318359375
tensor(11544.3174, grad_fn=<NegBackward0>) tensor(11544.3184, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11544.318359375
tensor(11544.3174, grad_fn=<NegBackward0>) tensor(11544.3184, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11544.3203125
tensor(11544.3174, grad_fn=<NegBackward0>) tensor(11544.3203, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.6259, 0.3741],
        [0.5484, 0.4516]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4895, 0.5105], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2114, 0.0911],
         [0.5494, 0.4000]],

        [[0.5957, 0.0966],
         [0.6618, 0.5317]],

        [[0.6496, 0.1035],
         [0.5955, 0.6692]],

        [[0.5947, 0.1064],
         [0.6696, 0.7085]],

        [[0.6847, 0.0916],
         [0.6913, 0.6383]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 34
Adjusted Rand Index: 0.09333333333333334
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5348832286524247
Average Adjusted Rand Index: 0.8106660083939566
[1.0, 0.5348832286524247] [1.0, 0.8106660083939566] [11357.853515625, 11544.3203125]
-------------------------------------
This iteration is 77
True Objective function: Loss = -11758.429727299665
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22613.6171875
inf tensor(22613.6172, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12513.4501953125
tensor(22613.6172, grad_fn=<NegBackward0>) tensor(12513.4502, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12468.3408203125
tensor(12513.4502, grad_fn=<NegBackward0>) tensor(12468.3408, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12113.0068359375
tensor(12468.3408, grad_fn=<NegBackward0>) tensor(12113.0068, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12021.7353515625
tensor(12113.0068, grad_fn=<NegBackward0>) tensor(12021.7354, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11980.603515625
tensor(12021.7354, grad_fn=<NegBackward0>) tensor(11980.6035, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11976.6689453125
tensor(11980.6035, grad_fn=<NegBackward0>) tensor(11976.6689, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11948.3271484375
tensor(11976.6689, grad_fn=<NegBackward0>) tensor(11948.3271, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11943.5361328125
tensor(11948.3271, grad_fn=<NegBackward0>) tensor(11943.5361, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11933.203125
tensor(11943.5361, grad_fn=<NegBackward0>) tensor(11933.2031, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11931.51171875
tensor(11933.2031, grad_fn=<NegBackward0>) tensor(11931.5117, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11923.703125
tensor(11931.5117, grad_fn=<NegBackward0>) tensor(11923.7031, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11915.3740234375
tensor(11923.7031, grad_fn=<NegBackward0>) tensor(11915.3740, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11915.337890625
tensor(11915.3740, grad_fn=<NegBackward0>) tensor(11915.3379, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11915.3154296875
tensor(11915.3379, grad_fn=<NegBackward0>) tensor(11915.3154, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11915.2978515625
tensor(11915.3154, grad_fn=<NegBackward0>) tensor(11915.2979, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11915.2841796875
tensor(11915.2979, grad_fn=<NegBackward0>) tensor(11915.2842, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11915.275390625
tensor(11915.2842, grad_fn=<NegBackward0>) tensor(11915.2754, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11915.2626953125
tensor(11915.2754, grad_fn=<NegBackward0>) tensor(11915.2627, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11915.255859375
tensor(11915.2627, grad_fn=<NegBackward0>) tensor(11915.2559, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11915.248046875
tensor(11915.2559, grad_fn=<NegBackward0>) tensor(11915.2480, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11915.2431640625
tensor(11915.2480, grad_fn=<NegBackward0>) tensor(11915.2432, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11915.236328125
tensor(11915.2432, grad_fn=<NegBackward0>) tensor(11915.2363, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11915.232421875
tensor(11915.2363, grad_fn=<NegBackward0>) tensor(11915.2324, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11915.23046875
tensor(11915.2324, grad_fn=<NegBackward0>) tensor(11915.2305, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11915.224609375
tensor(11915.2305, grad_fn=<NegBackward0>) tensor(11915.2246, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11915.224609375
tensor(11915.2246, grad_fn=<NegBackward0>) tensor(11915.2246, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11915.21875
tensor(11915.2246, grad_fn=<NegBackward0>) tensor(11915.2188, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11915.22265625
tensor(11915.2188, grad_fn=<NegBackward0>) tensor(11915.2227, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11915.2138671875
tensor(11915.2188, grad_fn=<NegBackward0>) tensor(11915.2139, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11915.2119140625
tensor(11915.2139, grad_fn=<NegBackward0>) tensor(11915.2119, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11915.2109375
tensor(11915.2119, grad_fn=<NegBackward0>) tensor(11915.2109, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11915.208984375
tensor(11915.2109, grad_fn=<NegBackward0>) tensor(11915.2090, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11915.2060546875
tensor(11915.2090, grad_fn=<NegBackward0>) tensor(11915.2061, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11915.20703125
tensor(11915.2061, grad_fn=<NegBackward0>) tensor(11915.2070, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11915.2060546875
tensor(11915.2061, grad_fn=<NegBackward0>) tensor(11915.2061, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11915.2021484375
tensor(11915.2061, grad_fn=<NegBackward0>) tensor(11915.2021, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11915.2001953125
tensor(11915.2021, grad_fn=<NegBackward0>) tensor(11915.2002, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11915.19921875
tensor(11915.2002, grad_fn=<NegBackward0>) tensor(11915.1992, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11915.1953125
tensor(11915.1992, grad_fn=<NegBackward0>) tensor(11915.1953, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11915.1884765625
tensor(11915.1953, grad_fn=<NegBackward0>) tensor(11915.1885, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11915.1845703125
tensor(11915.1885, grad_fn=<NegBackward0>) tensor(11915.1846, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11915.18359375
tensor(11915.1846, grad_fn=<NegBackward0>) tensor(11915.1836, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11915.1875
tensor(11915.1836, grad_fn=<NegBackward0>) tensor(11915.1875, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11915.1826171875
tensor(11915.1836, grad_fn=<NegBackward0>) tensor(11915.1826, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11915.1904296875
tensor(11915.1826, grad_fn=<NegBackward0>) tensor(11915.1904, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11915.181640625
tensor(11915.1826, grad_fn=<NegBackward0>) tensor(11915.1816, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11915.1796875
tensor(11915.1816, grad_fn=<NegBackward0>) tensor(11915.1797, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11915.18359375
tensor(11915.1797, grad_fn=<NegBackward0>) tensor(11915.1836, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11915.1806640625
tensor(11915.1797, grad_fn=<NegBackward0>) tensor(11915.1807, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11915.18359375
tensor(11915.1797, grad_fn=<NegBackward0>) tensor(11915.1836, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11915.177734375
tensor(11915.1797, grad_fn=<NegBackward0>) tensor(11915.1777, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11915.1728515625
tensor(11915.1777, grad_fn=<NegBackward0>) tensor(11915.1729, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11915.1748046875
tensor(11915.1729, grad_fn=<NegBackward0>) tensor(11915.1748, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11915.1728515625
tensor(11915.1729, grad_fn=<NegBackward0>) tensor(11915.1729, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11915.1748046875
tensor(11915.1729, grad_fn=<NegBackward0>) tensor(11915.1748, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11915.1708984375
tensor(11915.1729, grad_fn=<NegBackward0>) tensor(11915.1709, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11915.171875
tensor(11915.1709, grad_fn=<NegBackward0>) tensor(11915.1719, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11915.1767578125
tensor(11915.1709, grad_fn=<NegBackward0>) tensor(11915.1768, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11915.1708984375
tensor(11915.1709, grad_fn=<NegBackward0>) tensor(11915.1709, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11915.1689453125
tensor(11915.1709, grad_fn=<NegBackward0>) tensor(11915.1689, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11915.1689453125
tensor(11915.1689, grad_fn=<NegBackward0>) tensor(11915.1689, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11915.169921875
tensor(11915.1689, grad_fn=<NegBackward0>) tensor(11915.1699, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11915.169921875
tensor(11915.1689, grad_fn=<NegBackward0>) tensor(11915.1699, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11915.169921875
tensor(11915.1689, grad_fn=<NegBackward0>) tensor(11915.1699, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11915.1689453125
tensor(11915.1689, grad_fn=<NegBackward0>) tensor(11915.1689, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11915.17578125
tensor(11915.1689, grad_fn=<NegBackward0>) tensor(11915.1758, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11915.16796875
tensor(11915.1689, grad_fn=<NegBackward0>) tensor(11915.1680, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11915.169921875
tensor(11915.1680, grad_fn=<NegBackward0>) tensor(11915.1699, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11915.1689453125
tensor(11915.1680, grad_fn=<NegBackward0>) tensor(11915.1689, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11915.16796875
tensor(11915.1680, grad_fn=<NegBackward0>) tensor(11915.1680, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11915.1083984375
tensor(11915.1680, grad_fn=<NegBackward0>) tensor(11915.1084, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11915.109375
tensor(11915.1084, grad_fn=<NegBackward0>) tensor(11915.1094, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11915.1044921875
tensor(11915.1084, grad_fn=<NegBackward0>) tensor(11915.1045, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11915.10546875
tensor(11915.1045, grad_fn=<NegBackward0>) tensor(11915.1055, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11915.103515625
tensor(11915.1045, grad_fn=<NegBackward0>) tensor(11915.1035, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11915.1044921875
tensor(11915.1035, grad_fn=<NegBackward0>) tensor(11915.1045, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11915.103515625
tensor(11915.1035, grad_fn=<NegBackward0>) tensor(11915.1035, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11915.10546875
tensor(11915.1035, grad_fn=<NegBackward0>) tensor(11915.1055, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11915.103515625
tensor(11915.1035, grad_fn=<NegBackward0>) tensor(11915.1035, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11915.103515625
tensor(11915.1035, grad_fn=<NegBackward0>) tensor(11915.1035, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11915.09375
tensor(11915.1035, grad_fn=<NegBackward0>) tensor(11915.0938, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11915.0927734375
tensor(11915.0938, grad_fn=<NegBackward0>) tensor(11915.0928, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11915.09375
tensor(11915.0928, grad_fn=<NegBackward0>) tensor(11915.0938, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11915.09375
tensor(11915.0928, grad_fn=<NegBackward0>) tensor(11915.0938, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11915.09375
tensor(11915.0928, grad_fn=<NegBackward0>) tensor(11915.0938, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11915.0947265625
tensor(11915.0928, grad_fn=<NegBackward0>) tensor(11915.0947, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11915.09375
tensor(11915.0928, grad_fn=<NegBackward0>) tensor(11915.0938, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.7434, 0.2566],
        [0.4059, 0.5941]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4299, 0.5701], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2294, 0.0942],
         [0.6727, 0.3899]],

        [[0.5786, 0.0972],
         [0.5747, 0.6472]],

        [[0.6679, 0.1162],
         [0.6903, 0.5262]],

        [[0.5609, 0.1028],
         [0.5748, 0.6703]],

        [[0.6547, 0.1119],
         [0.5624, 0.5310]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 20
Adjusted Rand Index: 0.3543852647240511
Global Adjusted Rand Index: 0.46132806310569036
Average Adjusted Rand Index: 0.8708770529448102
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21179.1953125
inf tensor(21179.1953, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12254.09765625
tensor(21179.1953, grad_fn=<NegBackward0>) tensor(12254.0977, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12055.57421875
tensor(12254.0977, grad_fn=<NegBackward0>) tensor(12055.5742, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11951.5791015625
tensor(12055.5742, grad_fn=<NegBackward0>) tensor(11951.5791, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11951.1787109375
tensor(11951.5791, grad_fn=<NegBackward0>) tensor(11951.1787, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11951.0068359375
tensor(11951.1787, grad_fn=<NegBackward0>) tensor(11951.0068, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11950.931640625
tensor(11951.0068, grad_fn=<NegBackward0>) tensor(11950.9316, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11950.88671875
tensor(11950.9316, grad_fn=<NegBackward0>) tensor(11950.8867, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11950.853515625
tensor(11950.8867, grad_fn=<NegBackward0>) tensor(11950.8535, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11950.828125
tensor(11950.8535, grad_fn=<NegBackward0>) tensor(11950.8281, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11950.806640625
tensor(11950.8281, grad_fn=<NegBackward0>) tensor(11950.8066, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11950.24609375
tensor(11950.8066, grad_fn=<NegBackward0>) tensor(11950.2461, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11936.31640625
tensor(11950.2461, grad_fn=<NegBackward0>) tensor(11936.3164, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11936.068359375
tensor(11936.3164, grad_fn=<NegBackward0>) tensor(11936.0684, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11936.015625
tensor(11936.0684, grad_fn=<NegBackward0>) tensor(11936.0156, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11936.0048828125
tensor(11936.0156, grad_fn=<NegBackward0>) tensor(11936.0049, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11935.99609375
tensor(11936.0049, grad_fn=<NegBackward0>) tensor(11935.9961, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11935.9921875
tensor(11935.9961, grad_fn=<NegBackward0>) tensor(11935.9922, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11935.9873046875
tensor(11935.9922, grad_fn=<NegBackward0>) tensor(11935.9873, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11935.984375
tensor(11935.9873, grad_fn=<NegBackward0>) tensor(11935.9844, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11935.982421875
tensor(11935.9844, grad_fn=<NegBackward0>) tensor(11935.9824, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11935.978515625
tensor(11935.9824, grad_fn=<NegBackward0>) tensor(11935.9785, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11935.9775390625
tensor(11935.9785, grad_fn=<NegBackward0>) tensor(11935.9775, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11935.9755859375
tensor(11935.9775, grad_fn=<NegBackward0>) tensor(11935.9756, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11935.974609375
tensor(11935.9756, grad_fn=<NegBackward0>) tensor(11935.9746, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11935.97265625
tensor(11935.9746, grad_fn=<NegBackward0>) tensor(11935.9727, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11935.9716796875
tensor(11935.9727, grad_fn=<NegBackward0>) tensor(11935.9717, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11935.96875
tensor(11935.9717, grad_fn=<NegBackward0>) tensor(11935.9688, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11935.966796875
tensor(11935.9688, grad_fn=<NegBackward0>) tensor(11935.9668, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11935.962890625
tensor(11935.9668, grad_fn=<NegBackward0>) tensor(11935.9629, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11935.9599609375
tensor(11935.9629, grad_fn=<NegBackward0>) tensor(11935.9600, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11935.95703125
tensor(11935.9600, grad_fn=<NegBackward0>) tensor(11935.9570, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11935.9501953125
tensor(11935.9570, grad_fn=<NegBackward0>) tensor(11935.9502, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11935.951171875
tensor(11935.9502, grad_fn=<NegBackward0>) tensor(11935.9512, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11935.94921875
tensor(11935.9502, grad_fn=<NegBackward0>) tensor(11935.9492, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11935.9482421875
tensor(11935.9492, grad_fn=<NegBackward0>) tensor(11935.9482, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11935.9482421875
tensor(11935.9482, grad_fn=<NegBackward0>) tensor(11935.9482, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11935.947265625
tensor(11935.9482, grad_fn=<NegBackward0>) tensor(11935.9473, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11935.947265625
tensor(11935.9473, grad_fn=<NegBackward0>) tensor(11935.9473, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11935.947265625
tensor(11935.9473, grad_fn=<NegBackward0>) tensor(11935.9473, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11935.947265625
tensor(11935.9473, grad_fn=<NegBackward0>) tensor(11935.9473, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11935.9462890625
tensor(11935.9473, grad_fn=<NegBackward0>) tensor(11935.9463, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11935.9443359375
tensor(11935.9463, grad_fn=<NegBackward0>) tensor(11935.9443, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11935.9453125
tensor(11935.9443, grad_fn=<NegBackward0>) tensor(11935.9453, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11935.9443359375
tensor(11935.9443, grad_fn=<NegBackward0>) tensor(11935.9443, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11935.943359375
tensor(11935.9443, grad_fn=<NegBackward0>) tensor(11935.9434, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11935.9443359375
tensor(11935.9434, grad_fn=<NegBackward0>) tensor(11935.9443, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11935.943359375
tensor(11935.9434, grad_fn=<NegBackward0>) tensor(11935.9434, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11935.9423828125
tensor(11935.9434, grad_fn=<NegBackward0>) tensor(11935.9424, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11935.943359375
tensor(11935.9424, grad_fn=<NegBackward0>) tensor(11935.9434, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11935.943359375
tensor(11935.9424, grad_fn=<NegBackward0>) tensor(11935.9434, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11935.9658203125
tensor(11935.9424, grad_fn=<NegBackward0>) tensor(11935.9658, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11935.943359375
tensor(11935.9424, grad_fn=<NegBackward0>) tensor(11935.9434, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -11935.990234375
tensor(11935.9424, grad_fn=<NegBackward0>) tensor(11935.9902, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.4862, 0.5138],
        [0.3924, 0.6076]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5701, 0.4299], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3867, 0.0942],
         [0.5533, 0.2322]],

        [[0.7149, 0.0971],
         [0.5365, 0.6630]],

        [[0.5057, 0.1160],
         [0.6623, 0.6946]],

        [[0.6295, 0.1050],
         [0.5058, 0.5818]],

        [[0.7024, 0.1076],
         [0.6779, 0.6623]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 83
Adjusted Rand Index: 0.4304568527918782
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4451178972067646
Average Adjusted Rand Index: 0.8860913705583757
[0.46132806310569036, 0.4451178972067646] [0.8708770529448102, 0.8860913705583757] [11915.09375, 11935.990234375]
-------------------------------------
This iteration is 78
True Objective function: Loss = -11803.3271471895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22531.47265625
inf tensor(22531.4727, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12705.486328125
tensor(22531.4727, grad_fn=<NegBackward0>) tensor(12705.4863, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12555.2099609375
tensor(12705.4863, grad_fn=<NegBackward0>) tensor(12555.2100, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12149.4521484375
tensor(12555.2100, grad_fn=<NegBackward0>) tensor(12149.4521, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12001.6181640625
tensor(12149.4521, grad_fn=<NegBackward0>) tensor(12001.6182, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11911.845703125
tensor(12001.6182, grad_fn=<NegBackward0>) tensor(11911.8457, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11882.9560546875
tensor(11911.8457, grad_fn=<NegBackward0>) tensor(11882.9561, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11881.7998046875
tensor(11882.9561, grad_fn=<NegBackward0>) tensor(11881.7998, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11849.755859375
tensor(11881.7998, grad_fn=<NegBackward0>) tensor(11849.7559, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11811.341796875
tensor(11849.7559, grad_fn=<NegBackward0>) tensor(11811.3418, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11807.7109375
tensor(11811.3418, grad_fn=<NegBackward0>) tensor(11807.7109, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11807.4580078125
tensor(11807.7109, grad_fn=<NegBackward0>) tensor(11807.4580, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11806.3212890625
tensor(11807.4580, grad_fn=<NegBackward0>) tensor(11806.3213, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11806.236328125
tensor(11806.3213, grad_fn=<NegBackward0>) tensor(11806.2363, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11805.99609375
tensor(11806.2363, grad_fn=<NegBackward0>) tensor(11805.9961, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11800.4736328125
tensor(11805.9961, grad_fn=<NegBackward0>) tensor(11800.4736, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11800.4423828125
tensor(11800.4736, grad_fn=<NegBackward0>) tensor(11800.4424, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11800.41796875
tensor(11800.4424, grad_fn=<NegBackward0>) tensor(11800.4180, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11800.3955078125
tensor(11800.4180, grad_fn=<NegBackward0>) tensor(11800.3955, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11800.3798828125
tensor(11800.3955, grad_fn=<NegBackward0>) tensor(11800.3799, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11800.3623046875
tensor(11800.3799, grad_fn=<NegBackward0>) tensor(11800.3623, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11800.349609375
tensor(11800.3623, grad_fn=<NegBackward0>) tensor(11800.3496, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11800.3359375
tensor(11800.3496, grad_fn=<NegBackward0>) tensor(11800.3359, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11800.3193359375
tensor(11800.3359, grad_fn=<NegBackward0>) tensor(11800.3193, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11796.7841796875
tensor(11800.3193, grad_fn=<NegBackward0>) tensor(11796.7842, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11796.7734375
tensor(11796.7842, grad_fn=<NegBackward0>) tensor(11796.7734, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11796.7666015625
tensor(11796.7734, grad_fn=<NegBackward0>) tensor(11796.7666, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11796.759765625
tensor(11796.7666, grad_fn=<NegBackward0>) tensor(11796.7598, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11796.7529296875
tensor(11796.7598, grad_fn=<NegBackward0>) tensor(11796.7529, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11796.748046875
tensor(11796.7529, grad_fn=<NegBackward0>) tensor(11796.7480, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11796.7421875
tensor(11796.7480, grad_fn=<NegBackward0>) tensor(11796.7422, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11796.7392578125
tensor(11796.7422, grad_fn=<NegBackward0>) tensor(11796.7393, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11796.732421875
tensor(11796.7393, grad_fn=<NegBackward0>) tensor(11796.7324, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11796.7236328125
tensor(11796.7324, grad_fn=<NegBackward0>) tensor(11796.7236, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11796.7109375
tensor(11796.7236, grad_fn=<NegBackward0>) tensor(11796.7109, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11796.689453125
tensor(11796.7109, grad_fn=<NegBackward0>) tensor(11796.6895, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11796.6865234375
tensor(11796.6895, grad_fn=<NegBackward0>) tensor(11796.6865, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11796.6826171875
tensor(11796.6865, grad_fn=<NegBackward0>) tensor(11796.6826, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11796.6806640625
tensor(11796.6826, grad_fn=<NegBackward0>) tensor(11796.6807, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11796.6787109375
tensor(11796.6807, grad_fn=<NegBackward0>) tensor(11796.6787, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11796.6767578125
tensor(11796.6787, grad_fn=<NegBackward0>) tensor(11796.6768, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11796.673828125
tensor(11796.6768, grad_fn=<NegBackward0>) tensor(11796.6738, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11796.6748046875
tensor(11796.6738, grad_fn=<NegBackward0>) tensor(11796.6748, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11796.6728515625
tensor(11796.6738, grad_fn=<NegBackward0>) tensor(11796.6729, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11796.6708984375
tensor(11796.6729, grad_fn=<NegBackward0>) tensor(11796.6709, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11796.669921875
tensor(11796.6709, grad_fn=<NegBackward0>) tensor(11796.6699, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11796.6689453125
tensor(11796.6699, grad_fn=<NegBackward0>) tensor(11796.6689, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11796.6669921875
tensor(11796.6689, grad_fn=<NegBackward0>) tensor(11796.6670, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11796.6650390625
tensor(11796.6670, grad_fn=<NegBackward0>) tensor(11796.6650, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11796.6650390625
tensor(11796.6650, grad_fn=<NegBackward0>) tensor(11796.6650, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11796.669921875
tensor(11796.6650, grad_fn=<NegBackward0>) tensor(11796.6699, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11796.6630859375
tensor(11796.6650, grad_fn=<NegBackward0>) tensor(11796.6631, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11796.662109375
tensor(11796.6631, grad_fn=<NegBackward0>) tensor(11796.6621, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11796.6611328125
tensor(11796.6621, grad_fn=<NegBackward0>) tensor(11796.6611, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11796.6591796875
tensor(11796.6611, grad_fn=<NegBackward0>) tensor(11796.6592, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11796.6572265625
tensor(11796.6592, grad_fn=<NegBackward0>) tensor(11796.6572, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11796.65625
tensor(11796.6572, grad_fn=<NegBackward0>) tensor(11796.6562, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11795.150390625
tensor(11796.6562, grad_fn=<NegBackward0>) tensor(11795.1504, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11795.169921875
tensor(11795.1504, grad_fn=<NegBackward0>) tensor(11795.1699, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11795.177734375
tensor(11795.1504, grad_fn=<NegBackward0>) tensor(11795.1777, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11794.986328125
tensor(11795.1504, grad_fn=<NegBackward0>) tensor(11794.9863, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11794.994140625
tensor(11794.9863, grad_fn=<NegBackward0>) tensor(11794.9941, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11794.98828125
tensor(11794.9863, grad_fn=<NegBackward0>) tensor(11794.9883, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11794.9892578125
tensor(11794.9863, grad_fn=<NegBackward0>) tensor(11794.9893, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11794.9892578125
tensor(11794.9863, grad_fn=<NegBackward0>) tensor(11794.9893, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11794.9541015625
tensor(11794.9863, grad_fn=<NegBackward0>) tensor(11794.9541, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11794.9296875
tensor(11794.9541, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11794.9306640625
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9307, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11794.9287109375
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9287, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11794.9296875
tensor(11794.9287, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11794.9296875
tensor(11794.9287, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11794.9287109375
tensor(11794.9287, grad_fn=<NegBackward0>) tensor(11794.9287, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11794.927734375
tensor(11794.9287, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11794.9296875
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11794.927734375
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11794.9267578125
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9268, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11794.927734375
tensor(11794.9268, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11794.9296875
tensor(11794.9268, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11794.9267578125
tensor(11794.9268, grad_fn=<NegBackward0>) tensor(11794.9268, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11794.92578125
tensor(11794.9268, grad_fn=<NegBackward0>) tensor(11794.9258, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11794.943359375
tensor(11794.9258, grad_fn=<NegBackward0>) tensor(11794.9434, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11794.92578125
tensor(11794.9258, grad_fn=<NegBackward0>) tensor(11794.9258, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11794.927734375
tensor(11794.9258, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11794.92578125
tensor(11794.9258, grad_fn=<NegBackward0>) tensor(11794.9258, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11794.927734375
tensor(11794.9258, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11794.9248046875
tensor(11794.9258, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11794.9287109375
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9287, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11794.92578125
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9258, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11794.92578125
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9258, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11794.9248046875
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11794.9248046875
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11794.9248046875
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11794.9248046875
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11794.923828125
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9238, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11794.9228515625
tensor(11794.9238, grad_fn=<NegBackward0>) tensor(11794.9229, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11794.927734375
tensor(11794.9229, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11794.951171875
tensor(11794.9229, grad_fn=<NegBackward0>) tensor(11794.9512, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11794.9228515625
tensor(11794.9229, grad_fn=<NegBackward0>) tensor(11794.9229, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11794.923828125
tensor(11794.9229, grad_fn=<NegBackward0>) tensor(11794.9238, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11794.923828125
tensor(11794.9229, grad_fn=<NegBackward0>) tensor(11794.9238, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7665, 0.2335],
        [0.2104, 0.7896]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4606, 0.5394], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.1013],
         [0.6286, 0.4061]],

        [[0.5435, 0.0928],
         [0.5511, 0.5615]],

        [[0.6780, 0.1061],
         [0.5550, 0.6301]],

        [[0.6757, 0.1062],
         [0.6728, 0.6993]],

        [[0.7174, 0.1078],
         [0.5679, 0.5088]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999558239977
Average Adjusted Rand Index: 0.9919971467023199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20273.08203125
inf tensor(20273.0820, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12677.4462890625
tensor(20273.0820, grad_fn=<NegBackward0>) tensor(12677.4463, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12100.36328125
tensor(12677.4463, grad_fn=<NegBackward0>) tensor(12100.3633, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11927.6728515625
tensor(12100.3633, grad_fn=<NegBackward0>) tensor(11927.6729, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11854.22265625
tensor(11927.6729, grad_fn=<NegBackward0>) tensor(11854.2227, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11844.080078125
tensor(11854.2227, grad_fn=<NegBackward0>) tensor(11844.0801, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11834.9140625
tensor(11844.0801, grad_fn=<NegBackward0>) tensor(11834.9141, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11809.9921875
tensor(11834.9141, grad_fn=<NegBackward0>) tensor(11809.9922, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11809.890625
tensor(11809.9922, grad_fn=<NegBackward0>) tensor(11809.8906, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11809.8310546875
tensor(11809.8906, grad_fn=<NegBackward0>) tensor(11809.8311, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11809.76171875
tensor(11809.8311, grad_fn=<NegBackward0>) tensor(11809.7617, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11808.2421875
tensor(11809.7617, grad_fn=<NegBackward0>) tensor(11808.2422, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11796.8251953125
tensor(11808.2422, grad_fn=<NegBackward0>) tensor(11796.8252, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11796.80078125
tensor(11796.8252, grad_fn=<NegBackward0>) tensor(11796.8008, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11796.7841796875
tensor(11796.8008, grad_fn=<NegBackward0>) tensor(11796.7842, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11796.771484375
tensor(11796.7842, grad_fn=<NegBackward0>) tensor(11796.7715, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11796.7626953125
tensor(11796.7715, grad_fn=<NegBackward0>) tensor(11796.7627, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11796.7529296875
tensor(11796.7627, grad_fn=<NegBackward0>) tensor(11796.7529, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11796.74609375
tensor(11796.7529, grad_fn=<NegBackward0>) tensor(11796.7461, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11796.740234375
tensor(11796.7461, grad_fn=<NegBackward0>) tensor(11796.7402, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11796.7333984375
tensor(11796.7402, grad_fn=<NegBackward0>) tensor(11796.7334, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11796.732421875
tensor(11796.7334, grad_fn=<NegBackward0>) tensor(11796.7324, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11796.7255859375
tensor(11796.7324, grad_fn=<NegBackward0>) tensor(11796.7256, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11796.720703125
tensor(11796.7256, grad_fn=<NegBackward0>) tensor(11796.7207, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11796.7177734375
tensor(11796.7207, grad_fn=<NegBackward0>) tensor(11796.7178, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11796.7119140625
tensor(11796.7178, grad_fn=<NegBackward0>) tensor(11796.7119, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11796.701171875
tensor(11796.7119, grad_fn=<NegBackward0>) tensor(11796.7012, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11796.669921875
tensor(11796.7012, grad_fn=<NegBackward0>) tensor(11796.6699, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11796.6689453125
tensor(11796.6699, grad_fn=<NegBackward0>) tensor(11796.6689, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11796.6669921875
tensor(11796.6689, grad_fn=<NegBackward0>) tensor(11796.6670, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11796.6640625
tensor(11796.6670, grad_fn=<NegBackward0>) tensor(11796.6641, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11796.6630859375
tensor(11796.6641, grad_fn=<NegBackward0>) tensor(11796.6631, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11796.6630859375
tensor(11796.6631, grad_fn=<NegBackward0>) tensor(11796.6631, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11796.6611328125
tensor(11796.6631, grad_fn=<NegBackward0>) tensor(11796.6611, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11796.658203125
tensor(11796.6611, grad_fn=<NegBackward0>) tensor(11796.6582, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11796.650390625
tensor(11796.6582, grad_fn=<NegBackward0>) tensor(11796.6504, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11795.1689453125
tensor(11796.6504, grad_fn=<NegBackward0>) tensor(11795.1689, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11794.9697265625
tensor(11795.1689, grad_fn=<NegBackward0>) tensor(11794.9697, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11794.9384765625
tensor(11794.9697, grad_fn=<NegBackward0>) tensor(11794.9385, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11794.94140625
tensor(11794.9385, grad_fn=<NegBackward0>) tensor(11794.9414, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11794.93359375
tensor(11794.9385, grad_fn=<NegBackward0>) tensor(11794.9336, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11794.93359375
tensor(11794.9336, grad_fn=<NegBackward0>) tensor(11794.9336, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11794.9326171875
tensor(11794.9336, grad_fn=<NegBackward0>) tensor(11794.9326, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11794.9326171875
tensor(11794.9326, grad_fn=<NegBackward0>) tensor(11794.9326, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11794.931640625
tensor(11794.9326, grad_fn=<NegBackward0>) tensor(11794.9316, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11794.9296875
tensor(11794.9316, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11794.9306640625
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9307, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11794.9306640625
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9307, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11794.9296875
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11794.9296875
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11794.9296875
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9297, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11794.931640625
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9316, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11794.927734375
tensor(11794.9297, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11794.9375
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9375, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11794.9287109375
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9287, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11794.9326171875
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9326, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11794.927734375
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11794.9287109375
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9287, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11794.9287109375
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9287, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11794.927734375
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11794.92578125
tensor(11794.9277, grad_fn=<NegBackward0>) tensor(11794.9258, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11794.9248046875
tensor(11794.9258, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11794.9248046875
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11794.927734375
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11794.923828125
tensor(11794.9248, grad_fn=<NegBackward0>) tensor(11794.9238, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11794.923828125
tensor(11794.9238, grad_fn=<NegBackward0>) tensor(11794.9238, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11794.9248046875
tensor(11794.9238, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11794.93359375
tensor(11794.9238, grad_fn=<NegBackward0>) tensor(11794.9336, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11794.921875
tensor(11794.9238, grad_fn=<NegBackward0>) tensor(11794.9219, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11794.9248046875
tensor(11794.9219, grad_fn=<NegBackward0>) tensor(11794.9248, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11794.9228515625
tensor(11794.9219, grad_fn=<NegBackward0>) tensor(11794.9229, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11794.9462890625
tensor(11794.9219, grad_fn=<NegBackward0>) tensor(11794.9463, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11794.927734375
tensor(11794.9219, grad_fn=<NegBackward0>) tensor(11794.9277, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11794.9423828125
tensor(11794.9219, grad_fn=<NegBackward0>) tensor(11794.9424, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7875, 0.2125],
        [0.2328, 0.7672]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5412, 0.4588], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4066, 0.1013],
         [0.6120, 0.1998]],

        [[0.5889, 0.0928],
         [0.5919, 0.7088]],

        [[0.7014, 0.1063],
         [0.5693, 0.6770]],

        [[0.6523, 0.1062],
         [0.5192, 0.6288]],

        [[0.6235, 0.1078],
         [0.5783, 0.6854]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999558239977
Average Adjusted Rand Index: 0.9919971467023199
[0.9919999558239977, 0.9919999558239977] [0.9919971467023199, 0.9919971467023199] [11794.921875, 11794.9423828125]
-------------------------------------
This iteration is 79
True Objective function: Loss = -11833.92534900153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21858.8359375
inf tensor(21858.8359, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12167.0234375
tensor(21858.8359, grad_fn=<NegBackward0>) tensor(12167.0234, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12030.154296875
tensor(12167.0234, grad_fn=<NegBackward0>) tensor(12030.1543, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12028.0390625
tensor(12030.1543, grad_fn=<NegBackward0>) tensor(12028.0391, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12027.7529296875
tensor(12028.0391, grad_fn=<NegBackward0>) tensor(12027.7529, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12027.6513671875
tensor(12027.7529, grad_fn=<NegBackward0>) tensor(12027.6514, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12027.591796875
tensor(12027.6514, grad_fn=<NegBackward0>) tensor(12027.5918, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12027.5537109375
tensor(12027.5918, grad_fn=<NegBackward0>) tensor(12027.5537, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12027.52734375
tensor(12027.5537, grad_fn=<NegBackward0>) tensor(12027.5273, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12027.5087890625
tensor(12027.5273, grad_fn=<NegBackward0>) tensor(12027.5088, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12027.4931640625
tensor(12027.5088, grad_fn=<NegBackward0>) tensor(12027.4932, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12027.4833984375
tensor(12027.4932, grad_fn=<NegBackward0>) tensor(12027.4834, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12027.47265625
tensor(12027.4834, grad_fn=<NegBackward0>) tensor(12027.4727, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12027.466796875
tensor(12027.4727, grad_fn=<NegBackward0>) tensor(12027.4668, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12027.4599609375
tensor(12027.4668, grad_fn=<NegBackward0>) tensor(12027.4600, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12027.4560546875
tensor(12027.4600, grad_fn=<NegBackward0>) tensor(12027.4561, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12027.455078125
tensor(12027.4561, grad_fn=<NegBackward0>) tensor(12027.4551, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12027.4482421875
tensor(12027.4551, grad_fn=<NegBackward0>) tensor(12027.4482, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12027.4453125
tensor(12027.4482, grad_fn=<NegBackward0>) tensor(12027.4453, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12027.44140625
tensor(12027.4453, grad_fn=<NegBackward0>) tensor(12027.4414, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12027.4404296875
tensor(12027.4414, grad_fn=<NegBackward0>) tensor(12027.4404, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12027.44140625
tensor(12027.4404, grad_fn=<NegBackward0>) tensor(12027.4414, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -12027.4375
tensor(12027.4404, grad_fn=<NegBackward0>) tensor(12027.4375, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12027.435546875
tensor(12027.4375, grad_fn=<NegBackward0>) tensor(12027.4355, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12027.435546875
tensor(12027.4355, grad_fn=<NegBackward0>) tensor(12027.4355, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12027.4326171875
tensor(12027.4355, grad_fn=<NegBackward0>) tensor(12027.4326, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12027.431640625
tensor(12027.4326, grad_fn=<NegBackward0>) tensor(12027.4316, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12027.4296875
tensor(12027.4316, grad_fn=<NegBackward0>) tensor(12027.4297, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12027.4296875
tensor(12027.4297, grad_fn=<NegBackward0>) tensor(12027.4297, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12027.4296875
tensor(12027.4297, grad_fn=<NegBackward0>) tensor(12027.4297, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12027.4287109375
tensor(12027.4297, grad_fn=<NegBackward0>) tensor(12027.4287, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12027.4267578125
tensor(12027.4287, grad_fn=<NegBackward0>) tensor(12027.4268, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12027.4267578125
tensor(12027.4268, grad_fn=<NegBackward0>) tensor(12027.4268, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12027.4267578125
tensor(12027.4268, grad_fn=<NegBackward0>) tensor(12027.4268, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12027.42578125
tensor(12027.4268, grad_fn=<NegBackward0>) tensor(12027.4258, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12027.4248046875
tensor(12027.4258, grad_fn=<NegBackward0>) tensor(12027.4248, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12027.423828125
tensor(12027.4248, grad_fn=<NegBackward0>) tensor(12027.4238, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12027.423828125
tensor(12027.4238, grad_fn=<NegBackward0>) tensor(12027.4238, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12027.4228515625
tensor(12027.4238, grad_fn=<NegBackward0>) tensor(12027.4229, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12027.423828125
tensor(12027.4229, grad_fn=<NegBackward0>) tensor(12027.4238, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12027.423828125
tensor(12027.4229, grad_fn=<NegBackward0>) tensor(12027.4238, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -12027.4228515625
tensor(12027.4229, grad_fn=<NegBackward0>) tensor(12027.4229, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12027.421875
tensor(12027.4229, grad_fn=<NegBackward0>) tensor(12027.4219, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12027.4228515625
tensor(12027.4219, grad_fn=<NegBackward0>) tensor(12027.4229, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12027.421875
tensor(12027.4219, grad_fn=<NegBackward0>) tensor(12027.4219, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12027.4208984375
tensor(12027.4219, grad_fn=<NegBackward0>) tensor(12027.4209, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12027.4208984375
tensor(12027.4209, grad_fn=<NegBackward0>) tensor(12027.4209, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12027.4208984375
tensor(12027.4209, grad_fn=<NegBackward0>) tensor(12027.4209, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12027.42578125
tensor(12027.4209, grad_fn=<NegBackward0>) tensor(12027.4258, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12027.431640625
tensor(12027.4209, grad_fn=<NegBackward0>) tensor(12027.4316, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -12027.419921875
tensor(12027.4209, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12027.419921875
tensor(12027.4199, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12027.4208984375
tensor(12027.4199, grad_fn=<NegBackward0>) tensor(12027.4209, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12027.4287109375
tensor(12027.4199, grad_fn=<NegBackward0>) tensor(12027.4287, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12027.419921875
tensor(12027.4199, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12027.4189453125
tensor(12027.4199, grad_fn=<NegBackward0>) tensor(12027.4189, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12027.419921875
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12027.419921875
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12027.421875
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4219, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12027.419921875
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -12027.4189453125
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4189, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12027.4189453125
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4189, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12027.419921875
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12027.4228515625
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4229, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12027.427734375
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4277, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -12027.4189453125
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4189, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12027.4189453125
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4189, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12027.419921875
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12027.4189453125
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4189, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12027.41796875
tensor(12027.4189, grad_fn=<NegBackward0>) tensor(12027.4180, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12027.4208984375
tensor(12027.4180, grad_fn=<NegBackward0>) tensor(12027.4209, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12027.4189453125
tensor(12027.4180, grad_fn=<NegBackward0>) tensor(12027.4189, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -12027.4169921875
tensor(12027.4180, grad_fn=<NegBackward0>) tensor(12027.4170, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12027.4638671875
tensor(12027.4170, grad_fn=<NegBackward0>) tensor(12027.4639, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12027.41796875
tensor(12027.4170, grad_fn=<NegBackward0>) tensor(12027.4180, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12027.41796875
tensor(12027.4170, grad_fn=<NegBackward0>) tensor(12027.4180, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12027.416015625
tensor(12027.4170, grad_fn=<NegBackward0>) tensor(12027.4160, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12027.41796875
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4180, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12027.41796875
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4180, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12027.41796875
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4180, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -12027.416015625
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4160, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12027.419921875
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4199, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12027.4169921875
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4170, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12027.427734375
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4277, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12027.41796875
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.4180, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -12027.607421875
tensor(12027.4160, grad_fn=<NegBackward0>) tensor(12027.6074, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.5336, 0.4664],
        [0.4846, 0.5154]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4773, 0.5227], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2342, 0.0961],
         [0.5185, 0.3923]],

        [[0.5252, 0.0970],
         [0.5796, 0.6614]],

        [[0.5073, 0.1095],
         [0.6148, 0.5643]],

        [[0.7100, 0.0895],
         [0.5574, 0.7081]],

        [[0.6513, 0.1104],
         [0.5753, 0.7187]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 18
Adjusted Rand Index: 0.4036363636363636
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.439781368353115
Average Adjusted Rand Index: 0.8647264890570019
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22129.70703125
inf tensor(22129.7070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12671.3681640625
tensor(22129.7070, grad_fn=<NegBackward0>) tensor(12671.3682, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12161.0263671875
tensor(12671.3682, grad_fn=<NegBackward0>) tensor(12161.0264, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12083.69140625
tensor(12161.0264, grad_fn=<NegBackward0>) tensor(12083.6914, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11967.9638671875
tensor(12083.6914, grad_fn=<NegBackward0>) tensor(11967.9639, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11896.1171875
tensor(11967.9639, grad_fn=<NegBackward0>) tensor(11896.1172, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11889.5263671875
tensor(11896.1172, grad_fn=<NegBackward0>) tensor(11889.5264, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11871.4873046875
tensor(11889.5264, grad_fn=<NegBackward0>) tensor(11871.4873, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11870.73828125
tensor(11871.4873, grad_fn=<NegBackward0>) tensor(11870.7383, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11864.3095703125
tensor(11870.7383, grad_fn=<NegBackward0>) tensor(11864.3096, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11864.2607421875
tensor(11864.3096, grad_fn=<NegBackward0>) tensor(11864.2607, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11864.224609375
tensor(11864.2607, grad_fn=<NegBackward0>) tensor(11864.2246, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11864.1962890625
tensor(11864.2246, grad_fn=<NegBackward0>) tensor(11864.1963, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11864.173828125
tensor(11864.1963, grad_fn=<NegBackward0>) tensor(11864.1738, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11856.107421875
tensor(11864.1738, grad_fn=<NegBackward0>) tensor(11856.1074, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11856.064453125
tensor(11856.1074, grad_fn=<NegBackward0>) tensor(11856.0645, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11856.05078125
tensor(11856.0645, grad_fn=<NegBackward0>) tensor(11856.0508, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11856.0419921875
tensor(11856.0508, grad_fn=<NegBackward0>) tensor(11856.0420, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11856.0322265625
tensor(11856.0420, grad_fn=<NegBackward0>) tensor(11856.0322, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11856.0263671875
tensor(11856.0322, grad_fn=<NegBackward0>) tensor(11856.0264, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11856.029296875
tensor(11856.0264, grad_fn=<NegBackward0>) tensor(11856.0293, grad_fn=<NegBackward0>)
1
Iteration 2100: Loss = -11855.599609375
tensor(11856.0264, grad_fn=<NegBackward0>) tensor(11855.5996, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11853.1982421875
tensor(11855.5996, grad_fn=<NegBackward0>) tensor(11853.1982, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11853.1923828125
tensor(11853.1982, grad_fn=<NegBackward0>) tensor(11853.1924, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11853.181640625
tensor(11853.1924, grad_fn=<NegBackward0>) tensor(11853.1816, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11849.7353515625
tensor(11853.1816, grad_fn=<NegBackward0>) tensor(11849.7354, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11849.73046875
tensor(11849.7354, grad_fn=<NegBackward0>) tensor(11849.7305, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11849.728515625
tensor(11849.7305, grad_fn=<NegBackward0>) tensor(11849.7285, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11849.7255859375
tensor(11849.7285, grad_fn=<NegBackward0>) tensor(11849.7256, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11849.73828125
tensor(11849.7256, grad_fn=<NegBackward0>) tensor(11849.7383, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11849.7216796875
tensor(11849.7256, grad_fn=<NegBackward0>) tensor(11849.7217, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11849.7216796875
tensor(11849.7217, grad_fn=<NegBackward0>) tensor(11849.7217, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11849.7177734375
tensor(11849.7217, grad_fn=<NegBackward0>) tensor(11849.7178, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11849.7158203125
tensor(11849.7178, grad_fn=<NegBackward0>) tensor(11849.7158, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11849.7138671875
tensor(11849.7158, grad_fn=<NegBackward0>) tensor(11849.7139, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11849.712890625
tensor(11849.7139, grad_fn=<NegBackward0>) tensor(11849.7129, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11849.7109375
tensor(11849.7129, grad_fn=<NegBackward0>) tensor(11849.7109, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11849.6796875
tensor(11849.7109, grad_fn=<NegBackward0>) tensor(11849.6797, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11849.650390625
tensor(11849.6797, grad_fn=<NegBackward0>) tensor(11849.6504, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11849.650390625
tensor(11849.6504, grad_fn=<NegBackward0>) tensor(11849.6504, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11849.6494140625
tensor(11849.6504, grad_fn=<NegBackward0>) tensor(11849.6494, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11849.646484375
tensor(11849.6494, grad_fn=<NegBackward0>) tensor(11849.6465, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11849.6494140625
tensor(11849.6465, grad_fn=<NegBackward0>) tensor(11849.6494, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11849.646484375
tensor(11849.6465, grad_fn=<NegBackward0>) tensor(11849.6465, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11849.6455078125
tensor(11849.6465, grad_fn=<NegBackward0>) tensor(11849.6455, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11849.64453125
tensor(11849.6455, grad_fn=<NegBackward0>) tensor(11849.6445, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11849.6435546875
tensor(11849.6445, grad_fn=<NegBackward0>) tensor(11849.6436, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11849.6513671875
tensor(11849.6436, grad_fn=<NegBackward0>) tensor(11849.6514, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11849.6435546875
tensor(11849.6436, grad_fn=<NegBackward0>) tensor(11849.6436, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11849.642578125
tensor(11849.6436, grad_fn=<NegBackward0>) tensor(11849.6426, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11849.642578125
tensor(11849.6426, grad_fn=<NegBackward0>) tensor(11849.6426, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11849.6416015625
tensor(11849.6426, grad_fn=<NegBackward0>) tensor(11849.6416, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11849.603515625
tensor(11849.6416, grad_fn=<NegBackward0>) tensor(11849.6035, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11836.5693359375
tensor(11849.6035, grad_fn=<NegBackward0>) tensor(11836.5693, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11836.5693359375
tensor(11836.5693, grad_fn=<NegBackward0>) tensor(11836.5693, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11836.5673828125
tensor(11836.5693, grad_fn=<NegBackward0>) tensor(11836.5674, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11836.56640625
tensor(11836.5674, grad_fn=<NegBackward0>) tensor(11836.5664, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11836.56640625
tensor(11836.5664, grad_fn=<NegBackward0>) tensor(11836.5664, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11836.5654296875
tensor(11836.5664, grad_fn=<NegBackward0>) tensor(11836.5654, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11836.5673828125
tensor(11836.5654, grad_fn=<NegBackward0>) tensor(11836.5674, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11836.5654296875
tensor(11836.5654, grad_fn=<NegBackward0>) tensor(11836.5654, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11836.5634765625
tensor(11836.5654, grad_fn=<NegBackward0>) tensor(11836.5635, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11836.5634765625
tensor(11836.5635, grad_fn=<NegBackward0>) tensor(11836.5635, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11836.5615234375
tensor(11836.5635, grad_fn=<NegBackward0>) tensor(11836.5615, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11836.5634765625
tensor(11836.5615, grad_fn=<NegBackward0>) tensor(11836.5635, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11836.560546875
tensor(11836.5615, grad_fn=<NegBackward0>) tensor(11836.5605, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11836.560546875
tensor(11836.5605, grad_fn=<NegBackward0>) tensor(11836.5605, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11836.560546875
tensor(11836.5605, grad_fn=<NegBackward0>) tensor(11836.5605, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11836.5595703125
tensor(11836.5605, grad_fn=<NegBackward0>) tensor(11836.5596, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11836.58203125
tensor(11836.5596, grad_fn=<NegBackward0>) tensor(11836.5820, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11836.5634765625
tensor(11836.5596, grad_fn=<NegBackward0>) tensor(11836.5635, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11836.5595703125
tensor(11836.5596, grad_fn=<NegBackward0>) tensor(11836.5596, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11836.5703125
tensor(11836.5596, grad_fn=<NegBackward0>) tensor(11836.5703, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11836.5595703125
tensor(11836.5596, grad_fn=<NegBackward0>) tensor(11836.5596, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11836.55859375
tensor(11836.5596, grad_fn=<NegBackward0>) tensor(11836.5586, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11836.55859375
tensor(11836.5586, grad_fn=<NegBackward0>) tensor(11836.5586, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11836.55859375
tensor(11836.5586, grad_fn=<NegBackward0>) tensor(11836.5586, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11836.5595703125
tensor(11836.5586, grad_fn=<NegBackward0>) tensor(11836.5596, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11828.291015625
tensor(11836.5586, grad_fn=<NegBackward0>) tensor(11828.2910, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11828.2578125
tensor(11828.2910, grad_fn=<NegBackward0>) tensor(11828.2578, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11828.2587890625
tensor(11828.2578, grad_fn=<NegBackward0>) tensor(11828.2588, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11828.2568359375
tensor(11828.2578, grad_fn=<NegBackward0>) tensor(11828.2568, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11828.255859375
tensor(11828.2568, grad_fn=<NegBackward0>) tensor(11828.2559, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11828.2568359375
tensor(11828.2559, grad_fn=<NegBackward0>) tensor(11828.2568, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11828.2646484375
tensor(11828.2559, grad_fn=<NegBackward0>) tensor(11828.2646, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11828.2587890625
tensor(11828.2559, grad_fn=<NegBackward0>) tensor(11828.2588, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11828.2578125
tensor(11828.2559, grad_fn=<NegBackward0>) tensor(11828.2578, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11828.267578125
tensor(11828.2559, grad_fn=<NegBackward0>) tensor(11828.2676, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.6884, 0.3116],
        [0.2544, 0.7456]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4799, 0.5201], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1958, 0.0962],
         [0.5035, 0.4062]],

        [[0.6260, 0.1006],
         [0.6872, 0.7128]],

        [[0.7214, 0.1129],
         [0.6375, 0.5040]],

        [[0.7298, 0.0895],
         [0.6920, 0.6949]],

        [[0.6281, 0.1106],
         [0.7209, 0.6025]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9840313942303629
Average Adjusted Rand Index: 0.9839992163297293
[0.439781368353115, 0.9840313942303629] [0.8647264890570019, 0.9839992163297293] [12027.607421875, 11828.267578125]
-------------------------------------
This iteration is 80
True Objective function: Loss = -11312.417598073027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21640.21484375
inf tensor(21640.2148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11796.2919921875
tensor(21640.2148, grad_fn=<NegBackward0>) tensor(11796.2920, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11310.4560546875
tensor(11796.2920, grad_fn=<NegBackward0>) tensor(11310.4561, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11306.359375
tensor(11310.4561, grad_fn=<NegBackward0>) tensor(11306.3594, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11306.1015625
tensor(11306.3594, grad_fn=<NegBackward0>) tensor(11306.1016, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11305.9677734375
tensor(11306.1016, grad_fn=<NegBackward0>) tensor(11305.9678, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11305.8984375
tensor(11305.9678, grad_fn=<NegBackward0>) tensor(11305.8984, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11305.85546875
tensor(11305.8984, grad_fn=<NegBackward0>) tensor(11305.8555, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11305.826171875
tensor(11305.8555, grad_fn=<NegBackward0>) tensor(11305.8262, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11305.8056640625
tensor(11305.8262, grad_fn=<NegBackward0>) tensor(11305.8057, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11305.791015625
tensor(11305.8057, grad_fn=<NegBackward0>) tensor(11305.7910, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11305.779296875
tensor(11305.7910, grad_fn=<NegBackward0>) tensor(11305.7793, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11305.7685546875
tensor(11305.7793, grad_fn=<NegBackward0>) tensor(11305.7686, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11305.76171875
tensor(11305.7686, grad_fn=<NegBackward0>) tensor(11305.7617, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11305.7548828125
tensor(11305.7617, grad_fn=<NegBackward0>) tensor(11305.7549, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11305.75
tensor(11305.7549, grad_fn=<NegBackward0>) tensor(11305.7500, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11305.7451171875
tensor(11305.7500, grad_fn=<NegBackward0>) tensor(11305.7451, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11305.740234375
tensor(11305.7451, grad_fn=<NegBackward0>) tensor(11305.7402, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11305.7353515625
tensor(11305.7402, grad_fn=<NegBackward0>) tensor(11305.7354, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11305.732421875
tensor(11305.7354, grad_fn=<NegBackward0>) tensor(11305.7324, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11305.73046875
tensor(11305.7324, grad_fn=<NegBackward0>) tensor(11305.7305, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11305.7265625
tensor(11305.7305, grad_fn=<NegBackward0>) tensor(11305.7266, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11305.724609375
tensor(11305.7266, grad_fn=<NegBackward0>) tensor(11305.7246, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11305.7236328125
tensor(11305.7246, grad_fn=<NegBackward0>) tensor(11305.7236, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11305.720703125
tensor(11305.7236, grad_fn=<NegBackward0>) tensor(11305.7207, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11305.7177734375
tensor(11305.7207, grad_fn=<NegBackward0>) tensor(11305.7178, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11305.716796875
tensor(11305.7178, grad_fn=<NegBackward0>) tensor(11305.7168, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11305.7158203125
tensor(11305.7168, grad_fn=<NegBackward0>) tensor(11305.7158, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11305.7138671875
tensor(11305.7158, grad_fn=<NegBackward0>) tensor(11305.7139, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11305.7138671875
tensor(11305.7139, grad_fn=<NegBackward0>) tensor(11305.7139, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11305.7119140625
tensor(11305.7139, grad_fn=<NegBackward0>) tensor(11305.7119, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11305.7109375
tensor(11305.7119, grad_fn=<NegBackward0>) tensor(11305.7109, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11305.7109375
tensor(11305.7109, grad_fn=<NegBackward0>) tensor(11305.7109, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11305.7109375
tensor(11305.7109, grad_fn=<NegBackward0>) tensor(11305.7109, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11305.7099609375
tensor(11305.7109, grad_fn=<NegBackward0>) tensor(11305.7100, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11305.7099609375
tensor(11305.7100, grad_fn=<NegBackward0>) tensor(11305.7100, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11305.708984375
tensor(11305.7100, grad_fn=<NegBackward0>) tensor(11305.7090, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11305.7080078125
tensor(11305.7090, grad_fn=<NegBackward0>) tensor(11305.7080, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11305.7080078125
tensor(11305.7080, grad_fn=<NegBackward0>) tensor(11305.7080, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11305.7080078125
tensor(11305.7080, grad_fn=<NegBackward0>) tensor(11305.7080, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11305.70703125
tensor(11305.7080, grad_fn=<NegBackward0>) tensor(11305.7070, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11305.70703125
tensor(11305.7070, grad_fn=<NegBackward0>) tensor(11305.7070, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11305.71484375
tensor(11305.7070, grad_fn=<NegBackward0>) tensor(11305.7148, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11305.703125
tensor(11305.7070, grad_fn=<NegBackward0>) tensor(11305.7031, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11305.6904296875
tensor(11305.7031, grad_fn=<NegBackward0>) tensor(11305.6904, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11305.693359375
tensor(11305.6904, grad_fn=<NegBackward0>) tensor(11305.6934, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11305.69140625
tensor(11305.6904, grad_fn=<NegBackward0>) tensor(11305.6914, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11305.6982421875
tensor(11305.6904, grad_fn=<NegBackward0>) tensor(11305.6982, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -11305.689453125
tensor(11305.6904, grad_fn=<NegBackward0>) tensor(11305.6895, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11305.6904296875
tensor(11305.6895, grad_fn=<NegBackward0>) tensor(11305.6904, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11305.6923828125
tensor(11305.6895, grad_fn=<NegBackward0>) tensor(11305.6924, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11305.6884765625
tensor(11305.6895, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11305.6884765625
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11305.689453125
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6895, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11305.6884765625
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11305.689453125
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6895, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11305.689453125
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6895, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11305.689453125
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6895, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11305.6884765625
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11305.6875
tensor(11305.6885, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11305.6875
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11305.6884765625
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11305.6884765625
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11305.6875
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11305.689453125
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6895, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11305.6875
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11305.6904296875
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6904, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11305.6875
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11305.6884765625
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11305.6884765625
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11305.6884765625
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6885, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11305.6865234375
tensor(11305.6875, grad_fn=<NegBackward0>) tensor(11305.6865, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11305.6953125
tensor(11305.6865, grad_fn=<NegBackward0>) tensor(11305.6953, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11305.6875
tensor(11305.6865, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11305.6875
tensor(11305.6865, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11305.6875
tensor(11305.6865, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11305.6875
tensor(11305.6865, grad_fn=<NegBackward0>) tensor(11305.6875, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.7262, 0.2738],
        [0.2543, 0.7457]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4302, 0.5698], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3845, 0.0889],
         [0.5653, 0.2005]],

        [[0.6016, 0.1015],
         [0.6713, 0.7002]],

        [[0.6701, 0.0993],
         [0.7032, 0.6879]],

        [[0.6893, 0.0975],
         [0.7159, 0.7061]],

        [[0.7168, 0.1005],
         [0.7012, 0.5332]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919996552039955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20610.392578125
inf tensor(20610.3926, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11941.3466796875
tensor(20610.3926, grad_fn=<NegBackward0>) tensor(11941.3467, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11928.2138671875
tensor(11941.3467, grad_fn=<NegBackward0>) tensor(11928.2139, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11700.0087890625
tensor(11928.2139, grad_fn=<NegBackward0>) tensor(11700.0088, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11507.21875
tensor(11700.0088, grad_fn=<NegBackward0>) tensor(11507.2188, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11494.6875
tensor(11507.2188, grad_fn=<NegBackward0>) tensor(11494.6875, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11491.8544921875
tensor(11494.6875, grad_fn=<NegBackward0>) tensor(11491.8545, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11467.9228515625
tensor(11491.8545, grad_fn=<NegBackward0>) tensor(11467.9229, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11467.8046875
tensor(11467.9229, grad_fn=<NegBackward0>) tensor(11467.8047, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11467.72265625
tensor(11467.8047, grad_fn=<NegBackward0>) tensor(11467.7227, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11461.732421875
tensor(11467.7227, grad_fn=<NegBackward0>) tensor(11461.7324, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11461.6376953125
tensor(11461.7324, grad_fn=<NegBackward0>) tensor(11461.6377, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11458.3857421875
tensor(11461.6377, grad_fn=<NegBackward0>) tensor(11458.3857, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11458.2255859375
tensor(11458.3857, grad_fn=<NegBackward0>) tensor(11458.2256, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11458.20703125
tensor(11458.2256, grad_fn=<NegBackward0>) tensor(11458.2070, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11458.1875
tensor(11458.2070, grad_fn=<NegBackward0>) tensor(11458.1875, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11458.1513671875
tensor(11458.1875, grad_fn=<NegBackward0>) tensor(11458.1514, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11458.1416015625
tensor(11458.1514, grad_fn=<NegBackward0>) tensor(11458.1416, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11458.1337890625
tensor(11458.1416, grad_fn=<NegBackward0>) tensor(11458.1338, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11458.1259765625
tensor(11458.1338, grad_fn=<NegBackward0>) tensor(11458.1260, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11458.1201171875
tensor(11458.1260, grad_fn=<NegBackward0>) tensor(11458.1201, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11458.08203125
tensor(11458.1201, grad_fn=<NegBackward0>) tensor(11458.0820, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11457.7890625
tensor(11458.0820, grad_fn=<NegBackward0>) tensor(11457.7891, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11457.7822265625
tensor(11457.7891, grad_fn=<NegBackward0>) tensor(11457.7822, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11451.7392578125
tensor(11457.7822, grad_fn=<NegBackward0>) tensor(11451.7393, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11451.7353515625
tensor(11451.7393, grad_fn=<NegBackward0>) tensor(11451.7354, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11451.732421875
tensor(11451.7354, grad_fn=<NegBackward0>) tensor(11451.7324, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11451.7294921875
tensor(11451.7324, grad_fn=<NegBackward0>) tensor(11451.7295, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11451.7275390625
tensor(11451.7295, grad_fn=<NegBackward0>) tensor(11451.7275, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11451.7255859375
tensor(11451.7275, grad_fn=<NegBackward0>) tensor(11451.7256, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11451.7255859375
tensor(11451.7256, grad_fn=<NegBackward0>) tensor(11451.7256, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11451.72265625
tensor(11451.7256, grad_fn=<NegBackward0>) tensor(11451.7227, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11451.7216796875
tensor(11451.7227, grad_fn=<NegBackward0>) tensor(11451.7217, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11451.71875
tensor(11451.7217, grad_fn=<NegBackward0>) tensor(11451.7188, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11451.71875
tensor(11451.7188, grad_fn=<NegBackward0>) tensor(11451.7188, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11451.7177734375
tensor(11451.7188, grad_fn=<NegBackward0>) tensor(11451.7178, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11451.716796875
tensor(11451.7178, grad_fn=<NegBackward0>) tensor(11451.7168, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11451.7158203125
tensor(11451.7168, grad_fn=<NegBackward0>) tensor(11451.7158, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11451.7158203125
tensor(11451.7158, grad_fn=<NegBackward0>) tensor(11451.7158, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11451.71484375
tensor(11451.7158, grad_fn=<NegBackward0>) tensor(11451.7148, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11451.7138671875
tensor(11451.7148, grad_fn=<NegBackward0>) tensor(11451.7139, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11451.7138671875
tensor(11451.7139, grad_fn=<NegBackward0>) tensor(11451.7139, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11451.7119140625
tensor(11451.7139, grad_fn=<NegBackward0>) tensor(11451.7119, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11451.7119140625
tensor(11451.7119, grad_fn=<NegBackward0>) tensor(11451.7119, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11451.7109375
tensor(11451.7119, grad_fn=<NegBackward0>) tensor(11451.7109, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11451.7109375
tensor(11451.7109, grad_fn=<NegBackward0>) tensor(11451.7109, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11451.7109375
tensor(11451.7109, grad_fn=<NegBackward0>) tensor(11451.7109, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11451.7099609375
tensor(11451.7109, grad_fn=<NegBackward0>) tensor(11451.7100, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11451.71484375
tensor(11451.7100, grad_fn=<NegBackward0>) tensor(11451.7148, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11451.708984375
tensor(11451.7100, grad_fn=<NegBackward0>) tensor(11451.7090, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11451.7099609375
tensor(11451.7090, grad_fn=<NegBackward0>) tensor(11451.7100, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11451.7080078125
tensor(11451.7090, grad_fn=<NegBackward0>) tensor(11451.7080, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11451.7080078125
tensor(11451.7080, grad_fn=<NegBackward0>) tensor(11451.7080, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11451.70703125
tensor(11451.7080, grad_fn=<NegBackward0>) tensor(11451.7070, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11451.70703125
tensor(11451.7070, grad_fn=<NegBackward0>) tensor(11451.7070, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11451.70703125
tensor(11451.7070, grad_fn=<NegBackward0>) tensor(11451.7070, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11451.7060546875
tensor(11451.7070, grad_fn=<NegBackward0>) tensor(11451.7061, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11451.705078125
tensor(11451.7061, grad_fn=<NegBackward0>) tensor(11451.7051, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11451.7109375
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7109, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11451.705078125
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7051, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11451.71484375
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7148, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11451.705078125
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7051, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11451.705078125
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7051, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11451.7060546875
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7061, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11451.705078125
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7051, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11451.7041015625
tensor(11451.7051, grad_fn=<NegBackward0>) tensor(11451.7041, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11451.705078125
tensor(11451.7041, grad_fn=<NegBackward0>) tensor(11451.7051, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11451.7060546875
tensor(11451.7041, grad_fn=<NegBackward0>) tensor(11451.7061, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11451.703125
tensor(11451.7041, grad_fn=<NegBackward0>) tensor(11451.7031, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11451.69921875
tensor(11451.7031, grad_fn=<NegBackward0>) tensor(11451.6992, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11450.908203125
tensor(11451.6992, grad_fn=<NegBackward0>) tensor(11450.9082, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11450.833984375
tensor(11450.9082, grad_fn=<NegBackward0>) tensor(11450.8340, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11450.833984375
tensor(11450.8340, grad_fn=<NegBackward0>) tensor(11450.8340, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11450.8330078125
tensor(11450.8340, grad_fn=<NegBackward0>) tensor(11450.8330, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11450.83203125
tensor(11450.8330, grad_fn=<NegBackward0>) tensor(11450.8320, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11450.8349609375
tensor(11450.8320, grad_fn=<NegBackward0>) tensor(11450.8350, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11450.830078125
tensor(11450.8320, grad_fn=<NegBackward0>) tensor(11450.8301, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11450.8505859375
tensor(11450.8301, grad_fn=<NegBackward0>) tensor(11450.8506, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11450.83203125
tensor(11450.8301, grad_fn=<NegBackward0>) tensor(11450.8320, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11450.8173828125
tensor(11450.8301, grad_fn=<NegBackward0>) tensor(11450.8174, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11450.81640625
tensor(11450.8174, grad_fn=<NegBackward0>) tensor(11450.8164, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11450.81640625
tensor(11450.8164, grad_fn=<NegBackward0>) tensor(11450.8164, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11450.81640625
tensor(11450.8164, grad_fn=<NegBackward0>) tensor(11450.8164, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11450.814453125
tensor(11450.8164, grad_fn=<NegBackward0>) tensor(11450.8145, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11450.814453125
tensor(11450.8145, grad_fn=<NegBackward0>) tensor(11450.8145, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11450.8134765625
tensor(11450.8145, grad_fn=<NegBackward0>) tensor(11450.8135, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11450.8154296875
tensor(11450.8135, grad_fn=<NegBackward0>) tensor(11450.8154, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11450.9267578125
tensor(11450.8135, grad_fn=<NegBackward0>) tensor(11450.9268, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11450.8173828125
tensor(11450.8135, grad_fn=<NegBackward0>) tensor(11450.8174, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11450.814453125
tensor(11450.8135, grad_fn=<NegBackward0>) tensor(11450.8145, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11450.822265625
tensor(11450.8135, grad_fn=<NegBackward0>) tensor(11450.8223, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.6735, 0.3265],
        [0.2696, 0.7304]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9840, 0.0160], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1919, 0.0989],
         [0.7310, 0.3846]],

        [[0.6209, 0.1016],
         [0.7106, 0.6734]],

        [[0.5729, 0.1016],
         [0.6414, 0.6157]],

        [[0.5074, 0.0978],
         [0.7033, 0.7179]],

        [[0.6898, 0.1006],
         [0.5011, 0.6639]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6716375812828888
Average Adjusted Rand Index: 0.7910174896778944
[0.9919998213107827, 0.6716375812828888] [0.9919996552039955, 0.7910174896778944] [11305.6875, 11450.822265625]
-------------------------------------
This iteration is 81
True Objective function: Loss = -11606.487992668353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24497.7421875
inf tensor(24497.7422, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12115.2158203125
tensor(24497.7422, grad_fn=<NegBackward0>) tensor(12115.2158, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11662.6953125
tensor(12115.2158, grad_fn=<NegBackward0>) tensor(11662.6953, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11637.083984375
tensor(11662.6953, grad_fn=<NegBackward0>) tensor(11637.0840, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11631.7353515625
tensor(11637.0840, grad_fn=<NegBackward0>) tensor(11631.7354, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11630.7529296875
tensor(11631.7354, grad_fn=<NegBackward0>) tensor(11630.7529, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11616.1279296875
tensor(11630.7529, grad_fn=<NegBackward0>) tensor(11616.1279, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11616.068359375
tensor(11616.1279, grad_fn=<NegBackward0>) tensor(11616.0684, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11616.03515625
tensor(11616.0684, grad_fn=<NegBackward0>) tensor(11616.0352, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11616.01171875
tensor(11616.0352, grad_fn=<NegBackward0>) tensor(11616.0117, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11615.994140625
tensor(11616.0117, grad_fn=<NegBackward0>) tensor(11615.9941, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11615.9794921875
tensor(11615.9941, grad_fn=<NegBackward0>) tensor(11615.9795, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11615.9697265625
tensor(11615.9795, grad_fn=<NegBackward0>) tensor(11615.9697, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11615.9609375
tensor(11615.9697, grad_fn=<NegBackward0>) tensor(11615.9609, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11615.9541015625
tensor(11615.9609, grad_fn=<NegBackward0>) tensor(11615.9541, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11615.9482421875
tensor(11615.9541, grad_fn=<NegBackward0>) tensor(11615.9482, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11615.94140625
tensor(11615.9482, grad_fn=<NegBackward0>) tensor(11615.9414, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11615.9306640625
tensor(11615.9414, grad_fn=<NegBackward0>) tensor(11615.9307, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11615.9189453125
tensor(11615.9307, grad_fn=<NegBackward0>) tensor(11615.9189, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11615.9169921875
tensor(11615.9189, grad_fn=<NegBackward0>) tensor(11615.9170, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11615.9140625
tensor(11615.9170, grad_fn=<NegBackward0>) tensor(11615.9141, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11615.9111328125
tensor(11615.9141, grad_fn=<NegBackward0>) tensor(11615.9111, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11615.9091796875
tensor(11615.9111, grad_fn=<NegBackward0>) tensor(11615.9092, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11615.9072265625
tensor(11615.9092, grad_fn=<NegBackward0>) tensor(11615.9072, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11615.90625
tensor(11615.9072, grad_fn=<NegBackward0>) tensor(11615.9062, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11615.904296875
tensor(11615.9062, grad_fn=<NegBackward0>) tensor(11615.9043, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11615.90234375
tensor(11615.9043, grad_fn=<NegBackward0>) tensor(11615.9023, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11615.9013671875
tensor(11615.9023, grad_fn=<NegBackward0>) tensor(11615.9014, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11615.9013671875
tensor(11615.9014, grad_fn=<NegBackward0>) tensor(11615.9014, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11615.900390625
tensor(11615.9014, grad_fn=<NegBackward0>) tensor(11615.9004, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11615.8994140625
tensor(11615.9004, grad_fn=<NegBackward0>) tensor(11615.8994, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11615.8994140625
tensor(11615.8994, grad_fn=<NegBackward0>) tensor(11615.8994, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11615.8974609375
tensor(11615.8994, grad_fn=<NegBackward0>) tensor(11615.8975, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11615.8974609375
tensor(11615.8975, grad_fn=<NegBackward0>) tensor(11615.8975, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11615.8955078125
tensor(11615.8975, grad_fn=<NegBackward0>) tensor(11615.8955, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11615.9052734375
tensor(11615.8955, grad_fn=<NegBackward0>) tensor(11615.9053, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11615.8955078125
tensor(11615.8955, grad_fn=<NegBackward0>) tensor(11615.8955, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11615.8955078125
tensor(11615.8955, grad_fn=<NegBackward0>) tensor(11615.8955, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11615.89453125
tensor(11615.8955, grad_fn=<NegBackward0>) tensor(11615.8945, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11615.8935546875
tensor(11615.8945, grad_fn=<NegBackward0>) tensor(11615.8936, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11615.89453125
tensor(11615.8936, grad_fn=<NegBackward0>) tensor(11615.8945, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11615.892578125
tensor(11615.8936, grad_fn=<NegBackward0>) tensor(11615.8926, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11615.892578125
tensor(11615.8926, grad_fn=<NegBackward0>) tensor(11615.8926, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11615.892578125
tensor(11615.8926, grad_fn=<NegBackward0>) tensor(11615.8926, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11615.8916015625
tensor(11615.8926, grad_fn=<NegBackward0>) tensor(11615.8916, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11615.900390625
tensor(11615.8916, grad_fn=<NegBackward0>) tensor(11615.9004, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11615.892578125
tensor(11615.8916, grad_fn=<NegBackward0>) tensor(11615.8926, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11615.890625
tensor(11615.8916, grad_fn=<NegBackward0>) tensor(11615.8906, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11615.8916015625
tensor(11615.8906, grad_fn=<NegBackward0>) tensor(11615.8916, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11615.8896484375
tensor(11615.8906, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11615.890625
tensor(11615.8896, grad_fn=<NegBackward0>) tensor(11615.8906, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11615.8896484375
tensor(11615.8896, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11615.8896484375
tensor(11615.8896, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11615.890625
tensor(11615.8896, grad_fn=<NegBackward0>) tensor(11615.8906, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11615.888671875
tensor(11615.8896, grad_fn=<NegBackward0>) tensor(11615.8887, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11615.8896484375
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11615.8896484375
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11615.890625
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8906, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11615.888671875
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8887, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11615.892578125
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8926, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11615.8896484375
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11615.8896484375
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11615.888671875
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8887, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11615.888671875
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8887, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11615.888671875
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8887, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11615.8876953125
tensor(11615.8887, grad_fn=<NegBackward0>) tensor(11615.8877, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11615.8896484375
tensor(11615.8877, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11615.890625
tensor(11615.8877, grad_fn=<NegBackward0>) tensor(11615.8906, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11615.8896484375
tensor(11615.8877, grad_fn=<NegBackward0>) tensor(11615.8896, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11615.888671875
tensor(11615.8877, grad_fn=<NegBackward0>) tensor(11615.8887, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11615.888671875
tensor(11615.8877, grad_fn=<NegBackward0>) tensor(11615.8887, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.7018, 0.2982],
        [0.2358, 0.7642]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5479, 0.4521], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2004, 0.0996],
         [0.6374, 0.3991]],

        [[0.7265, 0.0954],
         [0.7007, 0.6913]],

        [[0.5595, 0.0998],
         [0.6040, 0.6164]],

        [[0.6121, 0.0949],
         [0.7003, 0.5524]],

        [[0.5330, 0.1055],
         [0.6327, 0.6996]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320488565437
Average Adjusted Rand Index: 0.9841601267189862
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23132.16796875
inf tensor(23132.1680, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11933.01953125
tensor(23132.1680, grad_fn=<NegBackward0>) tensor(11933.0195, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11868.720703125
tensor(11933.0195, grad_fn=<NegBackward0>) tensor(11868.7207, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11868.0625
tensor(11868.7207, grad_fn=<NegBackward0>) tensor(11868.0625, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11866.9365234375
tensor(11868.0625, grad_fn=<NegBackward0>) tensor(11866.9365, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11864.46484375
tensor(11866.9365, grad_fn=<NegBackward0>) tensor(11864.4648, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11864.3720703125
tensor(11864.4648, grad_fn=<NegBackward0>) tensor(11864.3721, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11864.318359375
tensor(11864.3721, grad_fn=<NegBackward0>) tensor(11864.3184, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11864.142578125
tensor(11864.3184, grad_fn=<NegBackward0>) tensor(11864.1426, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11864.0791015625
tensor(11864.1426, grad_fn=<NegBackward0>) tensor(11864.0791, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11864.0625
tensor(11864.0791, grad_fn=<NegBackward0>) tensor(11864.0625, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11864.0517578125
tensor(11864.0625, grad_fn=<NegBackward0>) tensor(11864.0518, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11864.033203125
tensor(11864.0518, grad_fn=<NegBackward0>) tensor(11864.0332, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11863.2392578125
tensor(11864.0332, grad_fn=<NegBackward0>) tensor(11863.2393, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11863.21875
tensor(11863.2393, grad_fn=<NegBackward0>) tensor(11863.2188, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11863.1455078125
tensor(11863.2188, grad_fn=<NegBackward0>) tensor(11863.1455, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11863.140625
tensor(11863.1455, grad_fn=<NegBackward0>) tensor(11863.1406, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11863.1357421875
tensor(11863.1406, grad_fn=<NegBackward0>) tensor(11863.1357, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11863.1328125
tensor(11863.1357, grad_fn=<NegBackward0>) tensor(11863.1328, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11863.1298828125
tensor(11863.1328, grad_fn=<NegBackward0>) tensor(11863.1299, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11863.1279296875
tensor(11863.1299, grad_fn=<NegBackward0>) tensor(11863.1279, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11863.1240234375
tensor(11863.1279, grad_fn=<NegBackward0>) tensor(11863.1240, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11863.12109375
tensor(11863.1240, grad_fn=<NegBackward0>) tensor(11863.1211, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11863.1201171875
tensor(11863.1211, grad_fn=<NegBackward0>) tensor(11863.1201, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11863.119140625
tensor(11863.1201, grad_fn=<NegBackward0>) tensor(11863.1191, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11863.1181640625
tensor(11863.1191, grad_fn=<NegBackward0>) tensor(11863.1182, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11863.1162109375
tensor(11863.1182, grad_fn=<NegBackward0>) tensor(11863.1162, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11863.115234375
tensor(11863.1162, grad_fn=<NegBackward0>) tensor(11863.1152, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11863.115234375
tensor(11863.1152, grad_fn=<NegBackward0>) tensor(11863.1152, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11863.1123046875
tensor(11863.1152, grad_fn=<NegBackward0>) tensor(11863.1123, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11863.11328125
tensor(11863.1123, grad_fn=<NegBackward0>) tensor(11863.1133, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11863.1123046875
tensor(11863.1123, grad_fn=<NegBackward0>) tensor(11863.1123, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11863.111328125
tensor(11863.1123, grad_fn=<NegBackward0>) tensor(11863.1113, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11863.111328125
tensor(11863.1113, grad_fn=<NegBackward0>) tensor(11863.1113, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11863.1103515625
tensor(11863.1113, grad_fn=<NegBackward0>) tensor(11863.1104, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11863.1103515625
tensor(11863.1104, grad_fn=<NegBackward0>) tensor(11863.1104, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11863.109375
tensor(11863.1104, grad_fn=<NegBackward0>) tensor(11863.1094, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11863.109375
tensor(11863.1094, grad_fn=<NegBackward0>) tensor(11863.1094, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11863.1083984375
tensor(11863.1094, grad_fn=<NegBackward0>) tensor(11863.1084, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11863.1083984375
tensor(11863.1084, grad_fn=<NegBackward0>) tensor(11863.1084, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11863.1064453125
tensor(11863.1084, grad_fn=<NegBackward0>) tensor(11863.1064, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11863.107421875
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1074, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11863.1064453125
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1064, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11863.109375
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1094, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11863.107421875
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1074, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11863.107421875
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1074, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11863.1064453125
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1064, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11863.1064453125
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1064, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11863.10546875
tensor(11863.1064, grad_fn=<NegBackward0>) tensor(11863.1055, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11863.109375
tensor(11863.1055, grad_fn=<NegBackward0>) tensor(11863.1094, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11863.1044921875
tensor(11863.1055, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11863.10546875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1055, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11863.107421875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1074, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11863.1044921875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11863.107421875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1074, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11863.10546875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1055, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11863.10546875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1055, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11863.10546875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1055, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -11863.1044921875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11863.1044921875
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11863.1064453125
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1064, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11863.103515625
tensor(11863.1045, grad_fn=<NegBackward0>) tensor(11863.1035, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11863.1044921875
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11863.103515625
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1035, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11863.103515625
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1035, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11863.109375
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1094, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11863.103515625
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1035, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11863.1044921875
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11863.1123046875
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1123, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11863.1044921875
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11863.1044921875
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11863.1025390625
tensor(11863.1035, grad_fn=<NegBackward0>) tensor(11863.1025, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11863.1044921875
tensor(11863.1025, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11863.1044921875
tensor(11863.1025, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11863.1044921875
tensor(11863.1025, grad_fn=<NegBackward0>) tensor(11863.1045, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11863.103515625
tensor(11863.1025, grad_fn=<NegBackward0>) tensor(11863.1035, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11863.103515625
tensor(11863.1025, grad_fn=<NegBackward0>) tensor(11863.1035, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.4493, 0.5507],
        [0.5783, 0.4217]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2442, 0.0998],
         [0.5167, 0.3712]],

        [[0.7225, 0.0949],
         [0.6965, 0.5382]],

        [[0.5068, 0.0899],
         [0.6789, 0.7206]],

        [[0.6646, 0.0946],
         [0.7047, 0.7174]],

        [[0.5524, 0.1055],
         [0.7059, 0.5049]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 29
Adjusted Rand Index: 0.16808080808080808
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 17
Adjusted Rand Index: 0.4304144935492726
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.14575487475996435
Average Adjusted Rand Index: 0.7196990603260162
[0.9840320488565437, 0.14575487475996435] [0.9841601267189862, 0.7196990603260162] [11615.888671875, 11863.103515625]
-------------------------------------
This iteration is 82
True Objective function: Loss = -11638.22047611485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20731.478515625
inf tensor(20731.4785, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12356.349609375
tensor(20731.4785, grad_fn=<NegBackward0>) tensor(12356.3496, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11859.2568359375
tensor(12356.3496, grad_fn=<NegBackward0>) tensor(11859.2568, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11670.3232421875
tensor(11859.2568, grad_fn=<NegBackward0>) tensor(11670.3232, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11644.443359375
tensor(11670.3232, grad_fn=<NegBackward0>) tensor(11644.4434, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11641.4296875
tensor(11644.4434, grad_fn=<NegBackward0>) tensor(11641.4297, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11639.712890625
tensor(11641.4297, grad_fn=<NegBackward0>) tensor(11639.7129, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11639.5537109375
tensor(11639.7129, grad_fn=<NegBackward0>) tensor(11639.5537, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11639.4697265625
tensor(11639.5537, grad_fn=<NegBackward0>) tensor(11639.4697, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11624.05078125
tensor(11639.4697, grad_fn=<NegBackward0>) tensor(11624.0508, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11624.0048828125
tensor(11624.0508, grad_fn=<NegBackward0>) tensor(11624.0049, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11623.966796875
tensor(11624.0049, grad_fn=<NegBackward0>) tensor(11623.9668, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11623.4482421875
tensor(11623.9668, grad_fn=<NegBackward0>) tensor(11623.4482, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11622.6650390625
tensor(11623.4482, grad_fn=<NegBackward0>) tensor(11622.6650, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11622.650390625
tensor(11622.6650, grad_fn=<NegBackward0>) tensor(11622.6504, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11622.63671875
tensor(11622.6504, grad_fn=<NegBackward0>) tensor(11622.6367, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11622.626953125
tensor(11622.6367, grad_fn=<NegBackward0>) tensor(11622.6270, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11622.6181640625
tensor(11622.6270, grad_fn=<NegBackward0>) tensor(11622.6182, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11622.611328125
tensor(11622.6182, grad_fn=<NegBackward0>) tensor(11622.6113, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11622.6044921875
tensor(11622.6113, grad_fn=<NegBackward0>) tensor(11622.6045, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11622.6005859375
tensor(11622.6045, grad_fn=<NegBackward0>) tensor(11622.6006, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11622.5947265625
tensor(11622.6006, grad_fn=<NegBackward0>) tensor(11622.5947, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11622.58984375
tensor(11622.5947, grad_fn=<NegBackward0>) tensor(11622.5898, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11622.5859375
tensor(11622.5898, grad_fn=<NegBackward0>) tensor(11622.5859, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11622.583984375
tensor(11622.5859, grad_fn=<NegBackward0>) tensor(11622.5840, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11622.580078125
tensor(11622.5840, grad_fn=<NegBackward0>) tensor(11622.5801, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11622.578125
tensor(11622.5801, grad_fn=<NegBackward0>) tensor(11622.5781, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11622.576171875
tensor(11622.5781, grad_fn=<NegBackward0>) tensor(11622.5762, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11622.5732421875
tensor(11622.5762, grad_fn=<NegBackward0>) tensor(11622.5732, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11622.572265625
tensor(11622.5732, grad_fn=<NegBackward0>) tensor(11622.5723, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11622.5693359375
tensor(11622.5723, grad_fn=<NegBackward0>) tensor(11622.5693, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11622.5693359375
tensor(11622.5693, grad_fn=<NegBackward0>) tensor(11622.5693, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11622.568359375
tensor(11622.5693, grad_fn=<NegBackward0>) tensor(11622.5684, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11622.56640625
tensor(11622.5684, grad_fn=<NegBackward0>) tensor(11622.5664, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11622.5654296875
tensor(11622.5664, grad_fn=<NegBackward0>) tensor(11622.5654, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11622.5634765625
tensor(11622.5654, grad_fn=<NegBackward0>) tensor(11622.5635, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11622.5615234375
tensor(11622.5635, grad_fn=<NegBackward0>) tensor(11622.5615, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11622.560546875
tensor(11622.5615, grad_fn=<NegBackward0>) tensor(11622.5605, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11622.5595703125
tensor(11622.5605, grad_fn=<NegBackward0>) tensor(11622.5596, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11622.5595703125
tensor(11622.5596, grad_fn=<NegBackward0>) tensor(11622.5596, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11622.55859375
tensor(11622.5596, grad_fn=<NegBackward0>) tensor(11622.5586, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11622.5576171875
tensor(11622.5586, grad_fn=<NegBackward0>) tensor(11622.5576, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11622.5576171875
tensor(11622.5576, grad_fn=<NegBackward0>) tensor(11622.5576, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11622.5556640625
tensor(11622.5576, grad_fn=<NegBackward0>) tensor(11622.5557, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11622.5546875
tensor(11622.5557, grad_fn=<NegBackward0>) tensor(11622.5547, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11622.5546875
tensor(11622.5547, grad_fn=<NegBackward0>) tensor(11622.5547, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11622.5556640625
tensor(11622.5547, grad_fn=<NegBackward0>) tensor(11622.5557, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11622.5546875
tensor(11622.5547, grad_fn=<NegBackward0>) tensor(11622.5547, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11622.5546875
tensor(11622.5547, grad_fn=<NegBackward0>) tensor(11622.5547, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11622.552734375
tensor(11622.5547, grad_fn=<NegBackward0>) tensor(11622.5527, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11622.5556640625
tensor(11622.5527, grad_fn=<NegBackward0>) tensor(11622.5557, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11622.5634765625
tensor(11622.5527, grad_fn=<NegBackward0>) tensor(11622.5635, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11622.552734375
tensor(11622.5527, grad_fn=<NegBackward0>) tensor(11622.5527, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11622.552734375
tensor(11622.5527, grad_fn=<NegBackward0>) tensor(11622.5527, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11622.5517578125
tensor(11622.5527, grad_fn=<NegBackward0>) tensor(11622.5518, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11622.5546875
tensor(11622.5518, grad_fn=<NegBackward0>) tensor(11622.5547, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11622.5517578125
tensor(11622.5518, grad_fn=<NegBackward0>) tensor(11622.5518, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11622.5517578125
tensor(11622.5518, grad_fn=<NegBackward0>) tensor(11622.5518, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11622.5556640625
tensor(11622.5518, grad_fn=<NegBackward0>) tensor(11622.5557, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11622.55078125
tensor(11622.5518, grad_fn=<NegBackward0>) tensor(11622.5508, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11622.55078125
tensor(11622.5508, grad_fn=<NegBackward0>) tensor(11622.5508, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11622.5537109375
tensor(11622.5508, grad_fn=<NegBackward0>) tensor(11622.5537, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11622.5517578125
tensor(11622.5508, grad_fn=<NegBackward0>) tensor(11622.5518, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11622.5498046875
tensor(11622.5508, grad_fn=<NegBackward0>) tensor(11622.5498, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11622.556640625
tensor(11622.5498, grad_fn=<NegBackward0>) tensor(11622.5566, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11622.548828125
tensor(11622.5498, grad_fn=<NegBackward0>) tensor(11622.5488, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11622.5478515625
tensor(11622.5488, grad_fn=<NegBackward0>) tensor(11622.5479, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11622.5478515625
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5479, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11622.5478515625
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5479, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11622.548828125
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5488, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11622.5537109375
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5537, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11622.5478515625
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5479, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11622.5478515625
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5479, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11622.548828125
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5488, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11622.546875
tensor(11622.5479, grad_fn=<NegBackward0>) tensor(11622.5469, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11622.546875
tensor(11622.5469, grad_fn=<NegBackward0>) tensor(11622.5469, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11622.5458984375
tensor(11622.5469, grad_fn=<NegBackward0>) tensor(11622.5459, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11622.5732421875
tensor(11622.5459, grad_fn=<NegBackward0>) tensor(11622.5732, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11622.546875
tensor(11622.5459, grad_fn=<NegBackward0>) tensor(11622.5469, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11622.5537109375
tensor(11622.5459, grad_fn=<NegBackward0>) tensor(11622.5537, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11622.5458984375
tensor(11622.5459, grad_fn=<NegBackward0>) tensor(11622.5459, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11622.5458984375
tensor(11622.5459, grad_fn=<NegBackward0>) tensor(11622.5459, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11622.5458984375
tensor(11622.5459, grad_fn=<NegBackward0>) tensor(11622.5459, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11622.5380859375
tensor(11622.5459, grad_fn=<NegBackward0>) tensor(11622.5381, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11622.546875
tensor(11622.5381, grad_fn=<NegBackward0>) tensor(11622.5469, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11622.537109375
tensor(11622.5381, grad_fn=<NegBackward0>) tensor(11622.5371, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11622.537109375
tensor(11622.5371, grad_fn=<NegBackward0>) tensor(11622.5371, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11622.5380859375
tensor(11622.5371, grad_fn=<NegBackward0>) tensor(11622.5381, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11622.5380859375
tensor(11622.5371, grad_fn=<NegBackward0>) tensor(11622.5381, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11622.560546875
tensor(11622.5371, grad_fn=<NegBackward0>) tensor(11622.5605, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11622.5439453125
tensor(11622.5371, grad_fn=<NegBackward0>) tensor(11622.5439, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11622.5380859375
tensor(11622.5371, grad_fn=<NegBackward0>) tensor(11622.5381, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.7411, 0.2589],
        [0.2437, 0.7563]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5273, 0.4727], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3978, 0.1093],
         [0.6147, 0.2043]],

        [[0.5576, 0.1054],
         [0.7240, 0.7064]],

        [[0.6559, 0.0894],
         [0.6370, 0.6833]],

        [[0.5848, 0.0984],
         [0.7048, 0.7121]],

        [[0.6071, 0.1098],
         [0.6288, 0.7001]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
Global Adjusted Rand Index: 0.9681922876370956
Average Adjusted Rand Index: 0.9681601078821229
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22251.40625
inf tensor(22251.4062, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12366.9765625
tensor(22251.4062, grad_fn=<NegBackward0>) tensor(12366.9766, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12277.14453125
tensor(12366.9766, grad_fn=<NegBackward0>) tensor(12277.1445, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11874.541015625
tensor(12277.1445, grad_fn=<NegBackward0>) tensor(11874.5410, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11798.83984375
tensor(11874.5410, grad_fn=<NegBackward0>) tensor(11798.8398, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11796.5986328125
tensor(11798.8398, grad_fn=<NegBackward0>) tensor(11796.5986, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11794.39453125
tensor(11796.5986, grad_fn=<NegBackward0>) tensor(11794.3945, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11793.9521484375
tensor(11794.3945, grad_fn=<NegBackward0>) tensor(11793.9521, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11793.724609375
tensor(11793.9521, grad_fn=<NegBackward0>) tensor(11793.7246, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11793.57421875
tensor(11793.7246, grad_fn=<NegBackward0>) tensor(11793.5742, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11793.4404296875
tensor(11793.5742, grad_fn=<NegBackward0>) tensor(11793.4404, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11793.283203125
tensor(11793.4404, grad_fn=<NegBackward0>) tensor(11793.2832, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11793.205078125
tensor(11793.2832, grad_fn=<NegBackward0>) tensor(11793.2051, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11793.107421875
tensor(11793.2051, grad_fn=<NegBackward0>) tensor(11793.1074, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11793.0673828125
tensor(11793.1074, grad_fn=<NegBackward0>) tensor(11793.0674, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11793.0361328125
tensor(11793.0674, grad_fn=<NegBackward0>) tensor(11793.0361, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11793.0087890625
tensor(11793.0361, grad_fn=<NegBackward0>) tensor(11793.0088, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11792.984375
tensor(11793.0088, grad_fn=<NegBackward0>) tensor(11792.9844, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11792.95703125
tensor(11792.9844, grad_fn=<NegBackward0>) tensor(11792.9570, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11792.1904296875
tensor(11792.9570, grad_fn=<NegBackward0>) tensor(11792.1904, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11790.00390625
tensor(11792.1904, grad_fn=<NegBackward0>) tensor(11790.0039, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11789.8115234375
tensor(11790.0039, grad_fn=<NegBackward0>) tensor(11789.8115, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11789.2255859375
tensor(11789.8115, grad_fn=<NegBackward0>) tensor(11789.2256, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11789.0205078125
tensor(11789.2256, grad_fn=<NegBackward0>) tensor(11789.0205, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11789.009765625
tensor(11789.0205, grad_fn=<NegBackward0>) tensor(11789.0098, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11789.0
tensor(11789.0098, grad_fn=<NegBackward0>) tensor(11789., grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11788.994140625
tensor(11789., grad_fn=<NegBackward0>) tensor(11788.9941, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11788.9873046875
tensor(11788.9941, grad_fn=<NegBackward0>) tensor(11788.9873, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11788.9814453125
tensor(11788.9873, grad_fn=<NegBackward0>) tensor(11788.9814, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11788.9755859375
tensor(11788.9814, grad_fn=<NegBackward0>) tensor(11788.9756, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11788.9716796875
tensor(11788.9756, grad_fn=<NegBackward0>) tensor(11788.9717, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11788.96484375
tensor(11788.9717, grad_fn=<NegBackward0>) tensor(11788.9648, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11788.9619140625
tensor(11788.9648, grad_fn=<NegBackward0>) tensor(11788.9619, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11788.958984375
tensor(11788.9619, grad_fn=<NegBackward0>) tensor(11788.9590, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11788.962890625
tensor(11788.9590, grad_fn=<NegBackward0>) tensor(11788.9629, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11788.953125
tensor(11788.9590, grad_fn=<NegBackward0>) tensor(11788.9531, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11788.9501953125
tensor(11788.9531, grad_fn=<NegBackward0>) tensor(11788.9502, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11788.9599609375
tensor(11788.9502, grad_fn=<NegBackward0>) tensor(11788.9600, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11788.9365234375
tensor(11788.9502, grad_fn=<NegBackward0>) tensor(11788.9365, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11788.9140625
tensor(11788.9365, grad_fn=<NegBackward0>) tensor(11788.9141, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11788.912109375
tensor(11788.9141, grad_fn=<NegBackward0>) tensor(11788.9121, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11788.9091796875
tensor(11788.9121, grad_fn=<NegBackward0>) tensor(11788.9092, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11788.9267578125
tensor(11788.9092, grad_fn=<NegBackward0>) tensor(11788.9268, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11788.90625
tensor(11788.9092, grad_fn=<NegBackward0>) tensor(11788.9062, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11788.908203125
tensor(11788.9062, grad_fn=<NegBackward0>) tensor(11788.9082, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11788.904296875
tensor(11788.9062, grad_fn=<NegBackward0>) tensor(11788.9043, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11788.9072265625
tensor(11788.9043, grad_fn=<NegBackward0>) tensor(11788.9072, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11788.90234375
tensor(11788.9043, grad_fn=<NegBackward0>) tensor(11788.9023, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11788.9091796875
tensor(11788.9023, grad_fn=<NegBackward0>) tensor(11788.9092, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11788.9013671875
tensor(11788.9023, grad_fn=<NegBackward0>) tensor(11788.9014, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11788.9013671875
tensor(11788.9014, grad_fn=<NegBackward0>) tensor(11788.9014, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11788.900390625
tensor(11788.9014, grad_fn=<NegBackward0>) tensor(11788.9004, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11788.89453125
tensor(11788.9004, grad_fn=<NegBackward0>) tensor(11788.8945, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11788.810546875
tensor(11788.8945, grad_fn=<NegBackward0>) tensor(11788.8105, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11788.7783203125
tensor(11788.8105, grad_fn=<NegBackward0>) tensor(11788.7783, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11788.7734375
tensor(11788.7783, grad_fn=<NegBackward0>) tensor(11788.7734, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11788.765625
tensor(11788.7734, grad_fn=<NegBackward0>) tensor(11788.7656, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11788.763671875
tensor(11788.7656, grad_fn=<NegBackward0>) tensor(11788.7637, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11788.7705078125
tensor(11788.7637, grad_fn=<NegBackward0>) tensor(11788.7705, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11788.7626953125
tensor(11788.7637, grad_fn=<NegBackward0>) tensor(11788.7627, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11788.7626953125
tensor(11788.7627, grad_fn=<NegBackward0>) tensor(11788.7627, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11788.7646484375
tensor(11788.7627, grad_fn=<NegBackward0>) tensor(11788.7646, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11788.759765625
tensor(11788.7627, grad_fn=<NegBackward0>) tensor(11788.7598, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11788.75390625
tensor(11788.7598, grad_fn=<NegBackward0>) tensor(11788.7539, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11788.75
tensor(11788.7539, grad_fn=<NegBackward0>) tensor(11788.7500, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11788.7509765625
tensor(11788.7500, grad_fn=<NegBackward0>) tensor(11788.7510, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11788.75
tensor(11788.7500, grad_fn=<NegBackward0>) tensor(11788.7500, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11788.7490234375
tensor(11788.7500, grad_fn=<NegBackward0>) tensor(11788.7490, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11788.751953125
tensor(11788.7490, grad_fn=<NegBackward0>) tensor(11788.7520, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11788.7490234375
tensor(11788.7490, grad_fn=<NegBackward0>) tensor(11788.7490, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11788.7470703125
tensor(11788.7490, grad_fn=<NegBackward0>) tensor(11788.7471, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11788.7470703125
tensor(11788.7471, grad_fn=<NegBackward0>) tensor(11788.7471, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11788.751953125
tensor(11788.7471, grad_fn=<NegBackward0>) tensor(11788.7520, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11788.748046875
tensor(11788.7471, grad_fn=<NegBackward0>) tensor(11788.7480, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11788.74609375
tensor(11788.7471, grad_fn=<NegBackward0>) tensor(11788.7461, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11788.7509765625
tensor(11788.7461, grad_fn=<NegBackward0>) tensor(11788.7510, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11788.74609375
tensor(11788.7461, grad_fn=<NegBackward0>) tensor(11788.7461, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11788.7548828125
tensor(11788.7461, grad_fn=<NegBackward0>) tensor(11788.7549, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11788.74609375
tensor(11788.7461, grad_fn=<NegBackward0>) tensor(11788.7461, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11788.7451171875
tensor(11788.7461, grad_fn=<NegBackward0>) tensor(11788.7451, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11788.744140625
tensor(11788.7451, grad_fn=<NegBackward0>) tensor(11788.7441, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11788.7744140625
tensor(11788.7441, grad_fn=<NegBackward0>) tensor(11788.7744, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11788.7451171875
tensor(11788.7441, grad_fn=<NegBackward0>) tensor(11788.7451, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11788.748046875
tensor(11788.7441, grad_fn=<NegBackward0>) tensor(11788.7480, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11788.744140625
tensor(11788.7441, grad_fn=<NegBackward0>) tensor(11788.7441, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11788.7431640625
tensor(11788.7441, grad_fn=<NegBackward0>) tensor(11788.7432, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11788.7529296875
tensor(11788.7432, grad_fn=<NegBackward0>) tensor(11788.7529, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11788.744140625
tensor(11788.7432, grad_fn=<NegBackward0>) tensor(11788.7441, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11788.744140625
tensor(11788.7432, grad_fn=<NegBackward0>) tensor(11788.7441, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11788.7431640625
tensor(11788.7432, grad_fn=<NegBackward0>) tensor(11788.7432, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11788.7431640625
tensor(11788.7432, grad_fn=<NegBackward0>) tensor(11788.7432, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11788.751953125
tensor(11788.7432, grad_fn=<NegBackward0>) tensor(11788.7520, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11788.7421875
tensor(11788.7432, grad_fn=<NegBackward0>) tensor(11788.7422, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11788.7431640625
tensor(11788.7422, grad_fn=<NegBackward0>) tensor(11788.7432, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11788.7421875
tensor(11788.7422, grad_fn=<NegBackward0>) tensor(11788.7422, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11788.7421875
tensor(11788.7422, grad_fn=<NegBackward0>) tensor(11788.7422, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11788.83984375
tensor(11788.7422, grad_fn=<NegBackward0>) tensor(11788.8398, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11788.7431640625
tensor(11788.7422, grad_fn=<NegBackward0>) tensor(11788.7432, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11788.74609375
tensor(11788.7422, grad_fn=<NegBackward0>) tensor(11788.7461, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11788.7470703125
tensor(11788.7422, grad_fn=<NegBackward0>) tensor(11788.7471, grad_fn=<NegBackward0>)
4
pi: tensor([[0.6212, 0.3788],
        [0.5285, 0.4715]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4738, 0.5262], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2149, 0.1096],
         [0.5847, 0.3971]],

        [[0.6031, 0.1054],
         [0.5410, 0.6723]],

        [[0.7253, 0.0895],
         [0.7110, 0.7290]],

        [[0.5216, 0.1024],
         [0.6528, 0.6843]],

        [[0.5634, 0.1099],
         [0.6975, 0.6140]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 31
Adjusted Rand Index: 0.1381684880788948
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
Global Adjusted Rand Index: 0.5002762332561931
Average Adjusted Rand Index: 0.7957938054979019
[0.9681922876370956, 0.5002762332561931] [0.9681601078821229, 0.7957938054979019] [11622.5380859375, 11788.744140625]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11542.978185995691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22431.5
inf tensor(22431.5000, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12367.6318359375
tensor(22431.5000, grad_fn=<NegBackward0>) tensor(12367.6318, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12277.98828125
tensor(12367.6318, grad_fn=<NegBackward0>) tensor(12277.9883, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11810.2822265625
tensor(12277.9883, grad_fn=<NegBackward0>) tensor(11810.2822, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11655.88671875
tensor(11810.2822, grad_fn=<NegBackward0>) tensor(11655.8867, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11599.12109375
tensor(11655.8867, grad_fn=<NegBackward0>) tensor(11599.1211, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11598.0244140625
tensor(11599.1211, grad_fn=<NegBackward0>) tensor(11598.0244, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11560.6767578125
tensor(11598.0244, grad_fn=<NegBackward0>) tensor(11560.6768, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11545.86328125
tensor(11560.6768, grad_fn=<NegBackward0>) tensor(11545.8633, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11545.7080078125
tensor(11545.8633, grad_fn=<NegBackward0>) tensor(11545.7080, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11535.705078125
tensor(11545.7080, grad_fn=<NegBackward0>) tensor(11535.7051, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11535.630859375
tensor(11535.7051, grad_fn=<NegBackward0>) tensor(11535.6309, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11535.580078125
tensor(11535.6309, grad_fn=<NegBackward0>) tensor(11535.5801, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11535.54296875
tensor(11535.5801, grad_fn=<NegBackward0>) tensor(11535.5430, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11535.51171875
tensor(11535.5430, grad_fn=<NegBackward0>) tensor(11535.5117, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11535.4873046875
tensor(11535.5117, grad_fn=<NegBackward0>) tensor(11535.4873, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11535.466796875
tensor(11535.4873, grad_fn=<NegBackward0>) tensor(11535.4668, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11535.44921875
tensor(11535.4668, grad_fn=<NegBackward0>) tensor(11535.4492, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11535.4345703125
tensor(11535.4492, grad_fn=<NegBackward0>) tensor(11535.4346, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11535.423828125
tensor(11535.4346, grad_fn=<NegBackward0>) tensor(11535.4238, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11535.4130859375
tensor(11535.4238, grad_fn=<NegBackward0>) tensor(11535.4131, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11535.404296875
tensor(11535.4131, grad_fn=<NegBackward0>) tensor(11535.4043, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11535.40234375
tensor(11535.4043, grad_fn=<NegBackward0>) tensor(11535.4023, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11535.390625
tensor(11535.4023, grad_fn=<NegBackward0>) tensor(11535.3906, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11535.384765625
tensor(11535.3906, grad_fn=<NegBackward0>) tensor(11535.3848, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11535.3779296875
tensor(11535.3848, grad_fn=<NegBackward0>) tensor(11535.3779, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11535.373046875
tensor(11535.3779, grad_fn=<NegBackward0>) tensor(11535.3730, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11535.3681640625
tensor(11535.3730, grad_fn=<NegBackward0>) tensor(11535.3682, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11535.365234375
tensor(11535.3682, grad_fn=<NegBackward0>) tensor(11535.3652, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11535.36328125
tensor(11535.3652, grad_fn=<NegBackward0>) tensor(11535.3633, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11535.3583984375
tensor(11535.3633, grad_fn=<NegBackward0>) tensor(11535.3584, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11535.3701171875
tensor(11535.3584, grad_fn=<NegBackward0>) tensor(11535.3701, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11535.3525390625
tensor(11535.3584, grad_fn=<NegBackward0>) tensor(11535.3525, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11535.3505859375
tensor(11535.3525, grad_fn=<NegBackward0>) tensor(11535.3506, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11535.349609375
tensor(11535.3506, grad_fn=<NegBackward0>) tensor(11535.3496, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11535.34765625
tensor(11535.3496, grad_fn=<NegBackward0>) tensor(11535.3477, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11535.345703125
tensor(11535.3477, grad_fn=<NegBackward0>) tensor(11535.3457, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11535.34375
tensor(11535.3457, grad_fn=<NegBackward0>) tensor(11535.3438, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11535.3408203125
tensor(11535.3438, grad_fn=<NegBackward0>) tensor(11535.3408, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11535.34375
tensor(11535.3408, grad_fn=<NegBackward0>) tensor(11535.3438, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11535.3388671875
tensor(11535.3408, grad_fn=<NegBackward0>) tensor(11535.3389, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11535.3369140625
tensor(11535.3389, grad_fn=<NegBackward0>) tensor(11535.3369, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11535.3359375
tensor(11535.3369, grad_fn=<NegBackward0>) tensor(11535.3359, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11535.3359375
tensor(11535.3359, grad_fn=<NegBackward0>) tensor(11535.3359, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11535.3349609375
tensor(11535.3359, grad_fn=<NegBackward0>) tensor(11535.3350, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11535.3349609375
tensor(11535.3350, grad_fn=<NegBackward0>) tensor(11535.3350, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11535.333984375
tensor(11535.3350, grad_fn=<NegBackward0>) tensor(11535.3340, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11535.3330078125
tensor(11535.3340, grad_fn=<NegBackward0>) tensor(11535.3330, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11535.33203125
tensor(11535.3330, grad_fn=<NegBackward0>) tensor(11535.3320, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11535.333984375
tensor(11535.3320, grad_fn=<NegBackward0>) tensor(11535.3340, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11535.3291015625
tensor(11535.3320, grad_fn=<NegBackward0>) tensor(11535.3291, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11535.3310546875
tensor(11535.3291, grad_fn=<NegBackward0>) tensor(11535.3311, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11535.3291015625
tensor(11535.3291, grad_fn=<NegBackward0>) tensor(11535.3291, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11535.328125
tensor(11535.3291, grad_fn=<NegBackward0>) tensor(11535.3281, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11535.3271484375
tensor(11535.3281, grad_fn=<NegBackward0>) tensor(11535.3271, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11535.328125
tensor(11535.3271, grad_fn=<NegBackward0>) tensor(11535.3281, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11535.326171875
tensor(11535.3271, grad_fn=<NegBackward0>) tensor(11535.3262, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11535.3271484375
tensor(11535.3262, grad_fn=<NegBackward0>) tensor(11535.3271, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11535.3251953125
tensor(11535.3262, grad_fn=<NegBackward0>) tensor(11535.3252, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11535.3251953125
tensor(11535.3252, grad_fn=<NegBackward0>) tensor(11535.3252, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11535.3251953125
tensor(11535.3252, grad_fn=<NegBackward0>) tensor(11535.3252, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11535.3251953125
tensor(11535.3252, grad_fn=<NegBackward0>) tensor(11535.3252, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11535.32421875
tensor(11535.3252, grad_fn=<NegBackward0>) tensor(11535.3242, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11535.3251953125
tensor(11535.3242, grad_fn=<NegBackward0>) tensor(11535.3252, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11535.326171875
tensor(11535.3242, grad_fn=<NegBackward0>) tensor(11535.3262, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11535.326171875
tensor(11535.3242, grad_fn=<NegBackward0>) tensor(11535.3262, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11535.32421875
tensor(11535.3242, grad_fn=<NegBackward0>) tensor(11535.3242, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11535.3232421875
tensor(11535.3242, grad_fn=<NegBackward0>) tensor(11535.3232, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11535.333984375
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3340, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11535.32421875
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3242, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11535.3232421875
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3232, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11535.32421875
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3242, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11535.326171875
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3262, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11535.330078125
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3301, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11535.34765625
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3477, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11535.3271484375
tensor(11535.3232, grad_fn=<NegBackward0>) tensor(11535.3271, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7425, 0.2575],
        [0.2615, 0.7385]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5090, 0.4910], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.0932],
         [0.6674, 0.4022]],

        [[0.5914, 0.0996],
         [0.5749, 0.7215]],

        [[0.5932, 0.0893],
         [0.5761, 0.6991]],

        [[0.7215, 0.1031],
         [0.6185, 0.5122]],

        [[0.6644, 0.1082],
         [0.6399, 0.6910]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9760961002009502
Average Adjusted Rand Index: 0.9759984953877163
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20801.556640625
inf tensor(20801.5566, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12361.890625
tensor(20801.5566, grad_fn=<NegBackward0>) tensor(12361.8906, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12332.3564453125
tensor(12361.8906, grad_fn=<NegBackward0>) tensor(12332.3564, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12007.060546875
tensor(12332.3564, grad_fn=<NegBackward0>) tensor(12007.0605, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11939.431640625
tensor(12007.0605, grad_fn=<NegBackward0>) tensor(11939.4316, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11852.0390625
tensor(11939.4316, grad_fn=<NegBackward0>) tensor(11852.0391, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11832.9501953125
tensor(11852.0391, grad_fn=<NegBackward0>) tensor(11832.9502, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11821.037109375
tensor(11832.9502, grad_fn=<NegBackward0>) tensor(11821.0371, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11820.517578125
tensor(11821.0371, grad_fn=<NegBackward0>) tensor(11820.5176, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11820.43359375
tensor(11820.5176, grad_fn=<NegBackward0>) tensor(11820.4336, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11820.3828125
tensor(11820.4336, grad_fn=<NegBackward0>) tensor(11820.3828, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11820.34765625
tensor(11820.3828, grad_fn=<NegBackward0>) tensor(11820.3477, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11820.314453125
tensor(11820.3477, grad_fn=<NegBackward0>) tensor(11820.3145, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11820.2783203125
tensor(11820.3145, grad_fn=<NegBackward0>) tensor(11820.2783, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11820.26171875
tensor(11820.2783, grad_fn=<NegBackward0>) tensor(11820.2617, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11820.25
tensor(11820.2617, grad_fn=<NegBackward0>) tensor(11820.2500, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11820.23828125
tensor(11820.2500, grad_fn=<NegBackward0>) tensor(11820.2383, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11820.23046875
tensor(11820.2383, grad_fn=<NegBackward0>) tensor(11820.2305, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11820.22265625
tensor(11820.2305, grad_fn=<NegBackward0>) tensor(11820.2227, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11820.2158203125
tensor(11820.2227, grad_fn=<NegBackward0>) tensor(11820.2158, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11820.208984375
tensor(11820.2158, grad_fn=<NegBackward0>) tensor(11820.2090, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11820.1982421875
tensor(11820.2090, grad_fn=<NegBackward0>) tensor(11820.1982, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11820.13671875
tensor(11820.1982, grad_fn=<NegBackward0>) tensor(11820.1367, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11820.1337890625
tensor(11820.1367, grad_fn=<NegBackward0>) tensor(11820.1338, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11820.1298828125
tensor(11820.1338, grad_fn=<NegBackward0>) tensor(11820.1299, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11820.134765625
tensor(11820.1299, grad_fn=<NegBackward0>) tensor(11820.1348, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11820.125
tensor(11820.1299, grad_fn=<NegBackward0>) tensor(11820.1250, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11820.1220703125
tensor(11820.1250, grad_fn=<NegBackward0>) tensor(11820.1221, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11820.125
tensor(11820.1221, grad_fn=<NegBackward0>) tensor(11820.1250, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11820.119140625
tensor(11820.1221, grad_fn=<NegBackward0>) tensor(11820.1191, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11820.130859375
tensor(11820.1191, grad_fn=<NegBackward0>) tensor(11820.1309, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11820.1005859375
tensor(11820.1191, grad_fn=<NegBackward0>) tensor(11820.1006, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11820.056640625
tensor(11820.1006, grad_fn=<NegBackward0>) tensor(11820.0566, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11820.05078125
tensor(11820.0566, grad_fn=<NegBackward0>) tensor(11820.0508, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11820.0439453125
tensor(11820.0508, grad_fn=<NegBackward0>) tensor(11820.0439, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11820.044921875
tensor(11820.0439, grad_fn=<NegBackward0>) tensor(11820.0449, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11820.041015625
tensor(11820.0439, grad_fn=<NegBackward0>) tensor(11820.0410, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11820.0400390625
tensor(11820.0410, grad_fn=<NegBackward0>) tensor(11820.0400, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11820.0224609375
tensor(11820.0400, grad_fn=<NegBackward0>) tensor(11820.0225, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11820.0205078125
tensor(11820.0225, grad_fn=<NegBackward0>) tensor(11820.0205, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11820.0185546875
tensor(11820.0205, grad_fn=<NegBackward0>) tensor(11820.0186, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11820.0185546875
tensor(11820.0186, grad_fn=<NegBackward0>) tensor(11820.0186, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11820.0185546875
tensor(11820.0186, grad_fn=<NegBackward0>) tensor(11820.0186, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11820.03125
tensor(11820.0186, grad_fn=<NegBackward0>) tensor(11820.0312, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11820.01953125
tensor(11820.0186, grad_fn=<NegBackward0>) tensor(11820.0195, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11820.0166015625
tensor(11820.0186, grad_fn=<NegBackward0>) tensor(11820.0166, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11820.0205078125
tensor(11820.0166, grad_fn=<NegBackward0>) tensor(11820.0205, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11820.0146484375
tensor(11820.0166, grad_fn=<NegBackward0>) tensor(11820.0146, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11820.01953125
tensor(11820.0146, grad_fn=<NegBackward0>) tensor(11820.0195, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11820.017578125
tensor(11820.0146, grad_fn=<NegBackward0>) tensor(11820.0176, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11820.0126953125
tensor(11820.0146, grad_fn=<NegBackward0>) tensor(11820.0127, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11820.013671875
tensor(11820.0127, grad_fn=<NegBackward0>) tensor(11820.0137, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11820.0185546875
tensor(11820.0127, grad_fn=<NegBackward0>) tensor(11820.0186, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11818.5107421875
tensor(11820.0127, grad_fn=<NegBackward0>) tensor(11818.5107, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11809.1748046875
tensor(11818.5107, grad_fn=<NegBackward0>) tensor(11809.1748, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11809.1640625
tensor(11809.1748, grad_fn=<NegBackward0>) tensor(11809.1641, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11809.162109375
tensor(11809.1641, grad_fn=<NegBackward0>) tensor(11809.1621, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11809.1611328125
tensor(11809.1621, grad_fn=<NegBackward0>) tensor(11809.1611, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11809.16796875
tensor(11809.1611, grad_fn=<NegBackward0>) tensor(11809.1680, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11809.0849609375
tensor(11809.1611, grad_fn=<NegBackward0>) tensor(11809.0850, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11809.083984375
tensor(11809.0850, grad_fn=<NegBackward0>) tensor(11809.0840, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11809.08203125
tensor(11809.0840, grad_fn=<NegBackward0>) tensor(11809.0820, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11809.0712890625
tensor(11809.0820, grad_fn=<NegBackward0>) tensor(11809.0713, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11809.0791015625
tensor(11809.0713, grad_fn=<NegBackward0>) tensor(11809.0791, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11809.0693359375
tensor(11809.0713, grad_fn=<NegBackward0>) tensor(11809.0693, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11809.068359375
tensor(11809.0693, grad_fn=<NegBackward0>) tensor(11809.0684, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11809.0693359375
tensor(11809.0684, grad_fn=<NegBackward0>) tensor(11809.0693, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11809.06640625
tensor(11809.0684, grad_fn=<NegBackward0>) tensor(11809.0664, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11809.06640625
tensor(11809.0664, grad_fn=<NegBackward0>) tensor(11809.0664, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11809.072265625
tensor(11809.0664, grad_fn=<NegBackward0>) tensor(11809.0723, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11809.0634765625
tensor(11809.0664, grad_fn=<NegBackward0>) tensor(11809.0635, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11809.0595703125
tensor(11809.0635, grad_fn=<NegBackward0>) tensor(11809.0596, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11809.0595703125
tensor(11809.0596, grad_fn=<NegBackward0>) tensor(11809.0596, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11809.05859375
tensor(11809.0596, grad_fn=<NegBackward0>) tensor(11809.0586, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11809.060546875
tensor(11809.0586, grad_fn=<NegBackward0>) tensor(11809.0605, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11809.095703125
tensor(11809.0586, grad_fn=<NegBackward0>) tensor(11809.0957, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11809.0576171875
tensor(11809.0586, grad_fn=<NegBackward0>) tensor(11809.0576, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11809.05859375
tensor(11809.0576, grad_fn=<NegBackward0>) tensor(11809.0586, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11809.0712890625
tensor(11809.0576, grad_fn=<NegBackward0>) tensor(11809.0713, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11809.0595703125
tensor(11809.0576, grad_fn=<NegBackward0>) tensor(11809.0596, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11809.05859375
tensor(11809.0576, grad_fn=<NegBackward0>) tensor(11809.0586, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11809.056640625
tensor(11809.0576, grad_fn=<NegBackward0>) tensor(11809.0566, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11809.0625
tensor(11809.0566, grad_fn=<NegBackward0>) tensor(11809.0625, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11809.05859375
tensor(11809.0566, grad_fn=<NegBackward0>) tensor(11809.0586, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11809.0595703125
tensor(11809.0566, grad_fn=<NegBackward0>) tensor(11809.0596, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11809.0693359375
tensor(11809.0566, grad_fn=<NegBackward0>) tensor(11809.0693, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11809.0576171875
tensor(11809.0566, grad_fn=<NegBackward0>) tensor(11809.0576, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.5674, 0.4326],
        [0.5053, 0.4947]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7505, 0.2495], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2357, 0.0924],
         [0.6393, 0.3824]],

        [[0.7006, 0.0996],
         [0.7205, 0.6444]],

        [[0.5457, 0.0893],
         [0.5194, 0.5383]],

        [[0.5673, 0.1025],
         [0.5189, 0.6740]],

        [[0.6560, 0.1082],
         [0.5924, 0.5432]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 28
Adjusted Rand Index: 0.18726837735140967
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 24
Adjusted Rand Index: 0.26343612334801764
Global Adjusted Rand Index: 0.16483615827610057
Average Adjusted Rand Index: 0.6901409001398855
[0.9760961002009502, 0.16483615827610057] [0.9759984953877163, 0.6901409001398855] [11535.3271484375, 11809.0576171875]
-------------------------------------
This iteration is 84
True Objective function: Loss = -11756.660175525833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21071.86328125
inf tensor(21071.8633, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11773.822265625
tensor(21071.8633, grad_fn=<NegBackward0>) tensor(11773.8223, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11750.5751953125
tensor(11773.8223, grad_fn=<NegBackward0>) tensor(11750.5752, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11750.0087890625
tensor(11750.5752, grad_fn=<NegBackward0>) tensor(11750.0088, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11749.7841796875
tensor(11750.0088, grad_fn=<NegBackward0>) tensor(11749.7842, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11749.6669921875
tensor(11749.7842, grad_fn=<NegBackward0>) tensor(11749.6670, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11749.595703125
tensor(11749.6670, grad_fn=<NegBackward0>) tensor(11749.5957, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11749.5478515625
tensor(11749.5957, grad_fn=<NegBackward0>) tensor(11749.5479, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11749.515625
tensor(11749.5479, grad_fn=<NegBackward0>) tensor(11749.5156, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11749.4921875
tensor(11749.5156, grad_fn=<NegBackward0>) tensor(11749.4922, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11749.474609375
tensor(11749.4922, grad_fn=<NegBackward0>) tensor(11749.4746, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11749.4619140625
tensor(11749.4746, grad_fn=<NegBackward0>) tensor(11749.4619, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11749.455078125
tensor(11749.4619, grad_fn=<NegBackward0>) tensor(11749.4551, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11749.4423828125
tensor(11749.4551, grad_fn=<NegBackward0>) tensor(11749.4424, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11749.435546875
tensor(11749.4424, grad_fn=<NegBackward0>) tensor(11749.4355, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11749.4296875
tensor(11749.4355, grad_fn=<NegBackward0>) tensor(11749.4297, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11749.423828125
tensor(11749.4297, grad_fn=<NegBackward0>) tensor(11749.4238, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11749.419921875
tensor(11749.4238, grad_fn=<NegBackward0>) tensor(11749.4199, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11749.416015625
tensor(11749.4199, grad_fn=<NegBackward0>) tensor(11749.4160, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11749.4130859375
tensor(11749.4160, grad_fn=<NegBackward0>) tensor(11749.4131, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11749.4111328125
tensor(11749.4131, grad_fn=<NegBackward0>) tensor(11749.4111, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11749.408203125
tensor(11749.4111, grad_fn=<NegBackward0>) tensor(11749.4082, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11749.40625
tensor(11749.4082, grad_fn=<NegBackward0>) tensor(11749.4062, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11749.404296875
tensor(11749.4062, grad_fn=<NegBackward0>) tensor(11749.4043, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11749.40234375
tensor(11749.4043, grad_fn=<NegBackward0>) tensor(11749.4023, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11749.400390625
tensor(11749.4023, grad_fn=<NegBackward0>) tensor(11749.4004, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11749.400390625
tensor(11749.4004, grad_fn=<NegBackward0>) tensor(11749.4004, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11749.3984375
tensor(11749.4004, grad_fn=<NegBackward0>) tensor(11749.3984, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11749.4072265625
tensor(11749.3984, grad_fn=<NegBackward0>) tensor(11749.4072, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11749.396484375
tensor(11749.3984, grad_fn=<NegBackward0>) tensor(11749.3965, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11749.3955078125
tensor(11749.3965, grad_fn=<NegBackward0>) tensor(11749.3955, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11749.3955078125
tensor(11749.3955, grad_fn=<NegBackward0>) tensor(11749.3955, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11749.39453125
tensor(11749.3955, grad_fn=<NegBackward0>) tensor(11749.3945, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11749.392578125
tensor(11749.3945, grad_fn=<NegBackward0>) tensor(11749.3926, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11749.392578125
tensor(11749.3926, grad_fn=<NegBackward0>) tensor(11749.3926, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11749.392578125
tensor(11749.3926, grad_fn=<NegBackward0>) tensor(11749.3926, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11749.3916015625
tensor(11749.3926, grad_fn=<NegBackward0>) tensor(11749.3916, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11749.390625
tensor(11749.3916, grad_fn=<NegBackward0>) tensor(11749.3906, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11749.3916015625
tensor(11749.3906, grad_fn=<NegBackward0>) tensor(11749.3916, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11749.390625
tensor(11749.3906, grad_fn=<NegBackward0>) tensor(11749.3906, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11749.3896484375
tensor(11749.3906, grad_fn=<NegBackward0>) tensor(11749.3896, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11749.3896484375
tensor(11749.3896, grad_fn=<NegBackward0>) tensor(11749.3896, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11749.390625
tensor(11749.3896, grad_fn=<NegBackward0>) tensor(11749.3906, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11749.390625
tensor(11749.3896, grad_fn=<NegBackward0>) tensor(11749.3906, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11749.388671875
tensor(11749.3896, grad_fn=<NegBackward0>) tensor(11749.3887, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11749.3896484375
tensor(11749.3887, grad_fn=<NegBackward0>) tensor(11749.3896, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11749.388671875
tensor(11749.3887, grad_fn=<NegBackward0>) tensor(11749.3887, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11749.388671875
tensor(11749.3887, grad_fn=<NegBackward0>) tensor(11749.3887, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11749.38671875
tensor(11749.3887, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11749.3876953125
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3877, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11749.3876953125
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3877, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11749.388671875
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3887, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11749.38671875
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11749.39453125
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3945, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11749.3876953125
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3877, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11749.38671875
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11749.38671875
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11749.38671875
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11749.388671875
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3887, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11749.38671875
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11749.384765625
tensor(11749.3867, grad_fn=<NegBackward0>) tensor(11749.3848, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11749.38671875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11749.3857421875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3857, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11749.3857421875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3857, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11749.3837890625
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3838, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11749.3857421875
tensor(11749.3838, grad_fn=<NegBackward0>) tensor(11749.3857, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11749.3837890625
tensor(11749.3838, grad_fn=<NegBackward0>) tensor(11749.3838, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11749.384765625
tensor(11749.3838, grad_fn=<NegBackward0>) tensor(11749.3848, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11749.384765625
tensor(11749.3838, grad_fn=<NegBackward0>) tensor(11749.3848, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11749.384765625
tensor(11749.3838, grad_fn=<NegBackward0>) tensor(11749.3848, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11749.416015625
tensor(11749.3838, grad_fn=<NegBackward0>) tensor(11749.4160, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11749.390625
tensor(11749.3838, grad_fn=<NegBackward0>) tensor(11749.3906, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.7949, 0.2051],
        [0.2363, 0.7637]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5112, 0.4888], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3937, 0.1091],
         [0.5649, 0.1974]],

        [[0.5434, 0.0998],
         [0.6753, 0.6777]],

        [[0.5703, 0.1070],
         [0.6873, 0.5908]],

        [[0.7196, 0.0947],
         [0.6973, 0.6113]],

        [[0.5891, 0.1070],
         [0.6122, 0.6277]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999558239977
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20592.236328125
inf tensor(20592.2363, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12566.6083984375
tensor(20592.2363, grad_fn=<NegBackward0>) tensor(12566.6084, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12176.0009765625
tensor(12566.6084, grad_fn=<NegBackward0>) tensor(12176.0010, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11838.8125
tensor(12176.0010, grad_fn=<NegBackward0>) tensor(11838.8125, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11779.515625
tensor(11838.8125, grad_fn=<NegBackward0>) tensor(11779.5156, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11758.873046875
tensor(11779.5156, grad_fn=<NegBackward0>) tensor(11758.8730, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11758.634765625
tensor(11758.8730, grad_fn=<NegBackward0>) tensor(11758.6348, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11758.501953125
tensor(11758.6348, grad_fn=<NegBackward0>) tensor(11758.5020, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11758.40234375
tensor(11758.5020, grad_fn=<NegBackward0>) tensor(11758.4023, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11749.7412109375
tensor(11758.4023, grad_fn=<NegBackward0>) tensor(11749.7412, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11749.705078125
tensor(11749.7412, grad_fn=<NegBackward0>) tensor(11749.7051, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11749.6640625
tensor(11749.7051, grad_fn=<NegBackward0>) tensor(11749.6641, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11749.638671875
tensor(11749.6641, grad_fn=<NegBackward0>) tensor(11749.6387, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11749.62109375
tensor(11749.6387, grad_fn=<NegBackward0>) tensor(11749.6211, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11749.603515625
tensor(11749.6211, grad_fn=<NegBackward0>) tensor(11749.6035, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11749.58984375
tensor(11749.6035, grad_fn=<NegBackward0>) tensor(11749.5898, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11749.5791015625
tensor(11749.5898, grad_fn=<NegBackward0>) tensor(11749.5791, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11749.5703125
tensor(11749.5791, grad_fn=<NegBackward0>) tensor(11749.5703, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11749.5615234375
tensor(11749.5703, grad_fn=<NegBackward0>) tensor(11749.5615, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11749.5546875
tensor(11749.5615, grad_fn=<NegBackward0>) tensor(11749.5547, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11749.548828125
tensor(11749.5547, grad_fn=<NegBackward0>) tensor(11749.5488, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11749.544921875
tensor(11749.5488, grad_fn=<NegBackward0>) tensor(11749.5449, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11749.5390625
tensor(11749.5449, grad_fn=<NegBackward0>) tensor(11749.5391, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11749.53515625
tensor(11749.5391, grad_fn=<NegBackward0>) tensor(11749.5352, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11749.53125
tensor(11749.5352, grad_fn=<NegBackward0>) tensor(11749.5312, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11749.529296875
tensor(11749.5312, grad_fn=<NegBackward0>) tensor(11749.5293, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11749.5263671875
tensor(11749.5293, grad_fn=<NegBackward0>) tensor(11749.5264, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11749.5244140625
tensor(11749.5264, grad_fn=<NegBackward0>) tensor(11749.5244, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11749.5224609375
tensor(11749.5244, grad_fn=<NegBackward0>) tensor(11749.5225, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11749.51953125
tensor(11749.5225, grad_fn=<NegBackward0>) tensor(11749.5195, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11749.5185546875
tensor(11749.5195, grad_fn=<NegBackward0>) tensor(11749.5186, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11749.515625
tensor(11749.5186, grad_fn=<NegBackward0>) tensor(11749.5156, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11749.5146484375
tensor(11749.5156, grad_fn=<NegBackward0>) tensor(11749.5146, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11749.5126953125
tensor(11749.5146, grad_fn=<NegBackward0>) tensor(11749.5127, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11749.5107421875
tensor(11749.5127, grad_fn=<NegBackward0>) tensor(11749.5107, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11749.51171875
tensor(11749.5107, grad_fn=<NegBackward0>) tensor(11749.5117, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11749.5087890625
tensor(11749.5107, grad_fn=<NegBackward0>) tensor(11749.5088, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11749.5087890625
tensor(11749.5088, grad_fn=<NegBackward0>) tensor(11749.5088, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11749.5078125
tensor(11749.5088, grad_fn=<NegBackward0>) tensor(11749.5078, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11749.505859375
tensor(11749.5078, grad_fn=<NegBackward0>) tensor(11749.5059, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11749.505859375
tensor(11749.5059, grad_fn=<NegBackward0>) tensor(11749.5059, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11749.5048828125
tensor(11749.5059, grad_fn=<NegBackward0>) tensor(11749.5049, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11749.5048828125
tensor(11749.5049, grad_fn=<NegBackward0>) tensor(11749.5049, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11749.5048828125
tensor(11749.5049, grad_fn=<NegBackward0>) tensor(11749.5049, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11749.5048828125
tensor(11749.5049, grad_fn=<NegBackward0>) tensor(11749.5049, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11749.50390625
tensor(11749.5049, grad_fn=<NegBackward0>) tensor(11749.5039, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11749.5146484375
tensor(11749.5039, grad_fn=<NegBackward0>) tensor(11749.5146, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11749.501953125
tensor(11749.5039, grad_fn=<NegBackward0>) tensor(11749.5020, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11749.5009765625
tensor(11749.5020, grad_fn=<NegBackward0>) tensor(11749.5010, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11749.5009765625
tensor(11749.5010, grad_fn=<NegBackward0>) tensor(11749.5010, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11749.5029296875
tensor(11749.5010, grad_fn=<NegBackward0>) tensor(11749.5029, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11749.5
tensor(11749.5010, grad_fn=<NegBackward0>) tensor(11749.5000, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11749.5
tensor(11749.5000, grad_fn=<NegBackward0>) tensor(11749.5000, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11749.5
tensor(11749.5000, grad_fn=<NegBackward0>) tensor(11749.5000, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11749.498046875
tensor(11749.5000, grad_fn=<NegBackward0>) tensor(11749.4980, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11749.4990234375
tensor(11749.4980, grad_fn=<NegBackward0>) tensor(11749.4990, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11749.4990234375
tensor(11749.4980, grad_fn=<NegBackward0>) tensor(11749.4990, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11749.5009765625
tensor(11749.4980, grad_fn=<NegBackward0>) tensor(11749.5010, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11749.4970703125
tensor(11749.4980, grad_fn=<NegBackward0>) tensor(11749.4971, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11749.5
tensor(11749.4971, grad_fn=<NegBackward0>) tensor(11749.5000, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11749.50390625
tensor(11749.4971, grad_fn=<NegBackward0>) tensor(11749.5039, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11749.4990234375
tensor(11749.4971, grad_fn=<NegBackward0>) tensor(11749.4990, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11749.4970703125
tensor(11749.4971, grad_fn=<NegBackward0>) tensor(11749.4971, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11749.4970703125
tensor(11749.4971, grad_fn=<NegBackward0>) tensor(11749.4971, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11749.49609375
tensor(11749.4971, grad_fn=<NegBackward0>) tensor(11749.4961, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11749.4970703125
tensor(11749.4961, grad_fn=<NegBackward0>) tensor(11749.4971, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11749.5048828125
tensor(11749.4961, grad_fn=<NegBackward0>) tensor(11749.5049, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11749.49609375
tensor(11749.4961, grad_fn=<NegBackward0>) tensor(11749.4961, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11749.384765625
tensor(11749.4961, grad_fn=<NegBackward0>) tensor(11749.3848, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11749.408203125
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.4082, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11749.38671875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11749.38671875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11749.384765625
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3848, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11749.3876953125
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3877, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11749.38671875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3867, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11749.3857421875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3857, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11749.408203125
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.4082, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11749.3857421875
tensor(11749.3848, grad_fn=<NegBackward0>) tensor(11749.3857, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.7649, 0.2351],
        [0.2029, 0.7971]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4888, 0.5112], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.1091],
         [0.7275, 0.3936]],

        [[0.6928, 0.0998],
         [0.5899, 0.6339]],

        [[0.6558, 0.1070],
         [0.6915, 0.5298]],

        [[0.7020, 0.0947],
         [0.6154, 0.5789]],

        [[0.6264, 0.1070],
         [0.6127, 0.6705]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999558239977
Average Adjusted Rand Index: 0.9919992163297293
[0.9919999558239977, 0.9919999558239977] [0.9919992163297293, 0.9919992163297293] [11749.390625, 11749.3857421875]
-------------------------------------
This iteration is 85
True Objective function: Loss = -11738.7880846895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21969.59765625
inf tensor(21969.5977, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12628.2802734375
tensor(21969.5977, grad_fn=<NegBackward0>) tensor(12628.2803, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12411.5087890625
tensor(12628.2803, grad_fn=<NegBackward0>) tensor(12411.5088, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12046.8896484375
tensor(12411.5088, grad_fn=<NegBackward0>) tensor(12046.8896, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12008.1865234375
tensor(12046.8896, grad_fn=<NegBackward0>) tensor(12008.1865, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12002.4755859375
tensor(12008.1865, grad_fn=<NegBackward0>) tensor(12002.4756, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11990.2509765625
tensor(12002.4756, grad_fn=<NegBackward0>) tensor(11990.2510, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11979.005859375
tensor(11990.2510, grad_fn=<NegBackward0>) tensor(11979.0059, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11977.982421875
tensor(11979.0059, grad_fn=<NegBackward0>) tensor(11977.9824, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11955.734375
tensor(11977.9824, grad_fn=<NegBackward0>) tensor(11955.7344, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11955.1455078125
tensor(11955.7344, grad_fn=<NegBackward0>) tensor(11955.1455, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11948.7587890625
tensor(11955.1455, grad_fn=<NegBackward0>) tensor(11948.7588, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11948.5185546875
tensor(11948.7588, grad_fn=<NegBackward0>) tensor(11948.5186, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11945.955078125
tensor(11948.5186, grad_fn=<NegBackward0>) tensor(11945.9551, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11931.24609375
tensor(11945.9551, grad_fn=<NegBackward0>) tensor(11931.2461, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11931.212890625
tensor(11931.2461, grad_fn=<NegBackward0>) tensor(11931.2129, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11914.7392578125
tensor(11931.2129, grad_fn=<NegBackward0>) tensor(11914.7393, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11914.58203125
tensor(11914.7393, grad_fn=<NegBackward0>) tensor(11914.5820, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11914.5595703125
tensor(11914.5820, grad_fn=<NegBackward0>) tensor(11914.5596, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11914.537109375
tensor(11914.5596, grad_fn=<NegBackward0>) tensor(11914.5371, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11914.5263671875
tensor(11914.5371, grad_fn=<NegBackward0>) tensor(11914.5264, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11914.5146484375
tensor(11914.5264, grad_fn=<NegBackward0>) tensor(11914.5146, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11914.5048828125
tensor(11914.5146, grad_fn=<NegBackward0>) tensor(11914.5049, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11914.5068359375
tensor(11914.5049, grad_fn=<NegBackward0>) tensor(11914.5068, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -11914.4921875
tensor(11914.5049, grad_fn=<NegBackward0>) tensor(11914.4922, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11914.486328125
tensor(11914.4922, grad_fn=<NegBackward0>) tensor(11914.4863, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11914.482421875
tensor(11914.4863, grad_fn=<NegBackward0>) tensor(11914.4824, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11914.4775390625
tensor(11914.4824, grad_fn=<NegBackward0>) tensor(11914.4775, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11914.484375
tensor(11914.4775, grad_fn=<NegBackward0>) tensor(11914.4844, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11914.470703125
tensor(11914.4775, grad_fn=<NegBackward0>) tensor(11914.4707, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11914.466796875
tensor(11914.4707, grad_fn=<NegBackward0>) tensor(11914.4668, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11914.4638671875
tensor(11914.4668, grad_fn=<NegBackward0>) tensor(11914.4639, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11914.462890625
tensor(11914.4639, grad_fn=<NegBackward0>) tensor(11914.4629, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11914.458984375
tensor(11914.4629, grad_fn=<NegBackward0>) tensor(11914.4590, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11914.4541015625
tensor(11914.4590, grad_fn=<NegBackward0>) tensor(11914.4541, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11910.5458984375
tensor(11914.4541, grad_fn=<NegBackward0>) tensor(11910.5459, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11910.541015625
tensor(11910.5459, grad_fn=<NegBackward0>) tensor(11910.5410, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11910.541015625
tensor(11910.5410, grad_fn=<NegBackward0>) tensor(11910.5410, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11910.54296875
tensor(11910.5410, grad_fn=<NegBackward0>) tensor(11910.5430, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11910.5361328125
tensor(11910.5410, grad_fn=<NegBackward0>) tensor(11910.5361, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11910.5361328125
tensor(11910.5361, grad_fn=<NegBackward0>) tensor(11910.5361, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11910.5341796875
tensor(11910.5361, grad_fn=<NegBackward0>) tensor(11910.5342, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11910.5341796875
tensor(11910.5342, grad_fn=<NegBackward0>) tensor(11910.5342, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11910.533203125
tensor(11910.5342, grad_fn=<NegBackward0>) tensor(11910.5332, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11910.53125
tensor(11910.5332, grad_fn=<NegBackward0>) tensor(11910.5312, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11910.5302734375
tensor(11910.5312, grad_fn=<NegBackward0>) tensor(11910.5303, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11910.5302734375
tensor(11910.5303, grad_fn=<NegBackward0>) tensor(11910.5303, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11910.5283203125
tensor(11910.5303, grad_fn=<NegBackward0>) tensor(11910.5283, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11910.52734375
tensor(11910.5283, grad_fn=<NegBackward0>) tensor(11910.5273, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11910.5322265625
tensor(11910.5273, grad_fn=<NegBackward0>) tensor(11910.5322, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11910.529296875
tensor(11910.5273, grad_fn=<NegBackward0>) tensor(11910.5293, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -11910.529296875
tensor(11910.5273, grad_fn=<NegBackward0>) tensor(11910.5293, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -11910.5263671875
tensor(11910.5273, grad_fn=<NegBackward0>) tensor(11910.5264, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11910.5234375
tensor(11910.5264, grad_fn=<NegBackward0>) tensor(11910.5234, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11910.5234375
tensor(11910.5234, grad_fn=<NegBackward0>) tensor(11910.5234, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11910.5244140625
tensor(11910.5234, grad_fn=<NegBackward0>) tensor(11910.5244, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11910.525390625
tensor(11910.5234, grad_fn=<NegBackward0>) tensor(11910.5254, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11910.5283203125
tensor(11910.5234, grad_fn=<NegBackward0>) tensor(11910.5283, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11910.5234375
tensor(11910.5234, grad_fn=<NegBackward0>) tensor(11910.5234, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11910.521484375
tensor(11910.5234, grad_fn=<NegBackward0>) tensor(11910.5215, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11910.5205078125
tensor(11910.5215, grad_fn=<NegBackward0>) tensor(11910.5205, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11910.5224609375
tensor(11910.5205, grad_fn=<NegBackward0>) tensor(11910.5225, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11910.5205078125
tensor(11910.5205, grad_fn=<NegBackward0>) tensor(11910.5205, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11910.51953125
tensor(11910.5205, grad_fn=<NegBackward0>) tensor(11910.5195, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11910.51953125
tensor(11910.5195, grad_fn=<NegBackward0>) tensor(11910.5195, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11910.53125
tensor(11910.5195, grad_fn=<NegBackward0>) tensor(11910.5312, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11910.5166015625
tensor(11910.5195, grad_fn=<NegBackward0>) tensor(11910.5166, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11910.521484375
tensor(11910.5166, grad_fn=<NegBackward0>) tensor(11910.5215, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11910.517578125
tensor(11910.5166, grad_fn=<NegBackward0>) tensor(11910.5176, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11910.515625
tensor(11910.5166, grad_fn=<NegBackward0>) tensor(11910.5156, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11910.51953125
tensor(11910.5156, grad_fn=<NegBackward0>) tensor(11910.5195, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11910.515625
tensor(11910.5156, grad_fn=<NegBackward0>) tensor(11910.5156, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11910.5146484375
tensor(11910.5156, grad_fn=<NegBackward0>) tensor(11910.5146, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11910.5146484375
tensor(11910.5146, grad_fn=<NegBackward0>) tensor(11910.5146, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11910.521484375
tensor(11910.5146, grad_fn=<NegBackward0>) tensor(11910.5215, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11910.578125
tensor(11910.5146, grad_fn=<NegBackward0>) tensor(11910.5781, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11910.5146484375
tensor(11910.5146, grad_fn=<NegBackward0>) tensor(11910.5146, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11910.5126953125
tensor(11910.5146, grad_fn=<NegBackward0>) tensor(11910.5127, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11910.517578125
tensor(11910.5127, grad_fn=<NegBackward0>) tensor(11910.5176, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11910.5068359375
tensor(11910.5127, grad_fn=<NegBackward0>) tensor(11910.5068, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11910.5390625
tensor(11910.5068, grad_fn=<NegBackward0>) tensor(11910.5391, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11910.49609375
tensor(11910.5068, grad_fn=<NegBackward0>) tensor(11910.4961, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11910.4951171875
tensor(11910.4961, grad_fn=<NegBackward0>) tensor(11910.4951, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11910.4970703125
tensor(11910.4951, grad_fn=<NegBackward0>) tensor(11910.4971, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11910.4951171875
tensor(11910.4951, grad_fn=<NegBackward0>) tensor(11910.4951, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11910.494140625
tensor(11910.4951, grad_fn=<NegBackward0>) tensor(11910.4941, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11910.4951171875
tensor(11910.4941, grad_fn=<NegBackward0>) tensor(11910.4951, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11910.494140625
tensor(11910.4941, grad_fn=<NegBackward0>) tensor(11910.4941, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11910.697265625
tensor(11910.4941, grad_fn=<NegBackward0>) tensor(11910.6973, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11906.771484375
tensor(11910.4941, grad_fn=<NegBackward0>) tensor(11906.7715, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11904.498046875
tensor(11906.7715, grad_fn=<NegBackward0>) tensor(11904.4980, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11904.47265625
tensor(11904.4980, grad_fn=<NegBackward0>) tensor(11904.4727, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11904.458984375
tensor(11904.4727, grad_fn=<NegBackward0>) tensor(11904.4590, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11904.4716796875
tensor(11904.4590, grad_fn=<NegBackward0>) tensor(11904.4717, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11904.484375
tensor(11904.4590, grad_fn=<NegBackward0>) tensor(11904.4844, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11904.4580078125
tensor(11904.4590, grad_fn=<NegBackward0>) tensor(11904.4580, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11904.45703125
tensor(11904.4580, grad_fn=<NegBackward0>) tensor(11904.4570, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11904.4599609375
tensor(11904.4570, grad_fn=<NegBackward0>) tensor(11904.4600, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11904.4619140625
tensor(11904.4570, grad_fn=<NegBackward0>) tensor(11904.4619, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11904.4560546875
tensor(11904.4570, grad_fn=<NegBackward0>) tensor(11904.4561, grad_fn=<NegBackward0>)
pi: tensor([[0.7510, 0.2490],
        [0.3948, 0.6052]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0784, 0.9216], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3963, 0.0824],
         [0.5020, 0.2108]],

        [[0.5725, 0.0968],
         [0.7064, 0.5265]],

        [[0.6158, 0.1043],
         [0.5827, 0.6779]],

        [[0.6375, 0.1001],
         [0.6477, 0.5845]],

        [[0.5802, 0.0972],
         [0.7270, 0.7228]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.022741429840595545
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5889885174450136
Average Adjusted Rand Index: 0.804548285968119
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24780.1015625
inf tensor(24780.1016, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11918.015625
tensor(24780.1016, grad_fn=<NegBackward0>) tensor(11918.0156, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11913.2568359375
tensor(11918.0156, grad_fn=<NegBackward0>) tensor(11913.2568, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11912.994140625
tensor(11913.2568, grad_fn=<NegBackward0>) tensor(11912.9941, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11912.8984375
tensor(11912.9941, grad_fn=<NegBackward0>) tensor(11912.8984, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11912.84765625
tensor(11912.8984, grad_fn=<NegBackward0>) tensor(11912.8477, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11912.8173828125
tensor(11912.8477, grad_fn=<NegBackward0>) tensor(11912.8174, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11912.7978515625
tensor(11912.8174, grad_fn=<NegBackward0>) tensor(11912.7979, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11912.783203125
tensor(11912.7979, grad_fn=<NegBackward0>) tensor(11912.7832, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11912.7724609375
tensor(11912.7832, grad_fn=<NegBackward0>) tensor(11912.7725, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11912.765625
tensor(11912.7725, grad_fn=<NegBackward0>) tensor(11912.7656, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11912.76171875
tensor(11912.7656, grad_fn=<NegBackward0>) tensor(11912.7617, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11912.7548828125
tensor(11912.7617, grad_fn=<NegBackward0>) tensor(11912.7549, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11912.7509765625
tensor(11912.7549, grad_fn=<NegBackward0>) tensor(11912.7510, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11912.748046875
tensor(11912.7510, grad_fn=<NegBackward0>) tensor(11912.7480, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11912.744140625
tensor(11912.7480, grad_fn=<NegBackward0>) tensor(11912.7441, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11912.7431640625
tensor(11912.7441, grad_fn=<NegBackward0>) tensor(11912.7432, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11912.7421875
tensor(11912.7432, grad_fn=<NegBackward0>) tensor(11912.7422, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11912.740234375
tensor(11912.7422, grad_fn=<NegBackward0>) tensor(11912.7402, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11912.7392578125
tensor(11912.7402, grad_fn=<NegBackward0>) tensor(11912.7393, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11912.736328125
tensor(11912.7393, grad_fn=<NegBackward0>) tensor(11912.7363, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11912.7373046875
tensor(11912.7363, grad_fn=<NegBackward0>) tensor(11912.7373, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -11912.736328125
tensor(11912.7363, grad_fn=<NegBackward0>) tensor(11912.7363, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11912.734375
tensor(11912.7363, grad_fn=<NegBackward0>) tensor(11912.7344, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11912.7333984375
tensor(11912.7344, grad_fn=<NegBackward0>) tensor(11912.7334, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11912.734375
tensor(11912.7334, grad_fn=<NegBackward0>) tensor(11912.7344, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11912.732421875
tensor(11912.7334, grad_fn=<NegBackward0>) tensor(11912.7324, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11912.732421875
tensor(11912.7324, grad_fn=<NegBackward0>) tensor(11912.7324, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11912.732421875
tensor(11912.7324, grad_fn=<NegBackward0>) tensor(11912.7324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11912.73046875
tensor(11912.7324, grad_fn=<NegBackward0>) tensor(11912.7305, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11912.73046875
tensor(11912.7305, grad_fn=<NegBackward0>) tensor(11912.7305, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11912.73046875
tensor(11912.7305, grad_fn=<NegBackward0>) tensor(11912.7305, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11912.73046875
tensor(11912.7305, grad_fn=<NegBackward0>) tensor(11912.7305, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11912.7294921875
tensor(11912.7305, grad_fn=<NegBackward0>) tensor(11912.7295, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11912.7294921875
tensor(11912.7295, grad_fn=<NegBackward0>) tensor(11912.7295, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11912.7294921875
tensor(11912.7295, grad_fn=<NegBackward0>) tensor(11912.7295, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11912.728515625
tensor(11912.7295, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11912.728515625
tensor(11912.7285, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11912.728515625
tensor(11912.7285, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11912.728515625
tensor(11912.7285, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11912.728515625
tensor(11912.7285, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11912.7275390625
tensor(11912.7285, grad_fn=<NegBackward0>) tensor(11912.7275, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11912.7275390625
tensor(11912.7275, grad_fn=<NegBackward0>) tensor(11912.7275, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11912.7275390625
tensor(11912.7275, grad_fn=<NegBackward0>) tensor(11912.7275, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11912.7314453125
tensor(11912.7275, grad_fn=<NegBackward0>) tensor(11912.7314, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11912.734375
tensor(11912.7275, grad_fn=<NegBackward0>) tensor(11912.7344, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11912.7265625
tensor(11912.7275, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11912.7333984375
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7334, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11912.7265625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11912.7265625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11912.7275390625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7275, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11912.7265625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11912.7275390625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7275, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11912.728515625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11912.7265625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11912.744140625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7441, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11912.7265625
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11912.7255859375
tensor(11912.7266, grad_fn=<NegBackward0>) tensor(11912.7256, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11912.7255859375
tensor(11912.7256, grad_fn=<NegBackward0>) tensor(11912.7256, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11912.7255859375
tensor(11912.7256, grad_fn=<NegBackward0>) tensor(11912.7256, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11912.7265625
tensor(11912.7256, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11912.7255859375
tensor(11912.7256, grad_fn=<NegBackward0>) tensor(11912.7256, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11912.7265625
tensor(11912.7256, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11912.7255859375
tensor(11912.7256, grad_fn=<NegBackward0>) tensor(11912.7256, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11912.724609375
tensor(11912.7256, grad_fn=<NegBackward0>) tensor(11912.7246, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11912.7265625
tensor(11912.7246, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11912.724609375
tensor(11912.7246, grad_fn=<NegBackward0>) tensor(11912.7246, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11912.7255859375
tensor(11912.7246, grad_fn=<NegBackward0>) tensor(11912.7256, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11912.728515625
tensor(11912.7246, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11912.728515625
tensor(11912.7246, grad_fn=<NegBackward0>) tensor(11912.7285, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11912.7265625
tensor(11912.7246, grad_fn=<NegBackward0>) tensor(11912.7266, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11912.73046875
tensor(11912.7246, grad_fn=<NegBackward0>) tensor(11912.7305, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6511, 0.3489],
        [0.3033, 0.6967]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5044, 0.4956], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3899, 0.1037],
         [0.5391, 0.2270]],

        [[0.7016, 0.0970],
         [0.5357, 0.7074]],

        [[0.5775, 0.1043],
         [0.7043, 0.5732]],

        [[0.6185, 0.0992],
         [0.6770, 0.5373]],

        [[0.7174, 0.0998],
         [0.6388, 0.6552]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 84
Adjusted Rand Index: 0.45744585590509607
Global Adjusted Rand Index: 0.434474679410968
Average Adjusted Rand Index: 0.8834863178833391
[0.5889885174450136, 0.434474679410968] [0.804548285968119, 0.8834863178833391] [11904.4677734375, 11912.73046875]
-------------------------------------
This iteration is 86
True Objective function: Loss = -11752.207801326182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22648.322265625
inf tensor(22648.3223, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12695.150390625
tensor(22648.3223, grad_fn=<NegBackward0>) tensor(12695.1504, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12467.177734375
tensor(12695.1504, grad_fn=<NegBackward0>) tensor(12467.1777, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12028.5830078125
tensor(12467.1777, grad_fn=<NegBackward0>) tensor(12028.5830, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12004.1259765625
tensor(12028.5830, grad_fn=<NegBackward0>) tensor(12004.1260, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12002.982421875
tensor(12004.1260, grad_fn=<NegBackward0>) tensor(12002.9824, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11994.748046875
tensor(12002.9824, grad_fn=<NegBackward0>) tensor(11994.7480, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11983.6357421875
tensor(11994.7480, grad_fn=<NegBackward0>) tensor(11983.6357, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11983.396484375
tensor(11983.6357, grad_fn=<NegBackward0>) tensor(11983.3965, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11983.26953125
tensor(11983.3965, grad_fn=<NegBackward0>) tensor(11983.2695, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11983.1640625
tensor(11983.2695, grad_fn=<NegBackward0>) tensor(11983.1641, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11974.0087890625
tensor(11983.1641, grad_fn=<NegBackward0>) tensor(11974.0088, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11955.2158203125
tensor(11974.0088, grad_fn=<NegBackward0>) tensor(11955.2158, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11955.068359375
tensor(11955.2158, grad_fn=<NegBackward0>) tensor(11955.0684, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11955.03125
tensor(11955.0684, grad_fn=<NegBackward0>) tensor(11955.0312, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11955.0009765625
tensor(11955.0312, grad_fn=<NegBackward0>) tensor(11955.0010, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11954.9775390625
tensor(11955.0010, grad_fn=<NegBackward0>) tensor(11954.9775, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11954.95703125
tensor(11954.9775, grad_fn=<NegBackward0>) tensor(11954.9570, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11954.9384765625
tensor(11954.9570, grad_fn=<NegBackward0>) tensor(11954.9385, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11954.923828125
tensor(11954.9385, grad_fn=<NegBackward0>) tensor(11954.9238, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11954.91015625
tensor(11954.9238, grad_fn=<NegBackward0>) tensor(11954.9102, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11954.9052734375
tensor(11954.9102, grad_fn=<NegBackward0>) tensor(11954.9053, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11954.8896484375
tensor(11954.9053, grad_fn=<NegBackward0>) tensor(11954.8896, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11954.880859375
tensor(11954.8896, grad_fn=<NegBackward0>) tensor(11954.8809, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11954.8740234375
tensor(11954.8809, grad_fn=<NegBackward0>) tensor(11954.8740, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11954.8662109375
tensor(11954.8740, grad_fn=<NegBackward0>) tensor(11954.8662, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11954.8720703125
tensor(11954.8662, grad_fn=<NegBackward0>) tensor(11954.8721, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11954.853515625
tensor(11954.8662, grad_fn=<NegBackward0>) tensor(11954.8535, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11954.8466796875
tensor(11954.8535, grad_fn=<NegBackward0>) tensor(11954.8467, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11954.849609375
tensor(11954.8467, grad_fn=<NegBackward0>) tensor(11954.8496, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11954.826171875
tensor(11954.8467, grad_fn=<NegBackward0>) tensor(11954.8262, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11954.822265625
tensor(11954.8262, grad_fn=<NegBackward0>) tensor(11954.8223, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11954.8203125
tensor(11954.8223, grad_fn=<NegBackward0>) tensor(11954.8203, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11954.8154296875
tensor(11954.8203, grad_fn=<NegBackward0>) tensor(11954.8154, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11954.8134765625
tensor(11954.8154, grad_fn=<NegBackward0>) tensor(11954.8135, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11954.8115234375
tensor(11954.8135, grad_fn=<NegBackward0>) tensor(11954.8115, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11954.80859375
tensor(11954.8115, grad_fn=<NegBackward0>) tensor(11954.8086, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11954.80859375
tensor(11954.8086, grad_fn=<NegBackward0>) tensor(11954.8086, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11954.8046875
tensor(11954.8086, grad_fn=<NegBackward0>) tensor(11954.8047, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11954.8037109375
tensor(11954.8047, grad_fn=<NegBackward0>) tensor(11954.8037, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11954.8017578125
tensor(11954.8037, grad_fn=<NegBackward0>) tensor(11954.8018, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11954.80078125
tensor(11954.8018, grad_fn=<NegBackward0>) tensor(11954.8008, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11954.7998046875
tensor(11954.8008, grad_fn=<NegBackward0>) tensor(11954.7998, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11954.7978515625
tensor(11954.7998, grad_fn=<NegBackward0>) tensor(11954.7979, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11954.7958984375
tensor(11954.7979, grad_fn=<NegBackward0>) tensor(11954.7959, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11954.8232421875
tensor(11954.7959, grad_fn=<NegBackward0>) tensor(11954.8232, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11954.7978515625
tensor(11954.7959, grad_fn=<NegBackward0>) tensor(11954.7979, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11954.794921875
tensor(11954.7959, grad_fn=<NegBackward0>) tensor(11954.7949, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11954.796875
tensor(11954.7949, grad_fn=<NegBackward0>) tensor(11954.7969, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11954.7939453125
tensor(11954.7949, grad_fn=<NegBackward0>) tensor(11954.7939, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11954.7919921875
tensor(11954.7939, grad_fn=<NegBackward0>) tensor(11954.7920, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11954.7900390625
tensor(11954.7920, grad_fn=<NegBackward0>) tensor(11954.7900, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11954.8056640625
tensor(11954.7900, grad_fn=<NegBackward0>) tensor(11954.8057, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11954.7900390625
tensor(11954.7900, grad_fn=<NegBackward0>) tensor(11954.7900, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11954.787109375
tensor(11954.7900, grad_fn=<NegBackward0>) tensor(11954.7871, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11954.787109375
tensor(11954.7871, grad_fn=<NegBackward0>) tensor(11954.7871, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11954.7841796875
tensor(11954.7871, grad_fn=<NegBackward0>) tensor(11954.7842, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11954.78125
tensor(11954.7842, grad_fn=<NegBackward0>) tensor(11954.7812, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11954.7802734375
tensor(11954.7812, grad_fn=<NegBackward0>) tensor(11954.7803, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11954.78515625
tensor(11954.7803, grad_fn=<NegBackward0>) tensor(11954.7852, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11954.779296875
tensor(11954.7803, grad_fn=<NegBackward0>) tensor(11954.7793, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11954.779296875
tensor(11954.7793, grad_fn=<NegBackward0>) tensor(11954.7793, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11954.779296875
tensor(11954.7793, grad_fn=<NegBackward0>) tensor(11954.7793, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11954.779296875
tensor(11954.7793, grad_fn=<NegBackward0>) tensor(11954.7793, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11954.78125
tensor(11954.7793, grad_fn=<NegBackward0>) tensor(11954.7812, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11954.7783203125
tensor(11954.7793, grad_fn=<NegBackward0>) tensor(11954.7783, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11954.7783203125
tensor(11954.7783, grad_fn=<NegBackward0>) tensor(11954.7783, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11954.7783203125
tensor(11954.7783, grad_fn=<NegBackward0>) tensor(11954.7783, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11954.783203125
tensor(11954.7783, grad_fn=<NegBackward0>) tensor(11954.7832, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11954.78125
tensor(11954.7783, grad_fn=<NegBackward0>) tensor(11954.7812, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11954.78125
tensor(11954.7783, grad_fn=<NegBackward0>) tensor(11954.7812, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11954.7763671875
tensor(11954.7783, grad_fn=<NegBackward0>) tensor(11954.7764, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11954.775390625
tensor(11954.7764, grad_fn=<NegBackward0>) tensor(11954.7754, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11954.77734375
tensor(11954.7754, grad_fn=<NegBackward0>) tensor(11954.7773, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11954.775390625
tensor(11954.7754, grad_fn=<NegBackward0>) tensor(11954.7754, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11954.78125
tensor(11954.7754, grad_fn=<NegBackward0>) tensor(11954.7812, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11954.775390625
tensor(11954.7754, grad_fn=<NegBackward0>) tensor(11954.7754, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11954.775390625
tensor(11954.7754, grad_fn=<NegBackward0>) tensor(11954.7754, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11954.76953125
tensor(11954.7754, grad_fn=<NegBackward0>) tensor(11954.7695, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11954.7734375
tensor(11954.7695, grad_fn=<NegBackward0>) tensor(11954.7734, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11954.7724609375
tensor(11954.7695, grad_fn=<NegBackward0>) tensor(11954.7725, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11954.7646484375
tensor(11954.7695, grad_fn=<NegBackward0>) tensor(11954.7646, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11954.7890625
tensor(11954.7646, grad_fn=<NegBackward0>) tensor(11954.7891, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11954.7646484375
tensor(11954.7646, grad_fn=<NegBackward0>) tensor(11954.7646, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11954.7646484375
tensor(11954.7646, grad_fn=<NegBackward0>) tensor(11954.7646, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11954.763671875
tensor(11954.7646, grad_fn=<NegBackward0>) tensor(11954.7637, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11954.763671875
tensor(11954.7637, grad_fn=<NegBackward0>) tensor(11954.7637, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11954.763671875
tensor(11954.7637, grad_fn=<NegBackward0>) tensor(11954.7637, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11954.763671875
tensor(11954.7637, grad_fn=<NegBackward0>) tensor(11954.7637, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11954.763671875
tensor(11954.7637, grad_fn=<NegBackward0>) tensor(11954.7637, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11954.7626953125
tensor(11954.7637, grad_fn=<NegBackward0>) tensor(11954.7627, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11954.7861328125
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7861, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11954.763671875
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7637, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11954.7626953125
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7627, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11954.7783203125
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7783, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11954.763671875
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7637, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11954.7841796875
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7842, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11954.7626953125
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7627, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11954.765625
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.7656, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11954.87890625
tensor(11954.7627, grad_fn=<NegBackward0>) tensor(11954.8789, grad_fn=<NegBackward0>)
2
pi: tensor([[0.5555, 0.4445],
        [0.5357, 0.4643]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4500, 0.5500], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2224, 0.1038],
         [0.6699, 0.4068]],

        [[0.5034, 0.1091],
         [0.5011, 0.6518]],

        [[0.5632, 0.0981],
         [0.5288, 0.7032]],

        [[0.6799, 0.0902],
         [0.6511, 0.5850]],

        [[0.6182, 0.0965],
         [0.5220, 0.6958]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 27
Adjusted Rand Index: 0.20599751849046874
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.500263788696875
Average Adjusted Rand Index: 0.8411995036980937
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21828.11328125
inf tensor(21828.1133, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12706.7353515625
tensor(21828.1133, grad_fn=<NegBackward0>) tensor(12706.7354, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12335.73046875
tensor(12706.7354, grad_fn=<NegBackward0>) tensor(12335.7305, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11984.0146484375
tensor(12335.7305, grad_fn=<NegBackward0>) tensor(11984.0146, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11930.955078125
tensor(11984.0146, grad_fn=<NegBackward0>) tensor(11930.9551, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11929.05078125
tensor(11930.9551, grad_fn=<NegBackward0>) tensor(11929.0508, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11928.6162109375
tensor(11929.0508, grad_fn=<NegBackward0>) tensor(11928.6162, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11928.484375
tensor(11928.6162, grad_fn=<NegBackward0>) tensor(11928.4844, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11928.400390625
tensor(11928.4844, grad_fn=<NegBackward0>) tensor(11928.4004, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11928.3447265625
tensor(11928.4004, grad_fn=<NegBackward0>) tensor(11928.3447, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11928.3017578125
tensor(11928.3447, grad_fn=<NegBackward0>) tensor(11928.3018, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11928.2705078125
tensor(11928.3018, grad_fn=<NegBackward0>) tensor(11928.2705, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11928.2470703125
tensor(11928.2705, grad_fn=<NegBackward0>) tensor(11928.2471, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11928.2275390625
tensor(11928.2471, grad_fn=<NegBackward0>) tensor(11928.2275, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11928.212890625
tensor(11928.2275, grad_fn=<NegBackward0>) tensor(11928.2129, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11928.2001953125
tensor(11928.2129, grad_fn=<NegBackward0>) tensor(11928.2002, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11928.1884765625
tensor(11928.2002, grad_fn=<NegBackward0>) tensor(11928.1885, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11928.181640625
tensor(11928.1885, grad_fn=<NegBackward0>) tensor(11928.1816, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11928.171875
tensor(11928.1816, grad_fn=<NegBackward0>) tensor(11928.1719, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11928.166015625
tensor(11928.1719, grad_fn=<NegBackward0>) tensor(11928.1660, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11928.16015625
tensor(11928.1660, grad_fn=<NegBackward0>) tensor(11928.1602, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11928.1552734375
tensor(11928.1602, grad_fn=<NegBackward0>) tensor(11928.1553, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11928.150390625
tensor(11928.1553, grad_fn=<NegBackward0>) tensor(11928.1504, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11928.146484375
tensor(11928.1504, grad_fn=<NegBackward0>) tensor(11928.1465, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11928.142578125
tensor(11928.1465, grad_fn=<NegBackward0>) tensor(11928.1426, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11928.138671875
tensor(11928.1426, grad_fn=<NegBackward0>) tensor(11928.1387, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11928.13671875
tensor(11928.1387, grad_fn=<NegBackward0>) tensor(11928.1367, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11928.134765625
tensor(11928.1367, grad_fn=<NegBackward0>) tensor(11928.1348, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11928.130859375
tensor(11928.1348, grad_fn=<NegBackward0>) tensor(11928.1309, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11928.130859375
tensor(11928.1309, grad_fn=<NegBackward0>) tensor(11928.1309, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11928.1279296875
tensor(11928.1309, grad_fn=<NegBackward0>) tensor(11928.1279, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11928.1259765625
tensor(11928.1279, grad_fn=<NegBackward0>) tensor(11928.1260, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11928.125
tensor(11928.1260, grad_fn=<NegBackward0>) tensor(11928.1250, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11928.1220703125
tensor(11928.1250, grad_fn=<NegBackward0>) tensor(11928.1221, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11928.119140625
tensor(11928.1221, grad_fn=<NegBackward0>) tensor(11928.1191, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11928.1162109375
tensor(11928.1191, grad_fn=<NegBackward0>) tensor(11928.1162, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11928.115234375
tensor(11928.1162, grad_fn=<NegBackward0>) tensor(11928.1152, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11928.115234375
tensor(11928.1152, grad_fn=<NegBackward0>) tensor(11928.1152, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11928.11328125
tensor(11928.1152, grad_fn=<NegBackward0>) tensor(11928.1133, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11928.11328125
tensor(11928.1133, grad_fn=<NegBackward0>) tensor(11928.1133, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11928.11328125
tensor(11928.1133, grad_fn=<NegBackward0>) tensor(11928.1133, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11928.1123046875
tensor(11928.1133, grad_fn=<NegBackward0>) tensor(11928.1123, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11928.111328125
tensor(11928.1123, grad_fn=<NegBackward0>) tensor(11928.1113, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11928.1171875
tensor(11928.1113, grad_fn=<NegBackward0>) tensor(11928.1172, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11928.1103515625
tensor(11928.1113, grad_fn=<NegBackward0>) tensor(11928.1104, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11928.109375
tensor(11928.1104, grad_fn=<NegBackward0>) tensor(11928.1094, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11928.1083984375
tensor(11928.1094, grad_fn=<NegBackward0>) tensor(11928.1084, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11928.109375
tensor(11928.1084, grad_fn=<NegBackward0>) tensor(11928.1094, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11928.1083984375
tensor(11928.1084, grad_fn=<NegBackward0>) tensor(11928.1084, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11928.1083984375
tensor(11928.1084, grad_fn=<NegBackward0>) tensor(11928.1084, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11928.1083984375
tensor(11928.1084, grad_fn=<NegBackward0>) tensor(11928.1084, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11928.107421875
tensor(11928.1084, grad_fn=<NegBackward0>) tensor(11928.1074, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11928.1259765625
tensor(11928.1074, grad_fn=<NegBackward0>) tensor(11928.1260, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11928.1064453125
tensor(11928.1074, grad_fn=<NegBackward0>) tensor(11928.1064, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11928.107421875
tensor(11928.1064, grad_fn=<NegBackward0>) tensor(11928.1074, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11928.1064453125
tensor(11928.1064, grad_fn=<NegBackward0>) tensor(11928.1064, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11928.109375
tensor(11928.1064, grad_fn=<NegBackward0>) tensor(11928.1094, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11928.10546875
tensor(11928.1064, grad_fn=<NegBackward0>) tensor(11928.1055, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11928.1103515625
tensor(11928.1055, grad_fn=<NegBackward0>) tensor(11928.1104, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11928.1044921875
tensor(11928.1055, grad_fn=<NegBackward0>) tensor(11928.1045, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11928.1083984375
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1084, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11928.1064453125
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1064, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11928.1044921875
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1045, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11928.107421875
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1074, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11928.1044921875
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1045, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11928.1044921875
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1045, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11928.1044921875
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1045, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11928.103515625
tensor(11928.1045, grad_fn=<NegBackward0>) tensor(11928.1035, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11928.11328125
tensor(11928.1035, grad_fn=<NegBackward0>) tensor(11928.1133, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11928.103515625
tensor(11928.1035, grad_fn=<NegBackward0>) tensor(11928.1035, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11928.1044921875
tensor(11928.1035, grad_fn=<NegBackward0>) tensor(11928.1045, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11928.10546875
tensor(11928.1035, grad_fn=<NegBackward0>) tensor(11928.1055, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11928.1025390625
tensor(11928.1035, grad_fn=<NegBackward0>) tensor(11928.1025, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11928.1025390625
tensor(11928.1025, grad_fn=<NegBackward0>) tensor(11928.1025, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11928.1025390625
tensor(11928.1025, grad_fn=<NegBackward0>) tensor(11928.1025, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11928.1025390625
tensor(11928.1025, grad_fn=<NegBackward0>) tensor(11928.1025, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11928.1005859375
tensor(11928.1025, grad_fn=<NegBackward0>) tensor(11928.1006, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11928.1015625
tensor(11928.1006, grad_fn=<NegBackward0>) tensor(11928.1016, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11928.1005859375
tensor(11928.1006, grad_fn=<NegBackward0>) tensor(11928.1006, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11928.1015625
tensor(11928.1006, grad_fn=<NegBackward0>) tensor(11928.1016, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11928.1015625
tensor(11928.1006, grad_fn=<NegBackward0>) tensor(11928.1016, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11928.1025390625
tensor(11928.1006, grad_fn=<NegBackward0>) tensor(11928.1025, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11928.1015625
tensor(11928.1006, grad_fn=<NegBackward0>) tensor(11928.1016, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11928.1103515625
tensor(11928.1006, grad_fn=<NegBackward0>) tensor(11928.1104, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.6710, 0.3290],
        [0.3816, 0.6184]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4422, 0.5578], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2224, 0.1038],
         [0.6488, 0.3991]],

        [[0.6128, 0.1091],
         [0.6769, 0.6724]],

        [[0.6767, 0.1002],
         [0.5446, 0.5969]],

        [[0.7015, 0.0903],
         [0.5911, 0.6745]],

        [[0.5236, 0.0978],
         [0.5881, 0.7152]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 26
Adjusted Rand Index: 0.22262626262626262
Global Adjusted Rand Index: 0.4946073375267982
Average Adjusted Rand Index: 0.8445252525252526
[0.500263788696875, 0.4946073375267982] [0.8411995036980937, 0.8445252525252526] [11954.7646484375, 11928.1103515625]
-------------------------------------
This iteration is 87
True Objective function: Loss = -11253.456947906672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21459.5
inf tensor(21459.5000, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11865.326171875
tensor(21459.5000, grad_fn=<NegBackward0>) tensor(11865.3262, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11816.61328125
tensor(11865.3262, grad_fn=<NegBackward0>) tensor(11816.6133, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11707.4111328125
tensor(11816.6133, grad_fn=<NegBackward0>) tensor(11707.4111, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11444.77734375
tensor(11707.4111, grad_fn=<NegBackward0>) tensor(11444.7773, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11322.341796875
tensor(11444.7773, grad_fn=<NegBackward0>) tensor(11322.3418, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11302.9296875
tensor(11322.3418, grad_fn=<NegBackward0>) tensor(11302.9297, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11302.4814453125
tensor(11302.9297, grad_fn=<NegBackward0>) tensor(11302.4814, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11302.2412109375
tensor(11302.4814, grad_fn=<NegBackward0>) tensor(11302.2412, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11296.087890625
tensor(11302.2412, grad_fn=<NegBackward0>) tensor(11296.0879, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11288.154296875
tensor(11296.0879, grad_fn=<NegBackward0>) tensor(11288.1543, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11288.08203125
tensor(11288.1543, grad_fn=<NegBackward0>) tensor(11288.0820, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11288.0224609375
tensor(11288.0820, grad_fn=<NegBackward0>) tensor(11288.0225, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11287.9755859375
tensor(11288.0225, grad_fn=<NegBackward0>) tensor(11287.9756, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11287.9296875
tensor(11287.9756, grad_fn=<NegBackward0>) tensor(11287.9297, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11287.8955078125
tensor(11287.9297, grad_fn=<NegBackward0>) tensor(11287.8955, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11287.873046875
tensor(11287.8955, grad_fn=<NegBackward0>) tensor(11287.8730, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11287.8427734375
tensor(11287.8730, grad_fn=<NegBackward0>) tensor(11287.8428, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11283.94921875
tensor(11287.8428, grad_fn=<NegBackward0>) tensor(11283.9492, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11283.9365234375
tensor(11283.9492, grad_fn=<NegBackward0>) tensor(11283.9365, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11283.92578125
tensor(11283.9365, grad_fn=<NegBackward0>) tensor(11283.9258, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11283.916015625
tensor(11283.9258, grad_fn=<NegBackward0>) tensor(11283.9160, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11283.9072265625
tensor(11283.9160, grad_fn=<NegBackward0>) tensor(11283.9072, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11283.8974609375
tensor(11283.9072, grad_fn=<NegBackward0>) tensor(11283.8975, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11276.892578125
tensor(11283.8975, grad_fn=<NegBackward0>) tensor(11276.8926, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11276.875
tensor(11276.8926, grad_fn=<NegBackward0>) tensor(11276.8750, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11276.869140625
tensor(11276.8750, grad_fn=<NegBackward0>) tensor(11276.8691, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11276.86328125
tensor(11276.8691, grad_fn=<NegBackward0>) tensor(11276.8633, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11276.86328125
tensor(11276.8633, grad_fn=<NegBackward0>) tensor(11276.8633, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11276.8544921875
tensor(11276.8633, grad_fn=<NegBackward0>) tensor(11276.8545, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11276.8515625
tensor(11276.8545, grad_fn=<NegBackward0>) tensor(11276.8516, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11276.8505859375
tensor(11276.8516, grad_fn=<NegBackward0>) tensor(11276.8506, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11276.841796875
tensor(11276.8506, grad_fn=<NegBackward0>) tensor(11276.8418, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11276.8291015625
tensor(11276.8418, grad_fn=<NegBackward0>) tensor(11276.8291, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11276.82421875
tensor(11276.8291, grad_fn=<NegBackward0>) tensor(11276.8242, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11276.8212890625
tensor(11276.8242, grad_fn=<NegBackward0>) tensor(11276.8213, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11276.765625
tensor(11276.8213, grad_fn=<NegBackward0>) tensor(11276.7656, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11276.7490234375
tensor(11276.7656, grad_fn=<NegBackward0>) tensor(11276.7490, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11265.7744140625
tensor(11276.7490, grad_fn=<NegBackward0>) tensor(11265.7744, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11265.7265625
tensor(11265.7744, grad_fn=<NegBackward0>) tensor(11265.7266, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11259.59375
tensor(11265.7266, grad_fn=<NegBackward0>) tensor(11259.5938, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11259.5869140625
tensor(11259.5938, grad_fn=<NegBackward0>) tensor(11259.5869, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11259.5849609375
tensor(11259.5869, grad_fn=<NegBackward0>) tensor(11259.5850, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11259.5830078125
tensor(11259.5850, grad_fn=<NegBackward0>) tensor(11259.5830, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11259.5810546875
tensor(11259.5830, grad_fn=<NegBackward0>) tensor(11259.5811, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11259.5810546875
tensor(11259.5811, grad_fn=<NegBackward0>) tensor(11259.5811, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11259.5791015625
tensor(11259.5811, grad_fn=<NegBackward0>) tensor(11259.5791, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11259.5791015625
tensor(11259.5791, grad_fn=<NegBackward0>) tensor(11259.5791, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11259.564453125
tensor(11259.5791, grad_fn=<NegBackward0>) tensor(11259.5645, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11257.4443359375
tensor(11259.5645, grad_fn=<NegBackward0>) tensor(11257.4443, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11257.4443359375
tensor(11257.4443, grad_fn=<NegBackward0>) tensor(11257.4443, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11257.439453125
tensor(11257.4443, grad_fn=<NegBackward0>) tensor(11257.4395, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11246.1376953125
tensor(11257.4395, grad_fn=<NegBackward0>) tensor(11246.1377, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11246.1435546875
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1436, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11246.1376953125
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1377, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11246.1357421875
tensor(11246.1377, grad_fn=<NegBackward0>) tensor(11246.1357, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11246.140625
tensor(11246.1357, grad_fn=<NegBackward0>) tensor(11246.1406, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11246.1337890625
tensor(11246.1357, grad_fn=<NegBackward0>) tensor(11246.1338, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11246.1337890625
tensor(11246.1338, grad_fn=<NegBackward0>) tensor(11246.1338, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11246.134765625
tensor(11246.1338, grad_fn=<NegBackward0>) tensor(11246.1348, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11246.1357421875
tensor(11246.1338, grad_fn=<NegBackward0>) tensor(11246.1357, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11246.1318359375
tensor(11246.1338, grad_fn=<NegBackward0>) tensor(11246.1318, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11246.1328125
tensor(11246.1318, grad_fn=<NegBackward0>) tensor(11246.1328, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11246.13671875
tensor(11246.1318, grad_fn=<NegBackward0>) tensor(11246.1367, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11246.130859375
tensor(11246.1318, grad_fn=<NegBackward0>) tensor(11246.1309, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11246.1279296875
tensor(11246.1309, grad_fn=<NegBackward0>) tensor(11246.1279, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11246.1181640625
tensor(11246.1279, grad_fn=<NegBackward0>) tensor(11246.1182, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11246.1123046875
tensor(11246.1182, grad_fn=<NegBackward0>) tensor(11246.1123, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11246.1123046875
tensor(11246.1123, grad_fn=<NegBackward0>) tensor(11246.1123, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11246.111328125
tensor(11246.1123, grad_fn=<NegBackward0>) tensor(11246.1113, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11246.111328125
tensor(11246.1113, grad_fn=<NegBackward0>) tensor(11246.1113, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11246.111328125
tensor(11246.1113, grad_fn=<NegBackward0>) tensor(11246.1113, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11246.11328125
tensor(11246.1113, grad_fn=<NegBackward0>) tensor(11246.1133, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11246.11328125
tensor(11246.1113, grad_fn=<NegBackward0>) tensor(11246.1133, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11246.1123046875
tensor(11246.1113, grad_fn=<NegBackward0>) tensor(11246.1123, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11246.1103515625
tensor(11246.1113, grad_fn=<NegBackward0>) tensor(11246.1104, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11246.109375
tensor(11246.1104, grad_fn=<NegBackward0>) tensor(11246.1094, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11246.109375
tensor(11246.1094, grad_fn=<NegBackward0>) tensor(11246.1094, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11246.109375
tensor(11246.1094, grad_fn=<NegBackward0>) tensor(11246.1094, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11246.1083984375
tensor(11246.1094, grad_fn=<NegBackward0>) tensor(11246.1084, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11246.1083984375
tensor(11246.1084, grad_fn=<NegBackward0>) tensor(11246.1084, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11246.107421875
tensor(11246.1084, grad_fn=<NegBackward0>) tensor(11246.1074, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11246.1083984375
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1084, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11246.1142578125
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1143, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11246.146484375
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1465, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11246.142578125
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1426, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11246.107421875
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1074, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11246.1083984375
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1084, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11246.1171875
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1172, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11246.111328125
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1113, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11246.14453125
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1445, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -11246.109375
tensor(11246.1074, grad_fn=<NegBackward0>) tensor(11246.1094, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.7118, 0.2882],
        [0.1880, 0.8120]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4530, 0.5470], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4058, 0.1012],
         [0.7249, 0.1993]],

        [[0.6820, 0.0918],
         [0.6593, 0.6346]],

        [[0.5574, 0.1057],
         [0.6545, 0.5927]],

        [[0.6952, 0.0922],
         [0.5720, 0.7267]],

        [[0.5489, 0.1036],
         [0.6470, 0.6216]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23120.919921875
inf tensor(23120.9199, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11867.5244140625
tensor(23120.9199, grad_fn=<NegBackward0>) tensor(11867.5244, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11848.6611328125
tensor(11867.5244, grad_fn=<NegBackward0>) tensor(11848.6611, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11548.46875
tensor(11848.6611, grad_fn=<NegBackward0>) tensor(11548.4688, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11457.3564453125
tensor(11548.4688, grad_fn=<NegBackward0>) tensor(11457.3564, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11437.673828125
tensor(11457.3564, grad_fn=<NegBackward0>) tensor(11437.6738, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11423.9619140625
tensor(11437.6738, grad_fn=<NegBackward0>) tensor(11423.9619, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11423.74609375
tensor(11423.9619, grad_fn=<NegBackward0>) tensor(11423.7461, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11423.59765625
tensor(11423.7461, grad_fn=<NegBackward0>) tensor(11423.5977, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11419.234375
tensor(11423.5977, grad_fn=<NegBackward0>) tensor(11419.2344, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11416.7841796875
tensor(11419.2344, grad_fn=<NegBackward0>) tensor(11416.7842, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11416.658203125
tensor(11416.7842, grad_fn=<NegBackward0>) tensor(11416.6582, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11410.1904296875
tensor(11416.6582, grad_fn=<NegBackward0>) tensor(11410.1904, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11410.1337890625
tensor(11410.1904, grad_fn=<NegBackward0>) tensor(11410.1338, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11410.1025390625
tensor(11410.1338, grad_fn=<NegBackward0>) tensor(11410.1025, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11410.08203125
tensor(11410.1025, grad_fn=<NegBackward0>) tensor(11410.0820, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11410.068359375
tensor(11410.0820, grad_fn=<NegBackward0>) tensor(11410.0684, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11410.0576171875
tensor(11410.0684, grad_fn=<NegBackward0>) tensor(11410.0576, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11410.0478515625
tensor(11410.0576, grad_fn=<NegBackward0>) tensor(11410.0479, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11410.041015625
tensor(11410.0479, grad_fn=<NegBackward0>) tensor(11410.0410, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11410.0322265625
tensor(11410.0410, grad_fn=<NegBackward0>) tensor(11410.0322, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11410.02734375
tensor(11410.0322, grad_fn=<NegBackward0>) tensor(11410.0273, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11409.361328125
tensor(11410.0273, grad_fn=<NegBackward0>) tensor(11409.3613, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11409.3046875
tensor(11409.3613, grad_fn=<NegBackward0>) tensor(11409.3047, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11409.298828125
tensor(11409.3047, grad_fn=<NegBackward0>) tensor(11409.2988, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11409.2958984375
tensor(11409.2988, grad_fn=<NegBackward0>) tensor(11409.2959, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11409.29296875
tensor(11409.2959, grad_fn=<NegBackward0>) tensor(11409.2930, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11409.291015625
tensor(11409.2930, grad_fn=<NegBackward0>) tensor(11409.2910, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11409.2861328125
tensor(11409.2910, grad_fn=<NegBackward0>) tensor(11409.2861, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11409.2861328125
tensor(11409.2861, grad_fn=<NegBackward0>) tensor(11409.2861, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11409.283203125
tensor(11409.2861, grad_fn=<NegBackward0>) tensor(11409.2832, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11409.28125
tensor(11409.2832, grad_fn=<NegBackward0>) tensor(11409.2812, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11409.2841796875
tensor(11409.2812, grad_fn=<NegBackward0>) tensor(11409.2842, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11409.279296875
tensor(11409.2812, grad_fn=<NegBackward0>) tensor(11409.2793, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11409.27734375
tensor(11409.2793, grad_fn=<NegBackward0>) tensor(11409.2773, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11409.279296875
tensor(11409.2773, grad_fn=<NegBackward0>) tensor(11409.2793, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11409.2763671875
tensor(11409.2773, grad_fn=<NegBackward0>) tensor(11409.2764, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11409.2744140625
tensor(11409.2764, grad_fn=<NegBackward0>) tensor(11409.2744, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11409.271484375
tensor(11409.2744, grad_fn=<NegBackward0>) tensor(11409.2715, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11409.1591796875
tensor(11409.2715, grad_fn=<NegBackward0>) tensor(11409.1592, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11409.1572265625
tensor(11409.1592, grad_fn=<NegBackward0>) tensor(11409.1572, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11409.15625
tensor(11409.1572, grad_fn=<NegBackward0>) tensor(11409.1562, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11409.1552734375
tensor(11409.1562, grad_fn=<NegBackward0>) tensor(11409.1553, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11409.1552734375
tensor(11409.1553, grad_fn=<NegBackward0>) tensor(11409.1553, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11409.1474609375
tensor(11409.1553, grad_fn=<NegBackward0>) tensor(11409.1475, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11408.998046875
tensor(11409.1475, grad_fn=<NegBackward0>) tensor(11408.9980, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11408.9970703125
tensor(11408.9980, grad_fn=<NegBackward0>) tensor(11408.9971, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11408.99609375
tensor(11408.9971, grad_fn=<NegBackward0>) tensor(11408.9961, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11408.998046875
tensor(11408.9961, grad_fn=<NegBackward0>) tensor(11408.9980, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11408.99609375
tensor(11408.9961, grad_fn=<NegBackward0>) tensor(11408.9961, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11409.00390625
tensor(11408.9961, grad_fn=<NegBackward0>) tensor(11409.0039, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11408.9951171875
tensor(11408.9961, grad_fn=<NegBackward0>) tensor(11408.9951, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11408.990234375
tensor(11408.9951, grad_fn=<NegBackward0>) tensor(11408.9902, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11408.98828125
tensor(11408.9902, grad_fn=<NegBackward0>) tensor(11408.9883, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11408.986328125
tensor(11408.9883, grad_fn=<NegBackward0>) tensor(11408.9863, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11408.9853515625
tensor(11408.9863, grad_fn=<NegBackward0>) tensor(11408.9854, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11408.986328125
tensor(11408.9854, grad_fn=<NegBackward0>) tensor(11408.9863, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11408.986328125
tensor(11408.9854, grad_fn=<NegBackward0>) tensor(11408.9863, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11408.9853515625
tensor(11408.9854, grad_fn=<NegBackward0>) tensor(11408.9854, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11408.9853515625
tensor(11408.9854, grad_fn=<NegBackward0>) tensor(11408.9854, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11408.9697265625
tensor(11408.9854, grad_fn=<NegBackward0>) tensor(11408.9697, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11408.96875
tensor(11408.9697, grad_fn=<NegBackward0>) tensor(11408.9688, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11408.9677734375
tensor(11408.9688, grad_fn=<NegBackward0>) tensor(11408.9678, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11408.96875
tensor(11408.9678, grad_fn=<NegBackward0>) tensor(11408.9688, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11408.9677734375
tensor(11408.9678, grad_fn=<NegBackward0>) tensor(11408.9678, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11408.9677734375
tensor(11408.9678, grad_fn=<NegBackward0>) tensor(11408.9678, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11408.966796875
tensor(11408.9678, grad_fn=<NegBackward0>) tensor(11408.9668, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11408.9677734375
tensor(11408.9668, grad_fn=<NegBackward0>) tensor(11408.9678, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11408.9677734375
tensor(11408.9668, grad_fn=<NegBackward0>) tensor(11408.9678, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11408.9658203125
tensor(11408.9668, grad_fn=<NegBackward0>) tensor(11408.9658, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11408.9658203125
tensor(11408.9658, grad_fn=<NegBackward0>) tensor(11408.9658, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11408.9658203125
tensor(11408.9658, grad_fn=<NegBackward0>) tensor(11408.9658, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11408.966796875
tensor(11408.9658, grad_fn=<NegBackward0>) tensor(11408.9668, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11408.9658203125
tensor(11408.9658, grad_fn=<NegBackward0>) tensor(11408.9658, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11408.984375
tensor(11408.9658, grad_fn=<NegBackward0>) tensor(11408.9844, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11408.9658203125
tensor(11408.9658, grad_fn=<NegBackward0>) tensor(11408.9658, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11408.96484375
tensor(11408.9658, grad_fn=<NegBackward0>) tensor(11408.9648, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11408.9658203125
tensor(11408.9648, grad_fn=<NegBackward0>) tensor(11408.9658, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11408.9677734375
tensor(11408.9648, grad_fn=<NegBackward0>) tensor(11408.9678, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11408.9638671875
tensor(11408.9648, grad_fn=<NegBackward0>) tensor(11408.9639, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11408.9677734375
tensor(11408.9639, grad_fn=<NegBackward0>) tensor(11408.9678, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11408.96484375
tensor(11408.9639, grad_fn=<NegBackward0>) tensor(11408.9648, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11408.958984375
tensor(11408.9639, grad_fn=<NegBackward0>) tensor(11408.9590, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11408.9375
tensor(11408.9590, grad_fn=<NegBackward0>) tensor(11408.9375, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11408.9375
tensor(11408.9375, grad_fn=<NegBackward0>) tensor(11408.9375, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11408.939453125
tensor(11408.9375, grad_fn=<NegBackward0>) tensor(11408.9395, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11408.9375
tensor(11408.9375, grad_fn=<NegBackward0>) tensor(11408.9375, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11408.9384765625
tensor(11408.9375, grad_fn=<NegBackward0>) tensor(11408.9385, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11408.953125
tensor(11408.9375, grad_fn=<NegBackward0>) tensor(11408.9531, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11408.9365234375
tensor(11408.9375, grad_fn=<NegBackward0>) tensor(11408.9365, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11408.943359375
tensor(11408.9365, grad_fn=<NegBackward0>) tensor(11408.9434, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11396.6044921875
tensor(11408.9365, grad_fn=<NegBackward0>) tensor(11396.6045, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11395.8701171875
tensor(11396.6045, grad_fn=<NegBackward0>) tensor(11395.8701, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11395.7587890625
tensor(11395.8701, grad_fn=<NegBackward0>) tensor(11395.7588, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11395.6357421875
tensor(11395.7588, grad_fn=<NegBackward0>) tensor(11395.6357, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11395.587890625
tensor(11395.6357, grad_fn=<NegBackward0>) tensor(11395.5879, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11395.5849609375
tensor(11395.5879, grad_fn=<NegBackward0>) tensor(11395.5850, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11395.525390625
tensor(11395.5850, grad_fn=<NegBackward0>) tensor(11395.5254, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11395.5234375
tensor(11395.5254, grad_fn=<NegBackward0>) tensor(11395.5234, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11395.5234375
tensor(11395.5234, grad_fn=<NegBackward0>) tensor(11395.5234, grad_fn=<NegBackward0>)
pi: tensor([[0.4634, 0.5366],
        [0.3289, 0.6711]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4551, 0.5449], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3959, 0.1007],
         [0.7295, 0.2029]],

        [[0.5649, 0.1069],
         [0.6005, 0.6724]],

        [[0.5484, 0.1060],
         [0.7210, 0.7246]],

        [[0.6460, 0.0922],
         [0.5474, 0.6508]],

        [[0.5067, 0.1034],
         [0.6930, 0.6713]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.05792931793547351
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5461747585661005
Average Adjusted Rand Index: 0.8035858635870948
[1.0, 0.5461747585661005] [1.0, 0.8035858635870948] [11246.109375, 11395.5234375]
-------------------------------------
This iteration is 88
True Objective function: Loss = -11652.978247567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21507.20703125
inf tensor(21507.2070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12401.908203125
tensor(21507.2070, grad_fn=<NegBackward0>) tensor(12401.9082, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12242.1591796875
tensor(12401.9082, grad_fn=<NegBackward0>) tensor(12242.1592, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11964.4072265625
tensor(12242.1592, grad_fn=<NegBackward0>) tensor(11964.4072, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11925.9296875
tensor(11964.4072, grad_fn=<NegBackward0>) tensor(11925.9297, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11922.83203125
tensor(11925.9297, grad_fn=<NegBackward0>) tensor(11922.8320, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11921.84375
tensor(11922.8320, grad_fn=<NegBackward0>) tensor(11921.8438, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11910.1806640625
tensor(11921.8438, grad_fn=<NegBackward0>) tensor(11910.1807, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11910.12890625
tensor(11910.1807, grad_fn=<NegBackward0>) tensor(11910.1289, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11910.0947265625
tensor(11910.1289, grad_fn=<NegBackward0>) tensor(11910.0947, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11910.0712890625
tensor(11910.0947, grad_fn=<NegBackward0>) tensor(11910.0713, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11910.05078125
tensor(11910.0713, grad_fn=<NegBackward0>) tensor(11910.0508, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11910.03125
tensor(11910.0508, grad_fn=<NegBackward0>) tensor(11910.0312, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11910.01953125
tensor(11910.0312, grad_fn=<NegBackward0>) tensor(11910.0195, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11910.009765625
tensor(11910.0195, grad_fn=<NegBackward0>) tensor(11910.0098, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11909.9921875
tensor(11910.0098, grad_fn=<NegBackward0>) tensor(11909.9922, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11909.9560546875
tensor(11909.9922, grad_fn=<NegBackward0>) tensor(11909.9561, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11909.951171875
tensor(11909.9561, grad_fn=<NegBackward0>) tensor(11909.9512, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11909.9462890625
tensor(11909.9512, grad_fn=<NegBackward0>) tensor(11909.9463, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11909.9443359375
tensor(11909.9463, grad_fn=<NegBackward0>) tensor(11909.9443, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11909.94140625
tensor(11909.9443, grad_fn=<NegBackward0>) tensor(11909.9414, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11909.9375
tensor(11909.9414, grad_fn=<NegBackward0>) tensor(11909.9375, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11909.9365234375
tensor(11909.9375, grad_fn=<NegBackward0>) tensor(11909.9365, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11909.9345703125
tensor(11909.9365, grad_fn=<NegBackward0>) tensor(11909.9346, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11909.931640625
tensor(11909.9346, grad_fn=<NegBackward0>) tensor(11909.9316, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11909.931640625
tensor(11909.9316, grad_fn=<NegBackward0>) tensor(11909.9316, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11909.9296875
tensor(11909.9316, grad_fn=<NegBackward0>) tensor(11909.9297, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11909.9287109375
tensor(11909.9297, grad_fn=<NegBackward0>) tensor(11909.9287, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11909.927734375
tensor(11909.9287, grad_fn=<NegBackward0>) tensor(11909.9277, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11909.92578125
tensor(11909.9277, grad_fn=<NegBackward0>) tensor(11909.9258, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11909.9248046875
tensor(11909.9258, grad_fn=<NegBackward0>) tensor(11909.9248, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11909.923828125
tensor(11909.9248, grad_fn=<NegBackward0>) tensor(11909.9238, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11909.923828125
tensor(11909.9238, grad_fn=<NegBackward0>) tensor(11909.9238, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11909.921875
tensor(11909.9238, grad_fn=<NegBackward0>) tensor(11909.9219, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11909.9208984375
tensor(11909.9219, grad_fn=<NegBackward0>) tensor(11909.9209, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11909.921875
tensor(11909.9209, grad_fn=<NegBackward0>) tensor(11909.9219, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11909.919921875
tensor(11909.9209, grad_fn=<NegBackward0>) tensor(11909.9199, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11909.9169921875
tensor(11909.9199, grad_fn=<NegBackward0>) tensor(11909.9170, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11909.9189453125
tensor(11909.9170, grad_fn=<NegBackward0>) tensor(11909.9189, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11909.9169921875
tensor(11909.9170, grad_fn=<NegBackward0>) tensor(11909.9170, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11909.91796875
tensor(11909.9170, grad_fn=<NegBackward0>) tensor(11909.9180, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11909.9169921875
tensor(11909.9170, grad_fn=<NegBackward0>) tensor(11909.9170, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11909.9169921875
tensor(11909.9170, grad_fn=<NegBackward0>) tensor(11909.9170, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11909.916015625
tensor(11909.9170, grad_fn=<NegBackward0>) tensor(11909.9160, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11909.916015625
tensor(11909.9160, grad_fn=<NegBackward0>) tensor(11909.9160, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11909.931640625
tensor(11909.9160, grad_fn=<NegBackward0>) tensor(11909.9316, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11909.9150390625
tensor(11909.9160, grad_fn=<NegBackward0>) tensor(11909.9150, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11909.9140625
tensor(11909.9150, grad_fn=<NegBackward0>) tensor(11909.9141, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11909.9150390625
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9150, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11909.9150390625
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9150, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11909.9150390625
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9150, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11909.9140625
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9141, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11909.9150390625
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9150, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11909.9150390625
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9150, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11909.9140625
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9141, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11909.9130859375
tensor(11909.9141, grad_fn=<NegBackward0>) tensor(11909.9131, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11909.9208984375
tensor(11909.9131, grad_fn=<NegBackward0>) tensor(11909.9209, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11909.912109375
tensor(11909.9131, grad_fn=<NegBackward0>) tensor(11909.9121, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11909.921875
tensor(11909.9121, grad_fn=<NegBackward0>) tensor(11909.9219, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11909.912109375
tensor(11909.9121, grad_fn=<NegBackward0>) tensor(11909.9121, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11907.9580078125
tensor(11909.9121, grad_fn=<NegBackward0>) tensor(11907.9580, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11907.9462890625
tensor(11907.9580, grad_fn=<NegBackward0>) tensor(11907.9463, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11907.9140625
tensor(11907.9463, grad_fn=<NegBackward0>) tensor(11907.9141, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11907.9091796875
tensor(11907.9141, grad_fn=<NegBackward0>) tensor(11907.9092, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11907.896484375
tensor(11907.9092, grad_fn=<NegBackward0>) tensor(11907.8965, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11907.89453125
tensor(11907.8965, grad_fn=<NegBackward0>) tensor(11907.8945, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11907.8662109375
tensor(11907.8945, grad_fn=<NegBackward0>) tensor(11907.8662, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11907.865234375
tensor(11907.8662, grad_fn=<NegBackward0>) tensor(11907.8652, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11907.86328125
tensor(11907.8652, grad_fn=<NegBackward0>) tensor(11907.8633, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11905.4931640625
tensor(11907.8633, grad_fn=<NegBackward0>) tensor(11905.4932, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11905.4921875
tensor(11905.4932, grad_fn=<NegBackward0>) tensor(11905.4922, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11905.4892578125
tensor(11905.4922, grad_fn=<NegBackward0>) tensor(11905.4893, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11905.4892578125
tensor(11905.4893, grad_fn=<NegBackward0>) tensor(11905.4893, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11905.48828125
tensor(11905.4893, grad_fn=<NegBackward0>) tensor(11905.4883, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11905.490234375
tensor(11905.4883, grad_fn=<NegBackward0>) tensor(11905.4902, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11905.490234375
tensor(11905.4883, grad_fn=<NegBackward0>) tensor(11905.4902, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11905.5087890625
tensor(11905.4883, grad_fn=<NegBackward0>) tensor(11905.5088, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11905.185546875
tensor(11905.4883, grad_fn=<NegBackward0>) tensor(11905.1855, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11905.2021484375
tensor(11905.1855, grad_fn=<NegBackward0>) tensor(11905.2021, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11905.1865234375
tensor(11905.1855, grad_fn=<NegBackward0>) tensor(11905.1865, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11905.1865234375
tensor(11905.1855, grad_fn=<NegBackward0>) tensor(11905.1865, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11905.185546875
tensor(11905.1855, grad_fn=<NegBackward0>) tensor(11905.1855, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11905.1875
tensor(11905.1855, grad_fn=<NegBackward0>) tensor(11905.1875, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11905.1845703125
tensor(11905.1855, grad_fn=<NegBackward0>) tensor(11905.1846, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11905.185546875
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11905.1855, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11905.1845703125
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11905.1846, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11905.201171875
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11905.2012, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11905.185546875
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11905.1855, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11905.1865234375
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11905.1865, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11905.1845703125
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11905.1846, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11905.1845703125
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11905.1846, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11901.55078125
tensor(11905.1846, grad_fn=<NegBackward0>) tensor(11901.5508, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11901.68359375
tensor(11901.5508, grad_fn=<NegBackward0>) tensor(11901.6836, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11901.5068359375
tensor(11901.5508, grad_fn=<NegBackward0>) tensor(11901.5068, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11901.4931640625
tensor(11901.5068, grad_fn=<NegBackward0>) tensor(11901.4932, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11901.490234375
tensor(11901.4932, grad_fn=<NegBackward0>) tensor(11901.4902, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11901.4912109375
tensor(11901.4902, grad_fn=<NegBackward0>) tensor(11901.4912, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11901.490234375
tensor(11901.4902, grad_fn=<NegBackward0>) tensor(11901.4902, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11901.5341796875
tensor(11901.4902, grad_fn=<NegBackward0>) tensor(11901.5342, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11901.263671875
tensor(11901.4902, grad_fn=<NegBackward0>) tensor(11901.2637, grad_fn=<NegBackward0>)
pi: tensor([[0.3293, 0.6707],
        [0.8340, 0.1660]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4325, 0.5675], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2650, 0.1084],
         [0.6189, 0.3439]],

        [[0.5686, 0.1023],
         [0.6582, 0.5423]],

        [[0.5346, 0.0980],
         [0.5507, 0.6056]],

        [[0.5247, 0.1018],
         [0.6665, 0.6818]],

        [[0.6258, 0.0998],
         [0.7224, 0.7186]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.772151675588645
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6691665204565022
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.06163219882070814
Average Adjusted Rand Index: 0.8724252553706456
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22264.435546875
inf tensor(22264.4355, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12367.7626953125
tensor(22264.4355, grad_fn=<NegBackward0>) tensor(12367.7627, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12306.4794921875
tensor(12367.7627, grad_fn=<NegBackward0>) tensor(12306.4795, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11825.46875
tensor(12306.4795, grad_fn=<NegBackward0>) tensor(11825.4688, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11705.8857421875
tensor(11825.4688, grad_fn=<NegBackward0>) tensor(11705.8857, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11696.8486328125
tensor(11705.8857, grad_fn=<NegBackward0>) tensor(11696.8486, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11680.9716796875
tensor(11696.8486, grad_fn=<NegBackward0>) tensor(11680.9717, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11667.2822265625
tensor(11680.9717, grad_fn=<NegBackward0>) tensor(11667.2822, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11667.2041015625
tensor(11667.2822, grad_fn=<NegBackward0>) tensor(11667.2041, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11665.5419921875
tensor(11667.2041, grad_fn=<NegBackward0>) tensor(11665.5420, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11665.5166015625
tensor(11665.5420, grad_fn=<NegBackward0>) tensor(11665.5166, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11665.4990234375
tensor(11665.5166, grad_fn=<NegBackward0>) tensor(11665.4990, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11665.486328125
tensor(11665.4990, grad_fn=<NegBackward0>) tensor(11665.4863, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11665.4736328125
tensor(11665.4863, grad_fn=<NegBackward0>) tensor(11665.4736, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11662.1220703125
tensor(11665.4736, grad_fn=<NegBackward0>) tensor(11662.1221, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11662.111328125
tensor(11662.1221, grad_fn=<NegBackward0>) tensor(11662.1113, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11662.10546875
tensor(11662.1113, grad_fn=<NegBackward0>) tensor(11662.1055, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11662.099609375
tensor(11662.1055, grad_fn=<NegBackward0>) tensor(11662.0996, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11662.095703125
tensor(11662.0996, grad_fn=<NegBackward0>) tensor(11662.0957, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11662.091796875
tensor(11662.0957, grad_fn=<NegBackward0>) tensor(11662.0918, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11662.0888671875
tensor(11662.0918, grad_fn=<NegBackward0>) tensor(11662.0889, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11662.0849609375
tensor(11662.0889, grad_fn=<NegBackward0>) tensor(11662.0850, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11662.0830078125
tensor(11662.0850, grad_fn=<NegBackward0>) tensor(11662.0830, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11653.0078125
tensor(11662.0830, grad_fn=<NegBackward0>) tensor(11653.0078, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11652.99609375
tensor(11653.0078, grad_fn=<NegBackward0>) tensor(11652.9961, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11647.39453125
tensor(11652.9961, grad_fn=<NegBackward0>) tensor(11647.3945, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11647.3916015625
tensor(11647.3945, grad_fn=<NegBackward0>) tensor(11647.3916, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11647.3896484375
tensor(11647.3916, grad_fn=<NegBackward0>) tensor(11647.3896, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11647.3876953125
tensor(11647.3896, grad_fn=<NegBackward0>) tensor(11647.3877, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11647.38671875
tensor(11647.3877, grad_fn=<NegBackward0>) tensor(11647.3867, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11647.38671875
tensor(11647.3867, grad_fn=<NegBackward0>) tensor(11647.3867, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11647.384765625
tensor(11647.3867, grad_fn=<NegBackward0>) tensor(11647.3848, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11647.3837890625
tensor(11647.3848, grad_fn=<NegBackward0>) tensor(11647.3838, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11647.3828125
tensor(11647.3838, grad_fn=<NegBackward0>) tensor(11647.3828, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11647.3818359375
tensor(11647.3828, grad_fn=<NegBackward0>) tensor(11647.3818, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11647.3818359375
tensor(11647.3818, grad_fn=<NegBackward0>) tensor(11647.3818, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11647.3798828125
tensor(11647.3818, grad_fn=<NegBackward0>) tensor(11647.3799, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11647.078125
tensor(11647.3799, grad_fn=<NegBackward0>) tensor(11647.0781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11647.0771484375
tensor(11647.0781, grad_fn=<NegBackward0>) tensor(11647.0771, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11647.076171875
tensor(11647.0771, grad_fn=<NegBackward0>) tensor(11647.0762, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11647.076171875
tensor(11647.0762, grad_fn=<NegBackward0>) tensor(11647.0762, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11647.076171875
tensor(11647.0762, grad_fn=<NegBackward0>) tensor(11647.0762, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11647.0751953125
tensor(11647.0762, grad_fn=<NegBackward0>) tensor(11647.0752, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11647.0771484375
tensor(11647.0752, grad_fn=<NegBackward0>) tensor(11647.0771, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11647.07421875
tensor(11647.0752, grad_fn=<NegBackward0>) tensor(11647.0742, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11647.0751953125
tensor(11647.0742, grad_fn=<NegBackward0>) tensor(11647.0752, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11647.07421875
tensor(11647.0742, grad_fn=<NegBackward0>) tensor(11647.0742, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11647.07421875
tensor(11647.0742, grad_fn=<NegBackward0>) tensor(11647.0742, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11647.072265625
tensor(11647.0742, grad_fn=<NegBackward0>) tensor(11647.0723, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11647.072265625
tensor(11647.0723, grad_fn=<NegBackward0>) tensor(11647.0723, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11647.076171875
tensor(11647.0723, grad_fn=<NegBackward0>) tensor(11647.0762, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11647.07421875
tensor(11647.0723, grad_fn=<NegBackward0>) tensor(11647.0742, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11647.0712890625
tensor(11647.0723, grad_fn=<NegBackward0>) tensor(11647.0713, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11647.0732421875
tensor(11647.0713, grad_fn=<NegBackward0>) tensor(11647.0732, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11647.0712890625
tensor(11647.0713, grad_fn=<NegBackward0>) tensor(11647.0713, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11647.0703125
tensor(11647.0713, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11647.0703125
tensor(11647.0703, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11647.07421875
tensor(11647.0703, grad_fn=<NegBackward0>) tensor(11647.0742, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11647.080078125
tensor(11647.0703, grad_fn=<NegBackward0>) tensor(11647.0801, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11647.0703125
tensor(11647.0703, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11647.0693359375
tensor(11647.0703, grad_fn=<NegBackward0>) tensor(11647.0693, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11647.0703125
tensor(11647.0693, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11647.0703125
tensor(11647.0693, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11647.0712890625
tensor(11647.0693, grad_fn=<NegBackward0>) tensor(11647.0713, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11647.0693359375
tensor(11647.0693, grad_fn=<NegBackward0>) tensor(11647.0693, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11647.0703125
tensor(11647.0693, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11647.068359375
tensor(11647.0693, grad_fn=<NegBackward0>) tensor(11647.0684, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11647.0703125
tensor(11647.0684, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11647.068359375
tensor(11647.0684, grad_fn=<NegBackward0>) tensor(11647.0684, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11647.0712890625
tensor(11647.0684, grad_fn=<NegBackward0>) tensor(11647.0713, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11647.0703125
tensor(11647.0684, grad_fn=<NegBackward0>) tensor(11647.0703, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11647.0712890625
tensor(11647.0684, grad_fn=<NegBackward0>) tensor(11647.0713, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11647.0693359375
tensor(11647.0684, grad_fn=<NegBackward0>) tensor(11647.0693, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11647.076171875
tensor(11647.0684, grad_fn=<NegBackward0>) tensor(11647.0762, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7672, 0.2328],
        [0.2708, 0.7292]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4467, 0.5533], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2056, 0.1104],
         [0.7057, 0.3989]],

        [[0.5225, 0.1006],
         [0.5037, 0.6275]],

        [[0.5959, 0.0983],
         [0.5626, 0.6817]],

        [[0.7305, 0.0997],
         [0.5435, 0.6296]],

        [[0.5229, 0.1003],
         [0.7115, 0.6502]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9919999681285151
Average Adjusted Rand Index: 0.9919995611635631
[0.06163219882070814, 0.9919999681285151] [0.8724252553706456, 0.9919995611635631] [11886.33984375, 11647.076171875]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11650.353001281203
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20161.099609375
inf tensor(20161.0996, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12344.69140625
tensor(20161.0996, grad_fn=<NegBackward0>) tensor(12344.6914, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11690.712890625
tensor(12344.6914, grad_fn=<NegBackward0>) tensor(11690.7129, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11660.0341796875
tensor(11690.7129, grad_fn=<NegBackward0>) tensor(11660.0342, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11654.9990234375
tensor(11660.0342, grad_fn=<NegBackward0>) tensor(11654.9990, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11653.828125
tensor(11654.9990, grad_fn=<NegBackward0>) tensor(11653.8281, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11653.654296875
tensor(11653.8281, grad_fn=<NegBackward0>) tensor(11653.6543, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11647.4931640625
tensor(11653.6543, grad_fn=<NegBackward0>) tensor(11647.4932, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11647.4013671875
tensor(11647.4932, grad_fn=<NegBackward0>) tensor(11647.4014, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11647.35546875
tensor(11647.4014, grad_fn=<NegBackward0>) tensor(11647.3555, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11647.318359375
tensor(11647.3555, grad_fn=<NegBackward0>) tensor(11647.3184, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11647.291015625
tensor(11647.3184, grad_fn=<NegBackward0>) tensor(11647.2910, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11647.2705078125
tensor(11647.2910, grad_fn=<NegBackward0>) tensor(11647.2705, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11647.2529296875
tensor(11647.2705, grad_fn=<NegBackward0>) tensor(11647.2529, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11647.240234375
tensor(11647.2529, grad_fn=<NegBackward0>) tensor(11647.2402, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11647.22265625
tensor(11647.2402, grad_fn=<NegBackward0>) tensor(11647.2227, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11647.1767578125
tensor(11647.2227, grad_fn=<NegBackward0>) tensor(11647.1768, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11644.3916015625
tensor(11647.1768, grad_fn=<NegBackward0>) tensor(11644.3916, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11641.8681640625
tensor(11644.3916, grad_fn=<NegBackward0>) tensor(11641.8682, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11641.861328125
tensor(11641.8682, grad_fn=<NegBackward0>) tensor(11641.8613, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11641.853515625
tensor(11641.8613, grad_fn=<NegBackward0>) tensor(11641.8535, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11641.8466796875
tensor(11641.8535, grad_fn=<NegBackward0>) tensor(11641.8467, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11641.8369140625
tensor(11641.8467, grad_fn=<NegBackward0>) tensor(11641.8369, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11641.783203125
tensor(11641.8369, grad_fn=<NegBackward0>) tensor(11641.7832, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11641.7353515625
tensor(11641.7832, grad_fn=<NegBackward0>) tensor(11641.7354, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11641.7333984375
tensor(11641.7354, grad_fn=<NegBackward0>) tensor(11641.7334, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11641.73046875
tensor(11641.7334, grad_fn=<NegBackward0>) tensor(11641.7305, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11641.7265625
tensor(11641.7305, grad_fn=<NegBackward0>) tensor(11641.7266, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11641.7255859375
tensor(11641.7266, grad_fn=<NegBackward0>) tensor(11641.7256, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11641.7236328125
tensor(11641.7256, grad_fn=<NegBackward0>) tensor(11641.7236, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11641.72265625
tensor(11641.7236, grad_fn=<NegBackward0>) tensor(11641.7227, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11641.720703125
tensor(11641.7227, grad_fn=<NegBackward0>) tensor(11641.7207, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11641.71875
tensor(11641.7207, grad_fn=<NegBackward0>) tensor(11641.7188, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11641.7177734375
tensor(11641.7188, grad_fn=<NegBackward0>) tensor(11641.7178, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11641.716796875
tensor(11641.7178, grad_fn=<NegBackward0>) tensor(11641.7168, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11641.7158203125
tensor(11641.7168, grad_fn=<NegBackward0>) tensor(11641.7158, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11641.71484375
tensor(11641.7158, grad_fn=<NegBackward0>) tensor(11641.7148, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11641.7138671875
tensor(11641.7148, grad_fn=<NegBackward0>) tensor(11641.7139, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11641.71484375
tensor(11641.7139, grad_fn=<NegBackward0>) tensor(11641.7148, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11641.712890625
tensor(11641.7139, grad_fn=<NegBackward0>) tensor(11641.7129, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11641.71484375
tensor(11641.7129, grad_fn=<NegBackward0>) tensor(11641.7148, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11641.7119140625
tensor(11641.7129, grad_fn=<NegBackward0>) tensor(11641.7119, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11641.7099609375
tensor(11641.7119, grad_fn=<NegBackward0>) tensor(11641.7100, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11641.7109375
tensor(11641.7100, grad_fn=<NegBackward0>) tensor(11641.7109, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11641.708984375
tensor(11641.7100, grad_fn=<NegBackward0>) tensor(11641.7090, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11641.7080078125
tensor(11641.7090, grad_fn=<NegBackward0>) tensor(11641.7080, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11641.708984375
tensor(11641.7080, grad_fn=<NegBackward0>) tensor(11641.7090, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11641.708984375
tensor(11641.7080, grad_fn=<NegBackward0>) tensor(11641.7090, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11641.7197265625
tensor(11641.7080, grad_fn=<NegBackward0>) tensor(11641.7197, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11641.70703125
tensor(11641.7080, grad_fn=<NegBackward0>) tensor(11641.7070, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11641.70703125
tensor(11641.7070, grad_fn=<NegBackward0>) tensor(11641.7070, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11641.70703125
tensor(11641.7070, grad_fn=<NegBackward0>) tensor(11641.7070, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11641.7060546875
tensor(11641.7070, grad_fn=<NegBackward0>) tensor(11641.7061, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11641.7060546875
tensor(11641.7061, grad_fn=<NegBackward0>) tensor(11641.7061, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11641.7060546875
tensor(11641.7061, grad_fn=<NegBackward0>) tensor(11641.7061, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11641.7080078125
tensor(11641.7061, grad_fn=<NegBackward0>) tensor(11641.7080, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11641.7119140625
tensor(11641.7061, grad_fn=<NegBackward0>) tensor(11641.7119, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11641.705078125
tensor(11641.7061, grad_fn=<NegBackward0>) tensor(11641.7051, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11641.705078125
tensor(11641.7051, grad_fn=<NegBackward0>) tensor(11641.7051, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11641.708984375
tensor(11641.7051, grad_fn=<NegBackward0>) tensor(11641.7090, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11641.7060546875
tensor(11641.7051, grad_fn=<NegBackward0>) tensor(11641.7061, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -11641.7060546875
tensor(11641.7051, grad_fn=<NegBackward0>) tensor(11641.7061, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -11641.701171875
tensor(11641.7051, grad_fn=<NegBackward0>) tensor(11641.7012, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11641.701171875
tensor(11641.7012, grad_fn=<NegBackward0>) tensor(11641.7012, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11641.703125
tensor(11641.7012, grad_fn=<NegBackward0>) tensor(11641.7031, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11641.7001953125
tensor(11641.7012, grad_fn=<NegBackward0>) tensor(11641.7002, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11641.701171875
tensor(11641.7002, grad_fn=<NegBackward0>) tensor(11641.7012, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11641.701171875
tensor(11641.7002, grad_fn=<NegBackward0>) tensor(11641.7012, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11641.701171875
tensor(11641.7002, grad_fn=<NegBackward0>) tensor(11641.7012, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11641.701171875
tensor(11641.7002, grad_fn=<NegBackward0>) tensor(11641.7012, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -11641.7001953125
tensor(11641.7002, grad_fn=<NegBackward0>) tensor(11641.7002, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11641.701171875
tensor(11641.7002, grad_fn=<NegBackward0>) tensor(11641.7012, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11641.69921875
tensor(11641.7002, grad_fn=<NegBackward0>) tensor(11641.6992, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11641.69921875
tensor(11641.6992, grad_fn=<NegBackward0>) tensor(11641.6992, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11639.84375
tensor(11641.6992, grad_fn=<NegBackward0>) tensor(11639.8438, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11639.849609375
tensor(11639.8438, grad_fn=<NegBackward0>) tensor(11639.8496, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11639.84375
tensor(11639.8438, grad_fn=<NegBackward0>) tensor(11639.8438, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11639.83984375
tensor(11639.8438, grad_fn=<NegBackward0>) tensor(11639.8398, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11639.83984375
tensor(11639.8398, grad_fn=<NegBackward0>) tensor(11639.8398, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11639.861328125
tensor(11639.8398, grad_fn=<NegBackward0>) tensor(11639.8613, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11639.8447265625
tensor(11639.8398, grad_fn=<NegBackward0>) tensor(11639.8447, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11639.837890625
tensor(11639.8398, grad_fn=<NegBackward0>) tensor(11639.8379, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11639.9833984375
tensor(11639.8379, grad_fn=<NegBackward0>) tensor(11639.9834, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11639.8330078125
tensor(11639.8379, grad_fn=<NegBackward0>) tensor(11639.8330, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11639.7216796875
tensor(11639.8330, grad_fn=<NegBackward0>) tensor(11639.7217, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11639.716796875
tensor(11639.7217, grad_fn=<NegBackward0>) tensor(11639.7168, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11639.720703125
tensor(11639.7168, grad_fn=<NegBackward0>) tensor(11639.7207, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11639.716796875
tensor(11639.7168, grad_fn=<NegBackward0>) tensor(11639.7168, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11639.7158203125
tensor(11639.7168, grad_fn=<NegBackward0>) tensor(11639.7158, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11639.7333984375
tensor(11639.7158, grad_fn=<NegBackward0>) tensor(11639.7334, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11639.7724609375
tensor(11639.7158, grad_fn=<NegBackward0>) tensor(11639.7725, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11639.7138671875
tensor(11639.7158, grad_fn=<NegBackward0>) tensor(11639.7139, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11639.7138671875
tensor(11639.7139, grad_fn=<NegBackward0>) tensor(11639.7139, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11639.8017578125
tensor(11639.7139, grad_fn=<NegBackward0>) tensor(11639.8018, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11639.71484375
tensor(11639.7139, grad_fn=<NegBackward0>) tensor(11639.7148, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11639.720703125
tensor(11639.7139, grad_fn=<NegBackward0>) tensor(11639.7207, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -11639.71875
tensor(11639.7139, grad_fn=<NegBackward0>) tensor(11639.7188, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -11639.8984375
tensor(11639.7139, grad_fn=<NegBackward0>) tensor(11639.8984, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7395, 0.2605],
        [0.3095, 0.6905]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4084, 0.5916], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2016, 0.0919],
         [0.6243, 0.3857]],

        [[0.5789, 0.1073],
         [0.6643, 0.7071]],

        [[0.5280, 0.0960],
         [0.6776, 0.5229]],

        [[0.6449, 0.1049],
         [0.5666, 0.6460]],

        [[0.5095, 0.1029],
         [0.5773, 0.5632]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23172.48828125
inf tensor(23172.4883, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11957.6513671875
tensor(23172.4883, grad_fn=<NegBackward0>) tensor(11957.6514, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11781.623046875
tensor(11957.6514, grad_fn=<NegBackward0>) tensor(11781.6230, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11772.287109375
tensor(11781.6230, grad_fn=<NegBackward0>) tensor(11772.2871, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11772.1708984375
tensor(11772.2871, grad_fn=<NegBackward0>) tensor(11772.1709, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11772.11328125
tensor(11772.1709, grad_fn=<NegBackward0>) tensor(11772.1133, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11772.08203125
tensor(11772.1133, grad_fn=<NegBackward0>) tensor(11772.0820, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11772.0595703125
tensor(11772.0820, grad_fn=<NegBackward0>) tensor(11772.0596, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11772.04296875
tensor(11772.0596, grad_fn=<NegBackward0>) tensor(11772.0430, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11772.03515625
tensor(11772.0430, grad_fn=<NegBackward0>) tensor(11772.0352, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11772.025390625
tensor(11772.0352, grad_fn=<NegBackward0>) tensor(11772.0254, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11772.0185546875
tensor(11772.0254, grad_fn=<NegBackward0>) tensor(11772.0186, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11772.0146484375
tensor(11772.0186, grad_fn=<NegBackward0>) tensor(11772.0146, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11772.0107421875
tensor(11772.0146, grad_fn=<NegBackward0>) tensor(11772.0107, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11772.0068359375
tensor(11772.0107, grad_fn=<NegBackward0>) tensor(11772.0068, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11772.005859375
tensor(11772.0068, grad_fn=<NegBackward0>) tensor(11772.0059, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11772.00390625
tensor(11772.0059, grad_fn=<NegBackward0>) tensor(11772.0039, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11772.001953125
tensor(11772.0039, grad_fn=<NegBackward0>) tensor(11772.0020, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11772.0
tensor(11772.0020, grad_fn=<NegBackward0>) tensor(11772., grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11771.998046875
tensor(11772., grad_fn=<NegBackward0>) tensor(11771.9980, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11771.998046875
tensor(11771.9980, grad_fn=<NegBackward0>) tensor(11771.9980, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11771.9951171875
tensor(11771.9980, grad_fn=<NegBackward0>) tensor(11771.9951, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11771.9951171875
tensor(11771.9951, grad_fn=<NegBackward0>) tensor(11771.9951, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11771.9951171875
tensor(11771.9951, grad_fn=<NegBackward0>) tensor(11771.9951, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11771.994140625
tensor(11771.9951, grad_fn=<NegBackward0>) tensor(11771.9941, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11771.9931640625
tensor(11771.9941, grad_fn=<NegBackward0>) tensor(11771.9932, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11771.9912109375
tensor(11771.9932, grad_fn=<NegBackward0>) tensor(11771.9912, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11771.994140625
tensor(11771.9912, grad_fn=<NegBackward0>) tensor(11771.9941, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11771.9912109375
tensor(11771.9912, grad_fn=<NegBackward0>) tensor(11771.9912, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11771.990234375
tensor(11771.9912, grad_fn=<NegBackward0>) tensor(11771.9902, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11771.990234375
tensor(11771.9902, grad_fn=<NegBackward0>) tensor(11771.9902, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11771.990234375
tensor(11771.9902, grad_fn=<NegBackward0>) tensor(11771.9902, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11771.9892578125
tensor(11771.9902, grad_fn=<NegBackward0>) tensor(11771.9893, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11771.990234375
tensor(11771.9893, grad_fn=<NegBackward0>) tensor(11771.9902, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11771.9873046875
tensor(11771.9893, grad_fn=<NegBackward0>) tensor(11771.9873, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11771.98828125
tensor(11771.9873, grad_fn=<NegBackward0>) tensor(11771.9883, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11771.98828125
tensor(11771.9873, grad_fn=<NegBackward0>) tensor(11771.9883, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -11771.9873046875
tensor(11771.9873, grad_fn=<NegBackward0>) tensor(11771.9873, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11771.98828125
tensor(11771.9873, grad_fn=<NegBackward0>) tensor(11771.9883, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11771.98828125
tensor(11771.9873, grad_fn=<NegBackward0>) tensor(11771.9883, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11771.9853515625
tensor(11771.9873, grad_fn=<NegBackward0>) tensor(11771.9854, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11771.9873046875
tensor(11771.9854, grad_fn=<NegBackward0>) tensor(11771.9873, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11771.986328125
tensor(11771.9854, grad_fn=<NegBackward0>) tensor(11771.9863, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11771.9912109375
tensor(11771.9854, grad_fn=<NegBackward0>) tensor(11771.9912, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -11771.9873046875
tensor(11771.9854, grad_fn=<NegBackward0>) tensor(11771.9873, grad_fn=<NegBackward0>)
4
Iteration 4500: Loss = -11771.986328125
tensor(11771.9854, grad_fn=<NegBackward0>) tensor(11771.9863, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4500 due to no improvement.
pi: tensor([[0.5015, 0.4985],
        [0.3645, 0.6355]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5949, 0.4051], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3805, 0.0914],
         [0.5172, 0.2205]],

        [[0.5155, 0.0998],
         [0.7191, 0.5653]],

        [[0.6966, 0.0952],
         [0.5563, 0.6050]],

        [[0.6423, 0.1055],
         [0.5897, 0.7233]],

        [[0.6829, 0.1032],
         [0.7097, 0.6772]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 74
Adjusted Rand Index: 0.22444941185092895
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
Global Adjusted Rand Index: 0.47782273746754195
Average Adjusted Rand Index: 0.8210450166831522
[0.9919999740011123, 0.47782273746754195] [0.9919993417272899, 0.8210450166831522] [11639.8984375, 11771.986328125]
-------------------------------------
This iteration is 90
True Objective function: Loss = -11447.335484727699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21946.3828125
inf tensor(21946.3828, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12205.984375
tensor(21946.3828, grad_fn=<NegBackward0>) tensor(12205.9844, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12061.4794921875
tensor(12205.9844, grad_fn=<NegBackward0>) tensor(12061.4795, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11750.49609375
tensor(12061.4795, grad_fn=<NegBackward0>) tensor(11750.4961, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11527.326171875
tensor(11750.4961, grad_fn=<NegBackward0>) tensor(11527.3262, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11514.177734375
tensor(11527.3262, grad_fn=<NegBackward0>) tensor(11514.1777, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11513.3603515625
tensor(11514.1777, grad_fn=<NegBackward0>) tensor(11513.3604, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11509.041015625
tensor(11513.3604, grad_fn=<NegBackward0>) tensor(11509.0410, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11500.90625
tensor(11509.0410, grad_fn=<NegBackward0>) tensor(11500.9062, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11500.4375
tensor(11500.9062, grad_fn=<NegBackward0>) tensor(11500.4375, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11484.11328125
tensor(11500.4375, grad_fn=<NegBackward0>) tensor(11484.1133, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11480.623046875
tensor(11484.1133, grad_fn=<NegBackward0>) tensor(11480.6230, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11480.5244140625
tensor(11480.6230, grad_fn=<NegBackward0>) tensor(11480.5244, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11480.4521484375
tensor(11480.5244, grad_fn=<NegBackward0>) tensor(11480.4521, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11471.505859375
tensor(11480.4521, grad_fn=<NegBackward0>) tensor(11471.5059, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11471.44921875
tensor(11471.5059, grad_fn=<NegBackward0>) tensor(11471.4492, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11463.181640625
tensor(11471.4492, grad_fn=<NegBackward0>) tensor(11463.1816, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11463.033203125
tensor(11463.1816, grad_fn=<NegBackward0>) tensor(11463.0332, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11463.009765625
tensor(11463.0332, grad_fn=<NegBackward0>) tensor(11463.0098, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11462.990234375
tensor(11463.0098, grad_fn=<NegBackward0>) tensor(11462.9902, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11462.97265625
tensor(11462.9902, grad_fn=<NegBackward0>) tensor(11462.9727, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11462.9541015625
tensor(11462.9727, grad_fn=<NegBackward0>) tensor(11462.9541, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11454.4775390625
tensor(11462.9541, grad_fn=<NegBackward0>) tensor(11454.4775, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11442.33203125
tensor(11454.4775, grad_fn=<NegBackward0>) tensor(11442.3320, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11442.318359375
tensor(11442.3320, grad_fn=<NegBackward0>) tensor(11442.3184, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11442.30859375
tensor(11442.3184, grad_fn=<NegBackward0>) tensor(11442.3086, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11442.30078125
tensor(11442.3086, grad_fn=<NegBackward0>) tensor(11442.3008, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11442.2919921875
tensor(11442.3008, grad_fn=<NegBackward0>) tensor(11442.2920, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11442.2841796875
tensor(11442.2920, grad_fn=<NegBackward0>) tensor(11442.2842, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11442.2744140625
tensor(11442.2842, grad_fn=<NegBackward0>) tensor(11442.2744, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11442.263671875
tensor(11442.2744, grad_fn=<NegBackward0>) tensor(11442.2637, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11442.2548828125
tensor(11442.2637, grad_fn=<NegBackward0>) tensor(11442.2549, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11440.9638671875
tensor(11442.2549, grad_fn=<NegBackward0>) tensor(11440.9639, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11440.9560546875
tensor(11440.9639, grad_fn=<NegBackward0>) tensor(11440.9561, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11440.953125
tensor(11440.9561, grad_fn=<NegBackward0>) tensor(11440.9531, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11440.953125
tensor(11440.9531, grad_fn=<NegBackward0>) tensor(11440.9531, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11440.9453125
tensor(11440.9531, grad_fn=<NegBackward0>) tensor(11440.9453, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11440.943359375
tensor(11440.9453, grad_fn=<NegBackward0>) tensor(11440.9434, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11440.9404296875
tensor(11440.9434, grad_fn=<NegBackward0>) tensor(11440.9404, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11440.927734375
tensor(11440.9404, grad_fn=<NegBackward0>) tensor(11440.9277, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11440.9189453125
tensor(11440.9277, grad_fn=<NegBackward0>) tensor(11440.9189, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11440.908203125
tensor(11440.9189, grad_fn=<NegBackward0>) tensor(11440.9082, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11440.90625
tensor(11440.9082, grad_fn=<NegBackward0>) tensor(11440.9062, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11440.904296875
tensor(11440.9062, grad_fn=<NegBackward0>) tensor(11440.9043, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11440.90234375
tensor(11440.9043, grad_fn=<NegBackward0>) tensor(11440.9023, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11440.904296875
tensor(11440.9023, grad_fn=<NegBackward0>) tensor(11440.9043, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11440.9013671875
tensor(11440.9023, grad_fn=<NegBackward0>) tensor(11440.9014, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11440.8984375
tensor(11440.9014, grad_fn=<NegBackward0>) tensor(11440.8984, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11440.9033203125
tensor(11440.8984, grad_fn=<NegBackward0>) tensor(11440.9033, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11440.896484375
tensor(11440.8984, grad_fn=<NegBackward0>) tensor(11440.8965, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11440.90234375
tensor(11440.8965, grad_fn=<NegBackward0>) tensor(11440.9023, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11440.89453125
tensor(11440.8965, grad_fn=<NegBackward0>) tensor(11440.8945, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11440.8935546875
tensor(11440.8945, grad_fn=<NegBackward0>) tensor(11440.8936, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11440.892578125
tensor(11440.8936, grad_fn=<NegBackward0>) tensor(11440.8926, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11440.900390625
tensor(11440.8926, grad_fn=<NegBackward0>) tensor(11440.9004, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11440.89453125
tensor(11440.8926, grad_fn=<NegBackward0>) tensor(11440.8945, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11440.890625
tensor(11440.8926, grad_fn=<NegBackward0>) tensor(11440.8906, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11440.890625
tensor(11440.8906, grad_fn=<NegBackward0>) tensor(11440.8906, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11440.8896484375
tensor(11440.8906, grad_fn=<NegBackward0>) tensor(11440.8896, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11440.8935546875
tensor(11440.8896, grad_fn=<NegBackward0>) tensor(11440.8936, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11440.8876953125
tensor(11440.8896, grad_fn=<NegBackward0>) tensor(11440.8877, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11440.8876953125
tensor(11440.8877, grad_fn=<NegBackward0>) tensor(11440.8877, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11440.8857421875
tensor(11440.8877, grad_fn=<NegBackward0>) tensor(11440.8857, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11440.88671875
tensor(11440.8857, grad_fn=<NegBackward0>) tensor(11440.8867, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11440.888671875
tensor(11440.8857, grad_fn=<NegBackward0>) tensor(11440.8887, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11440.884765625
tensor(11440.8857, grad_fn=<NegBackward0>) tensor(11440.8848, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11440.884765625
tensor(11440.8848, grad_fn=<NegBackward0>) tensor(11440.8848, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11440.8857421875
tensor(11440.8848, grad_fn=<NegBackward0>) tensor(11440.8857, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11440.8837890625
tensor(11440.8848, grad_fn=<NegBackward0>) tensor(11440.8838, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11440.8896484375
tensor(11440.8838, grad_fn=<NegBackward0>) tensor(11440.8896, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11440.8828125
tensor(11440.8838, grad_fn=<NegBackward0>) tensor(11440.8828, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11440.888671875
tensor(11440.8828, grad_fn=<NegBackward0>) tensor(11440.8887, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11440.8828125
tensor(11440.8828, grad_fn=<NegBackward0>) tensor(11440.8828, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11440.880859375
tensor(11440.8828, grad_fn=<NegBackward0>) tensor(11440.8809, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11440.8916015625
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8916, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11440.8828125
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8828, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11440.8818359375
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8818, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11440.8818359375
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8818, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11440.880859375
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8809, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11440.8818359375
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8818, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11440.88671875
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8867, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11440.8818359375
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8818, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11440.880859375
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8809, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11440.8798828125
tensor(11440.8809, grad_fn=<NegBackward0>) tensor(11440.8799, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11440.8798828125
tensor(11440.8799, grad_fn=<NegBackward0>) tensor(11440.8799, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11440.888671875
tensor(11440.8799, grad_fn=<NegBackward0>) tensor(11440.8887, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11440.8798828125
tensor(11440.8799, grad_fn=<NegBackward0>) tensor(11440.8799, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11440.8916015625
tensor(11440.8799, grad_fn=<NegBackward0>) tensor(11440.8916, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11440.87890625
tensor(11440.8799, grad_fn=<NegBackward0>) tensor(11440.8789, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11440.8974609375
tensor(11440.8789, grad_fn=<NegBackward0>) tensor(11440.8975, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11440.8779296875
tensor(11440.8789, grad_fn=<NegBackward0>) tensor(11440.8779, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11441.0537109375
tensor(11440.8779, grad_fn=<NegBackward0>) tensor(11441.0537, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11440.87890625
tensor(11440.8779, grad_fn=<NegBackward0>) tensor(11440.8789, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11440.9248046875
tensor(11440.8779, grad_fn=<NegBackward0>) tensor(11440.9248, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11440.87890625
tensor(11440.8779, grad_fn=<NegBackward0>) tensor(11440.8789, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11440.896484375
tensor(11440.8779, grad_fn=<NegBackward0>) tensor(11440.8965, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.7405, 0.2595],
        [0.2922, 0.7078]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4495, 0.5505], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2017, 0.0988],
         [0.6107, 0.3931]],

        [[0.6018, 0.0906],
         [0.6164, 0.6203]],

        [[0.5640, 0.0971],
         [0.5940, 0.5645]],

        [[0.5474, 0.0988],
         [0.5446, 0.7028]],

        [[0.5125, 0.0980],
         [0.5649, 0.6328]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320153662032
Average Adjusted Rand Index: 0.9839993417272901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21176.630859375
inf tensor(21176.6309, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12192.8955078125
tensor(21176.6309, grad_fn=<NegBackward0>) tensor(12192.8955, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11818.2265625
tensor(12192.8955, grad_fn=<NegBackward0>) tensor(11818.2266, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11584.0439453125
tensor(11818.2266, grad_fn=<NegBackward0>) tensor(11584.0439, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11510.0009765625
tensor(11584.0439, grad_fn=<NegBackward0>) tensor(11510.0010, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11469.009765625
tensor(11510.0010, grad_fn=<NegBackward0>) tensor(11469.0098, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11446.837890625
tensor(11469.0098, grad_fn=<NegBackward0>) tensor(11446.8379, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11443.1005859375
tensor(11446.8379, grad_fn=<NegBackward0>) tensor(11443.1006, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11442.9921875
tensor(11443.1006, grad_fn=<NegBackward0>) tensor(11442.9922, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11442.9208984375
tensor(11442.9922, grad_fn=<NegBackward0>) tensor(11442.9209, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11442.8681640625
tensor(11442.9209, grad_fn=<NegBackward0>) tensor(11442.8682, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11442.8310546875
tensor(11442.8682, grad_fn=<NegBackward0>) tensor(11442.8311, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11442.8017578125
tensor(11442.8311, grad_fn=<NegBackward0>) tensor(11442.8018, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11442.779296875
tensor(11442.8018, grad_fn=<NegBackward0>) tensor(11442.7793, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11442.7607421875
tensor(11442.7793, grad_fn=<NegBackward0>) tensor(11442.7607, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11442.74609375
tensor(11442.7607, grad_fn=<NegBackward0>) tensor(11442.7461, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11442.7314453125
tensor(11442.7461, grad_fn=<NegBackward0>) tensor(11442.7314, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11442.7216796875
tensor(11442.7314, grad_fn=<NegBackward0>) tensor(11442.7217, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11442.7138671875
tensor(11442.7217, grad_fn=<NegBackward0>) tensor(11442.7139, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11442.705078125
tensor(11442.7139, grad_fn=<NegBackward0>) tensor(11442.7051, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11442.69921875
tensor(11442.7051, grad_fn=<NegBackward0>) tensor(11442.6992, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11442.693359375
tensor(11442.6992, grad_fn=<NegBackward0>) tensor(11442.6934, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11442.6884765625
tensor(11442.6934, grad_fn=<NegBackward0>) tensor(11442.6885, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11442.6845703125
tensor(11442.6885, grad_fn=<NegBackward0>) tensor(11442.6846, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11442.6806640625
tensor(11442.6846, grad_fn=<NegBackward0>) tensor(11442.6807, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11442.677734375
tensor(11442.6807, grad_fn=<NegBackward0>) tensor(11442.6777, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11442.67578125
tensor(11442.6777, grad_fn=<NegBackward0>) tensor(11442.6758, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11442.671875
tensor(11442.6758, grad_fn=<NegBackward0>) tensor(11442.6719, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11442.6708984375
tensor(11442.6719, grad_fn=<NegBackward0>) tensor(11442.6709, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11442.66796875
tensor(11442.6709, grad_fn=<NegBackward0>) tensor(11442.6680, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11442.666015625
tensor(11442.6680, grad_fn=<NegBackward0>) tensor(11442.6660, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11442.6650390625
tensor(11442.6660, grad_fn=<NegBackward0>) tensor(11442.6650, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11442.662109375
tensor(11442.6650, grad_fn=<NegBackward0>) tensor(11442.6621, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11442.6630859375
tensor(11442.6621, grad_fn=<NegBackward0>) tensor(11442.6631, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11442.6591796875
tensor(11442.6621, grad_fn=<NegBackward0>) tensor(11442.6592, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11442.66015625
tensor(11442.6592, grad_fn=<NegBackward0>) tensor(11442.6602, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11442.6572265625
tensor(11442.6592, grad_fn=<NegBackward0>) tensor(11442.6572, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11442.65625
tensor(11442.6572, grad_fn=<NegBackward0>) tensor(11442.6562, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11442.65625
tensor(11442.6562, grad_fn=<NegBackward0>) tensor(11442.6562, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11442.654296875
tensor(11442.6562, grad_fn=<NegBackward0>) tensor(11442.6543, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11442.66796875
tensor(11442.6543, grad_fn=<NegBackward0>) tensor(11442.6680, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11442.654296875
tensor(11442.6543, grad_fn=<NegBackward0>) tensor(11442.6543, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11442.6513671875
tensor(11442.6543, grad_fn=<NegBackward0>) tensor(11442.6514, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11442.65234375
tensor(11442.6514, grad_fn=<NegBackward0>) tensor(11442.6523, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11442.6513671875
tensor(11442.6514, grad_fn=<NegBackward0>) tensor(11442.6514, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11442.65234375
tensor(11442.6514, grad_fn=<NegBackward0>) tensor(11442.6523, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11442.6494140625
tensor(11442.6514, grad_fn=<NegBackward0>) tensor(11442.6494, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11442.6484375
tensor(11442.6494, grad_fn=<NegBackward0>) tensor(11442.6484, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11442.6484375
tensor(11442.6484, grad_fn=<NegBackward0>) tensor(11442.6484, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11442.6494140625
tensor(11442.6484, grad_fn=<NegBackward0>) tensor(11442.6494, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11442.6474609375
tensor(11442.6484, grad_fn=<NegBackward0>) tensor(11442.6475, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11442.6474609375
tensor(11442.6475, grad_fn=<NegBackward0>) tensor(11442.6475, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11442.646484375
tensor(11442.6475, grad_fn=<NegBackward0>) tensor(11442.6465, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11442.6455078125
tensor(11442.6465, grad_fn=<NegBackward0>) tensor(11442.6455, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11442.2109375
tensor(11442.6455, grad_fn=<NegBackward0>) tensor(11442.2109, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11442.2080078125
tensor(11442.2109, grad_fn=<NegBackward0>) tensor(11442.2080, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11442.216796875
tensor(11442.2080, grad_fn=<NegBackward0>) tensor(11442.2168, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11442.2080078125
tensor(11442.2080, grad_fn=<NegBackward0>) tensor(11442.2080, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11442.20703125
tensor(11442.2080, grad_fn=<NegBackward0>) tensor(11442.2070, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11442.2138671875
tensor(11442.2070, grad_fn=<NegBackward0>) tensor(11442.2139, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11442.2060546875
tensor(11442.2070, grad_fn=<NegBackward0>) tensor(11442.2061, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11442.2119140625
tensor(11442.2061, grad_fn=<NegBackward0>) tensor(11442.2119, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11442.20703125
tensor(11442.2061, grad_fn=<NegBackward0>) tensor(11442.2070, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11442.216796875
tensor(11442.2061, grad_fn=<NegBackward0>) tensor(11442.2168, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11442.205078125
tensor(11442.2061, grad_fn=<NegBackward0>) tensor(11442.2051, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11442.205078125
tensor(11442.2051, grad_fn=<NegBackward0>) tensor(11442.2051, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11442.2080078125
tensor(11442.2051, grad_fn=<NegBackward0>) tensor(11442.2080, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11442.205078125
tensor(11442.2051, grad_fn=<NegBackward0>) tensor(11442.2051, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11442.2080078125
tensor(11442.2051, grad_fn=<NegBackward0>) tensor(11442.2080, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11442.205078125
tensor(11442.2051, grad_fn=<NegBackward0>) tensor(11442.2051, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11442.2080078125
tensor(11442.2051, grad_fn=<NegBackward0>) tensor(11442.2080, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11442.203125
tensor(11442.2051, grad_fn=<NegBackward0>) tensor(11442.2031, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11442.2041015625
tensor(11442.2031, grad_fn=<NegBackward0>) tensor(11442.2041, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11442.2109375
tensor(11442.2031, grad_fn=<NegBackward0>) tensor(11442.2109, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11442.2041015625
tensor(11442.2031, grad_fn=<NegBackward0>) tensor(11442.2041, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11442.2041015625
tensor(11442.2031, grad_fn=<NegBackward0>) tensor(11442.2041, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11442.2041015625
tensor(11442.2031, grad_fn=<NegBackward0>) tensor(11442.2041, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.7411, 0.2589],
        [0.2933, 0.7067]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4497, 0.5503], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.0981],
         [0.7143, 0.3940]],

        [[0.6384, 0.0920],
         [0.5425, 0.7161]],

        [[0.6324, 0.0971],
         [0.5862, 0.5302]],

        [[0.6883, 0.0988],
         [0.6169, 0.5998]],

        [[0.5495, 0.0980],
         [0.6485, 0.5839]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.992
[0.9840320153662032, 0.9919999775871758] [0.9839993417272901, 0.992] [11440.896484375, 11442.2041015625]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11695.200029873846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22154.115234375
inf tensor(22154.1152, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12247.5556640625
tensor(22154.1152, grad_fn=<NegBackward0>) tensor(12247.5557, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11836.08203125
tensor(12247.5557, grad_fn=<NegBackward0>) tensor(11836.0820, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11816.4775390625
tensor(11836.0820, grad_fn=<NegBackward0>) tensor(11816.4775, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11802.345703125
tensor(11816.4775, grad_fn=<NegBackward0>) tensor(11802.3457, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11723.4580078125
tensor(11802.3457, grad_fn=<NegBackward0>) tensor(11723.4580, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11707.798828125
tensor(11723.4580, grad_fn=<NegBackward0>) tensor(11707.7988, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11686.1552734375
tensor(11707.7988, grad_fn=<NegBackward0>) tensor(11686.1553, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11686.123046875
tensor(11686.1553, grad_fn=<NegBackward0>) tensor(11686.1230, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11686.0966796875
tensor(11686.1230, grad_fn=<NegBackward0>) tensor(11686.0967, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11682.3974609375
tensor(11686.0967, grad_fn=<NegBackward0>) tensor(11682.3975, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11682.38671875
tensor(11682.3975, grad_fn=<NegBackward0>) tensor(11682.3867, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11682.37890625
tensor(11682.3867, grad_fn=<NegBackward0>) tensor(11682.3789, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11682.3720703125
tensor(11682.3789, grad_fn=<NegBackward0>) tensor(11682.3721, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11682.3662109375
tensor(11682.3721, grad_fn=<NegBackward0>) tensor(11682.3662, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11682.357421875
tensor(11682.3662, grad_fn=<NegBackward0>) tensor(11682.3574, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11682.34765625
tensor(11682.3574, grad_fn=<NegBackward0>) tensor(11682.3477, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11682.3447265625
tensor(11682.3477, grad_fn=<NegBackward0>) tensor(11682.3447, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11682.33984375
tensor(11682.3447, grad_fn=<NegBackward0>) tensor(11682.3398, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11682.337890625
tensor(11682.3398, grad_fn=<NegBackward0>) tensor(11682.3379, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11682.3359375
tensor(11682.3379, grad_fn=<NegBackward0>) tensor(11682.3359, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11682.3349609375
tensor(11682.3359, grad_fn=<NegBackward0>) tensor(11682.3350, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11682.3330078125
tensor(11682.3350, grad_fn=<NegBackward0>) tensor(11682.3330, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11682.3310546875
tensor(11682.3330, grad_fn=<NegBackward0>) tensor(11682.3311, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11682.330078125
tensor(11682.3311, grad_fn=<NegBackward0>) tensor(11682.3301, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11682.328125
tensor(11682.3301, grad_fn=<NegBackward0>) tensor(11682.3281, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11682.3134765625
tensor(11682.3281, grad_fn=<NegBackward0>) tensor(11682.3135, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11682.30859375
tensor(11682.3135, grad_fn=<NegBackward0>) tensor(11682.3086, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11682.30859375
tensor(11682.3086, grad_fn=<NegBackward0>) tensor(11682.3086, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11682.3076171875
tensor(11682.3086, grad_fn=<NegBackward0>) tensor(11682.3076, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11682.3056640625
tensor(11682.3076, grad_fn=<NegBackward0>) tensor(11682.3057, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11682.3095703125
tensor(11682.3057, grad_fn=<NegBackward0>) tensor(11682.3096, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11682.3056640625
tensor(11682.3057, grad_fn=<NegBackward0>) tensor(11682.3057, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11682.3046875
tensor(11682.3057, grad_fn=<NegBackward0>) tensor(11682.3047, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11682.3046875
tensor(11682.3047, grad_fn=<NegBackward0>) tensor(11682.3047, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11682.3046875
tensor(11682.3047, grad_fn=<NegBackward0>) tensor(11682.3047, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11682.3037109375
tensor(11682.3047, grad_fn=<NegBackward0>) tensor(11682.3037, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11682.3046875
tensor(11682.3037, grad_fn=<NegBackward0>) tensor(11682.3047, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11682.3037109375
tensor(11682.3037, grad_fn=<NegBackward0>) tensor(11682.3037, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11682.3115234375
tensor(11682.3037, grad_fn=<NegBackward0>) tensor(11682.3115, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11682.302734375
tensor(11682.3037, grad_fn=<NegBackward0>) tensor(11682.3027, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11682.3017578125
tensor(11682.3027, grad_fn=<NegBackward0>) tensor(11682.3018, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11682.3037109375
tensor(11682.3018, grad_fn=<NegBackward0>) tensor(11682.3037, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11682.3017578125
tensor(11682.3018, grad_fn=<NegBackward0>) tensor(11682.3018, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11682.30859375
tensor(11682.3018, grad_fn=<NegBackward0>) tensor(11682.3086, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11682.3017578125
tensor(11682.3018, grad_fn=<NegBackward0>) tensor(11682.3018, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11682.3037109375
tensor(11682.3018, grad_fn=<NegBackward0>) tensor(11682.3037, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11682.30078125
tensor(11682.3018, grad_fn=<NegBackward0>) tensor(11682.3008, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11682.3017578125
tensor(11682.3008, grad_fn=<NegBackward0>) tensor(11682.3018, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11682.3017578125
tensor(11682.3008, grad_fn=<NegBackward0>) tensor(11682.3018, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11682.30078125
tensor(11682.3008, grad_fn=<NegBackward0>) tensor(11682.3008, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11682.3017578125
tensor(11682.3008, grad_fn=<NegBackward0>) tensor(11682.3018, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11682.0751953125
tensor(11682.3008, grad_fn=<NegBackward0>) tensor(11682.0752, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11682.0439453125
tensor(11682.0752, grad_fn=<NegBackward0>) tensor(11682.0439, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11682.04296875
tensor(11682.0439, grad_fn=<NegBackward0>) tensor(11682.0430, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11682.04296875
tensor(11682.0430, grad_fn=<NegBackward0>) tensor(11682.0430, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11682.041015625
tensor(11682.0430, grad_fn=<NegBackward0>) tensor(11682.0410, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11682.041015625
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0410, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11682.044921875
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0449, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11682.0419921875
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0420, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11682.0419921875
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0420, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -11682.041015625
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0410, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11682.041015625
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0410, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11682.0458984375
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0459, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11682.0400390625
tensor(11682.0410, grad_fn=<NegBackward0>) tensor(11682.0400, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11682.046875
tensor(11682.0400, grad_fn=<NegBackward0>) tensor(11682.0469, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11682.0361328125
tensor(11682.0400, grad_fn=<NegBackward0>) tensor(11682.0361, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11682.037109375
tensor(11682.0361, grad_fn=<NegBackward0>) tensor(11682.0371, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11682.0380859375
tensor(11682.0361, grad_fn=<NegBackward0>) tensor(11682.0381, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11682.037109375
tensor(11682.0361, grad_fn=<NegBackward0>) tensor(11682.0371, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11682.041015625
tensor(11682.0361, grad_fn=<NegBackward0>) tensor(11682.0410, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11682.037109375
tensor(11682.0361, grad_fn=<NegBackward0>) tensor(11682.0371, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.7305, 0.2695],
        [0.2037, 0.7963]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5504, 0.4496], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1935, 0.0940],
         [0.5963, 0.3939]],

        [[0.5887, 0.1006],
         [0.7223, 0.6732]],

        [[0.6192, 0.1033],
         [0.7192, 0.6643]],

        [[0.5059, 0.1134],
         [0.6156, 0.7183]],

        [[0.5937, 0.1059],
         [0.5082, 0.5883]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961975210904
Average Adjusted Rand Index: 0.9759943230151148
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22420.59765625
inf tensor(22420.5977, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12396.337890625
tensor(22420.5977, grad_fn=<NegBackward0>) tensor(12396.3379, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12014.072265625
tensor(12396.3379, grad_fn=<NegBackward0>) tensor(12014.0723, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11862.4453125
tensor(12014.0723, grad_fn=<NegBackward0>) tensor(11862.4453, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11825.8583984375
tensor(11862.4453, grad_fn=<NegBackward0>) tensor(11825.8584, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11814.7841796875
tensor(11825.8584, grad_fn=<NegBackward0>) tensor(11814.7842, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11814.173828125
tensor(11814.7842, grad_fn=<NegBackward0>) tensor(11814.1738, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11810.71875
tensor(11814.1738, grad_fn=<NegBackward0>) tensor(11810.7188, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11810.474609375
tensor(11810.7188, grad_fn=<NegBackward0>) tensor(11810.4746, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11810.412109375
tensor(11810.4746, grad_fn=<NegBackward0>) tensor(11810.4121, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11810.3642578125
tensor(11810.4121, grad_fn=<NegBackward0>) tensor(11810.3643, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11810.3203125
tensor(11810.3643, grad_fn=<NegBackward0>) tensor(11810.3203, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11810.279296875
tensor(11810.3203, grad_fn=<NegBackward0>) tensor(11810.2793, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11806.1982421875
tensor(11810.2793, grad_fn=<NegBackward0>) tensor(11806.1982, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11806.177734375
tensor(11806.1982, grad_fn=<NegBackward0>) tensor(11806.1777, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11806.1640625
tensor(11806.1777, grad_fn=<NegBackward0>) tensor(11806.1641, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11806.150390625
tensor(11806.1641, grad_fn=<NegBackward0>) tensor(11806.1504, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11806.1416015625
tensor(11806.1504, grad_fn=<NegBackward0>) tensor(11806.1416, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11806.1328125
tensor(11806.1416, grad_fn=<NegBackward0>) tensor(11806.1328, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11806.125
tensor(11806.1328, grad_fn=<NegBackward0>) tensor(11806.1250, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11806.1162109375
tensor(11806.1250, grad_fn=<NegBackward0>) tensor(11806.1162, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11806.107421875
tensor(11806.1162, grad_fn=<NegBackward0>) tensor(11806.1074, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11806.0986328125
tensor(11806.1074, grad_fn=<NegBackward0>) tensor(11806.0986, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11806.0966796875
tensor(11806.0986, grad_fn=<NegBackward0>) tensor(11806.0967, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11806.091796875
tensor(11806.0967, grad_fn=<NegBackward0>) tensor(11806.0918, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11806.087890625
tensor(11806.0918, grad_fn=<NegBackward0>) tensor(11806.0879, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11806.0849609375
tensor(11806.0879, grad_fn=<NegBackward0>) tensor(11806.0850, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11806.0830078125
tensor(11806.0850, grad_fn=<NegBackward0>) tensor(11806.0830, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11806.080078125
tensor(11806.0830, grad_fn=<NegBackward0>) tensor(11806.0801, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11806.076171875
tensor(11806.0801, grad_fn=<NegBackward0>) tensor(11806.0762, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11806.0654296875
tensor(11806.0762, grad_fn=<NegBackward0>) tensor(11806.0654, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11806.056640625
tensor(11806.0654, grad_fn=<NegBackward0>) tensor(11806.0566, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11805.9931640625
tensor(11806.0566, grad_fn=<NegBackward0>) tensor(11805.9932, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11805.990234375
tensor(11805.9932, grad_fn=<NegBackward0>) tensor(11805.9902, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11805.9892578125
tensor(11805.9902, grad_fn=<NegBackward0>) tensor(11805.9893, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11805.9873046875
tensor(11805.9893, grad_fn=<NegBackward0>) tensor(11805.9873, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11805.9873046875
tensor(11805.9873, grad_fn=<NegBackward0>) tensor(11805.9873, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11805.98828125
tensor(11805.9873, grad_fn=<NegBackward0>) tensor(11805.9883, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11805.9853515625
tensor(11805.9873, grad_fn=<NegBackward0>) tensor(11805.9854, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11805.9833984375
tensor(11805.9854, grad_fn=<NegBackward0>) tensor(11805.9834, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11805.984375
tensor(11805.9834, grad_fn=<NegBackward0>) tensor(11805.9844, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11805.982421875
tensor(11805.9834, grad_fn=<NegBackward0>) tensor(11805.9824, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11805.98828125
tensor(11805.9824, grad_fn=<NegBackward0>) tensor(11805.9883, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11805.98046875
tensor(11805.9824, grad_fn=<NegBackward0>) tensor(11805.9805, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11805.98046875
tensor(11805.9805, grad_fn=<NegBackward0>) tensor(11805.9805, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11805.98046875
tensor(11805.9805, grad_fn=<NegBackward0>) tensor(11805.9805, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11805.9794921875
tensor(11805.9805, grad_fn=<NegBackward0>) tensor(11805.9795, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11805.98046875
tensor(11805.9795, grad_fn=<NegBackward0>) tensor(11805.9805, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11805.9794921875
tensor(11805.9795, grad_fn=<NegBackward0>) tensor(11805.9795, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11805.9775390625
tensor(11805.9795, grad_fn=<NegBackward0>) tensor(11805.9775, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11805.98046875
tensor(11805.9775, grad_fn=<NegBackward0>) tensor(11805.9805, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11805.9765625
tensor(11805.9775, grad_fn=<NegBackward0>) tensor(11805.9766, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11805.978515625
tensor(11805.9766, grad_fn=<NegBackward0>) tensor(11805.9785, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11805.9775390625
tensor(11805.9766, grad_fn=<NegBackward0>) tensor(11805.9775, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11805.9765625
tensor(11805.9766, grad_fn=<NegBackward0>) tensor(11805.9766, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11805.9755859375
tensor(11805.9766, grad_fn=<NegBackward0>) tensor(11805.9756, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11805.9755859375
tensor(11805.9756, grad_fn=<NegBackward0>) tensor(11805.9756, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11805.974609375
tensor(11805.9756, grad_fn=<NegBackward0>) tensor(11805.9746, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11805.974609375
tensor(11805.9746, grad_fn=<NegBackward0>) tensor(11805.9746, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11805.9755859375
tensor(11805.9746, grad_fn=<NegBackward0>) tensor(11805.9756, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11805.974609375
tensor(11805.9746, grad_fn=<NegBackward0>) tensor(11805.9746, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11805.974609375
tensor(11805.9746, grad_fn=<NegBackward0>) tensor(11805.9746, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11805.9736328125
tensor(11805.9746, grad_fn=<NegBackward0>) tensor(11805.9736, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11805.974609375
tensor(11805.9736, grad_fn=<NegBackward0>) tensor(11805.9746, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11805.974609375
tensor(11805.9736, grad_fn=<NegBackward0>) tensor(11805.9746, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11805.9736328125
tensor(11805.9736, grad_fn=<NegBackward0>) tensor(11805.9736, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11805.97265625
tensor(11805.9736, grad_fn=<NegBackward0>) tensor(11805.9727, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11805.9736328125
tensor(11805.9727, grad_fn=<NegBackward0>) tensor(11805.9736, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11805.9736328125
tensor(11805.9727, grad_fn=<NegBackward0>) tensor(11805.9736, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11805.9736328125
tensor(11805.9727, grad_fn=<NegBackward0>) tensor(11805.9736, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11805.98046875
tensor(11805.9727, grad_fn=<NegBackward0>) tensor(11805.9805, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11805.9736328125
tensor(11805.9727, grad_fn=<NegBackward0>) tensor(11805.9736, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6485, 0.3515],
        [0.1807, 0.8193]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 9.2386e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1861, 0.1315],
         [0.5534, 0.3976]],

        [[0.5225, 0.1007],
         [0.6404, 0.7263]],

        [[0.5248, 0.1033],
         [0.6114, 0.5143]],

        [[0.6269, 0.1135],
         [0.6195, 0.6265]],

        [[0.5689, 0.1059],
         [0.6752, 0.5894]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6521663475533844
Average Adjusted Rand Index: 0.7839943230151147
[0.9760961975210904, 0.6521663475533844] [0.9759943230151148, 0.7839943230151147] [11682.037109375, 11805.9736328125]
-------------------------------------
This iteration is 92
True Objective function: Loss = -11498.823827549195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22721.529296875
inf tensor(22721.5293, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12154.7880859375
tensor(22721.5293, grad_fn=<NegBackward0>) tensor(12154.7881, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12122.291015625
tensor(12154.7881, grad_fn=<NegBackward0>) tensor(12122.2910, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12105.8193359375
tensor(12122.2910, grad_fn=<NegBackward0>) tensor(12105.8193, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12102.306640625
tensor(12105.8193, grad_fn=<NegBackward0>) tensor(12102.3066, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12085.3818359375
tensor(12102.3066, grad_fn=<NegBackward0>) tensor(12085.3818, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11762.048828125
tensor(12085.3818, grad_fn=<NegBackward0>) tensor(11762.0488, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11607.203125
tensor(11762.0488, grad_fn=<NegBackward0>) tensor(11607.2031, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11543.8896484375
tensor(11607.2031, grad_fn=<NegBackward0>) tensor(11543.8896, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11519.6640625
tensor(11543.8896, grad_fn=<NegBackward0>) tensor(11519.6641, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11505.1708984375
tensor(11519.6641, grad_fn=<NegBackward0>) tensor(11505.1709, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11505.0712890625
tensor(11505.1709, grad_fn=<NegBackward0>) tensor(11505.0713, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11505.0244140625
tensor(11505.0713, grad_fn=<NegBackward0>) tensor(11505.0244, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11504.9931640625
tensor(11505.0244, grad_fn=<NegBackward0>) tensor(11504.9932, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11504.96875
tensor(11504.9932, grad_fn=<NegBackward0>) tensor(11504.9688, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11504.9521484375
tensor(11504.9688, grad_fn=<NegBackward0>) tensor(11504.9521, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11504.9375
tensor(11504.9521, grad_fn=<NegBackward0>) tensor(11504.9375, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11504.83203125
tensor(11504.9375, grad_fn=<NegBackward0>) tensor(11504.8320, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11501.62890625
tensor(11504.8320, grad_fn=<NegBackward0>) tensor(11501.6289, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11501.6181640625
tensor(11501.6289, grad_fn=<NegBackward0>) tensor(11501.6182, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11501.609375
tensor(11501.6182, grad_fn=<NegBackward0>) tensor(11501.6094, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11501.603515625
tensor(11501.6094, grad_fn=<NegBackward0>) tensor(11501.6035, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11501.5966796875
tensor(11501.6035, grad_fn=<NegBackward0>) tensor(11501.5967, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11501.591796875
tensor(11501.5967, grad_fn=<NegBackward0>) tensor(11501.5918, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11501.5888671875
tensor(11501.5918, grad_fn=<NegBackward0>) tensor(11501.5889, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11501.5859375
tensor(11501.5889, grad_fn=<NegBackward0>) tensor(11501.5859, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11501.5830078125
tensor(11501.5859, grad_fn=<NegBackward0>) tensor(11501.5830, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11501.5791015625
tensor(11501.5830, grad_fn=<NegBackward0>) tensor(11501.5791, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11501.576171875
tensor(11501.5791, grad_fn=<NegBackward0>) tensor(11501.5762, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11501.5693359375
tensor(11501.5762, grad_fn=<NegBackward0>) tensor(11501.5693, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11501.5673828125
tensor(11501.5693, grad_fn=<NegBackward0>) tensor(11501.5674, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11501.564453125
tensor(11501.5674, grad_fn=<NegBackward0>) tensor(11501.5645, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11501.5634765625
tensor(11501.5645, grad_fn=<NegBackward0>) tensor(11501.5635, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11501.556640625
tensor(11501.5635, grad_fn=<NegBackward0>) tensor(11501.5566, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11490.271484375
tensor(11501.5566, grad_fn=<NegBackward0>) tensor(11490.2715, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11490.265625
tensor(11490.2715, grad_fn=<NegBackward0>) tensor(11490.2656, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11490.2412109375
tensor(11490.2656, grad_fn=<NegBackward0>) tensor(11490.2412, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11490.240234375
tensor(11490.2412, grad_fn=<NegBackward0>) tensor(11490.2402, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11490.23828125
tensor(11490.2402, grad_fn=<NegBackward0>) tensor(11490.2383, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11490.236328125
tensor(11490.2383, grad_fn=<NegBackward0>) tensor(11490.2363, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11490.2373046875
tensor(11490.2363, grad_fn=<NegBackward0>) tensor(11490.2373, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11490.236328125
tensor(11490.2363, grad_fn=<NegBackward0>) tensor(11490.2363, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11490.234375
tensor(11490.2363, grad_fn=<NegBackward0>) tensor(11490.2344, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11490.234375
tensor(11490.2344, grad_fn=<NegBackward0>) tensor(11490.2344, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11490.234375
tensor(11490.2344, grad_fn=<NegBackward0>) tensor(11490.2344, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11490.232421875
tensor(11490.2344, grad_fn=<NegBackward0>) tensor(11490.2324, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11490.236328125
tensor(11490.2324, grad_fn=<NegBackward0>) tensor(11490.2363, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11490.232421875
tensor(11490.2324, grad_fn=<NegBackward0>) tensor(11490.2324, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11490.2314453125
tensor(11490.2324, grad_fn=<NegBackward0>) tensor(11490.2314, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11490.2314453125
tensor(11490.2314, grad_fn=<NegBackward0>) tensor(11490.2314, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11490.23046875
tensor(11490.2314, grad_fn=<NegBackward0>) tensor(11490.2305, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11490.23046875
tensor(11490.2305, grad_fn=<NegBackward0>) tensor(11490.2305, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11490.2294921875
tensor(11490.2305, grad_fn=<NegBackward0>) tensor(11490.2295, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11490.2294921875
tensor(11490.2295, grad_fn=<NegBackward0>) tensor(11490.2295, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11490.228515625
tensor(11490.2295, grad_fn=<NegBackward0>) tensor(11490.2285, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11490.2275390625
tensor(11490.2285, grad_fn=<NegBackward0>) tensor(11490.2275, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11490.2275390625
tensor(11490.2275, grad_fn=<NegBackward0>) tensor(11490.2275, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11490.2177734375
tensor(11490.2275, grad_fn=<NegBackward0>) tensor(11490.2178, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11490.2177734375
tensor(11490.2178, grad_fn=<NegBackward0>) tensor(11490.2178, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11490.216796875
tensor(11490.2178, grad_fn=<NegBackward0>) tensor(11490.2168, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11490.216796875
tensor(11490.2168, grad_fn=<NegBackward0>) tensor(11490.2168, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11490.2373046875
tensor(11490.2168, grad_fn=<NegBackward0>) tensor(11490.2373, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11490.21484375
tensor(11490.2168, grad_fn=<NegBackward0>) tensor(11490.2148, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11490.2177734375
tensor(11490.2148, grad_fn=<NegBackward0>) tensor(11490.2178, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11490.2158203125
tensor(11490.2148, grad_fn=<NegBackward0>) tensor(11490.2158, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11490.21484375
tensor(11490.2148, grad_fn=<NegBackward0>) tensor(11490.2148, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11490.2158203125
tensor(11490.2148, grad_fn=<NegBackward0>) tensor(11490.2158, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11490.2158203125
tensor(11490.2148, grad_fn=<NegBackward0>) tensor(11490.2158, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11490.2138671875
tensor(11490.2148, grad_fn=<NegBackward0>) tensor(11490.2139, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11490.21484375
tensor(11490.2139, grad_fn=<NegBackward0>) tensor(11490.2148, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11490.21484375
tensor(11490.2139, grad_fn=<NegBackward0>) tensor(11490.2148, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11490.21484375
tensor(11490.2139, grad_fn=<NegBackward0>) tensor(11490.2148, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11490.2177734375
tensor(11490.2139, grad_fn=<NegBackward0>) tensor(11490.2178, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11490.2197265625
tensor(11490.2139, grad_fn=<NegBackward0>) tensor(11490.2197, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.7408, 0.2592],
        [0.2771, 0.7229]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3935, 0.6065], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3959, 0.0963],
         [0.7128, 0.1991]],

        [[0.7093, 0.1009],
         [0.6915, 0.5710]],

        [[0.6745, 0.0948],
         [0.5254, 0.5395]],

        [[0.6356, 0.1109],
         [0.6670, 0.6531]],

        [[0.5782, 0.1090],
         [0.5426, 0.5622]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999558239977
Average Adjusted Rand Index: 0.9919905858125253
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22660.634765625
inf tensor(22660.6348, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12117.8837890625
tensor(22660.6348, grad_fn=<NegBackward0>) tensor(12117.8838, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11786.9443359375
tensor(12117.8838, grad_fn=<NegBackward0>) tensor(11786.9443, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11778.037109375
tensor(11786.9443, grad_fn=<NegBackward0>) tensor(11778.0371, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11771.220703125
tensor(11778.0371, grad_fn=<NegBackward0>) tensor(11771.2207, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11768.6904296875
tensor(11771.2207, grad_fn=<NegBackward0>) tensor(11768.6904, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11767.7841796875
tensor(11768.6904, grad_fn=<NegBackward0>) tensor(11767.7842, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11767.666015625
tensor(11767.7842, grad_fn=<NegBackward0>) tensor(11767.6660, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11761.224609375
tensor(11767.6660, grad_fn=<NegBackward0>) tensor(11761.2246, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11751.1171875
tensor(11761.2246, grad_fn=<NegBackward0>) tensor(11751.1172, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11750.83984375
tensor(11751.1172, grad_fn=<NegBackward0>) tensor(11750.8398, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11745.259765625
tensor(11750.8398, grad_fn=<NegBackward0>) tensor(11745.2598, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11744.6767578125
tensor(11745.2598, grad_fn=<NegBackward0>) tensor(11744.6768, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11743.98046875
tensor(11744.6768, grad_fn=<NegBackward0>) tensor(11743.9805, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11743.9609375
tensor(11743.9805, grad_fn=<NegBackward0>) tensor(11743.9609, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11743.9521484375
tensor(11743.9609, grad_fn=<NegBackward0>) tensor(11743.9521, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11743.9453125
tensor(11743.9521, grad_fn=<NegBackward0>) tensor(11743.9453, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11743.939453125
tensor(11743.9453, grad_fn=<NegBackward0>) tensor(11743.9395, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11743.935546875
tensor(11743.9395, grad_fn=<NegBackward0>) tensor(11743.9355, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11743.9326171875
tensor(11743.9355, grad_fn=<NegBackward0>) tensor(11743.9326, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11743.9248046875
tensor(11743.9326, grad_fn=<NegBackward0>) tensor(11743.9248, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11743.919921875
tensor(11743.9248, grad_fn=<NegBackward0>) tensor(11743.9199, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11743.9169921875
tensor(11743.9199, grad_fn=<NegBackward0>) tensor(11743.9170, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11743.9130859375
tensor(11743.9170, grad_fn=<NegBackward0>) tensor(11743.9131, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11738.525390625
tensor(11743.9131, grad_fn=<NegBackward0>) tensor(11738.5254, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11738.501953125
tensor(11738.5254, grad_fn=<NegBackward0>) tensor(11738.5020, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11738.494140625
tensor(11738.5020, grad_fn=<NegBackward0>) tensor(11738.4941, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11738.48046875
tensor(11738.4941, grad_fn=<NegBackward0>) tensor(11738.4805, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11738.4775390625
tensor(11738.4805, grad_fn=<NegBackward0>) tensor(11738.4775, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11738.4736328125
tensor(11738.4775, grad_fn=<NegBackward0>) tensor(11738.4736, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11738.4580078125
tensor(11738.4736, grad_fn=<NegBackward0>) tensor(11738.4580, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11738.4580078125
tensor(11738.4580, grad_fn=<NegBackward0>) tensor(11738.4580, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11738.45703125
tensor(11738.4580, grad_fn=<NegBackward0>) tensor(11738.4570, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11738.4560546875
tensor(11738.4570, grad_fn=<NegBackward0>) tensor(11738.4561, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11738.455078125
tensor(11738.4561, grad_fn=<NegBackward0>) tensor(11738.4551, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11738.453125
tensor(11738.4551, grad_fn=<NegBackward0>) tensor(11738.4531, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11738.453125
tensor(11738.4531, grad_fn=<NegBackward0>) tensor(11738.4531, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11738.451171875
tensor(11738.4531, grad_fn=<NegBackward0>) tensor(11738.4512, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11738.4501953125
tensor(11738.4512, grad_fn=<NegBackward0>) tensor(11738.4502, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11738.4501953125
tensor(11738.4502, grad_fn=<NegBackward0>) tensor(11738.4502, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11738.4501953125
tensor(11738.4502, grad_fn=<NegBackward0>) tensor(11738.4502, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11738.4501953125
tensor(11738.4502, grad_fn=<NegBackward0>) tensor(11738.4502, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11738.44921875
tensor(11738.4502, grad_fn=<NegBackward0>) tensor(11738.4492, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11738.44921875
tensor(11738.4492, grad_fn=<NegBackward0>) tensor(11738.4492, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11738.4482421875
tensor(11738.4492, grad_fn=<NegBackward0>) tensor(11738.4482, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11738.4482421875
tensor(11738.4482, grad_fn=<NegBackward0>) tensor(11738.4482, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11738.4453125
tensor(11738.4482, grad_fn=<NegBackward0>) tensor(11738.4453, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11738.4453125
tensor(11738.4453, grad_fn=<NegBackward0>) tensor(11738.4453, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11738.4462890625
tensor(11738.4453, grad_fn=<NegBackward0>) tensor(11738.4463, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11738.4443359375
tensor(11738.4453, grad_fn=<NegBackward0>) tensor(11738.4443, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11738.443359375
tensor(11738.4443, grad_fn=<NegBackward0>) tensor(11738.4434, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11738.4443359375
tensor(11738.4434, grad_fn=<NegBackward0>) tensor(11738.4443, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11738.4453125
tensor(11738.4434, grad_fn=<NegBackward0>) tensor(11738.4453, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11738.447265625
tensor(11738.4434, grad_fn=<NegBackward0>) tensor(11738.4473, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11738.443359375
tensor(11738.4434, grad_fn=<NegBackward0>) tensor(11738.4434, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11738.443359375
tensor(11738.4434, grad_fn=<NegBackward0>) tensor(11738.4434, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11738.4423828125
tensor(11738.4434, grad_fn=<NegBackward0>) tensor(11738.4424, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11738.443359375
tensor(11738.4424, grad_fn=<NegBackward0>) tensor(11738.4434, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11738.443359375
tensor(11738.4424, grad_fn=<NegBackward0>) tensor(11738.4434, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11738.4443359375
tensor(11738.4424, grad_fn=<NegBackward0>) tensor(11738.4443, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11738.4423828125
tensor(11738.4424, grad_fn=<NegBackward0>) tensor(11738.4424, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11738.4248046875
tensor(11738.4424, grad_fn=<NegBackward0>) tensor(11738.4248, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11738.4189453125
tensor(11738.4248, grad_fn=<NegBackward0>) tensor(11738.4189, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11738.4375
tensor(11738.4189, grad_fn=<NegBackward0>) tensor(11738.4375, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11738.4150390625
tensor(11738.4189, grad_fn=<NegBackward0>) tensor(11738.4150, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11738.4130859375
tensor(11738.4150, grad_fn=<NegBackward0>) tensor(11738.4131, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11738.412109375
tensor(11738.4131, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11738.4130859375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4131, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11738.4140625
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4141, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11738.4140625
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4141, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11738.4130859375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4131, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11738.412109375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11738.4130859375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4131, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11738.4130859375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4131, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11738.41796875
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4180, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11738.412109375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11738.412109375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11738.412109375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11738.412109375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11738.412109375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11738.4130859375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4131, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11738.412109375
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11738.41015625
tensor(11738.4121, grad_fn=<NegBackward0>) tensor(11738.4102, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11738.412109375
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11738.4111328125
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4111, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11738.41015625
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4102, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11738.4111328125
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4111, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11738.4111328125
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4111, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11738.412109375
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4121, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11738.4111328125
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4111, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11738.421875
tensor(11738.4102, grad_fn=<NegBackward0>) tensor(11738.4219, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.3614, 0.6386],
        [0.5540, 0.4460]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4194, 0.5806], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3132, 0.0964],
         [0.5324, 0.2691]],

        [[0.6199, 0.0967],
         [0.6951, 0.6283]],

        [[0.5942, 0.0943],
         [0.5142, 0.5529]],

        [[0.7241, 0.1076],
         [0.6686, 0.6881]],

        [[0.5705, 0.0991],
         [0.5757, 0.5641]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080890789891884
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448427857772554
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6691937320900481
Global Adjusted Rand Index: 0.04301394899809318
Average Adjusted Rand Index: 0.8253873820894981
[0.9919999558239977, 0.04301394899809318] [0.9919905858125253, 0.8253873820894981] [11490.2197265625, 11738.421875]
-------------------------------------
This iteration is 93
True Objective function: Loss = -11562.155776703867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21562.37109375
inf tensor(21562.3711, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12354.87890625
tensor(21562.3711, grad_fn=<NegBackward0>) tensor(12354.8789, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12307.0625
tensor(12354.8789, grad_fn=<NegBackward0>) tensor(12307.0625, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11829.5986328125
tensor(12307.0625, grad_fn=<NegBackward0>) tensor(11829.5986, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11729.9443359375
tensor(11829.5986, grad_fn=<NegBackward0>) tensor(11729.9443, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11689.572265625
tensor(11729.9443, grad_fn=<NegBackward0>) tensor(11689.5723, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11681.18359375
tensor(11689.5723, grad_fn=<NegBackward0>) tensor(11681.1836, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11633.4111328125
tensor(11681.1836, grad_fn=<NegBackward0>) tensor(11633.4111, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11605.8515625
tensor(11633.4111, grad_fn=<NegBackward0>) tensor(11605.8516, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11604.0615234375
tensor(11605.8516, grad_fn=<NegBackward0>) tensor(11604.0615, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11603.875
tensor(11604.0615, grad_fn=<NegBackward0>) tensor(11603.8750, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11593.421875
tensor(11603.8750, grad_fn=<NegBackward0>) tensor(11593.4219, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11592.8134765625
tensor(11593.4219, grad_fn=<NegBackward0>) tensor(11592.8135, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11581.4443359375
tensor(11592.8135, grad_fn=<NegBackward0>) tensor(11581.4443, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11581.3173828125
tensor(11581.4443, grad_fn=<NegBackward0>) tensor(11581.3174, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11560.763671875
tensor(11581.3174, grad_fn=<NegBackward0>) tensor(11560.7637, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11560.7177734375
tensor(11560.7637, grad_fn=<NegBackward0>) tensor(11560.7178, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11560.689453125
tensor(11560.7178, grad_fn=<NegBackward0>) tensor(11560.6895, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11560.66796875
tensor(11560.6895, grad_fn=<NegBackward0>) tensor(11560.6680, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11560.6474609375
tensor(11560.6680, grad_fn=<NegBackward0>) tensor(11560.6475, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11556.01953125
tensor(11560.6475, grad_fn=<NegBackward0>) tensor(11556.0195, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11555.8916015625
tensor(11556.0195, grad_fn=<NegBackward0>) tensor(11555.8916, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11555.875
tensor(11555.8916, grad_fn=<NegBackward0>) tensor(11555.8750, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11555.8671875
tensor(11555.8750, grad_fn=<NegBackward0>) tensor(11555.8672, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11555.85546875
tensor(11555.8672, grad_fn=<NegBackward0>) tensor(11555.8555, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11555.8486328125
tensor(11555.8555, grad_fn=<NegBackward0>) tensor(11555.8486, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11555.841796875
tensor(11555.8486, grad_fn=<NegBackward0>) tensor(11555.8418, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11555.8359375
tensor(11555.8418, grad_fn=<NegBackward0>) tensor(11555.8359, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11555.830078125
tensor(11555.8359, grad_fn=<NegBackward0>) tensor(11555.8301, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11555.8271484375
tensor(11555.8301, grad_fn=<NegBackward0>) tensor(11555.8271, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11555.822265625
tensor(11555.8271, grad_fn=<NegBackward0>) tensor(11555.8223, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11555.8173828125
tensor(11555.8223, grad_fn=<NegBackward0>) tensor(11555.8174, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11555.814453125
tensor(11555.8174, grad_fn=<NegBackward0>) tensor(11555.8145, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11555.8134765625
tensor(11555.8145, grad_fn=<NegBackward0>) tensor(11555.8135, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11555.80859375
tensor(11555.8135, grad_fn=<NegBackward0>) tensor(11555.8086, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11555.806640625
tensor(11555.8086, grad_fn=<NegBackward0>) tensor(11555.8066, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11555.8037109375
tensor(11555.8066, grad_fn=<NegBackward0>) tensor(11555.8037, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11555.80078125
tensor(11555.8037, grad_fn=<NegBackward0>) tensor(11555.8008, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11555.798828125
tensor(11555.8008, grad_fn=<NegBackward0>) tensor(11555.7988, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11555.7958984375
tensor(11555.7988, grad_fn=<NegBackward0>) tensor(11555.7959, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11555.7939453125
tensor(11555.7959, grad_fn=<NegBackward0>) tensor(11555.7939, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11555.80078125
tensor(11555.7939, grad_fn=<NegBackward0>) tensor(11555.8008, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11555.7890625
tensor(11555.7939, grad_fn=<NegBackward0>) tensor(11555.7891, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11555.7861328125
tensor(11555.7891, grad_fn=<NegBackward0>) tensor(11555.7861, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11555.783203125
tensor(11555.7861, grad_fn=<NegBackward0>) tensor(11555.7832, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11555.78125
tensor(11555.7832, grad_fn=<NegBackward0>) tensor(11555.7812, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11555.779296875
tensor(11555.7812, grad_fn=<NegBackward0>) tensor(11555.7793, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11555.7783203125
tensor(11555.7793, grad_fn=<NegBackward0>) tensor(11555.7783, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11555.7783203125
tensor(11555.7783, grad_fn=<NegBackward0>) tensor(11555.7783, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11555.77734375
tensor(11555.7783, grad_fn=<NegBackward0>) tensor(11555.7773, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11555.7763671875
tensor(11555.7773, grad_fn=<NegBackward0>) tensor(11555.7764, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11555.7724609375
tensor(11555.7764, grad_fn=<NegBackward0>) tensor(11555.7725, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11555.7783203125
tensor(11555.7725, grad_fn=<NegBackward0>) tensor(11555.7783, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11555.7666015625
tensor(11555.7725, grad_fn=<NegBackward0>) tensor(11555.7666, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11555.76953125
tensor(11555.7666, grad_fn=<NegBackward0>) tensor(11555.7695, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11555.767578125
tensor(11555.7666, grad_fn=<NegBackward0>) tensor(11555.7676, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11555.7646484375
tensor(11555.7666, grad_fn=<NegBackward0>) tensor(11555.7646, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11555.7646484375
tensor(11555.7646, grad_fn=<NegBackward0>) tensor(11555.7646, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11555.765625
tensor(11555.7646, grad_fn=<NegBackward0>) tensor(11555.7656, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11555.7626953125
tensor(11555.7646, grad_fn=<NegBackward0>) tensor(11555.7627, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11555.763671875
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7637, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11555.7626953125
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7627, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11555.7705078125
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7705, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11555.76171875
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7617, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11555.759765625
tensor(11555.7617, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11555.7607421875
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7607, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11555.759765625
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11555.7646484375
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7646, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11555.7607421875
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7607, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11555.759765625
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11555.7587890625
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7588, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11555.7626953125
tensor(11555.7588, grad_fn=<NegBackward0>) tensor(11555.7627, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11555.759765625
tensor(11555.7588, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11555.7578125
tensor(11555.7588, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11555.759765625
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11555.765625
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7656, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11555.759765625
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11555.7578125
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11555.763671875
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7637, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11555.7822265625
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7822, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11555.7568359375
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7568, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11555.7568359375
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7568, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11555.76171875
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7617, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11555.7568359375
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7568, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11555.759765625
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11555.7578125
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11555.755859375
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7559, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11555.751953125
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7520, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11555.751953125
tensor(11555.7520, grad_fn=<NegBackward0>) tensor(11555.7520, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11555.75390625
tensor(11555.7520, grad_fn=<NegBackward0>) tensor(11555.7539, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11555.7529296875
tensor(11555.7520, grad_fn=<NegBackward0>) tensor(11555.7529, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11555.7568359375
tensor(11555.7520, grad_fn=<NegBackward0>) tensor(11555.7568, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11555.75390625
tensor(11555.7520, grad_fn=<NegBackward0>) tensor(11555.7539, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11555.765625
tensor(11555.7520, grad_fn=<NegBackward0>) tensor(11555.7656, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.7214, 0.2786],
        [0.2845, 0.7155]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4692, 0.5308], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1916, 0.0998],
         [0.6447, 0.4005]],

        [[0.7221, 0.1031],
         [0.7066, 0.5987]],

        [[0.5690, 0.0969],
         [0.6242, 0.6506]],

        [[0.5131, 0.1006],
         [0.5715, 0.5333]],

        [[0.6504, 0.1059],
         [0.5956, 0.6456]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9841616161616162
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24771.591796875
inf tensor(24771.5918, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12042.7314453125
tensor(24771.5918, grad_fn=<NegBackward0>) tensor(12042.7314, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11738.11328125
tensor(12042.7314, grad_fn=<NegBackward0>) tensor(11738.1133, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11616.6728515625
tensor(11738.1133, grad_fn=<NegBackward0>) tensor(11616.6729, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11565.3681640625
tensor(11616.6729, grad_fn=<NegBackward0>) tensor(11565.3682, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11556.380859375
tensor(11565.3682, grad_fn=<NegBackward0>) tensor(11556.3809, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11556.1728515625
tensor(11556.3809, grad_fn=<NegBackward0>) tensor(11556.1729, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11556.0654296875
tensor(11556.1729, grad_fn=<NegBackward0>) tensor(11556.0654, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11556.0
tensor(11556.0654, grad_fn=<NegBackward0>) tensor(11556., grad_fn=<NegBackward0>)
Iteration 900: Loss = -11555.955078125
tensor(11556., grad_fn=<NegBackward0>) tensor(11555.9551, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11555.921875
tensor(11555.9551, grad_fn=<NegBackward0>) tensor(11555.9219, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11555.8955078125
tensor(11555.9219, grad_fn=<NegBackward0>) tensor(11555.8955, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11555.87890625
tensor(11555.8955, grad_fn=<NegBackward0>) tensor(11555.8789, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11555.8603515625
tensor(11555.8789, grad_fn=<NegBackward0>) tensor(11555.8604, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11555.84765625
tensor(11555.8604, grad_fn=<NegBackward0>) tensor(11555.8477, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11555.8369140625
tensor(11555.8477, grad_fn=<NegBackward0>) tensor(11555.8369, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11555.8291015625
tensor(11555.8369, grad_fn=<NegBackward0>) tensor(11555.8291, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11555.82421875
tensor(11555.8291, grad_fn=<NegBackward0>) tensor(11555.8242, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11555.8134765625
tensor(11555.8242, grad_fn=<NegBackward0>) tensor(11555.8135, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11555.8046875
tensor(11555.8135, grad_fn=<NegBackward0>) tensor(11555.8047, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11555.7978515625
tensor(11555.8047, grad_fn=<NegBackward0>) tensor(11555.7979, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11555.794921875
tensor(11555.7979, grad_fn=<NegBackward0>) tensor(11555.7949, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11555.791015625
tensor(11555.7949, grad_fn=<NegBackward0>) tensor(11555.7910, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11555.7890625
tensor(11555.7910, grad_fn=<NegBackward0>) tensor(11555.7891, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11555.78515625
tensor(11555.7891, grad_fn=<NegBackward0>) tensor(11555.7852, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11555.78125
tensor(11555.7852, grad_fn=<NegBackward0>) tensor(11555.7812, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11555.7802734375
tensor(11555.7812, grad_fn=<NegBackward0>) tensor(11555.7803, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11555.77734375
tensor(11555.7803, grad_fn=<NegBackward0>) tensor(11555.7773, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11555.7763671875
tensor(11555.7773, grad_fn=<NegBackward0>) tensor(11555.7764, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11555.7744140625
tensor(11555.7764, grad_fn=<NegBackward0>) tensor(11555.7744, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11555.7724609375
tensor(11555.7744, grad_fn=<NegBackward0>) tensor(11555.7725, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11555.771484375
tensor(11555.7725, grad_fn=<NegBackward0>) tensor(11555.7715, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11555.77734375
tensor(11555.7715, grad_fn=<NegBackward0>) tensor(11555.7773, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11555.76953125
tensor(11555.7715, grad_fn=<NegBackward0>) tensor(11555.7695, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11555.7744140625
tensor(11555.7695, grad_fn=<NegBackward0>) tensor(11555.7744, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11555.7666015625
tensor(11555.7695, grad_fn=<NegBackward0>) tensor(11555.7666, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11555.7763671875
tensor(11555.7666, grad_fn=<NegBackward0>) tensor(11555.7764, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11555.7646484375
tensor(11555.7666, grad_fn=<NegBackward0>) tensor(11555.7646, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11555.7763671875
tensor(11555.7646, grad_fn=<NegBackward0>) tensor(11555.7764, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11555.763671875
tensor(11555.7646, grad_fn=<NegBackward0>) tensor(11555.7637, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11555.779296875
tensor(11555.7637, grad_fn=<NegBackward0>) tensor(11555.7793, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11555.7626953125
tensor(11555.7637, grad_fn=<NegBackward0>) tensor(11555.7627, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11555.7626953125
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7627, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11555.7626953125
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7627, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11555.76953125
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7695, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11555.76171875
tensor(11555.7627, grad_fn=<NegBackward0>) tensor(11555.7617, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11555.7607421875
tensor(11555.7617, grad_fn=<NegBackward0>) tensor(11555.7607, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11555.76171875
tensor(11555.7607, grad_fn=<NegBackward0>) tensor(11555.7617, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11555.759765625
tensor(11555.7607, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11555.759765625
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11555.759765625
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11555.7587890625
tensor(11555.7598, grad_fn=<NegBackward0>) tensor(11555.7588, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11555.7587890625
tensor(11555.7588, grad_fn=<NegBackward0>) tensor(11555.7588, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11555.7578125
tensor(11555.7588, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11555.7578125
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11555.7578125
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11555.7578125
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11555.7587890625
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7588, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11555.7578125
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11555.7578125
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11555.7587890625
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7588, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11555.7568359375
tensor(11555.7578, grad_fn=<NegBackward0>) tensor(11555.7568, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11555.7568359375
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7568, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11555.755859375
tensor(11555.7568, grad_fn=<NegBackward0>) tensor(11555.7559, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11555.755859375
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7559, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11555.755859375
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7559, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11555.755859375
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7559, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11555.759765625
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7598, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11555.755859375
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7559, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11555.7568359375
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7568, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11555.755859375
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7559, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11555.75390625
tensor(11555.7559, grad_fn=<NegBackward0>) tensor(11555.7539, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11555.7626953125
tensor(11555.7539, grad_fn=<NegBackward0>) tensor(11555.7627, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11555.7578125
tensor(11555.7539, grad_fn=<NegBackward0>) tensor(11555.7578, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11555.771484375
tensor(11555.7539, grad_fn=<NegBackward0>) tensor(11555.7715, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11555.7548828125
tensor(11555.7539, grad_fn=<NegBackward0>) tensor(11555.7549, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11555.76171875
tensor(11555.7539, grad_fn=<NegBackward0>) tensor(11555.7617, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.7188, 0.2812],
        [0.2858, 0.7142]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4677, 0.5323], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1912, 0.0998],
         [0.5558, 0.4009]],

        [[0.5105, 0.1037],
         [0.6889, 0.6312]],

        [[0.5171, 0.0971],
         [0.6283, 0.6947]],

        [[0.5990, 0.1006],
         [0.5141, 0.6383]],

        [[0.7290, 0.1059],
         [0.7275, 0.5545]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9841616161616162
[0.9840320165194881, 0.9840320165194881] [0.9841616161616162, 0.9841616161616162] [11555.765625, 11555.76171875]
-------------------------------------
This iteration is 94
True Objective function: Loss = -11414.110165600014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20775.75390625
inf tensor(20775.7539, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11705.21875
tensor(20775.7539, grad_fn=<NegBackward0>) tensor(11705.2188, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11545.9326171875
tensor(11705.2188, grad_fn=<NegBackward0>) tensor(11545.9326, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11473.001953125
tensor(11545.9326, grad_fn=<NegBackward0>) tensor(11473.0020, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11452.4775390625
tensor(11473.0020, grad_fn=<NegBackward0>) tensor(11452.4775, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11438.3125
tensor(11452.4775, grad_fn=<NegBackward0>) tensor(11438.3125, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11415.109375
tensor(11438.3125, grad_fn=<NegBackward0>) tensor(11415.1094, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11414.98828125
tensor(11415.1094, grad_fn=<NegBackward0>) tensor(11414.9883, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11414.9072265625
tensor(11414.9883, grad_fn=<NegBackward0>) tensor(11414.9072, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11414.65234375
tensor(11414.9072, grad_fn=<NegBackward0>) tensor(11414.6523, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11414.2529296875
tensor(11414.6523, grad_fn=<NegBackward0>) tensor(11414.2529, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11412.8671875
tensor(11414.2529, grad_fn=<NegBackward0>) tensor(11412.8672, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11404.8095703125
tensor(11412.8672, grad_fn=<NegBackward0>) tensor(11404.8096, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11404.7734375
tensor(11404.8096, grad_fn=<NegBackward0>) tensor(11404.7734, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11404.7587890625
tensor(11404.7734, grad_fn=<NegBackward0>) tensor(11404.7588, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11404.74609375
tensor(11404.7588, grad_fn=<NegBackward0>) tensor(11404.7461, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11404.7353515625
tensor(11404.7461, grad_fn=<NegBackward0>) tensor(11404.7354, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11404.7265625
tensor(11404.7354, grad_fn=<NegBackward0>) tensor(11404.7266, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11404.71875
tensor(11404.7266, grad_fn=<NegBackward0>) tensor(11404.7188, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11404.712890625
tensor(11404.7188, grad_fn=<NegBackward0>) tensor(11404.7129, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11404.7080078125
tensor(11404.7129, grad_fn=<NegBackward0>) tensor(11404.7080, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11404.7021484375
tensor(11404.7080, grad_fn=<NegBackward0>) tensor(11404.7021, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11404.69921875
tensor(11404.7021, grad_fn=<NegBackward0>) tensor(11404.6992, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11404.6953125
tensor(11404.6992, grad_fn=<NegBackward0>) tensor(11404.6953, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11404.6904296875
tensor(11404.6953, grad_fn=<NegBackward0>) tensor(11404.6904, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11404.689453125
tensor(11404.6904, grad_fn=<NegBackward0>) tensor(11404.6895, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11404.6875
tensor(11404.6895, grad_fn=<NegBackward0>) tensor(11404.6875, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11404.68359375
tensor(11404.6875, grad_fn=<NegBackward0>) tensor(11404.6836, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11404.681640625
tensor(11404.6836, grad_fn=<NegBackward0>) tensor(11404.6816, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11404.6806640625
tensor(11404.6816, grad_fn=<NegBackward0>) tensor(11404.6807, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11404.6787109375
tensor(11404.6807, grad_fn=<NegBackward0>) tensor(11404.6787, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11404.6767578125
tensor(11404.6787, grad_fn=<NegBackward0>) tensor(11404.6768, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11404.67578125
tensor(11404.6768, grad_fn=<NegBackward0>) tensor(11404.6758, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11404.6748046875
tensor(11404.6758, grad_fn=<NegBackward0>) tensor(11404.6748, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11404.673828125
tensor(11404.6748, grad_fn=<NegBackward0>) tensor(11404.6738, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11404.6728515625
tensor(11404.6738, grad_fn=<NegBackward0>) tensor(11404.6729, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11404.6708984375
tensor(11404.6729, grad_fn=<NegBackward0>) tensor(11404.6709, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11404.669921875
tensor(11404.6709, grad_fn=<NegBackward0>) tensor(11404.6699, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11404.6708984375
tensor(11404.6699, grad_fn=<NegBackward0>) tensor(11404.6709, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11404.6689453125
tensor(11404.6699, grad_fn=<NegBackward0>) tensor(11404.6689, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11404.66796875
tensor(11404.6689, grad_fn=<NegBackward0>) tensor(11404.6680, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11404.6669921875
tensor(11404.6680, grad_fn=<NegBackward0>) tensor(11404.6670, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11404.6669921875
tensor(11404.6670, grad_fn=<NegBackward0>) tensor(11404.6670, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11404.6708984375
tensor(11404.6670, grad_fn=<NegBackward0>) tensor(11404.6709, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11404.666015625
tensor(11404.6670, grad_fn=<NegBackward0>) tensor(11404.6660, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11404.666015625
tensor(11404.6660, grad_fn=<NegBackward0>) tensor(11404.6660, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11404.6650390625
tensor(11404.6660, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11404.6650390625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11404.6806640625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6807, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11404.6630859375
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6631, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11404.662109375
tensor(11404.6631, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11404.6630859375
tensor(11404.6621, grad_fn=<NegBackward0>) tensor(11404.6631, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11404.662109375
tensor(11404.6621, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11404.6669921875
tensor(11404.6621, grad_fn=<NegBackward0>) tensor(11404.6670, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11404.662109375
tensor(11404.6621, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11404.6611328125
tensor(11404.6621, grad_fn=<NegBackward0>) tensor(11404.6611, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11404.66015625
tensor(11404.6611, grad_fn=<NegBackward0>) tensor(11404.6602, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11404.6611328125
tensor(11404.6602, grad_fn=<NegBackward0>) tensor(11404.6611, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11404.662109375
tensor(11404.6602, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11404.66015625
tensor(11404.6602, grad_fn=<NegBackward0>) tensor(11404.6602, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11404.66015625
tensor(11404.6602, grad_fn=<NegBackward0>) tensor(11404.6602, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11404.6611328125
tensor(11404.6602, grad_fn=<NegBackward0>) tensor(11404.6611, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11404.6611328125
tensor(11404.6602, grad_fn=<NegBackward0>) tensor(11404.6611, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11404.6591796875
tensor(11404.6602, grad_fn=<NegBackward0>) tensor(11404.6592, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11404.66015625
tensor(11404.6592, grad_fn=<NegBackward0>) tensor(11404.6602, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11404.658203125
tensor(11404.6592, grad_fn=<NegBackward0>) tensor(11404.6582, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11404.6591796875
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6592, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11404.6689453125
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6689, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11404.662109375
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11404.658203125
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6582, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11404.6591796875
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6592, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11404.6611328125
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6611, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11404.658203125
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6582, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11404.658203125
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6582, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11404.6572265625
tensor(11404.6582, grad_fn=<NegBackward0>) tensor(11404.6572, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11404.658203125
tensor(11404.6572, grad_fn=<NegBackward0>) tensor(11404.6582, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11404.658203125
tensor(11404.6572, grad_fn=<NegBackward0>) tensor(11404.6582, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11404.6611328125
tensor(11404.6572, grad_fn=<NegBackward0>) tensor(11404.6611, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -11404.658203125
tensor(11404.6572, grad_fn=<NegBackward0>) tensor(11404.6582, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -11404.6708984375
tensor(11404.6572, grad_fn=<NegBackward0>) tensor(11404.6709, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7551, 0.2449],
        [0.2417, 0.7583]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4858, 0.5142], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4165, 0.0896],
         [0.6203, 0.1969]],

        [[0.5621, 0.0962],
         [0.5525, 0.6294]],

        [[0.5811, 0.1055],
         [0.7067, 0.6574]],

        [[0.7143, 0.0944],
         [0.7251, 0.5957]],

        [[0.6001, 0.0960],
         [0.6773, 0.6562]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.984032031618886
Average Adjusted Rand Index: 0.9839993417272901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22332.2734375
inf tensor(22332.2734, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12306.9462890625
tensor(22332.2734, grad_fn=<NegBackward0>) tensor(12306.9463, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11839.2197265625
tensor(12306.9463, grad_fn=<NegBackward0>) tensor(11839.2197, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11618.271484375
tensor(11839.2197, grad_fn=<NegBackward0>) tensor(11618.2715, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11498.833984375
tensor(11618.2715, grad_fn=<NegBackward0>) tensor(11498.8340, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11422.240234375
tensor(11498.8340, grad_fn=<NegBackward0>) tensor(11422.2402, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11421.8154296875
tensor(11422.2402, grad_fn=<NegBackward0>) tensor(11421.8154, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11415.6787109375
tensor(11421.8154, grad_fn=<NegBackward0>) tensor(11415.6787, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11413.3349609375
tensor(11415.6787, grad_fn=<NegBackward0>) tensor(11413.3350, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11411.9404296875
tensor(11413.3350, grad_fn=<NegBackward0>) tensor(11411.9404, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11411.79296875
tensor(11411.9404, grad_fn=<NegBackward0>) tensor(11411.7930, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11411.7412109375
tensor(11411.7930, grad_fn=<NegBackward0>) tensor(11411.7412, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11411.703125
tensor(11411.7412, grad_fn=<NegBackward0>) tensor(11411.7031, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11411.671875
tensor(11411.7031, grad_fn=<NegBackward0>) tensor(11411.6719, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11411.6416015625
tensor(11411.6719, grad_fn=<NegBackward0>) tensor(11411.6416, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11411.52734375
tensor(11411.6416, grad_fn=<NegBackward0>) tensor(11411.5273, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11411.3828125
tensor(11411.5273, grad_fn=<NegBackward0>) tensor(11411.3828, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11411.369140625
tensor(11411.3828, grad_fn=<NegBackward0>) tensor(11411.3691, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11411.3583984375
tensor(11411.3691, grad_fn=<NegBackward0>) tensor(11411.3584, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11411.3466796875
tensor(11411.3584, grad_fn=<NegBackward0>) tensor(11411.3467, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11411.3388671875
tensor(11411.3467, grad_fn=<NegBackward0>) tensor(11411.3389, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11411.3310546875
tensor(11411.3389, grad_fn=<NegBackward0>) tensor(11411.3311, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11411.3251953125
tensor(11411.3311, grad_fn=<NegBackward0>) tensor(11411.3252, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11411.318359375
tensor(11411.3252, grad_fn=<NegBackward0>) tensor(11411.3184, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11411.3125
tensor(11411.3184, grad_fn=<NegBackward0>) tensor(11411.3125, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11411.310546875
tensor(11411.3125, grad_fn=<NegBackward0>) tensor(11411.3105, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11411.3037109375
tensor(11411.3105, grad_fn=<NegBackward0>) tensor(11411.3037, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11411.298828125
tensor(11411.3037, grad_fn=<NegBackward0>) tensor(11411.2988, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11411.296875
tensor(11411.2988, grad_fn=<NegBackward0>) tensor(11411.2969, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11411.29296875
tensor(11411.2969, grad_fn=<NegBackward0>) tensor(11411.2930, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11404.94921875
tensor(11411.2930, grad_fn=<NegBackward0>) tensor(11404.9492, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11404.9033203125
tensor(11404.9492, grad_fn=<NegBackward0>) tensor(11404.9033, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11404.9033203125
tensor(11404.9033, grad_fn=<NegBackward0>) tensor(11404.9033, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11404.8984375
tensor(11404.9033, grad_fn=<NegBackward0>) tensor(11404.8984, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11404.8837890625
tensor(11404.8984, grad_fn=<NegBackward0>) tensor(11404.8838, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11404.6884765625
tensor(11404.8838, grad_fn=<NegBackward0>) tensor(11404.6885, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11404.6845703125
tensor(11404.6885, grad_fn=<NegBackward0>) tensor(11404.6846, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11404.68359375
tensor(11404.6846, grad_fn=<NegBackward0>) tensor(11404.6836, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11404.681640625
tensor(11404.6836, grad_fn=<NegBackward0>) tensor(11404.6816, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11404.6806640625
tensor(11404.6816, grad_fn=<NegBackward0>) tensor(11404.6807, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11404.685546875
tensor(11404.6807, grad_fn=<NegBackward0>) tensor(11404.6855, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11404.6767578125
tensor(11404.6807, grad_fn=<NegBackward0>) tensor(11404.6768, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11404.67578125
tensor(11404.6768, grad_fn=<NegBackward0>) tensor(11404.6758, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11404.67578125
tensor(11404.6758, grad_fn=<NegBackward0>) tensor(11404.6758, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11404.6748046875
tensor(11404.6758, grad_fn=<NegBackward0>) tensor(11404.6748, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11404.6728515625
tensor(11404.6748, grad_fn=<NegBackward0>) tensor(11404.6729, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11404.6728515625
tensor(11404.6729, grad_fn=<NegBackward0>) tensor(11404.6729, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11404.671875
tensor(11404.6729, grad_fn=<NegBackward0>) tensor(11404.6719, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11404.671875
tensor(11404.6719, grad_fn=<NegBackward0>) tensor(11404.6719, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11404.6728515625
tensor(11404.6719, grad_fn=<NegBackward0>) tensor(11404.6729, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11404.6689453125
tensor(11404.6719, grad_fn=<NegBackward0>) tensor(11404.6689, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11404.669921875
tensor(11404.6689, grad_fn=<NegBackward0>) tensor(11404.6699, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11404.66796875
tensor(11404.6689, grad_fn=<NegBackward0>) tensor(11404.6680, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11404.6689453125
tensor(11404.6680, grad_fn=<NegBackward0>) tensor(11404.6689, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11404.6689453125
tensor(11404.6680, grad_fn=<NegBackward0>) tensor(11404.6689, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11404.66796875
tensor(11404.6680, grad_fn=<NegBackward0>) tensor(11404.6680, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11404.6669921875
tensor(11404.6680, grad_fn=<NegBackward0>) tensor(11404.6670, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11404.6669921875
tensor(11404.6670, grad_fn=<NegBackward0>) tensor(11404.6670, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11404.6650390625
tensor(11404.6670, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11404.666015625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6660, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11404.6650390625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11404.6650390625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11404.6689453125
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6689, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11404.666015625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6660, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11404.6650390625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11404.666015625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6660, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11404.6650390625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11404.6640625
tensor(11404.6650, grad_fn=<NegBackward0>) tensor(11404.6641, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11404.6650390625
tensor(11404.6641, grad_fn=<NegBackward0>) tensor(11404.6650, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11404.6611328125
tensor(11404.6641, grad_fn=<NegBackward0>) tensor(11404.6611, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11404.662109375
tensor(11404.6611, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11404.662109375
tensor(11404.6611, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11404.662109375
tensor(11404.6611, grad_fn=<NegBackward0>) tensor(11404.6621, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11404.6640625
tensor(11404.6611, grad_fn=<NegBackward0>) tensor(11404.6641, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11404.6669921875
tensor(11404.6611, grad_fn=<NegBackward0>) tensor(11404.6670, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.7520, 0.2480],
        [0.2460, 0.7540]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4811, 0.5189], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4160, 0.0896],
         [0.6960, 0.1969]],

        [[0.6499, 0.0962],
         [0.7243, 0.5691]],

        [[0.6386, 0.1055],
         [0.7077, 0.5608]],

        [[0.7211, 0.0944],
         [0.5784, 0.5820]],

        [[0.5119, 0.0960],
         [0.6330, 0.6170]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.984032031618886
Average Adjusted Rand Index: 0.9839993417272901
[0.984032031618886, 0.984032031618886] [0.9839993417272901, 0.9839993417272901] [11404.6708984375, 11404.6669921875]
-------------------------------------
This iteration is 95
True Objective function: Loss = -11458.191035573027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22540.05078125
inf tensor(22540.0508, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12080.76171875
tensor(22540.0508, grad_fn=<NegBackward0>) tensor(12080.7617, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11783.4697265625
tensor(12080.7617, grad_fn=<NegBackward0>) tensor(11783.4697, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11724.3759765625
tensor(11783.4697, grad_fn=<NegBackward0>) tensor(11724.3760, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11714.5654296875
tensor(11724.3760, grad_fn=<NegBackward0>) tensor(11714.5654, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11702.1171875
tensor(11714.5654, grad_fn=<NegBackward0>) tensor(11702.1172, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11701.900390625
tensor(11702.1172, grad_fn=<NegBackward0>) tensor(11701.9004, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11700.837890625
tensor(11701.9004, grad_fn=<NegBackward0>) tensor(11700.8379, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11697.548828125
tensor(11700.8379, grad_fn=<NegBackward0>) tensor(11697.5488, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11693.099609375
tensor(11697.5488, grad_fn=<NegBackward0>) tensor(11693.0996, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11681.7236328125
tensor(11693.0996, grad_fn=<NegBackward0>) tensor(11681.7236, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11676.08984375
tensor(11681.7236, grad_fn=<NegBackward0>) tensor(11676.0898, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11676.0244140625
tensor(11676.0898, grad_fn=<NegBackward0>) tensor(11676.0244, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11676.0107421875
tensor(11676.0244, grad_fn=<NegBackward0>) tensor(11676.0107, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11676.0048828125
tensor(11676.0107, grad_fn=<NegBackward0>) tensor(11676.0049, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11675.998046875
tensor(11676.0049, grad_fn=<NegBackward0>) tensor(11675.9980, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11675.9931640625
tensor(11675.9980, grad_fn=<NegBackward0>) tensor(11675.9932, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11675.986328125
tensor(11675.9932, grad_fn=<NegBackward0>) tensor(11675.9863, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11675.9267578125
tensor(11675.9863, grad_fn=<NegBackward0>) tensor(11675.9268, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11675.900390625
tensor(11675.9268, grad_fn=<NegBackward0>) tensor(11675.9004, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11675.8974609375
tensor(11675.9004, grad_fn=<NegBackward0>) tensor(11675.8975, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11675.8955078125
tensor(11675.8975, grad_fn=<NegBackward0>) tensor(11675.8955, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11675.8935546875
tensor(11675.8955, grad_fn=<NegBackward0>) tensor(11675.8936, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11675.890625
tensor(11675.8936, grad_fn=<NegBackward0>) tensor(11675.8906, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11675.88671875
tensor(11675.8906, grad_fn=<NegBackward0>) tensor(11675.8867, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11675.8837890625
tensor(11675.8867, grad_fn=<NegBackward0>) tensor(11675.8838, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11675.8828125
tensor(11675.8838, grad_fn=<NegBackward0>) tensor(11675.8828, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11675.880859375
tensor(11675.8828, grad_fn=<NegBackward0>) tensor(11675.8809, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11675.8798828125
tensor(11675.8809, grad_fn=<NegBackward0>) tensor(11675.8799, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11675.87890625
tensor(11675.8799, grad_fn=<NegBackward0>) tensor(11675.8789, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11675.876953125
tensor(11675.8789, grad_fn=<NegBackward0>) tensor(11675.8770, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11675.8759765625
tensor(11675.8770, grad_fn=<NegBackward0>) tensor(11675.8760, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11675.8798828125
tensor(11675.8760, grad_fn=<NegBackward0>) tensor(11675.8799, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11675.875
tensor(11675.8760, grad_fn=<NegBackward0>) tensor(11675.8750, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11675.8759765625
tensor(11675.8750, grad_fn=<NegBackward0>) tensor(11675.8760, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11675.8798828125
tensor(11675.8750, grad_fn=<NegBackward0>) tensor(11675.8799, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -11675.875
tensor(11675.8750, grad_fn=<NegBackward0>) tensor(11675.8750, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11675.875
tensor(11675.8750, grad_fn=<NegBackward0>) tensor(11675.8750, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11675.8740234375
tensor(11675.8750, grad_fn=<NegBackward0>) tensor(11675.8740, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11675.873046875
tensor(11675.8740, grad_fn=<NegBackward0>) tensor(11675.8730, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11675.8740234375
tensor(11675.8730, grad_fn=<NegBackward0>) tensor(11675.8740, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11675.873046875
tensor(11675.8730, grad_fn=<NegBackward0>) tensor(11675.8730, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11675.87109375
tensor(11675.8730, grad_fn=<NegBackward0>) tensor(11675.8711, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11675.87109375
tensor(11675.8711, grad_fn=<NegBackward0>) tensor(11675.8711, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11675.87109375
tensor(11675.8711, grad_fn=<NegBackward0>) tensor(11675.8711, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11675.8701171875
tensor(11675.8711, grad_fn=<NegBackward0>) tensor(11675.8701, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11675.8701171875
tensor(11675.8701, grad_fn=<NegBackward0>) tensor(11675.8701, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11675.8701171875
tensor(11675.8701, grad_fn=<NegBackward0>) tensor(11675.8701, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11675.87109375
tensor(11675.8701, grad_fn=<NegBackward0>) tensor(11675.8711, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11675.87109375
tensor(11675.8701, grad_fn=<NegBackward0>) tensor(11675.8711, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11675.869140625
tensor(11675.8701, grad_fn=<NegBackward0>) tensor(11675.8691, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11675.869140625
tensor(11675.8691, grad_fn=<NegBackward0>) tensor(11675.8691, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11675.869140625
tensor(11675.8691, grad_fn=<NegBackward0>) tensor(11675.8691, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11675.869140625
tensor(11675.8691, grad_fn=<NegBackward0>) tensor(11675.8691, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11675.869140625
tensor(11675.8691, grad_fn=<NegBackward0>) tensor(11675.8691, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11675.8681640625
tensor(11675.8691, grad_fn=<NegBackward0>) tensor(11675.8682, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11675.8720703125
tensor(11675.8682, grad_fn=<NegBackward0>) tensor(11675.8721, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11675.8681640625
tensor(11675.8682, grad_fn=<NegBackward0>) tensor(11675.8682, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11675.869140625
tensor(11675.8682, grad_fn=<NegBackward0>) tensor(11675.8691, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11675.8671875
tensor(11675.8682, grad_fn=<NegBackward0>) tensor(11675.8672, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11675.8671875
tensor(11675.8672, grad_fn=<NegBackward0>) tensor(11675.8672, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11675.86328125
tensor(11675.8672, grad_fn=<NegBackward0>) tensor(11675.8633, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11675.86328125
tensor(11675.8633, grad_fn=<NegBackward0>) tensor(11675.8633, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11675.8623046875
tensor(11675.8633, grad_fn=<NegBackward0>) tensor(11675.8623, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11675.8623046875
tensor(11675.8623, grad_fn=<NegBackward0>) tensor(11675.8623, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11675.873046875
tensor(11675.8623, grad_fn=<NegBackward0>) tensor(11675.8730, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11675.861328125
tensor(11675.8623, grad_fn=<NegBackward0>) tensor(11675.8613, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11675.8623046875
tensor(11675.8613, grad_fn=<NegBackward0>) tensor(11675.8623, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11675.86328125
tensor(11675.8613, grad_fn=<NegBackward0>) tensor(11675.8633, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11675.861328125
tensor(11675.8613, grad_fn=<NegBackward0>) tensor(11675.8613, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11675.861328125
tensor(11675.8613, grad_fn=<NegBackward0>) tensor(11675.8613, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11675.865234375
tensor(11675.8613, grad_fn=<NegBackward0>) tensor(11675.8652, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11675.8603515625
tensor(11675.8613, grad_fn=<NegBackward0>) tensor(11675.8604, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11675.8876953125
tensor(11675.8604, grad_fn=<NegBackward0>) tensor(11675.8877, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11675.859375
tensor(11675.8604, grad_fn=<NegBackward0>) tensor(11675.8594, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11675.8662109375
tensor(11675.8594, grad_fn=<NegBackward0>) tensor(11675.8662, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11675.8603515625
tensor(11675.8594, grad_fn=<NegBackward0>) tensor(11675.8604, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11675.859375
tensor(11675.8594, grad_fn=<NegBackward0>) tensor(11675.8594, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11675.861328125
tensor(11675.8594, grad_fn=<NegBackward0>) tensor(11675.8613, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11675.8525390625
tensor(11675.8594, grad_fn=<NegBackward0>) tensor(11675.8525, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11675.8525390625
tensor(11675.8525, grad_fn=<NegBackward0>) tensor(11675.8525, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11675.8515625
tensor(11675.8525, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11675.8525390625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8525, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11675.8525390625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8525, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11676.0205078125
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11676.0205, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11675.8525390625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8525, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11675.8525390625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8525, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11675.8525390625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8525, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11675.8515625
tensor(11675.8516, grad_fn=<NegBackward0>) tensor(11675.8516, grad_fn=<NegBackward0>)
pi: tensor([[0.1517, 0.8483],
        [0.6319, 0.3681]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5521, 0.4479], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3570, 0.0971],
         [0.6932, 0.2445]],

        [[0.6080, 0.1075],
         [0.5245, 0.5671]],

        [[0.5186, 0.0930],
         [0.7246, 0.5974]],

        [[0.6421, 0.0999],
         [0.5381, 0.5672]],

        [[0.7173, 0.0885],
         [0.6494, 0.6995]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 80
Adjusted Rand Index: 0.35333678220382825
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.4852686308492201
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.1084533705272011
Average Adjusted Rand Index: 0.751720298940339
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21277.212890625
inf tensor(21277.2129, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12094.5556640625
tensor(21277.2129, grad_fn=<NegBackward0>) tensor(12094.5557, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11708.21484375
tensor(12094.5557, grad_fn=<NegBackward0>) tensor(11708.2148, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11521.5146484375
tensor(11708.2148, grad_fn=<NegBackward0>) tensor(11521.5146, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11495.2978515625
tensor(11521.5146, grad_fn=<NegBackward0>) tensor(11495.2979, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11478.0673828125
tensor(11495.2979, grad_fn=<NegBackward0>) tensor(11478.0674, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11477.8544921875
tensor(11478.0674, grad_fn=<NegBackward0>) tensor(11477.8545, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11464.244140625
tensor(11477.8545, grad_fn=<NegBackward0>) tensor(11464.2441, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11464.1240234375
tensor(11464.2441, grad_fn=<NegBackward0>) tensor(11464.1240, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11464.0712890625
tensor(11464.1240, grad_fn=<NegBackward0>) tensor(11464.0713, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11464.0302734375
tensor(11464.0713, grad_fn=<NegBackward0>) tensor(11464.0303, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11451.2314453125
tensor(11464.0303, grad_fn=<NegBackward0>) tensor(11451.2314, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11451.197265625
tensor(11451.2314, grad_fn=<NegBackward0>) tensor(11451.1973, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11451.17578125
tensor(11451.1973, grad_fn=<NegBackward0>) tensor(11451.1758, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11450.0869140625
tensor(11451.1758, grad_fn=<NegBackward0>) tensor(11450.0869, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11449.9169921875
tensor(11450.0869, grad_fn=<NegBackward0>) tensor(11449.9170, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11449.90625
tensor(11449.9170, grad_fn=<NegBackward0>) tensor(11449.9062, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11449.89453125
tensor(11449.9062, grad_fn=<NegBackward0>) tensor(11449.8945, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11449.884765625
tensor(11449.8945, grad_fn=<NegBackward0>) tensor(11449.8848, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11449.87890625
tensor(11449.8848, grad_fn=<NegBackward0>) tensor(11449.8789, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11449.873046875
tensor(11449.8789, grad_fn=<NegBackward0>) tensor(11449.8730, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11449.8681640625
tensor(11449.8730, grad_fn=<NegBackward0>) tensor(11449.8682, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11449.8642578125
tensor(11449.8682, grad_fn=<NegBackward0>) tensor(11449.8643, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11449.8623046875
tensor(11449.8643, grad_fn=<NegBackward0>) tensor(11449.8623, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11449.857421875
tensor(11449.8623, grad_fn=<NegBackward0>) tensor(11449.8574, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11449.8564453125
tensor(11449.8574, grad_fn=<NegBackward0>) tensor(11449.8564, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11449.853515625
tensor(11449.8564, grad_fn=<NegBackward0>) tensor(11449.8535, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11449.8515625
tensor(11449.8535, grad_fn=<NegBackward0>) tensor(11449.8516, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11449.8486328125
tensor(11449.8516, grad_fn=<NegBackward0>) tensor(11449.8486, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11449.8466796875
tensor(11449.8486, grad_fn=<NegBackward0>) tensor(11449.8467, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11449.845703125
tensor(11449.8467, grad_fn=<NegBackward0>) tensor(11449.8457, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11449.84375
tensor(11449.8457, grad_fn=<NegBackward0>) tensor(11449.8438, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11449.841796875
tensor(11449.8438, grad_fn=<NegBackward0>) tensor(11449.8418, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11449.841796875
tensor(11449.8418, grad_fn=<NegBackward0>) tensor(11449.8418, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11449.83984375
tensor(11449.8418, grad_fn=<NegBackward0>) tensor(11449.8398, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11449.8388671875
tensor(11449.8398, grad_fn=<NegBackward0>) tensor(11449.8389, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11449.837890625
tensor(11449.8389, grad_fn=<NegBackward0>) tensor(11449.8379, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11449.837890625
tensor(11449.8379, grad_fn=<NegBackward0>) tensor(11449.8379, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11449.837890625
tensor(11449.8379, grad_fn=<NegBackward0>) tensor(11449.8379, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11449.8359375
tensor(11449.8379, grad_fn=<NegBackward0>) tensor(11449.8359, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11449.833984375
tensor(11449.8359, grad_fn=<NegBackward0>) tensor(11449.8340, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11449.8349609375
tensor(11449.8340, grad_fn=<NegBackward0>) tensor(11449.8350, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11449.833984375
tensor(11449.8340, grad_fn=<NegBackward0>) tensor(11449.8340, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11449.8369140625
tensor(11449.8340, grad_fn=<NegBackward0>) tensor(11449.8369, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11449.8330078125
tensor(11449.8340, grad_fn=<NegBackward0>) tensor(11449.8330, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11449.833984375
tensor(11449.8330, grad_fn=<NegBackward0>) tensor(11449.8340, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11449.8310546875
tensor(11449.8330, grad_fn=<NegBackward0>) tensor(11449.8311, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11449.833984375
tensor(11449.8311, grad_fn=<NegBackward0>) tensor(11449.8340, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11449.8291015625
tensor(11449.8311, grad_fn=<NegBackward0>) tensor(11449.8291, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11449.8271484375
tensor(11449.8291, grad_fn=<NegBackward0>) tensor(11449.8271, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11449.82421875
tensor(11449.8271, grad_fn=<NegBackward0>) tensor(11449.8242, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11449.82421875
tensor(11449.8242, grad_fn=<NegBackward0>) tensor(11449.8242, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11449.826171875
tensor(11449.8242, grad_fn=<NegBackward0>) tensor(11449.8262, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11449.8232421875
tensor(11449.8242, grad_fn=<NegBackward0>) tensor(11449.8232, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11449.830078125
tensor(11449.8232, grad_fn=<NegBackward0>) tensor(11449.8301, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11449.82421875
tensor(11449.8232, grad_fn=<NegBackward0>) tensor(11449.8242, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11449.8271484375
tensor(11449.8232, grad_fn=<NegBackward0>) tensor(11449.8271, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11449.826171875
tensor(11449.8232, grad_fn=<NegBackward0>) tensor(11449.8262, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -11449.822265625
tensor(11449.8232, grad_fn=<NegBackward0>) tensor(11449.8223, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11449.8212890625
tensor(11449.8223, grad_fn=<NegBackward0>) tensor(11449.8213, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11449.8212890625
tensor(11449.8213, grad_fn=<NegBackward0>) tensor(11449.8213, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11449.822265625
tensor(11449.8213, grad_fn=<NegBackward0>) tensor(11449.8223, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11449.8212890625
tensor(11449.8213, grad_fn=<NegBackward0>) tensor(11449.8213, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11449.822265625
tensor(11449.8213, grad_fn=<NegBackward0>) tensor(11449.8223, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11449.8203125
tensor(11449.8213, grad_fn=<NegBackward0>) tensor(11449.8203, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11449.826171875
tensor(11449.8203, grad_fn=<NegBackward0>) tensor(11449.8262, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11449.8193359375
tensor(11449.8203, grad_fn=<NegBackward0>) tensor(11449.8193, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11449.8203125
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8203, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11449.8203125
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8203, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11449.8212890625
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8213, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11449.8193359375
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8193, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11449.8349609375
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8350, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11449.828125
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8281, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11449.8232421875
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8232, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11449.822265625
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8223, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11449.818359375
tensor(11449.8193, grad_fn=<NegBackward0>) tensor(11449.8184, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11449.8203125
tensor(11449.8184, grad_fn=<NegBackward0>) tensor(11449.8203, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11449.8173828125
tensor(11449.8184, grad_fn=<NegBackward0>) tensor(11449.8174, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11449.8251953125
tensor(11449.8174, grad_fn=<NegBackward0>) tensor(11449.8252, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11449.8193359375
tensor(11449.8174, grad_fn=<NegBackward0>) tensor(11449.8193, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11449.8203125
tensor(11449.8174, grad_fn=<NegBackward0>) tensor(11449.8203, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11449.83203125
tensor(11449.8174, grad_fn=<NegBackward0>) tensor(11449.8320, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11449.8193359375
tensor(11449.8174, grad_fn=<NegBackward0>) tensor(11449.8193, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.7244, 0.2756],
        [0.2447, 0.7553]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5411, 0.4589], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3937, 0.0986],
         [0.5899, 0.2049]],

        [[0.6741, 0.1015],
         [0.5100, 0.6013]],

        [[0.7227, 0.0939],
         [0.5212, 0.5241]],

        [[0.7045, 0.0961],
         [0.6739, 0.5540]],

        [[0.5841, 0.0885],
         [0.6676, 0.6780]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320099978856
Average Adjusted Rand Index: 0.9839984326594585
[0.1084533705272011, 0.9840320099978856] [0.751720298940339, 0.9839984326594585] [11675.857421875, 11449.8193359375]
-------------------------------------
This iteration is 96
True Objective function: Loss = -11461.529943808191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21312.1484375
inf tensor(21312.1484, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12179.580078125
tensor(21312.1484, grad_fn=<NegBackward0>) tensor(12179.5801, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12144.458984375
tensor(12179.5801, grad_fn=<NegBackward0>) tensor(12144.4590, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11758.23046875
tensor(12144.4590, grad_fn=<NegBackward0>) tensor(11758.2305, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11713.849609375
tensor(11758.2305, grad_fn=<NegBackward0>) tensor(11713.8496, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11700.7587890625
tensor(11713.8496, grad_fn=<NegBackward0>) tensor(11700.7588, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11685.4970703125
tensor(11700.7588, grad_fn=<NegBackward0>) tensor(11685.4971, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11663.53125
tensor(11685.4971, grad_fn=<NegBackward0>) tensor(11663.5312, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11663.4169921875
tensor(11663.5312, grad_fn=<NegBackward0>) tensor(11663.4170, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11656.1123046875
tensor(11663.4170, grad_fn=<NegBackward0>) tensor(11656.1123, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11655.787109375
tensor(11656.1123, grad_fn=<NegBackward0>) tensor(11655.7871, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11644.802734375
tensor(11655.7871, grad_fn=<NegBackward0>) tensor(11644.8027, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11644.771484375
tensor(11644.8027, grad_fn=<NegBackward0>) tensor(11644.7715, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11628.38671875
tensor(11644.7715, grad_fn=<NegBackward0>) tensor(11628.3867, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11623.15234375
tensor(11628.3867, grad_fn=<NegBackward0>) tensor(11623.1523, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11614.7646484375
tensor(11623.1523, grad_fn=<NegBackward0>) tensor(11614.7646, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11614.71875
tensor(11614.7646, grad_fn=<NegBackward0>) tensor(11614.7188, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11614.7080078125
tensor(11614.7188, grad_fn=<NegBackward0>) tensor(11614.7080, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11614.6875
tensor(11614.7080, grad_fn=<NegBackward0>) tensor(11614.6875, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11608.1611328125
tensor(11614.6875, grad_fn=<NegBackward0>) tensor(11608.1611, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11608.1171875
tensor(11608.1611, grad_fn=<NegBackward0>) tensor(11608.1172, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11608.111328125
tensor(11608.1172, grad_fn=<NegBackward0>) tensor(11608.1113, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11608.107421875
tensor(11608.1113, grad_fn=<NegBackward0>) tensor(11608.1074, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11608.1025390625
tensor(11608.1074, grad_fn=<NegBackward0>) tensor(11608.1025, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11608.0986328125
tensor(11608.1025, grad_fn=<NegBackward0>) tensor(11608.0986, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11608.095703125
tensor(11608.0986, grad_fn=<NegBackward0>) tensor(11608.0957, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11608.09375
tensor(11608.0957, grad_fn=<NegBackward0>) tensor(11608.0938, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11608.0908203125
tensor(11608.0938, grad_fn=<NegBackward0>) tensor(11608.0908, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11608.087890625
tensor(11608.0908, grad_fn=<NegBackward0>) tensor(11608.0879, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11608.0869140625
tensor(11608.0879, grad_fn=<NegBackward0>) tensor(11608.0869, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11608.083984375
tensor(11608.0869, grad_fn=<NegBackward0>) tensor(11608.0840, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11608.0732421875
tensor(11608.0840, grad_fn=<NegBackward0>) tensor(11608.0732, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11608.068359375
tensor(11608.0732, grad_fn=<NegBackward0>) tensor(11608.0684, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11608.064453125
tensor(11608.0684, grad_fn=<NegBackward0>) tensor(11608.0645, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11608.064453125
tensor(11608.0645, grad_fn=<NegBackward0>) tensor(11608.0645, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11608.0654296875
tensor(11608.0645, grad_fn=<NegBackward0>) tensor(11608.0654, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11608.0625
tensor(11608.0645, grad_fn=<NegBackward0>) tensor(11608.0625, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11608.0615234375
tensor(11608.0625, grad_fn=<NegBackward0>) tensor(11608.0615, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11608.060546875
tensor(11608.0615, grad_fn=<NegBackward0>) tensor(11608.0605, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11608.060546875
tensor(11608.0605, grad_fn=<NegBackward0>) tensor(11608.0605, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11608.060546875
tensor(11608.0605, grad_fn=<NegBackward0>) tensor(11608.0605, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11608.0595703125
tensor(11608.0605, grad_fn=<NegBackward0>) tensor(11608.0596, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11608.05859375
tensor(11608.0596, grad_fn=<NegBackward0>) tensor(11608.0586, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11608.0576171875
tensor(11608.0586, grad_fn=<NegBackward0>) tensor(11608.0576, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11608.05859375
tensor(11608.0576, grad_fn=<NegBackward0>) tensor(11608.0586, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11608.056640625
tensor(11608.0576, grad_fn=<NegBackward0>) tensor(11608.0566, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11608.0546875
tensor(11608.0566, grad_fn=<NegBackward0>) tensor(11608.0547, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11608.0556640625
tensor(11608.0547, grad_fn=<NegBackward0>) tensor(11608.0557, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11608.0546875
tensor(11608.0547, grad_fn=<NegBackward0>) tensor(11608.0547, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11608.0546875
tensor(11608.0547, grad_fn=<NegBackward0>) tensor(11608.0547, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11608.0546875
tensor(11608.0547, grad_fn=<NegBackward0>) tensor(11608.0547, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11608.0537109375
tensor(11608.0547, grad_fn=<NegBackward0>) tensor(11608.0537, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11608.052734375
tensor(11608.0537, grad_fn=<NegBackward0>) tensor(11608.0527, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11608.046875
tensor(11608.0527, grad_fn=<NegBackward0>) tensor(11608.0469, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11592.8974609375
tensor(11608.0469, grad_fn=<NegBackward0>) tensor(11592.8975, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11592.8935546875
tensor(11592.8975, grad_fn=<NegBackward0>) tensor(11592.8936, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11592.892578125
tensor(11592.8936, grad_fn=<NegBackward0>) tensor(11592.8926, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11592.892578125
tensor(11592.8926, grad_fn=<NegBackward0>) tensor(11592.8926, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11592.892578125
tensor(11592.8926, grad_fn=<NegBackward0>) tensor(11592.8926, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11592.8935546875
tensor(11592.8926, grad_fn=<NegBackward0>) tensor(11592.8936, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11592.892578125
tensor(11592.8926, grad_fn=<NegBackward0>) tensor(11592.8926, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11592.8916015625
tensor(11592.8926, grad_fn=<NegBackward0>) tensor(11592.8916, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11592.8916015625
tensor(11592.8916, grad_fn=<NegBackward0>) tensor(11592.8916, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11592.8935546875
tensor(11592.8916, grad_fn=<NegBackward0>) tensor(11592.8936, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11592.890625
tensor(11592.8916, grad_fn=<NegBackward0>) tensor(11592.8906, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11592.892578125
tensor(11592.8906, grad_fn=<NegBackward0>) tensor(11592.8926, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11592.8935546875
tensor(11592.8906, grad_fn=<NegBackward0>) tensor(11592.8936, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11592.8896484375
tensor(11592.8906, grad_fn=<NegBackward0>) tensor(11592.8896, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11592.888671875
tensor(11592.8896, grad_fn=<NegBackward0>) tensor(11592.8887, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11592.908203125
tensor(11592.8887, grad_fn=<NegBackward0>) tensor(11592.9082, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11592.8291015625
tensor(11592.8887, grad_fn=<NegBackward0>) tensor(11592.8291, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11592.8369140625
tensor(11592.8291, grad_fn=<NegBackward0>) tensor(11592.8369, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11592.828125
tensor(11592.8291, grad_fn=<NegBackward0>) tensor(11592.8281, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11592.826171875
tensor(11592.8281, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11592.8271484375
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8271, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11592.8330078125
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8330, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11592.841796875
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8418, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11592.8037109375
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8037, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11592.794921875
tensor(11592.8037, grad_fn=<NegBackward0>) tensor(11592.7949, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11592.794921875
tensor(11592.7949, grad_fn=<NegBackward0>) tensor(11592.7949, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11592.794921875
tensor(11592.7949, grad_fn=<NegBackward0>) tensor(11592.7949, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11592.794921875
tensor(11592.7949, grad_fn=<NegBackward0>) tensor(11592.7949, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11592.8671875
tensor(11592.7949, grad_fn=<NegBackward0>) tensor(11592.8672, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11592.7939453125
tensor(11592.7949, grad_fn=<NegBackward0>) tensor(11592.7939, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11592.796875
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7969, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11592.796875
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7969, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11592.7958984375
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7959, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11592.7939453125
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7939, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11592.7939453125
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7939, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11592.7958984375
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7959, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11592.7939453125
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7939, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11592.8046875
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.8047, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11592.76953125
tensor(11592.7939, grad_fn=<NegBackward0>) tensor(11592.7695, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11592.7646484375
tensor(11592.7695, grad_fn=<NegBackward0>) tensor(11592.7646, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11592.7724609375
tensor(11592.7646, grad_fn=<NegBackward0>) tensor(11592.7725, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11592.763671875
tensor(11592.7646, grad_fn=<NegBackward0>) tensor(11592.7637, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11592.7861328125
tensor(11592.7637, grad_fn=<NegBackward0>) tensor(11592.7861, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11592.7900390625
tensor(11592.7637, grad_fn=<NegBackward0>) tensor(11592.7900, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11592.7626953125
tensor(11592.7637, grad_fn=<NegBackward0>) tensor(11592.7627, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11592.7646484375
tensor(11592.7627, grad_fn=<NegBackward0>) tensor(11592.7646, grad_fn=<NegBackward0>)
1
pi: tensor([[0.6713, 0.3287],
        [0.2380, 0.7620]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9941, 0.0059], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.1055],
         [0.5445, 0.3996]],

        [[0.6246, 0.0926],
         [0.5723, 0.5538]],

        [[0.6512, 0.1033],
         [0.5129, 0.5867]],

        [[0.5212, 0.1039],
         [0.6852, 0.7139]],

        [[0.6766, 0.0993],
         [0.7061, 0.6065]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6849276777072695
Average Adjusted Rand Index: 0.792
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22111.8515625
inf tensor(22111.8516, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12174.5830078125
tensor(22111.8516, grad_fn=<NegBackward0>) tensor(12174.5830, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11904.9462890625
tensor(12174.5830, grad_fn=<NegBackward0>) tensor(11904.9463, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11717.8154296875
tensor(11904.9463, grad_fn=<NegBackward0>) tensor(11717.8154, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11694.72265625
tensor(11717.8154, grad_fn=<NegBackward0>) tensor(11694.7227, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11677.3916015625
tensor(11694.7227, grad_fn=<NegBackward0>) tensor(11677.3916, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11670.0810546875
tensor(11677.3916, grad_fn=<NegBackward0>) tensor(11670.0811, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11655.40234375
tensor(11670.0811, grad_fn=<NegBackward0>) tensor(11655.4023, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11647.1494140625
tensor(11655.4023, grad_fn=<NegBackward0>) tensor(11647.1494, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11630.8564453125
tensor(11647.1494, grad_fn=<NegBackward0>) tensor(11630.8564, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11630.060546875
tensor(11630.8564, grad_fn=<NegBackward0>) tensor(11630.0605, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11630.021484375
tensor(11630.0605, grad_fn=<NegBackward0>) tensor(11630.0215, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11626.80859375
tensor(11630.0215, grad_fn=<NegBackward0>) tensor(11626.8086, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11626.7666015625
tensor(11626.8086, grad_fn=<NegBackward0>) tensor(11626.7666, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11626.7099609375
tensor(11626.7666, grad_fn=<NegBackward0>) tensor(11626.7100, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11608.9072265625
tensor(11626.7100, grad_fn=<NegBackward0>) tensor(11608.9072, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11608.86328125
tensor(11608.9072, grad_fn=<NegBackward0>) tensor(11608.8633, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11593.0537109375
tensor(11608.8633, grad_fn=<NegBackward0>) tensor(11593.0537, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11593.0341796875
tensor(11593.0537, grad_fn=<NegBackward0>) tensor(11593.0342, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11593.0263671875
tensor(11593.0342, grad_fn=<NegBackward0>) tensor(11593.0264, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11593.01953125
tensor(11593.0264, grad_fn=<NegBackward0>) tensor(11593.0195, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11593.013671875
tensor(11593.0195, grad_fn=<NegBackward0>) tensor(11593.0137, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11593.009765625
tensor(11593.0137, grad_fn=<NegBackward0>) tensor(11593.0098, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11593.0048828125
tensor(11593.0098, grad_fn=<NegBackward0>) tensor(11593.0049, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11593.001953125
tensor(11593.0049, grad_fn=<NegBackward0>) tensor(11593.0020, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11592.9970703125
tensor(11593.0020, grad_fn=<NegBackward0>) tensor(11592.9971, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11592.9951171875
tensor(11592.9971, grad_fn=<NegBackward0>) tensor(11592.9951, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11592.9921875
tensor(11592.9951, grad_fn=<NegBackward0>) tensor(11592.9922, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11592.990234375
tensor(11592.9922, grad_fn=<NegBackward0>) tensor(11592.9902, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11592.9892578125
tensor(11592.9902, grad_fn=<NegBackward0>) tensor(11592.9893, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11592.986328125
tensor(11592.9893, grad_fn=<NegBackward0>) tensor(11592.9863, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11592.9833984375
tensor(11592.9863, grad_fn=<NegBackward0>) tensor(11592.9834, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11592.9814453125
tensor(11592.9834, grad_fn=<NegBackward0>) tensor(11592.9814, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11592.9775390625
tensor(11592.9814, grad_fn=<NegBackward0>) tensor(11592.9775, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11592.9169921875
tensor(11592.9775, grad_fn=<NegBackward0>) tensor(11592.9170, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11592.9169921875
tensor(11592.9170, grad_fn=<NegBackward0>) tensor(11592.9170, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11592.9150390625
tensor(11592.9170, grad_fn=<NegBackward0>) tensor(11592.9150, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11592.9140625
tensor(11592.9150, grad_fn=<NegBackward0>) tensor(11592.9141, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11592.912109375
tensor(11592.9141, grad_fn=<NegBackward0>) tensor(11592.9121, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11592.9111328125
tensor(11592.9121, grad_fn=<NegBackward0>) tensor(11592.9111, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11592.9130859375
tensor(11592.9111, grad_fn=<NegBackward0>) tensor(11592.9131, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11592.91015625
tensor(11592.9111, grad_fn=<NegBackward0>) tensor(11592.9102, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11592.9091796875
tensor(11592.9102, grad_fn=<NegBackward0>) tensor(11592.9092, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11592.9072265625
tensor(11592.9092, grad_fn=<NegBackward0>) tensor(11592.9072, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11592.8984375
tensor(11592.9072, grad_fn=<NegBackward0>) tensor(11592.8984, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11592.8974609375
tensor(11592.8984, grad_fn=<NegBackward0>) tensor(11592.8975, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11592.8974609375
tensor(11592.8975, grad_fn=<NegBackward0>) tensor(11592.8975, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11592.896484375
tensor(11592.8975, grad_fn=<NegBackward0>) tensor(11592.8965, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11592.896484375
tensor(11592.8965, grad_fn=<NegBackward0>) tensor(11592.8965, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11592.8955078125
tensor(11592.8965, grad_fn=<NegBackward0>) tensor(11592.8955, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11592.8994140625
tensor(11592.8955, grad_fn=<NegBackward0>) tensor(11592.8994, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11592.89453125
tensor(11592.8955, grad_fn=<NegBackward0>) tensor(11592.8945, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11592.8955078125
tensor(11592.8945, grad_fn=<NegBackward0>) tensor(11592.8955, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11592.8935546875
tensor(11592.8945, grad_fn=<NegBackward0>) tensor(11592.8936, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11592.8828125
tensor(11592.8936, grad_fn=<NegBackward0>) tensor(11592.8828, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11592.8447265625
tensor(11592.8828, grad_fn=<NegBackward0>) tensor(11592.8447, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11592.845703125
tensor(11592.8447, grad_fn=<NegBackward0>) tensor(11592.8457, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11592.845703125
tensor(11592.8447, grad_fn=<NegBackward0>) tensor(11592.8457, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11592.845703125
tensor(11592.8447, grad_fn=<NegBackward0>) tensor(11592.8457, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11592.8447265625
tensor(11592.8447, grad_fn=<NegBackward0>) tensor(11592.8447, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11592.84375
tensor(11592.8447, grad_fn=<NegBackward0>) tensor(11592.8438, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11592.8447265625
tensor(11592.8438, grad_fn=<NegBackward0>) tensor(11592.8447, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11592.8427734375
tensor(11592.8438, grad_fn=<NegBackward0>) tensor(11592.8428, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11592.8291015625
tensor(11592.8428, grad_fn=<NegBackward0>) tensor(11592.8291, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11592.8330078125
tensor(11592.8291, grad_fn=<NegBackward0>) tensor(11592.8330, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11592.826171875
tensor(11592.8291, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11592.826171875
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11592.826171875
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11592.826171875
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11592.8271484375
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8271, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11592.83203125
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8320, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11592.826171875
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11592.826171875
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11592.8271484375
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8271, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11592.8271484375
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8271, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11592.826171875
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11592.8271484375
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8271, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11592.8251953125
tensor(11592.8262, grad_fn=<NegBackward0>) tensor(11592.8252, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11592.826171875
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11592.826171875
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11592.8251953125
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8252, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11592.8310546875
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8311, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11592.826171875
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11592.826171875
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11592.8251953125
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8252, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11592.8251953125
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8252, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11592.8330078125
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8330, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11592.8291015625
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8291, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11592.8271484375
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8271, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11592.826171875
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8262, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11592.8271484375
tensor(11592.8252, grad_fn=<NegBackward0>) tensor(11592.8271, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7638, 0.2362],
        [0.3275, 0.6725]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.8288e-06, 9.9999e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4010, 0.1610],
         [0.5095, 0.1952]],

        [[0.5991, 0.0926],
         [0.5791, 0.5306]],

        [[0.6242, 0.1034],
         [0.5111, 0.6172]],

        [[0.5421, 0.1040],
         [0.6220, 0.6609]],

        [[0.5141, 0.0994],
         [0.5902, 0.7296]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6849276777072695
Average Adjusted Rand Index: 0.792
[0.6849276777072695, 0.6849276777072695] [0.792, 0.792] [11592.779296875, 11592.8271484375]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11508.403278016836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22382.205078125
inf tensor(22382.2051, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12192.5537109375
tensor(22382.2051, grad_fn=<NegBackward0>) tensor(12192.5537, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11856.6630859375
tensor(12192.5537, grad_fn=<NegBackward0>) tensor(11856.6631, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11674.642578125
tensor(11856.6631, grad_fn=<NegBackward0>) tensor(11674.6426, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11609.5908203125
tensor(11674.6426, grad_fn=<NegBackward0>) tensor(11609.5908, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11589.724609375
tensor(11609.5908, grad_fn=<NegBackward0>) tensor(11589.7246, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11558.51171875
tensor(11589.7246, grad_fn=<NegBackward0>) tensor(11558.5117, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11535.76171875
tensor(11558.5117, grad_fn=<NegBackward0>) tensor(11535.7617, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11533.556640625
tensor(11535.7617, grad_fn=<NegBackward0>) tensor(11533.5566, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11527.609375
tensor(11533.5566, grad_fn=<NegBackward0>) tensor(11527.6094, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11522.109375
tensor(11527.6094, grad_fn=<NegBackward0>) tensor(11522.1094, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11517.591796875
tensor(11522.1094, grad_fn=<NegBackward0>) tensor(11517.5918, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11517.505859375
tensor(11517.5918, grad_fn=<NegBackward0>) tensor(11517.5059, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11507.7333984375
tensor(11517.5059, grad_fn=<NegBackward0>) tensor(11507.7334, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11507.689453125
tensor(11507.7334, grad_fn=<NegBackward0>) tensor(11507.6895, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11504.935546875
tensor(11507.6895, grad_fn=<NegBackward0>) tensor(11504.9355, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11504.9072265625
tensor(11504.9355, grad_fn=<NegBackward0>) tensor(11504.9072, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11504.875
tensor(11504.9072, grad_fn=<NegBackward0>) tensor(11504.8750, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11504.8212890625
tensor(11504.8750, grad_fn=<NegBackward0>) tensor(11504.8213, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11504.806640625
tensor(11504.8213, grad_fn=<NegBackward0>) tensor(11504.8066, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11504.4248046875
tensor(11504.8066, grad_fn=<NegBackward0>) tensor(11504.4248, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11503.0634765625
tensor(11504.4248, grad_fn=<NegBackward0>) tensor(11503.0635, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11494.11328125
tensor(11503.0635, grad_fn=<NegBackward0>) tensor(11494.1133, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11494.103515625
tensor(11494.1133, grad_fn=<NegBackward0>) tensor(11494.1035, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11494.095703125
tensor(11494.1035, grad_fn=<NegBackward0>) tensor(11494.0957, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11494.08984375
tensor(11494.0957, grad_fn=<NegBackward0>) tensor(11494.0898, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11494.083984375
tensor(11494.0898, grad_fn=<NegBackward0>) tensor(11494.0840, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11494.0771484375
tensor(11494.0840, grad_fn=<NegBackward0>) tensor(11494.0771, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11494.072265625
tensor(11494.0771, grad_fn=<NegBackward0>) tensor(11494.0723, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11494.06640625
tensor(11494.0723, grad_fn=<NegBackward0>) tensor(11494.0664, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11494.0615234375
tensor(11494.0664, grad_fn=<NegBackward0>) tensor(11494.0615, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11494.05859375
tensor(11494.0615, grad_fn=<NegBackward0>) tensor(11494.0586, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11494.0546875
tensor(11494.0586, grad_fn=<NegBackward0>) tensor(11494.0547, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11494.052734375
tensor(11494.0547, grad_fn=<NegBackward0>) tensor(11494.0527, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11494.0498046875
tensor(11494.0527, grad_fn=<NegBackward0>) tensor(11494.0498, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11494.0458984375
tensor(11494.0498, grad_fn=<NegBackward0>) tensor(11494.0459, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11493.9921875
tensor(11494.0459, grad_fn=<NegBackward0>) tensor(11493.9922, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11493.9833984375
tensor(11493.9922, grad_fn=<NegBackward0>) tensor(11493.9834, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11493.98828125
tensor(11493.9834, grad_fn=<NegBackward0>) tensor(11493.9883, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11493.974609375
tensor(11493.9834, grad_fn=<NegBackward0>) tensor(11493.9746, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11493.9736328125
tensor(11493.9746, grad_fn=<NegBackward0>) tensor(11493.9736, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11493.9658203125
tensor(11493.9736, grad_fn=<NegBackward0>) tensor(11493.9658, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11493.9345703125
tensor(11493.9658, grad_fn=<NegBackward0>) tensor(11493.9346, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11493.9345703125
tensor(11493.9346, grad_fn=<NegBackward0>) tensor(11493.9346, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11493.9326171875
tensor(11493.9346, grad_fn=<NegBackward0>) tensor(11493.9326, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11493.935546875
tensor(11493.9326, grad_fn=<NegBackward0>) tensor(11493.9355, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11493.9306640625
tensor(11493.9326, grad_fn=<NegBackward0>) tensor(11493.9307, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11493.931640625
tensor(11493.9307, grad_fn=<NegBackward0>) tensor(11493.9316, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11493.9287109375
tensor(11493.9307, grad_fn=<NegBackward0>) tensor(11493.9287, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11493.9287109375
tensor(11493.9287, grad_fn=<NegBackward0>) tensor(11493.9287, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11493.927734375
tensor(11493.9287, grad_fn=<NegBackward0>) tensor(11493.9277, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11493.9287109375
tensor(11493.9277, grad_fn=<NegBackward0>) tensor(11493.9287, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11493.9287109375
tensor(11493.9277, grad_fn=<NegBackward0>) tensor(11493.9287, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11493.927734375
tensor(11493.9277, grad_fn=<NegBackward0>) tensor(11493.9277, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11493.92578125
tensor(11493.9277, grad_fn=<NegBackward0>) tensor(11493.9258, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11493.9248046875
tensor(11493.9258, grad_fn=<NegBackward0>) tensor(11493.9248, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11493.923828125
tensor(11493.9248, grad_fn=<NegBackward0>) tensor(11493.9238, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11493.923828125
tensor(11493.9238, grad_fn=<NegBackward0>) tensor(11493.9238, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11493.921875
tensor(11493.9238, grad_fn=<NegBackward0>) tensor(11493.9219, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11493.921875
tensor(11493.9219, grad_fn=<NegBackward0>) tensor(11493.9219, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11493.921875
tensor(11493.9219, grad_fn=<NegBackward0>) tensor(11493.9219, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11493.9208984375
tensor(11493.9219, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11493.9208984375
tensor(11493.9209, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11493.9208984375
tensor(11493.9209, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11493.919921875
tensor(11493.9209, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11493.9189453125
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9189, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11493.91796875
tensor(11493.9189, grad_fn=<NegBackward0>) tensor(11493.9180, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11493.91796875
tensor(11493.9180, grad_fn=<NegBackward0>) tensor(11493.9180, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11493.916015625
tensor(11493.9180, grad_fn=<NegBackward0>) tensor(11493.9160, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11493.9169921875
tensor(11493.9160, grad_fn=<NegBackward0>) tensor(11493.9170, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11493.916015625
tensor(11493.9160, grad_fn=<NegBackward0>) tensor(11493.9160, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11493.916015625
tensor(11493.9160, grad_fn=<NegBackward0>) tensor(11493.9160, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11493.9150390625
tensor(11493.9160, grad_fn=<NegBackward0>) tensor(11493.9150, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11493.9150390625
tensor(11493.9150, grad_fn=<NegBackward0>) tensor(11493.9150, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11493.9267578125
tensor(11493.9150, grad_fn=<NegBackward0>) tensor(11493.9268, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11493.919921875
tensor(11493.9150, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11493.9150390625
tensor(11493.9150, grad_fn=<NegBackward0>) tensor(11493.9150, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11493.9130859375
tensor(11493.9150, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11493.9140625
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9141, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11493.9140625
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9141, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11493.9140625
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9141, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11493.9130859375
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11493.923828125
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9238, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11493.9130859375
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11493.9130859375
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11493.916015625
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9160, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11493.9130859375
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11493.916015625
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9160, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11493.9140625
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9141, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -11493.9365234375
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9365, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -11493.912109375
tensor(11493.9131, grad_fn=<NegBackward0>) tensor(11493.9121, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11493.9130859375
tensor(11493.9121, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11493.9521484375
tensor(11493.9121, grad_fn=<NegBackward0>) tensor(11493.9521, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11493.9130859375
tensor(11493.9121, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11493.9130859375
tensor(11493.9121, grad_fn=<NegBackward0>) tensor(11493.9131, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11493.916015625
tensor(11493.9121, grad_fn=<NegBackward0>) tensor(11493.9160, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[0.7725, 0.2275],
        [0.2321, 0.7679]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4903, 0.5097], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1841, 0.1020],
         [0.6922, 0.3910]],

        [[0.5933, 0.1116],
         [0.6239, 0.5082]],

        [[0.6002, 0.1145],
         [0.5003, 0.5437]],

        [[0.5021, 0.0998],
         [0.6644, 0.5819]],

        [[0.6717, 0.0976],
         [0.6342, 0.6213]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9840320050933414
Average Adjusted Rand Index: 0.983998902890853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22585.2109375
inf tensor(22585.2109, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11549.9033203125
tensor(22585.2109, grad_fn=<NegBackward0>) tensor(11549.9033, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11497.181640625
tensor(11549.9033, grad_fn=<NegBackward0>) tensor(11497.1816, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11496.677734375
tensor(11497.1816, grad_fn=<NegBackward0>) tensor(11496.6777, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11496.5322265625
tensor(11496.6777, grad_fn=<NegBackward0>) tensor(11496.5322, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11496.4599609375
tensor(11496.5322, grad_fn=<NegBackward0>) tensor(11496.4600, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11496.416015625
tensor(11496.4600, grad_fn=<NegBackward0>) tensor(11496.4160, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11496.3896484375
tensor(11496.4160, grad_fn=<NegBackward0>) tensor(11496.3896, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11496.3701171875
tensor(11496.3896, grad_fn=<NegBackward0>) tensor(11496.3701, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11496.357421875
tensor(11496.3701, grad_fn=<NegBackward0>) tensor(11496.3574, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11496.345703125
tensor(11496.3574, grad_fn=<NegBackward0>) tensor(11496.3457, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11496.3388671875
tensor(11496.3457, grad_fn=<NegBackward0>) tensor(11496.3389, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11496.33203125
tensor(11496.3389, grad_fn=<NegBackward0>) tensor(11496.3320, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11496.326171875
tensor(11496.3320, grad_fn=<NegBackward0>) tensor(11496.3262, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11496.322265625
tensor(11496.3262, grad_fn=<NegBackward0>) tensor(11496.3223, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11496.3193359375
tensor(11496.3223, grad_fn=<NegBackward0>) tensor(11496.3193, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11496.3203125
tensor(11496.3193, grad_fn=<NegBackward0>) tensor(11496.3203, grad_fn=<NegBackward0>)
1
Iteration 1700: Loss = -11496.314453125
tensor(11496.3193, grad_fn=<NegBackward0>) tensor(11496.3145, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11496.314453125
tensor(11496.3145, grad_fn=<NegBackward0>) tensor(11496.3145, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11496.30859375
tensor(11496.3145, grad_fn=<NegBackward0>) tensor(11496.3086, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11496.30859375
tensor(11496.3086, grad_fn=<NegBackward0>) tensor(11496.3086, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11496.3046875
tensor(11496.3086, grad_fn=<NegBackward0>) tensor(11496.3047, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11494.0234375
tensor(11496.3047, grad_fn=<NegBackward0>) tensor(11494.0234, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11494.0205078125
tensor(11494.0234, grad_fn=<NegBackward0>) tensor(11494.0205, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11494.021484375
tensor(11494.0205, grad_fn=<NegBackward0>) tensor(11494.0215, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11494.0185546875
tensor(11494.0205, grad_fn=<NegBackward0>) tensor(11494.0186, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11494.017578125
tensor(11494.0186, grad_fn=<NegBackward0>) tensor(11494.0176, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11494.02734375
tensor(11494.0176, grad_fn=<NegBackward0>) tensor(11494.0273, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11494.01953125
tensor(11494.0176, grad_fn=<NegBackward0>) tensor(11494.0195, grad_fn=<NegBackward0>)
2
Iteration 2900: Loss = -11494.015625
tensor(11494.0176, grad_fn=<NegBackward0>) tensor(11494.0156, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11494.0146484375
tensor(11494.0156, grad_fn=<NegBackward0>) tensor(11494.0146, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11494.013671875
tensor(11494.0146, grad_fn=<NegBackward0>) tensor(11494.0137, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11494.0126953125
tensor(11494.0137, grad_fn=<NegBackward0>) tensor(11494.0127, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11493.9794921875
tensor(11494.0127, grad_fn=<NegBackward0>) tensor(11493.9795, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11493.9248046875
tensor(11493.9795, grad_fn=<NegBackward0>) tensor(11493.9248, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11493.923828125
tensor(11493.9248, grad_fn=<NegBackward0>) tensor(11493.9238, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11493.9248046875
tensor(11493.9238, grad_fn=<NegBackward0>) tensor(11493.9248, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11493.9228515625
tensor(11493.9238, grad_fn=<NegBackward0>) tensor(11493.9229, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11493.92578125
tensor(11493.9229, grad_fn=<NegBackward0>) tensor(11493.9258, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11493.921875
tensor(11493.9229, grad_fn=<NegBackward0>) tensor(11493.9219, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11493.9462890625
tensor(11493.9219, grad_fn=<NegBackward0>) tensor(11493.9463, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11493.919921875
tensor(11493.9219, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11493.9208984375
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11493.9208984375
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11493.9208984375
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11493.919921875
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11493.9208984375
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11493.9208984375
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11493.9208984375
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11493.919921875
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11493.9189453125
tensor(11493.9199, grad_fn=<NegBackward0>) tensor(11493.9189, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11493.919921875
tensor(11493.9189, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11493.9208984375
tensor(11493.9189, grad_fn=<NegBackward0>) tensor(11493.9209, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -11493.919921875
tensor(11493.9189, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -11493.9248046875
tensor(11493.9189, grad_fn=<NegBackward0>) tensor(11493.9248, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -11493.919921875
tensor(11493.9189, grad_fn=<NegBackward0>) tensor(11493.9199, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5500 due to no improvement.
pi: tensor([[0.7670, 0.2330],
        [0.2284, 0.7716]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5104, 0.4896], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3908, 0.1020],
         [0.7067, 0.1841]],

        [[0.6702, 0.1115],
         [0.5456, 0.6835]],

        [[0.5998, 0.1143],
         [0.6932, 0.7038]],

        [[0.6588, 0.0998],
         [0.6291, 0.5228]],

        [[0.7107, 0.0975],
         [0.6701, 0.5197]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9840320050933414
Average Adjusted Rand Index: 0.983998902890853
[0.9840320050933414, 0.9840320050933414] [0.983998902890853, 0.983998902890853] [11493.916015625, 11493.919921875]
-------------------------------------
This iteration is 98
True Objective function: Loss = -11422.467566951182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21083.240234375
inf tensor(21083.2402, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12097.3388671875
tensor(21083.2402, grad_fn=<NegBackward0>) tensor(12097.3389, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12053.9814453125
tensor(12097.3389, grad_fn=<NegBackward0>) tensor(12053.9814, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11608.431640625
tensor(12053.9814, grad_fn=<NegBackward0>) tensor(11608.4316, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11546.193359375
tensor(11608.4316, grad_fn=<NegBackward0>) tensor(11546.1934, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11506.78515625
tensor(11546.1934, grad_fn=<NegBackward0>) tensor(11506.7852, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11497.48046875
tensor(11506.7852, grad_fn=<NegBackward0>) tensor(11497.4805, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11483.3779296875
tensor(11497.4805, grad_fn=<NegBackward0>) tensor(11483.3779, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11470.7626953125
tensor(11483.3779, grad_fn=<NegBackward0>) tensor(11470.7627, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11470.67578125
tensor(11470.7627, grad_fn=<NegBackward0>) tensor(11470.6758, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11470.6005859375
tensor(11470.6758, grad_fn=<NegBackward0>) tensor(11470.6006, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11462.208984375
tensor(11470.6006, grad_fn=<NegBackward0>) tensor(11462.2090, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11445.97265625
tensor(11462.2090, grad_fn=<NegBackward0>) tensor(11445.9727, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11434.41015625
tensor(11445.9727, grad_fn=<NegBackward0>) tensor(11434.4102, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11434.388671875
tensor(11434.4102, grad_fn=<NegBackward0>) tensor(11434.3887, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11434.3720703125
tensor(11434.3887, grad_fn=<NegBackward0>) tensor(11434.3721, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11434.3603515625
tensor(11434.3721, grad_fn=<NegBackward0>) tensor(11434.3604, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11434.34765625
tensor(11434.3604, grad_fn=<NegBackward0>) tensor(11434.3477, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11434.3388671875
tensor(11434.3477, grad_fn=<NegBackward0>) tensor(11434.3389, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11434.33203125
tensor(11434.3389, grad_fn=<NegBackward0>) tensor(11434.3320, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11434.3251953125
tensor(11434.3320, grad_fn=<NegBackward0>) tensor(11434.3252, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11434.3193359375
tensor(11434.3252, grad_fn=<NegBackward0>) tensor(11434.3193, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11434.3115234375
tensor(11434.3193, grad_fn=<NegBackward0>) tensor(11434.3115, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11432.224609375
tensor(11434.3115, grad_fn=<NegBackward0>) tensor(11432.2246, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11432.2138671875
tensor(11432.2246, grad_fn=<NegBackward0>) tensor(11432.2139, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11432.2080078125
tensor(11432.2139, grad_fn=<NegBackward0>) tensor(11432.2080, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11432.1943359375
tensor(11432.2080, grad_fn=<NegBackward0>) tensor(11432.1943, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11432.0849609375
tensor(11432.1943, grad_fn=<NegBackward0>) tensor(11432.0850, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11432.080078125
tensor(11432.0850, grad_fn=<NegBackward0>) tensor(11432.0801, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11432.0791015625
tensor(11432.0801, grad_fn=<NegBackward0>) tensor(11432.0791, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11432.076171875
tensor(11432.0791, grad_fn=<NegBackward0>) tensor(11432.0762, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11432.0732421875
tensor(11432.0762, grad_fn=<NegBackward0>) tensor(11432.0732, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11416.9462890625
tensor(11432.0732, grad_fn=<NegBackward0>) tensor(11416.9463, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11416.9423828125
tensor(11416.9463, grad_fn=<NegBackward0>) tensor(11416.9424, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11416.9404296875
tensor(11416.9424, grad_fn=<NegBackward0>) tensor(11416.9404, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11416.939453125
tensor(11416.9404, grad_fn=<NegBackward0>) tensor(11416.9395, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11416.9365234375
tensor(11416.9395, grad_fn=<NegBackward0>) tensor(11416.9365, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11416.9365234375
tensor(11416.9365, grad_fn=<NegBackward0>) tensor(11416.9365, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11416.935546875
tensor(11416.9365, grad_fn=<NegBackward0>) tensor(11416.9355, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11416.9345703125
tensor(11416.9355, grad_fn=<NegBackward0>) tensor(11416.9346, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11416.9326171875
tensor(11416.9346, grad_fn=<NegBackward0>) tensor(11416.9326, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11416.93359375
tensor(11416.9326, grad_fn=<NegBackward0>) tensor(11416.9336, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11416.9326171875
tensor(11416.9326, grad_fn=<NegBackward0>) tensor(11416.9326, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11416.931640625
tensor(11416.9326, grad_fn=<NegBackward0>) tensor(11416.9316, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11416.9306640625
tensor(11416.9316, grad_fn=<NegBackward0>) tensor(11416.9307, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11416.9306640625
tensor(11416.9307, grad_fn=<NegBackward0>) tensor(11416.9307, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11416.9296875
tensor(11416.9307, grad_fn=<NegBackward0>) tensor(11416.9297, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11416.9296875
tensor(11416.9297, grad_fn=<NegBackward0>) tensor(11416.9297, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11416.9296875
tensor(11416.9297, grad_fn=<NegBackward0>) tensor(11416.9297, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11416.927734375
tensor(11416.9297, grad_fn=<NegBackward0>) tensor(11416.9277, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11416.9296875
tensor(11416.9277, grad_fn=<NegBackward0>) tensor(11416.9297, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11416.927734375
tensor(11416.9277, grad_fn=<NegBackward0>) tensor(11416.9277, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11416.9267578125
tensor(11416.9277, grad_fn=<NegBackward0>) tensor(11416.9268, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11416.9306640625
tensor(11416.9268, grad_fn=<NegBackward0>) tensor(11416.9307, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11416.931640625
tensor(11416.9268, grad_fn=<NegBackward0>) tensor(11416.9316, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11416.9248046875
tensor(11416.9268, grad_fn=<NegBackward0>) tensor(11416.9248, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11416.9267578125
tensor(11416.9248, grad_fn=<NegBackward0>) tensor(11416.9268, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11416.92578125
tensor(11416.9248, grad_fn=<NegBackward0>) tensor(11416.9258, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11416.92578125
tensor(11416.9248, grad_fn=<NegBackward0>) tensor(11416.9258, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11416.9248046875
tensor(11416.9248, grad_fn=<NegBackward0>) tensor(11416.9248, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11416.92578125
tensor(11416.9248, grad_fn=<NegBackward0>) tensor(11416.9258, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11416.923828125
tensor(11416.9248, grad_fn=<NegBackward0>) tensor(11416.9238, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11416.9091796875
tensor(11416.9238, grad_fn=<NegBackward0>) tensor(11416.9092, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11416.908203125
tensor(11416.9092, grad_fn=<NegBackward0>) tensor(11416.9082, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11416.90625
tensor(11416.9082, grad_fn=<NegBackward0>) tensor(11416.9062, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11416.9072265625
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9072, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11416.9072265625
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9072, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11416.90625
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9062, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11416.908203125
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9082, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11416.908203125
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9082, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11416.90625
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9062, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11416.9130859375
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9131, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11416.9072265625
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9072, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11416.90625
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9062, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11416.9052734375
tensor(11416.9062, grad_fn=<NegBackward0>) tensor(11416.9053, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11416.908203125
tensor(11416.9053, grad_fn=<NegBackward0>) tensor(11416.9082, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11416.9111328125
tensor(11416.9053, grad_fn=<NegBackward0>) tensor(11416.9111, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -11416.9052734375
tensor(11416.9053, grad_fn=<NegBackward0>) tensor(11416.9053, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11416.9052734375
tensor(11416.9053, grad_fn=<NegBackward0>) tensor(11416.9053, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11416.9052734375
tensor(11416.9053, grad_fn=<NegBackward0>) tensor(11416.9053, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11416.9033203125
tensor(11416.9053, grad_fn=<NegBackward0>) tensor(11416.9033, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11416.896484375
tensor(11416.9033, grad_fn=<NegBackward0>) tensor(11416.8965, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11416.8974609375
tensor(11416.8965, grad_fn=<NegBackward0>) tensor(11416.8975, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11416.89453125
tensor(11416.8965, grad_fn=<NegBackward0>) tensor(11416.8945, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11416.896484375
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.8965, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11416.9384765625
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.9385, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11416.8955078125
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.8955, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11416.9150390625
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.9150, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11416.89453125
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.8945, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11416.9013671875
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.9014, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11416.8955078125
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.8955, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11416.9130859375
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.9131, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11416.896484375
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.8965, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11416.8974609375
tensor(11416.8945, grad_fn=<NegBackward0>) tensor(11416.8975, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.7550, 0.2450],
        [0.2517, 0.7483]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5375, 0.4625], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1982, 0.1001],
         [0.6628, 0.3884]],

        [[0.5445, 0.0946],
         [0.5994, 0.5029]],

        [[0.5335, 0.0942],
         [0.5471, 0.5245]],

        [[0.6854, 0.1053],
         [0.5463, 0.6521]],

        [[0.6430, 0.1056],
         [0.5118, 0.7259]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22695.40234375
inf tensor(22695.4023, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12100.6640625
tensor(22695.4023, grad_fn=<NegBackward0>) tensor(12100.6641, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12066.2041015625
tensor(12100.6641, grad_fn=<NegBackward0>) tensor(12066.2041, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11686.4462890625
tensor(12066.2041, grad_fn=<NegBackward0>) tensor(11686.4463, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11598.4169921875
tensor(11686.4463, grad_fn=<NegBackward0>) tensor(11598.4170, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11567.14453125
tensor(11598.4170, grad_fn=<NegBackward0>) tensor(11567.1445, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11536.0224609375
tensor(11567.1445, grad_fn=<NegBackward0>) tensor(11536.0225, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11502.5078125
tensor(11536.0225, grad_fn=<NegBackward0>) tensor(11502.5078, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11474.66015625
tensor(11502.5078, grad_fn=<NegBackward0>) tensor(11474.6602, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11473.50390625
tensor(11474.6602, grad_fn=<NegBackward0>) tensor(11473.5039, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11472.595703125
tensor(11473.5039, grad_fn=<NegBackward0>) tensor(11472.5957, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11472.36328125
tensor(11472.5957, grad_fn=<NegBackward0>) tensor(11472.3633, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11443.7275390625
tensor(11472.3633, grad_fn=<NegBackward0>) tensor(11443.7275, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11436.2177734375
tensor(11443.7275, grad_fn=<NegBackward0>) tensor(11436.2178, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11425.6328125
tensor(11436.2178, grad_fn=<NegBackward0>) tensor(11425.6328, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11417.2265625
tensor(11425.6328, grad_fn=<NegBackward0>) tensor(11417.2266, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11417.16796875
tensor(11417.2266, grad_fn=<NegBackward0>) tensor(11417.1680, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11417.142578125
tensor(11417.1680, grad_fn=<NegBackward0>) tensor(11417.1426, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11417.1201171875
tensor(11417.1426, grad_fn=<NegBackward0>) tensor(11417.1201, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11417.1025390625
tensor(11417.1201, grad_fn=<NegBackward0>) tensor(11417.1025, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11417.0849609375
tensor(11417.1025, grad_fn=<NegBackward0>) tensor(11417.0850, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11417.0703125
tensor(11417.0850, grad_fn=<NegBackward0>) tensor(11417.0703, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11417.0517578125
tensor(11417.0703, grad_fn=<NegBackward0>) tensor(11417.0518, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11417.0390625
tensor(11417.0518, grad_fn=<NegBackward0>) tensor(11417.0391, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11417.0263671875
tensor(11417.0391, grad_fn=<NegBackward0>) tensor(11417.0264, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11417.0185546875
tensor(11417.0264, grad_fn=<NegBackward0>) tensor(11417.0186, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11417.0087890625
tensor(11417.0186, grad_fn=<NegBackward0>) tensor(11417.0088, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11416.9921875
tensor(11417.0088, grad_fn=<NegBackward0>) tensor(11416.9922, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11416.9853515625
tensor(11416.9922, grad_fn=<NegBackward0>) tensor(11416.9854, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11416.98046875
tensor(11416.9854, grad_fn=<NegBackward0>) tensor(11416.9805, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11416.9775390625
tensor(11416.9805, grad_fn=<NegBackward0>) tensor(11416.9775, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11416.974609375
tensor(11416.9775, grad_fn=<NegBackward0>) tensor(11416.9746, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11416.9716796875
tensor(11416.9746, grad_fn=<NegBackward0>) tensor(11416.9717, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11416.96875
tensor(11416.9717, grad_fn=<NegBackward0>) tensor(11416.9688, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11416.966796875
tensor(11416.9688, grad_fn=<NegBackward0>) tensor(11416.9668, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11416.9658203125
tensor(11416.9668, grad_fn=<NegBackward0>) tensor(11416.9658, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11416.962890625
tensor(11416.9658, grad_fn=<NegBackward0>) tensor(11416.9629, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11416.962890625
tensor(11416.9629, grad_fn=<NegBackward0>) tensor(11416.9629, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11416.9599609375
tensor(11416.9629, grad_fn=<NegBackward0>) tensor(11416.9600, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11416.9580078125
tensor(11416.9600, grad_fn=<NegBackward0>) tensor(11416.9580, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11416.9580078125
tensor(11416.9580, grad_fn=<NegBackward0>) tensor(11416.9580, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11416.9560546875
tensor(11416.9580, grad_fn=<NegBackward0>) tensor(11416.9561, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11416.95703125
tensor(11416.9561, grad_fn=<NegBackward0>) tensor(11416.9570, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11416.953125
tensor(11416.9561, grad_fn=<NegBackward0>) tensor(11416.9531, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11416.9541015625
tensor(11416.9531, grad_fn=<NegBackward0>) tensor(11416.9541, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11416.9521484375
tensor(11416.9531, grad_fn=<NegBackward0>) tensor(11416.9521, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11416.951171875
tensor(11416.9521, grad_fn=<NegBackward0>) tensor(11416.9512, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11416.951171875
tensor(11416.9512, grad_fn=<NegBackward0>) tensor(11416.9512, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11416.951171875
tensor(11416.9512, grad_fn=<NegBackward0>) tensor(11416.9512, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11416.94921875
tensor(11416.9512, grad_fn=<NegBackward0>) tensor(11416.9492, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11416.94921875
tensor(11416.9492, grad_fn=<NegBackward0>) tensor(11416.9492, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11416.9482421875
tensor(11416.9492, grad_fn=<NegBackward0>) tensor(11416.9482, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11416.95703125
tensor(11416.9482, grad_fn=<NegBackward0>) tensor(11416.9570, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11416.947265625
tensor(11416.9482, grad_fn=<NegBackward0>) tensor(11416.9473, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11416.9541015625
tensor(11416.9473, grad_fn=<NegBackward0>) tensor(11416.9541, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11416.9482421875
tensor(11416.9473, grad_fn=<NegBackward0>) tensor(11416.9482, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11416.947265625
tensor(11416.9473, grad_fn=<NegBackward0>) tensor(11416.9473, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11416.9462890625
tensor(11416.9473, grad_fn=<NegBackward0>) tensor(11416.9463, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11416.9453125
tensor(11416.9463, grad_fn=<NegBackward0>) tensor(11416.9453, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11416.9462890625
tensor(11416.9453, grad_fn=<NegBackward0>) tensor(11416.9463, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11416.9443359375
tensor(11416.9453, grad_fn=<NegBackward0>) tensor(11416.9443, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11416.9443359375
tensor(11416.9443, grad_fn=<NegBackward0>) tensor(11416.9443, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11416.943359375
tensor(11416.9443, grad_fn=<NegBackward0>) tensor(11416.9434, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11416.9443359375
tensor(11416.9434, grad_fn=<NegBackward0>) tensor(11416.9443, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11416.9443359375
tensor(11416.9434, grad_fn=<NegBackward0>) tensor(11416.9443, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11416.94921875
tensor(11416.9434, grad_fn=<NegBackward0>) tensor(11416.9492, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11416.94140625
tensor(11416.9434, grad_fn=<NegBackward0>) tensor(11416.9414, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11416.9423828125
tensor(11416.9414, grad_fn=<NegBackward0>) tensor(11416.9424, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11416.943359375
tensor(11416.9414, grad_fn=<NegBackward0>) tensor(11416.9434, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11416.94140625
tensor(11416.9414, grad_fn=<NegBackward0>) tensor(11416.9414, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11416.94140625
tensor(11416.9414, grad_fn=<NegBackward0>) tensor(11416.9414, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11416.94140625
tensor(11416.9414, grad_fn=<NegBackward0>) tensor(11416.9414, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11416.873046875
tensor(11416.9414, grad_fn=<NegBackward0>) tensor(11416.8730, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11416.876953125
tensor(11416.8730, grad_fn=<NegBackward0>) tensor(11416.8770, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11416.875
tensor(11416.8730, grad_fn=<NegBackward0>) tensor(11416.8750, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11416.93359375
tensor(11416.8730, grad_fn=<NegBackward0>) tensor(11416.9336, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11416.873046875
tensor(11416.8730, grad_fn=<NegBackward0>) tensor(11416.8730, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11416.8720703125
tensor(11416.8730, grad_fn=<NegBackward0>) tensor(11416.8721, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11416.9423828125
tensor(11416.8721, grad_fn=<NegBackward0>) tensor(11416.9424, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11416.8701171875
tensor(11416.8721, grad_fn=<NegBackward0>) tensor(11416.8701, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11416.8701171875
tensor(11416.8701, grad_fn=<NegBackward0>) tensor(11416.8701, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11416.87109375
tensor(11416.8701, grad_fn=<NegBackward0>) tensor(11416.8711, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11416.880859375
tensor(11416.8701, grad_fn=<NegBackward0>) tensor(11416.8809, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11416.8662109375
tensor(11416.8701, grad_fn=<NegBackward0>) tensor(11416.8662, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11416.865234375
tensor(11416.8662, grad_fn=<NegBackward0>) tensor(11416.8652, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11416.865234375
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8652, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11416.865234375
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8652, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11416.865234375
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8652, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11416.865234375
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8652, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11416.8681640625
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8682, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11416.865234375
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8652, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11416.865234375
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8652, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11416.8642578125
tensor(11416.8652, grad_fn=<NegBackward0>) tensor(11416.8643, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11416.8681640625
tensor(11416.8643, grad_fn=<NegBackward0>) tensor(11416.8682, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11416.8642578125
tensor(11416.8643, grad_fn=<NegBackward0>) tensor(11416.8643, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11416.8681640625
tensor(11416.8643, grad_fn=<NegBackward0>) tensor(11416.8682, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11416.880859375
tensor(11416.8643, grad_fn=<NegBackward0>) tensor(11416.8809, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11416.8701171875
tensor(11416.8643, grad_fn=<NegBackward0>) tensor(11416.8701, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11416.8779296875
tensor(11416.8643, grad_fn=<NegBackward0>) tensor(11416.8779, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11416.8642578125
tensor(11416.8643, grad_fn=<NegBackward0>) tensor(11416.8643, grad_fn=<NegBackward0>)
pi: tensor([[0.7483, 0.2517],
        [0.2447, 0.7553]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4626, 0.5374], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3884, 0.1001],
         [0.6872, 0.1987]],

        [[0.6985, 0.0945],
         [0.5589, 0.5471]],

        [[0.6510, 0.0942],
         [0.6086, 0.6898]],

        [[0.6174, 0.1054],
         [0.6718, 0.6499]],

        [[0.7138, 0.1056],
         [0.5261, 0.7165]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919992163297293
[0.9919999997943784, 0.9919999997943784] [0.9919992163297293, 0.9919992163297293] [11416.8974609375, 11416.8671875]
-------------------------------------
This iteration is 99
True Objective function: Loss = -11287.95061648354
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21985.13671875
inf tensor(21985.1367, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12004.2666015625
tensor(21985.1367, grad_fn=<NegBackward0>) tensor(12004.2666, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12001.537109375
tensor(12004.2666, grad_fn=<NegBackward0>) tensor(12001.5371, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11979.5224609375
tensor(12001.5371, grad_fn=<NegBackward0>) tensor(11979.5225, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11559.333984375
tensor(11979.5225, grad_fn=<NegBackward0>) tensor(11559.3340, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11490.578125
tensor(11559.3340, grad_fn=<NegBackward0>) tensor(11490.5781, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11489.1328125
tensor(11490.5781, grad_fn=<NegBackward0>) tensor(11489.1328, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11476.3330078125
tensor(11489.1328, grad_fn=<NegBackward0>) tensor(11476.3330, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11466.08203125
tensor(11476.3330, grad_fn=<NegBackward0>) tensor(11466.0820, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11453.5419921875
tensor(11466.0820, grad_fn=<NegBackward0>) tensor(11453.5420, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11453.3837890625
tensor(11453.5420, grad_fn=<NegBackward0>) tensor(11453.3838, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11453.2509765625
tensor(11453.3838, grad_fn=<NegBackward0>) tensor(11453.2510, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11444.767578125
tensor(11453.2510, grad_fn=<NegBackward0>) tensor(11444.7676, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11444.6982421875
tensor(11444.7676, grad_fn=<NegBackward0>) tensor(11444.6982, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11444.6474609375
tensor(11444.6982, grad_fn=<NegBackward0>) tensor(11444.6475, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11444.607421875
tensor(11444.6475, grad_fn=<NegBackward0>) tensor(11444.6074, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11444.564453125
tensor(11444.6074, grad_fn=<NegBackward0>) tensor(11444.5645, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11435.08203125
tensor(11444.5645, grad_fn=<NegBackward0>) tensor(11435.0820, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11435.0537109375
tensor(11435.0820, grad_fn=<NegBackward0>) tensor(11435.0537, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11435.0302734375
tensor(11435.0537, grad_fn=<NegBackward0>) tensor(11435.0303, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11435.009765625
tensor(11435.0303, grad_fn=<NegBackward0>) tensor(11435.0098, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11434.9892578125
tensor(11435.0098, grad_fn=<NegBackward0>) tensor(11434.9893, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11434.958984375
tensor(11434.9893, grad_fn=<NegBackward0>) tensor(11434.9590, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11434.896484375
tensor(11434.9590, grad_fn=<NegBackward0>) tensor(11434.8965, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11434.04296875
tensor(11434.8965, grad_fn=<NegBackward0>) tensor(11434.0430, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11431.71875
tensor(11434.0430, grad_fn=<NegBackward0>) tensor(11431.7188, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11431.0185546875
tensor(11431.7188, grad_fn=<NegBackward0>) tensor(11431.0186, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11427.5732421875
tensor(11431.0186, grad_fn=<NegBackward0>) tensor(11427.5732, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11424.68359375
tensor(11427.5732, grad_fn=<NegBackward0>) tensor(11424.6836, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11419.3134765625
tensor(11424.6836, grad_fn=<NegBackward0>) tensor(11419.3135, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11418.5927734375
tensor(11419.3135, grad_fn=<NegBackward0>) tensor(11418.5928, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11418.3388671875
tensor(11418.5928, grad_fn=<NegBackward0>) tensor(11418.3389, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11418.2587890625
tensor(11418.3389, grad_fn=<NegBackward0>) tensor(11418.2588, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11418.23828125
tensor(11418.2588, grad_fn=<NegBackward0>) tensor(11418.2383, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11418.2314453125
tensor(11418.2383, grad_fn=<NegBackward0>) tensor(11418.2314, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11418.2333984375
tensor(11418.2314, grad_fn=<NegBackward0>) tensor(11418.2334, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11418.2099609375
tensor(11418.2314, grad_fn=<NegBackward0>) tensor(11418.2100, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11418.2060546875
tensor(11418.2100, grad_fn=<NegBackward0>) tensor(11418.2061, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11418.220703125
tensor(11418.2061, grad_fn=<NegBackward0>) tensor(11418.2207, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11418.201171875
tensor(11418.2061, grad_fn=<NegBackward0>) tensor(11418.2012, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11418.19921875
tensor(11418.2012, grad_fn=<NegBackward0>) tensor(11418.1992, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11418.1953125
tensor(11418.1992, grad_fn=<NegBackward0>) tensor(11418.1953, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11418.1689453125
tensor(11418.1953, grad_fn=<NegBackward0>) tensor(11418.1689, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11418.1533203125
tensor(11418.1689, grad_fn=<NegBackward0>) tensor(11418.1533, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11418.15234375
tensor(11418.1533, grad_fn=<NegBackward0>) tensor(11418.1523, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11418.1513671875
tensor(11418.1523, grad_fn=<NegBackward0>) tensor(11418.1514, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11418.1484375
tensor(11418.1514, grad_fn=<NegBackward0>) tensor(11418.1484, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11418.1474609375
tensor(11418.1484, grad_fn=<NegBackward0>) tensor(11418.1475, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11418.1494140625
tensor(11418.1475, grad_fn=<NegBackward0>) tensor(11418.1494, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11418.14453125
tensor(11418.1475, grad_fn=<NegBackward0>) tensor(11418.1445, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11418.14453125
tensor(11418.1445, grad_fn=<NegBackward0>) tensor(11418.1445, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11418.14453125
tensor(11418.1445, grad_fn=<NegBackward0>) tensor(11418.1445, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11418.1435546875
tensor(11418.1445, grad_fn=<NegBackward0>) tensor(11418.1436, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11418.14453125
tensor(11418.1436, grad_fn=<NegBackward0>) tensor(11418.1445, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11418.146484375
tensor(11418.1436, grad_fn=<NegBackward0>) tensor(11418.1465, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11418.1416015625
tensor(11418.1436, grad_fn=<NegBackward0>) tensor(11418.1416, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11418.1416015625
tensor(11418.1416, grad_fn=<NegBackward0>) tensor(11418.1416, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11418.142578125
tensor(11418.1416, grad_fn=<NegBackward0>) tensor(11418.1426, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11418.140625
tensor(11418.1416, grad_fn=<NegBackward0>) tensor(11418.1406, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11418.142578125
tensor(11418.1406, grad_fn=<NegBackward0>) tensor(11418.1426, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11418.1396484375
tensor(11418.1406, grad_fn=<NegBackward0>) tensor(11418.1396, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11418.14453125
tensor(11418.1396, grad_fn=<NegBackward0>) tensor(11418.1445, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11418.1396484375
tensor(11418.1396, grad_fn=<NegBackward0>) tensor(11418.1396, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11418.1376953125
tensor(11418.1396, grad_fn=<NegBackward0>) tensor(11418.1377, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11418.1396484375
tensor(11418.1377, grad_fn=<NegBackward0>) tensor(11418.1396, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11418.14453125
tensor(11418.1377, grad_fn=<NegBackward0>) tensor(11418.1445, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11418.1435546875
tensor(11418.1377, grad_fn=<NegBackward0>) tensor(11418.1436, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -11418.1357421875
tensor(11418.1377, grad_fn=<NegBackward0>) tensor(11418.1357, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11418.1357421875
tensor(11418.1357, grad_fn=<NegBackward0>) tensor(11418.1357, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11418.14453125
tensor(11418.1357, grad_fn=<NegBackward0>) tensor(11418.1445, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11418.1357421875
tensor(11418.1357, grad_fn=<NegBackward0>) tensor(11418.1357, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11418.134765625
tensor(11418.1357, grad_fn=<NegBackward0>) tensor(11418.1348, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11418.1357421875
tensor(11418.1348, grad_fn=<NegBackward0>) tensor(11418.1357, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11418.1337890625
tensor(11418.1348, grad_fn=<NegBackward0>) tensor(11418.1338, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11418.1337890625
tensor(11418.1338, grad_fn=<NegBackward0>) tensor(11418.1338, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11418.1337890625
tensor(11418.1338, grad_fn=<NegBackward0>) tensor(11418.1338, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11418.1318359375
tensor(11418.1338, grad_fn=<NegBackward0>) tensor(11418.1318, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11418.1337890625
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1338, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11418.1337890625
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1338, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11418.1328125
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1328, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11418.1337890625
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1338, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11418.1318359375
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1318, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11418.1396484375
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1396, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11418.1337890625
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1338, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11418.1318359375
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1318, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11418.1298828125
tensor(11418.1318, grad_fn=<NegBackward0>) tensor(11418.1299, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11418.138671875
tensor(11418.1299, grad_fn=<NegBackward0>) tensor(11418.1387, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11418.1279296875
tensor(11418.1299, grad_fn=<NegBackward0>) tensor(11418.1279, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11418.126953125
tensor(11418.1279, grad_fn=<NegBackward0>) tensor(11418.1270, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11418.126953125
tensor(11418.1270, grad_fn=<NegBackward0>) tensor(11418.1270, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11418.07421875
tensor(11418.1270, grad_fn=<NegBackward0>) tensor(11418.0742, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11418.0771484375
tensor(11418.0742, grad_fn=<NegBackward0>) tensor(11418.0771, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11418.072265625
tensor(11418.0742, grad_fn=<NegBackward0>) tensor(11418.0723, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11418.1669921875
tensor(11418.0723, grad_fn=<NegBackward0>) tensor(11418.1670, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11418.0712890625
tensor(11418.0723, grad_fn=<NegBackward0>) tensor(11418.0713, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11418.1513671875
tensor(11418.0713, grad_fn=<NegBackward0>) tensor(11418.1514, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11418.0712890625
tensor(11418.0713, grad_fn=<NegBackward0>) tensor(11418.0713, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11418.076171875
tensor(11418.0713, grad_fn=<NegBackward0>) tensor(11418.0762, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11418.0712890625
tensor(11418.0713, grad_fn=<NegBackward0>) tensor(11418.0713, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11418.0751953125
tensor(11418.0713, grad_fn=<NegBackward0>) tensor(11418.0752, grad_fn=<NegBackward0>)
1
pi: tensor([[0.6850, 0.3150],
        [0.3064, 0.6936]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1872, 0.8128], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3975, 0.1036],
         [0.5978, 0.2095]],

        [[0.7220, 0.1019],
         [0.6170, 0.5962]],

        [[0.5893, 0.0945],
         [0.6165, 0.5542]],

        [[0.5468, 0.0876],
         [0.5128, 0.7214]],

        [[0.5042, 0.0997],
         [0.7126, 0.6891]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.07045526565965987
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.552526234725856
Average Adjusted Rand Index: 0.8140910531319321
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22442.470703125
inf tensor(22442.4707, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11969.234375
tensor(22442.4707, grad_fn=<NegBackward0>) tensor(11969.2344, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11304.46484375
tensor(11969.2344, grad_fn=<NegBackward0>) tensor(11304.4648, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11282.9521484375
tensor(11304.4648, grad_fn=<NegBackward0>) tensor(11282.9521, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11282.1083984375
tensor(11282.9521, grad_fn=<NegBackward0>) tensor(11282.1084, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11281.90625
tensor(11282.1084, grad_fn=<NegBackward0>) tensor(11281.9062, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11281.798828125
tensor(11281.9062, grad_fn=<NegBackward0>) tensor(11281.7988, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11281.7314453125
tensor(11281.7988, grad_fn=<NegBackward0>) tensor(11281.7314, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11281.6865234375
tensor(11281.7314, grad_fn=<NegBackward0>) tensor(11281.6865, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11281.65234375
tensor(11281.6865, grad_fn=<NegBackward0>) tensor(11281.6523, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11281.630859375
tensor(11281.6523, grad_fn=<NegBackward0>) tensor(11281.6309, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11281.611328125
tensor(11281.6309, grad_fn=<NegBackward0>) tensor(11281.6113, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11281.5966796875
tensor(11281.6113, grad_fn=<NegBackward0>) tensor(11281.5967, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11281.580078125
tensor(11281.5967, grad_fn=<NegBackward0>) tensor(11281.5801, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11281.56640625
tensor(11281.5801, grad_fn=<NegBackward0>) tensor(11281.5664, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11281.5576171875
tensor(11281.5664, grad_fn=<NegBackward0>) tensor(11281.5576, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11281.55078125
tensor(11281.5576, grad_fn=<NegBackward0>) tensor(11281.5508, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11281.546875
tensor(11281.5508, grad_fn=<NegBackward0>) tensor(11281.5469, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11281.541015625
tensor(11281.5469, grad_fn=<NegBackward0>) tensor(11281.5410, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11281.5380859375
tensor(11281.5410, grad_fn=<NegBackward0>) tensor(11281.5381, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11281.533203125
tensor(11281.5381, grad_fn=<NegBackward0>) tensor(11281.5332, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11281.5302734375
tensor(11281.5332, grad_fn=<NegBackward0>) tensor(11281.5303, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11281.52734375
tensor(11281.5303, grad_fn=<NegBackward0>) tensor(11281.5273, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11281.525390625
tensor(11281.5273, grad_fn=<NegBackward0>) tensor(11281.5254, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11281.521484375
tensor(11281.5254, grad_fn=<NegBackward0>) tensor(11281.5215, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11281.51953125
tensor(11281.5215, grad_fn=<NegBackward0>) tensor(11281.5195, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11281.51953125
tensor(11281.5195, grad_fn=<NegBackward0>) tensor(11281.5195, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11281.51953125
tensor(11281.5195, grad_fn=<NegBackward0>) tensor(11281.5195, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11281.5166015625
tensor(11281.5195, grad_fn=<NegBackward0>) tensor(11281.5166, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11281.515625
tensor(11281.5166, grad_fn=<NegBackward0>) tensor(11281.5156, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11281.5126953125
tensor(11281.5156, grad_fn=<NegBackward0>) tensor(11281.5127, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11281.5126953125
tensor(11281.5127, grad_fn=<NegBackward0>) tensor(11281.5127, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11281.5126953125
tensor(11281.5127, grad_fn=<NegBackward0>) tensor(11281.5127, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11281.509765625
tensor(11281.5127, grad_fn=<NegBackward0>) tensor(11281.5098, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11281.509765625
tensor(11281.5098, grad_fn=<NegBackward0>) tensor(11281.5098, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11281.5087890625
tensor(11281.5098, grad_fn=<NegBackward0>) tensor(11281.5088, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11281.509765625
tensor(11281.5088, grad_fn=<NegBackward0>) tensor(11281.5098, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11281.5087890625
tensor(11281.5088, grad_fn=<NegBackward0>) tensor(11281.5088, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11281.5087890625
tensor(11281.5088, grad_fn=<NegBackward0>) tensor(11281.5088, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11281.5078125
tensor(11281.5088, grad_fn=<NegBackward0>) tensor(11281.5078, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11281.5068359375
tensor(11281.5078, grad_fn=<NegBackward0>) tensor(11281.5068, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11281.505859375
tensor(11281.5068, grad_fn=<NegBackward0>) tensor(11281.5059, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11281.5068359375
tensor(11281.5059, grad_fn=<NegBackward0>) tensor(11281.5068, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11281.5048828125
tensor(11281.5059, grad_fn=<NegBackward0>) tensor(11281.5049, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11281.505859375
tensor(11281.5049, grad_fn=<NegBackward0>) tensor(11281.5059, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11281.5068359375
tensor(11281.5049, grad_fn=<NegBackward0>) tensor(11281.5068, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11281.50390625
tensor(11281.5049, grad_fn=<NegBackward0>) tensor(11281.5039, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11281.50390625
tensor(11281.5039, grad_fn=<NegBackward0>) tensor(11281.5039, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11281.50390625
tensor(11281.5039, grad_fn=<NegBackward0>) tensor(11281.5039, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11281.5048828125
tensor(11281.5039, grad_fn=<NegBackward0>) tensor(11281.5049, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11281.50390625
tensor(11281.5039, grad_fn=<NegBackward0>) tensor(11281.5039, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11281.501953125
tensor(11281.5039, grad_fn=<NegBackward0>) tensor(11281.5020, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11281.5
tensor(11281.5020, grad_fn=<NegBackward0>) tensor(11281.5000, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11281.5
tensor(11281.5000, grad_fn=<NegBackward0>) tensor(11281.5000, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11281.5029296875
tensor(11281.5000, grad_fn=<NegBackward0>) tensor(11281.5029, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11281.498046875
tensor(11281.5000, grad_fn=<NegBackward0>) tensor(11281.4980, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11281.5
tensor(11281.4980, grad_fn=<NegBackward0>) tensor(11281.5000, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11281.5
tensor(11281.4980, grad_fn=<NegBackward0>) tensor(11281.5000, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11281.5087890625
tensor(11281.4980, grad_fn=<NegBackward0>) tensor(11281.5088, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11281.498046875
tensor(11281.4980, grad_fn=<NegBackward0>) tensor(11281.4980, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11281.4990234375
tensor(11281.4980, grad_fn=<NegBackward0>) tensor(11281.4990, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11281.4990234375
tensor(11281.4980, grad_fn=<NegBackward0>) tensor(11281.4990, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11281.4970703125
tensor(11281.4980, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11281.5078125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.5078, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11281.4970703125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11281.4970703125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11281.4970703125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11281.4970703125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11281.498046875
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4980, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11281.498046875
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4980, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11281.4970703125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11281.5048828125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.5049, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11281.4970703125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11281.4990234375
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4990, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11281.5087890625
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.5088, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11281.4970703125
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11281.49609375
tensor(11281.4971, grad_fn=<NegBackward0>) tensor(11281.4961, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11281.4990234375
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.4990, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11281.5009765625
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.5010, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11281.49609375
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.4961, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11281.4970703125
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11281.49609375
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.4961, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11281.4970703125
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11281.4970703125
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11281.513671875
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.5137, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11281.4970703125
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.4971, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11281.5009765625
tensor(11281.4961, grad_fn=<NegBackward0>) tensor(11281.5010, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7362, 0.2638],
        [0.3159, 0.6841]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5399, 0.4601], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2018, 0.0938],
         [0.6772, 0.4014]],

        [[0.6983, 0.1019],
         [0.5805, 0.6591]],

        [[0.5244, 0.0945],
         [0.5727, 0.6057]],

        [[0.6228, 0.0876],
         [0.7282, 0.5094]],

        [[0.7214, 0.0992],
         [0.7062, 0.6793]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
[0.552526234725856, 1.0] [0.8140910531319321, 1.0] [11418.06640625, 11281.5009765625]
